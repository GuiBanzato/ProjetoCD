title,summary,category
ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles,"Deaf and hard-of-hearing (DHH) students face significant barriers in
accessing science, technology, engineering, and mathematics (STEM) education,
notably due to the scarcity of STEM resources in signed languages. To help
address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia
articles on STEM topics in English, interpreted into over 300 hours of American
Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset
focused on STEM, facilitating the development of AI resources for STEM
education in ASL. We identify several use cases of ASL STEM Wiki with
human-centered applications. For example, because this dataset highlights the
frequent use of fingerspelling for technical concepts, which inhibits DHH
students' ability to learn, we develop models to identify fingerspelled words
-- which can later be used to query for appropriate ASL signs to suggest to
interpreters.",cs.AI
Using Language Models to Disambiguate Lexical Choices in Translation,"In translation, a concept represented by a single word in a source language
can have multiple variations in a target language. The task of lexical
selection requires using context to identify which variation is most
appropriate for a source text. We work with native speakers of nine languages
to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual
concept variation when translating from English. We evaluate recent LLMs and
neural machine translation systems on DTAiLS, with the best-performing model,
GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use
language models to generate English rules describing target-language concept
variations. Providing weaker models with high-quality lexical rules improves
accuracy substantially, in some cases reaching or outperforming GPT-4.",cs.AI
GazeSearch: Radiology Findings Search Benchmark,"Medical eye-tracking data is an important information source for
understanding how radiologists visually interpret medical images. This
information not only improves the accuracy of deep learning models for X-ray
analysis but also their interpretability, enhancing transparency in
decision-making. However, the current eye-tracking data is dispersed,
unprocessed, and ambiguous, making it difficult to derive meaningful insights.
Therefore, there is a need to create a new dataset with more focus and
purposeful eyetracking data, improving its utility for diagnostic applications.
In this work, we propose a refinement method inspired by the target-present
visual search challenge: there is a specific finding and fixations are guided
to locate it. After refining the existing eye-tracking datasets, we transform
them into a curated visual search dataset, called GazeSearch, specifically for
radiology findings, where each fixation sequence is purposefully aligned to the
task of locating a particular finding. Subsequently, we introduce a scan path
prediction baseline, called ChestSearch, specifically tailored to GazeSearch.
Finally, we employ the newly introduced GazeSearch as a benchmark to evaluate
the performance of current state-of-the-art methods, offering a comprehensive
assessment for visual search in the medical imaging domain.",cs.AI
LLMs as Method Actors: A Model for Prompt Engineering and Architecture,"We introduce ""Method Actors"" as a mental model for guiding LLM prompt
engineering and prompt architecture. Under this mental model, LLMs should be
thought of as actors; prompts as scripts and cues; and LLM responses as
performances. We apply this mental model to the task of improving LLM
performance at playing Connections, a New York Times word puzzle game that
prior research identified as a challenging benchmark for evaluating LLM
reasoning. Our experiments with GPT-4o show that a ""Method Actors"" approach can
significantly improve LLM performance over both a vanilla and ""Chain of
Thoughts"" approach. A vanilla approach solves 27% of Connections puzzles in our
dataset and a ""Chain of Thoughts"" approach solves 41% of puzzles, whereas our
strongest ""Method Actor"" approach solves 86% of puzzles. We also test OpenAI's
newest model designed specifically for complex reasoning tasks, o1-preview.
When asked to solve a puzzle all at once, o1-preview solves 79% of Connections
puzzles in our dataset, and when allowed to build puzzle solutions one guess at
a time over multiple API calls, o1-preview solves 100% of the puzzles.
Incorporating a ""Method Actor"" prompt architecture increases the percentage of
puzzles that o1-preview solves perfectly from 76% to 87%.",cs.AI
Quantitative Assessment of Intersectional Empathetic Bias and Understanding,"A growing amount of literature critiques the current operationalizations of
empathy based on loose definitions of the construct. Such definitions
negatively affect dataset quality, model robustness, and evaluation
reliability. We propose an empathy evaluation framework that operationalizes
empathy close to its psychological origins. The framework measures the variance
in responses of LLMs to prompts using existing metrics for empathy and
emotional valence. The variance is introduced through the controlled generation
of the prompts by varying social biases affecting context understanding, thus
impacting empathetic understanding. The control over generation ensures high
theoretical validity of the constructs in the prompt dataset. Also, it makes
high-quality translation, especially into languages that currently have
little-to-no way of evaluating empathy or bias, such as the Slavonic family,
more manageable. Using chosen LLMs and various prompt types, we demonstrate the
empathy evaluation with the framework, including multiple-choice answers and
free generation. The variance in our initial evaluation sample is small and we
were unable to measure convincing differences between the empathetic
understanding in contexts given by different social groups. However, the
results are promising because the models showed significant alterations their
reasoning chains needed to capture the relatively subtle changes in the
prompts. This provides the basis for future research into the construction of
the evaluation sample and statistical methods for measuring the results.",cs.AI
Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?,"Political misinformation poses significant challenges to democratic
processes, shaping public opinion and trust in media. Manual fact-checking
methods face issues of scalability and annotator bias, while machine learning
models require large, costly labelled datasets. This study investigates the use
of state-of-the-art large language models (LLMs) as reliable annotators for
detecting political factuality in news articles. Using open-source LLMs, we
create a politically diverse dataset, labelled for bias through LLM-generated
annotations. These annotations are validated by human experts and further
evaluated by LLM-based judges to assess the accuracy and reliability of the
annotations. Our approach offers a scalable and robust alternative to
traditional fact-checking, enhancing transparency and public trust in media.",cs.AI
On Differentially Private String Distances,"Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.",cs.AI
"Multi-Dimensional Reconfigurable, Physically Composable Hybrid Diffractive Optical Neural Network","Diffractive optical neural networks (DONNs), leveraging free-space light wave
propagation for ultra-parallel, high-efficiency computing, have emerged as
promising artificial intelligence (AI) accelerators. However, their inherent
lack of reconfigurability due to fixed optical structures post-fabrication
hinders practical deployment in the face of dynamic AI workloads and evolving
applications. To overcome this challenge, we introduce, for the first time, a
multi-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a
physically composable architecture that unlocks a new degree of freedom and
unprecedented versatility in DONNs. By leveraging full-system learnability,
MDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially
expanded functionality and superior task adaptability through the
differentiable learning of system variables. Furthermore, MDR-HDONN adopts a
hybrid optical/photonic design, combining the reconfigurability of integrated
photonics with the ultra-parallelism of free-space diffractive systems.
Extensive evaluations demonstrate that MDR-HDONN has digital-comparable
accuracy on various task adaptations with 74x faster speed and 194x lower
energy. Compared to prior DONNs, MDR-HDONN shows exponentially larger
functional space with 5x faster training speed, paving the way for a new
paradigm of versatile, composable, hybrid optical/photonic AI computing. We
will open-source our codes.",cs.AI
Continuous-Time Analysis of Adaptive Optimization and Normalization,"Adaptive optimization algorithms, particularly Adam and its variant AdamW,
are fundamental components of modern deep learning. However, their training
dynamics lack comprehensive theoretical understanding, with limited insight
into why common practices - such as specific hyperparameter choices and
normalization layers - contribute to successful generalization. This work
presents a continuous-time formulation of Adam and AdamW, facilitating a
tractable analysis of training dynamics that can shed light on such practical
questions. We theoretically derive a stable region for Adam's hyperparameters
$(\beta, \gamma)$ that ensures bounded updates, empirically verifying these
predictions by observing unstable exponential growth of parameter updates
outside this region. Furthermore, we theoretically justify the success of
normalization layers by uncovering an implicit meta-adaptive effect of
scale-invariant architectural components. This insight leads to an explicit
optimizer, $2$-Adam, which we generalize to $k$-Adam - an optimizer that
applies an adaptive normalization procedure $k$ times, encompassing Adam
(corresponding to $k=1$) and Adam with a normalization layer (corresponding to
$k=2$). Overall, our continuous-time formulation of Adam facilitates a
principled analysis, offering deeper understanding of optimal hyperparameter
choices and architectural decisions in modern deep learning.",cs.AI
Topology-aware Reinforcement Feature Space Reconstruction for Graph Data,"Feature space is an environment where data points are vectorized to represent
the original dataset. Reconstructing a good feature space is essential to
augment the AI power of data, improve model generalization, and increase the
availability of downstream ML models. Existing literature, such as feature
transformation and feature selection, is labor-intensive (e.g., heavy reliance
on empirical experience) and mostly designed for tabular data. Moreover, these
methods regard data samples as independent, which ignores the unique
topological structure when applied to graph data, thus resulting in a
suboptimal reconstruction feature space. Can we consider the topological
information to automatically reconstruct feature space for graph data without
heavy experiential knowledge? To fill this gap, we leverage topology-aware
reinforcement learning to automate and optimize feature space reconstruction
for graph data. Our approach combines the extraction of core subgraphs to
capture essential structural information with a graph neural network (GNN) to
encode topological features and reduce computing complexity. Then we introduce
three reinforcement agents within a hierarchical structure to systematically
generate meaningful features through an iterative process, effectively
reconstructing the feature space. This framework provides a principled solution
for attributed graph feature space reconstruction. The extensive experiments
demonstrate the effectiveness and efficiency of including topological
awareness.",cs.AI
Aioli: A Unified Optimization Framework for Language Model Data Mixing,"Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.",cs.AI
"A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics","Machine learning methods have a groundbreaking impact in many application
domains, but their application on real robotic platforms is still limited.
Despite the many challenges associated with combining machine learning
technology with robotics, robot learning remains one of the most promising
directions for enhancing the capabilities of robots. When deploying
learning-based approaches on real robots, extra effort is required to address
the challenges posed by various real-world factors. To investigate the key
factors influencing real-world deployment and to encourage original solutions
from different researchers, we organized the Robot Air Hockey Challenge at the
NeurIPS 2023 conference. We selected the air hockey task as a benchmark,
encompassing low-level robotics problems and high-level tactics. Different from
other machine learning-centric benchmarks, participants need to tackle
practical challenges in robotics, such as the sim-to-real gap, low-level
control issues, safety problems, real-time requirements, and the limited
availability of real-world data. Furthermore, we focus on a dynamic
environment, removing the typical assumption of quasi-static motions of other
real-world benchmarks. The competition's results show that solutions combining
learning-based approaches with prior knowledge outperform those relying solely
on data when real-world deployment is challenging. Our ablation study reveals
which real-world factors may be overlooked when building a learning-based
solution. The successful real-world air hockey deployment of best-performing
agents sets the foundation for future competitions and follow-up research
directions.",cs.AI
Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification,"Convolutional Neural Networks (CNNs) have seen significant performance
improvements in recent years. However, due to their size and complexity, they
function as black-boxes, leading to transparency concerns. State-of-the-art
saliency methods generate local explanations that highlight the area in the
input image where a class is identified but cannot explain how a concept of
interest contributes to the prediction, which is essential for bias mitigation.
On the other hand, concept-based methods, such as TCAV (Testing with Concept
Activation Vectors), provide insights into how sensitive is the network to a
concept, but cannot compute its attribution in a specific prediction nor show
its location within the input image. This paper introduces a novel post-hoc
explainability framework, Visual-TCAV, which aims to bridge the gap between
these methods by providing both local and global explanations for CNN-based
image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to
generate saliency maps that show where concepts are recognized by the network.
Moreover, it can estimate the attribution of these concepts to the output of
any class using a generalization of Integrated Gradients. This framework is
evaluated on popular CNN architectures, with its validity further confirmed via
experiments where ground truth for explanations is known, and a comparison with
TCAV. Our code will be made available soon.",cs.AI
Asterisk*: Keep it Simple,"This paper describes Asterisk, a compact GPT-based model for generating text
embeddings. The model uses a minimalist architecture with two layers, two
attention heads, and 256 embedding dimensions. By applying knowledge
distillation from larger pretrained models, we explore the trade-offs between
model size and performance while minimizing computational and memory
requirements. The model is primarily evaluated and optimized for classification
tasks, with experimental results showing its moderate performance in zero-shot
classification across various downstream applications. With additional
configuration, the model performance can approach or even surpass that of
larger architectures on specific classification tasks.",cs.AI
Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning,"The integration of unmanned platforms equipped with advanced sensors promises
to enhance situational awareness and mitigate the ""fog of war"" in military
operations. However, managing the vast influx of data from these platforms
poses a significant challenge for Command and Control (C2) systems. This study
presents a novel multi-agent learning framework to address this challenge. Our
method enables autonomous and secure communication between agents and humans,
which in turn enables real-time formation of an interpretable Common
Operational Picture (COP). Each agent encodes its perceptions and actions into
compact vectors, which are then transmitted, received and decoded to form a COP
encompassing the current state of all agents (friendly and enemy) on the
battlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP
models and agent's action selection policies. We demonstrate resilience to
degraded conditions such as denied GPS and disrupted communications.
Experimental validation is performed in the Starcraft-2 simulation environment
to evaluate the precision of the COPs and robustness of policies. We report
less than 5% error in COPs and policies resilient to various adversarial
conditions. In summary, our contributions include a method for autonomous COP
formation, increased resilience through distributed prediction, and joint
training of COP models and multi-agent RL policies. This research advances
adaptive and resilient C2, facilitating effective control of heterogeneous
unmanned platforms.",cs.AI
Tell What You Hear From What You See -- Video to Audio Generation Through Text,"The content of visual and audio scenes is multi-faceted such that a video can
be paired with various audio and vice-versa. Thereby, in video-to-audio
generation task, it is imperative to introduce steering approaches for
controlling the generated audio. While Video-to-Audio generation is a
well-established generative task, existing methods lack such controllability.
In this work, we propose VATT, a multi-modal generative framework that takes a
video and an optional text prompt as input, and generates audio and optional
textual description of the audio. Such a framework has two advantages: i)
Video-to-Audio generation process can be refined and controlled via text which
complements the context of visual information, and ii) The model can suggest
what audio to generate for the video by generating audio captions. VATT
consists of two key modules: VATT Converter, a LLM that is fine-tuned for
instructions and includes a projection layer that maps video features to the
LLM vector space; and VATT Audio, a transformer that generates audio tokens
from visual frames and from optional text prompt using iterative parallel
decoding. The audio tokens are converted to a waveform by pretrained neural
codec. Experiments show that when VATT is compared to existing video-to-audio
generation methods in objective metrics, it achieves competitive performance
when the audio caption is not provided. When the audio caption is provided as a
prompt, VATT achieves even more refined performance (lowest KLD score of 1.41).
Furthermore, subjective studies show that VATT Audio has been chosen as
preferred generated audio than audio generated by existing methods. VATT
enables controllable video-to-audio generation through text as well as
suggesting text prompts for videos through audio captions, unlocking novel
applications such as text-guided video-to-audio generation and video-to-audio
captioning.",cs.AI
Improving Molecular Graph Generation with Flow Matching and Optimal Transport,"Generating molecular graphs is crucial in drug design and discovery but
remains challenging due to the complex interdependencies between nodes and
edges. While diffusion models have demonstrated their potentiality in molecular
graph design, they often suffer from unstable training and inefficient
sampling. To enhance generation performance and training stability, we propose
GGFlow, a discrete flow matching generative model incorporating optimal
transport for molecular graphs and it incorporates an edge-augmented graph
transformer to enable the direct communications among chemical bounds.
Additionally, GGFlow introduces a novel goal-guided generation framework to
control the generative trajectory of our model, aiming to design novel
molecular structures with the desired properties. GGFlow demonstrates superior
performance on both unconditional and conditional molecule generation tasks,
outperforming existing baselines and underscoring its effectiveness and
potential for wider application.",cs.AI
The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
conversational tasks. Embodying an LLM as a virtual human allows users to
engage in face-to-face social interactions in Virtual Reality. However, the
influence of person- and task-related factors in social interactions with
LLM-controlled agents remains unclear. In this study, forty-six participants
interacted with a virtual agent whose persona was manipulated as extravert or
introvert in three different conversational tasks (small talk, knowledge test,
convincing). Social-evaluation, emotional experience, and realism were assessed
using ratings. Interactive engagement was measured by quantifying participants'
words and conversational turns. Finally, we measured participants' willingness
to ask the agent for help during the knowledge test. Our findings show that the
extraverted agent was more positively evaluated, elicited a more pleasant
experience and greater engagement, and was assessed as more realistic compared
to the introverted agent. Whereas persona did not affect the tendency to ask
for help, participants were generally more confident in the answer when they
had help of the LLM. Variation of personality traits of LLM-controlled embodied
virtual agents, therefore, affects social-emotional processing and behavior in
virtual interactions. Embodied virtual agents allow the presentation of
naturalistic social encounters in a virtual environment.",cs.AI
SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection,"Developing robust drone detection systems is often constrained by the limited
availability of large-scale annotated training data and the high costs
associated with real-world data collection. However, leveraging synthetic data
generated via game engine-based simulations provides a promising and
cost-effective solution to overcome this issue. Therefore, we present
SynDroneVision, a synthetic dataset specifically designed for RGB-based drone
detection in surveillance applications. Featuring diverse backgrounds, lighting
conditions, and drone models, SynDroneVision offers a comprehensive training
foundation for deep learning algorithms. To evaluate the dataset's
effectiveness, we perform a comparative analysis across a selection of recent
YOLO detection models. Our findings demonstrate that SynDroneVision is a
valuable resource for real-world data enrichment, achieving notable
enhancements in model performance and robustness, while significantly reducing
the time and costs of real-world data acquisition. SynDroneVision will be
publicly released upon paper acceptance.",cs.AI
Knowledge Distillation Neural Network for Predicting Car-following Behaviour of Human-driven and Autonomous Vehicles,"As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and
Human-driven vehicles (HDVs), understanding the car-following behaviour is
important to improve traffic efficiency and road safety. Using a real-world
trajectory dataset, this study uses descriptive and statistical analysis to
investigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV
and HDV-HDV in mixed traffic. The ANOVA test showed that car-following
behaviours across different vehicle pairs are statistically significant
(p-value < 0.05).
  We also introduce a data-driven Knowledge Distillation Neural Network (KDNN)
model for predicting car-following behaviour in terms of speed. The KDNN model
demonstrates comparable predictive accuracy to its teacher network, a Long
Short-Term Memory (LSTM) network, and outperforms both the standalone student
network, a Multilayer Perceptron (MLP), and traditional physics-based models
like the Gipps model. Notably, the KDNN model better prevents collisions,
measured by minimum Time-to-Collision (TTC), and operates with lower
computational power, making it ideal for AVs or driving simulators requiring
efficient computing.",cs.AI
Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey,"Deep reinforcement learning has led to dramatic breakthroughs in the field of
artificial intelligence for the past few years. As the amount of rollout
experience data and the size of neural networks for deep reinforcement learning
have grown continuously, handling the training process and reducing the time
consumption using parallel and distributed computing is becoming an urgent and
essential desire. In this paper, we perform a broad and thorough investigation
on training acceleration methodologies for deep reinforcement learning based on
parallel and distributed computing, providing a comprehensive survey in this
field with state-of-the-art methods and pointers to core references. In
particular, a taxonomy of literature is provided, along with a discussion of
emerging topics and open issues. This incorporates learning system
architectures, simulation parallelism, computing parallelism, distributed
synchronization mechanisms, and deep evolutionary reinforcement learning.
Further, we compare 16 current open-source libraries and platforms with
criteria of facilitating rapid development. Finally, we extrapolate future
directions that deserve further research.",cs.AI
Expectation vs. Reality: Towards Verification of Psychological Games,"Game theory provides an effective way to model strategic interactions among
rational agents. In the context of formal verification, these ideas can be used
to produce guarantees on the correctness of multi-agent systems, with a diverse
range of applications from computer security to autonomous driving.
Psychological games (PGs) were developed as a way to model and analyse agents
with belief-dependent motivations, opening up the possibility to model how
human emotions can influence behaviour. In PGs, players' utilities depend not
only on what actually happens (which strategies players choose to adopt), but
also on what the players had expected to happen (their belief as to the
strategies that would be played). Despite receiving much attention in fields
such as economics and psychology, very little consideration has been given to
their applicability to problems in computer science, nor to practical
algorithms and tool support. In this paper, we start to bridge that gap,
proposing methods to solve PGs and implementing them within PRISM-games, a
formal verification tool for stochastic games. We discuss how to model these
games, highlight specific challenges for their analysis and illustrate the
usefulness of our approach on several case studies, including human behaviour
in traffic scenarios.",cs.AI
Tangled Program Graphs as an alternative to DRL-based control algorithms for UAVs,"Deep reinforcement learning (DRL) is currently the most popular AI-based
approach to autonomous vehicle control. An agent, trained for this purpose in
simulation, can interact with the real environment with a human-level
performance. Despite very good results in terms of selected metrics, this
approach has some significant drawbacks: high computational requirements and
low explainability. Because of that, a DRL-based agent cannot be used in some
control tasks, especially when safety is the key issue. Therefore we propose to
use Tangled Program Graphs (TPGs) as an alternative for deep reinforcement
learning in control-related tasks. In this approach, input signals are
processed by simple programs that are combined in a graph structure. As a
result, TPGs are less computationally demanding and their actions can be
explained based on the graph structure. In this paper, we present our studies
on the use of TPGs as an alternative for DRL in control-related tasks. In
particular, we consider the problem of navigating an unmanned aerial vehicle
(UAV) through the unknown environment based solely on the on-board LiDAR
sensor. The results of our work show promising prospects for the use of TPGs in
control related-tasks.",cs.AI
Solving 7x7 Killall-Go with Seki Database,"Game solving is the process of finding the theoretical outcome for a game,
assuming that all player choices are optimal. This paper focuses on a technique
that can reduce the heuristic search space significantly for 7x7 Killall-Go. In
Go and Killall-Go, live patterns are stones that are protected from opponent
capture. Mutual life, also referred to as seki, is when both players' stones
achieve life by sharing liberties with their opponent. Whichever player
attempts to capture the opponent first will leave their own stones vulnerable.
Therefore, it is critical to recognize seki patterns to avoid putting oneself
in jeopardy. Recognizing seki can reduce the search depth significantly. In
this paper, we enumerate all seki patterns up to a predetermined area size,
then store these patterns into a seki table. This allows us to recognize seki
during search, which significantly improves solving efficiency for the game of
Killall-Go. Experiments show that a day-long, unsolvable position can be solved
in 482 seconds with the addition of a seki table. For general positions, a 10%
to 20% improvement in wall clock time and node count is observed.",cs.AI
Open-set object detection: towards unified problem formulation and benchmarking,"In real-world applications where confidence is key, like autonomous driving,
the accurate detection and appropriate handling of classes differing from those
used during training are crucial. Despite the proposal of various unknown
object detection approaches, we have observed widespread inconsistencies among
them regarding the datasets, metrics, and scenarios used, alongside a notable
absence of a clear definition for unknown objects, which hampers meaningful
evaluation. To counter these issues, we introduce two benchmarks: a unified
VOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear
hierarchical object definition besides new evaluation metrics. Complementing
the benchmark, we exploit recent self-supervised Vision Transformers
performance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD),
through OW-DETR++. State-of-the-art methods are extensively evaluated on the
proposed benchmarks. This study provides a clear problem definition, ensures
consistent evaluations, and draws new conclusions about effectiveness of OSOD
strategies.",cs.AI
Training objective drives the consistency of representational similarity across datasets,"The Platonic Representation Hypothesis claims that recent foundation models
are converging to a shared representation space as a function of their
downstream task performance, irrespective of the objectives and data modalities
used to train these models. Representational similarity is generally measured
for individual datasets and is not necessarily consistent across datasets.
Thus, one may wonder whether this convergence of model representations is
confounded by the datasets commonly used in machine learning. Here, we propose
a systematic way to measure how representational similarity between models
varies with the set of stimuli used to construct the representations. We find
that the objective function is the most crucial factor in determining the
consistency of representational similarities across datasets. Specifically,
self-supervised vision models learn representations whose relative pairwise
similarities generalize better from one dataset to another compared to those of
image classification or image-text models. Moreover, the correspondence between
representational similarities and the models' task behavior is
dataset-dependent, being most strongly pronounced for single-domain datasets.
Our work provides a framework for systematically measuring similarities of
model representations across datasets and linking those similarities to
differences in task behavior.",cs.AI
A Nerf-Based Color Consistency Method for Remote Sensing Images,"Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.",cs.AI
CRepair: CVAE-based Automatic Vulnerability Repair Technology,"Software vulnerabilities are flaws in computer software systems that pose
significant threats to the integrity, security, and reliability of modern
software and its application data. These vulnerabilities can lead to
substantial economic losses across various industries. Manual vulnerability
repair is not only time-consuming but also prone to errors. To address the
challenges of vulnerability repair, researchers have proposed various
solutions, with learning-based automatic vulnerability repair techniques
gaining widespread attention. However, existing methods often focus on learning
more vulnerability data to improve repair outcomes, while neglecting the
diverse characteristics of vulnerable code, and suffer from imprecise
vulnerability localization.To address these shortcomings, this paper proposes
CRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing
security vulnerabilities in system code. We first preprocess the vulnerability
data using a prompt-based method to serve as input to the model. Then, we apply
causal inference techniques to map the vulnerability feature data to
probability distributions. By employing multi-sample feature fusion, we capture
diverse vulnerability feature information. Finally, conditional control is used
to guide the model in repairing the vulnerabilities.Experimental results
demonstrate that the proposed method significantly outperforms other benchmark
models, achieving a perfect repair rate of 52%. The effectiveness of the
approach is validated from multiple perspectives, advancing AI-driven code
vulnerability repair and showing promising applications.",cs.AI
SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,"Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.",cs.AI
Towards Scalable Foundation Models for Digital Dermatology,"The growing demand for accurate and equitable AI models in digital
dermatology faces a significant challenge: the lack of diverse, high-quality
labeled data. In this work, we investigate the potential of domain-specific
foundation models for dermatology in addressing this challenge. We utilize
self-supervised learning (SSL) techniques to pre-train models on a dataset of
over 240,000 dermatological images from public and private collections. Our
study considers several SSL methods and compares the resulting foundation
models against domain-agnostic models like those pre-trained on ImageNet and
state-of-the-art models such as MONET across 12 downstream tasks. Unlike
previous research, we emphasize the development of smaller models that are more
suitable for resource-limited clinical settings, facilitating easier adaptation
to a broad range of use cases. Results show that models pre-trained in this
work not only outperform general-purpose models but also approach the
performance of models 50 times larger on clinically relevant diagnostic tasks.
To promote further research in this direction, we publicly release both the
training code and the foundation models, which can benefit clinicians in
dermatological applications.",cs.AI
WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models,"Recent advancements in large language models (LLMs) have driven a
revolutionary paradigm shift in process automation from Robotic Process
Automation to Agentic Process Automation by automating the workflow
orchestration procedure based on LLMs. However, existing LLMs (even the
advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in
workflow orchestration. To address this limitation, we present WorkflowLLM, a
data-centric framework elaborately designed to enhance the capability of LLMs
in workflow orchestration. It first constructs a large-scale fine-tuning
dataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83
applications across 28 categories. Specifically, the construction process can
be divided into three phases: (1) Data Collection: we collect real-world
workflow data from Apple Shortcuts and RoutineHub, transcribing them into
Python-style code. We further equip them with generated hierarchical thought
via ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task
queries to enrich the diversity and complexity of workflows. (3) Workflow
Generation: we leverage an annotator model trained on collected data to
generate workflows for synthesized queries. Finally, we merge the synthetic
samples that pass quality confirmation with the collected samples to obtain the
WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain
WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong
capacity to orchestrate complex workflows, while also achieving notable
generalization performance on previously unseen APIs. Additionally,
WorkflowBench exhibits robust zero-shot generalization capabilities on an
out-of-distribution task planning dataset, T-Eval. Our data and code are
available at https://github.com/OpenBMB/WorkflowLLM.",cs.AI
ICE-T: A Multi-Faceted Concept for Teaching Machine Learning,"The topics of Artificial intelligence (AI) and especially Machine Learning
(ML) are increasingly making their way into educational curricula. To
facilitate the access for students, a variety of platforms, visual tools, and
digital games are already being used to introduce ML concepts and strengthen
the understanding of how AI works. We take a look at didactic principles that
are employed for teaching computer science, define criteria, and, based on
those, evaluate a selection of prominent existing platforms, tools, and games.
Additionally, we criticize the approach of portraying ML mostly as a black-box
and the resulting missing focus on creating an understanding of data,
algorithms, and models that come with it. To tackle this issue, we present a
concept that covers intermodal transfer, computational and explanatory
thinking, ICE-T, as an extension of known didactic principles. With our
multi-faceted concept, we believe that planners of learning units, creators of
learning platforms and educators can improve on teaching ML.",cs.AI
VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM,"Generating accurate and consistent visual aids is a critical challenge in
mathematics education, where visual representations like geometric shapes and
functions play a pivotal role in enhancing student comprehension. This paper
introduces a novel multi-agent framework that leverages Large Language Models
(LLMs) to automate the creation of complex mathematical visualizations
alongside coherent problem text. Our approach not only simplifies the
generation of precise visual aids but also aligns these aids with the problem's
core mathematical concepts, improving both problem creation and assessment. By
integrating multiple agents, each responsible for distinct tasks such as
numeric calculation, geometry validation, and visualization, our system
delivers mathematically accurate and contextually relevant problems with visual
aids. Evaluation across Geometry and Function problem types shows that our
method significantly outperforms basic LLMs in terms of text coherence,
consistency, relevance and similarity, while maintaining the essential
geometrical and functional integrity of the original problems. Although some
challenges remain in ensuring consistent visual outputs, our framework
demonstrates the immense potential of LLMs in transforming the way educators
generate and utilize visual aids in math education.",cs.AI
Learning the rules of peptide self-assembly through data mining with large language models,"Peptides are ubiquitous and important biologically derived molecules, that
have been found to self-assemble to form a wide array of structures. Extensive
research has explored the impacts of both internal chemical composition and
external environmental stimuli on the self-assembly behaviour of these systems.
However, there is yet to be a systematic study that gathers this rich
literature data and collectively examines these experimental factors to provide
a global picture of the fundamental rules that govern protein self-assembly
behavior. In this work, we curate a peptide assembly database through a
combination of manual processing by human experts and literature mining
facilitated by a large language model. As a result, we collect more than 1,000
experimental data entries with information about peptide sequence, experimental
conditions and corresponding self-assembly phases. Utilizing the collected
data, ML models are trained and evaluated, demonstrating excellent accuracy
(>80\%) and efficiency in peptide assembly phase classification. Moreover, we
fine-tune our GPT model for peptide literature mining with the developed
dataset, which exhibits markedly superior performance in extracting information
from academic publications relative to the pre-trained model. We find that this
workflow can substantially improve efficiency when exploring potential
self-assembling peptide candidates, through guiding experimental work, while
also deepening our understanding of the mechanisms governing peptide
self-assembly. In doing so, novel structures can be accessed for a range of
applications including sensing, catalysis and biomaterials.",cs.AI
WeatherGFM: Learning A Weather Generalist Foundation Model via In-context Learning,"The Earth's weather system encompasses intricate weather data modalities and
diverse weather understanding tasks, which hold significant value to human
life. Existing data-driven models focus on single weather understanding tasks
(e.g., weather forecasting). Although these models have achieved promising
results, they fail to tackle various complex tasks within a single and unified
model. Moreover, the paradigm that relies on limited real observations for a
single scenario hinders the model's performance upper bound. In response to
these limitations, we draw inspiration from the in-context learning paradigm
employed in state-of-the-art visual foundation models and large language
models. In this paper, we introduce the first generalist weather foundation
model (WeatherGFM), designed to address a wide spectrum of weather
understanding tasks in a unified manner. More specifically, we initially unify
the representation and definition of the diverse weather understanding tasks.
Subsequently, we devised weather prompt formats to manage different weather
data modalities, namely single, multiple, and temporal modalities. Finally, we
adopt a visual prompting question-answering paradigm for the training of
unified weather understanding tasks. Extensive experiments indicate that our
WeatherGFM can effectively handle up to ten weather understanding tasks,
including weather forecasting, super-resolution, weather image translation, and
post-processing. Our method also showcases generalization ability on unseen
tasks.",cs.AI
Web Archives Metadata Generation with GPT-4o: Challenges and Insights,"Current metadata creation for web archives is time consuming and costly due
to reliance on human effort. This paper explores the use of gpt-4o for metadata
generation within the Web Archive Singapore, focusing on scalability,
efficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files
using data reduction techniques, achieving a notable 99.9% reduction in
metadata generation costs. By prompt engineering, we generated titles and
abstracts, which were evaluated both intrinsically using Levenshtein Distance
and BERTScore, and extrinsically with human cataloguers using McNemar's test.
Results indicate that while our method offers significant cost savings and
efficiency gains, human curated metadata maintains an edge in quality. The
study identifies key challenges including content inaccuracies, hallucinations,
and translation issues, suggesting that Large Language Models (LLMs) should
serve as complements rather than replacements for human cataloguers. Future
work will focus on refining prompts, improving content filtering, and
addressing privacy concerns through experimentation with smaller models. This
research advances the integration of LLMs in web archiving, offering valuable
insights into their current capabilities and outlining directions for future
enhancements. The code is available at
https://github.com/masamune-prog/warc2summary for further development and use
by institutions facing similar challenges.",cs.AI
Benchmarking Distributional Alignment of Large Language Models,"Language models (LMs) are increasingly used as simulacra for people, yet
their ability to match the distribution of views of a specific demographic
group and be \textit{distributionally aligned} remains uncertain. This notion
of distributional alignment is complex, as there is significant variation in
the types of attributes that are simulated. Prior works have underexplored the
role of three critical variables -- the question domain, steering method, and
distribution expression method -- which motivates our contribution of a
benchmark explicitly addressing these dimensions. We construct a dataset
expanding beyond political values, create human baselines for this task, and
evaluate the extent to which an LM can align with a particular group's opinion
distribution to inform design choices of such simulation systems. Our analysis
reveals open problems regarding if, and how, LMs can be used to simulate
humans, and that LLMs can more accurately describe the opinion distribution
than simulate such distributions.",cs.AI
Advancing Meteorological Forecasting: AI-based Approach to Synoptic Weather Map Analysis,"As global warming increases the complexity of weather patterns; the precision
of weather forecasting becomes increasingly important. Our study proposes a
novel preprocessing method and convolutional autoencoder model developed to
improve the interpretation of synoptic weather maps. These are critical for
meteorologists seeking a thorough understanding of weather conditions. This
model could recognize historical synoptic weather maps that nearly match
current atmospheric conditions, marking a significant step forward in modern
technology in meteorological forecasting. This comprises unsupervised learning
models like VQ-VQE, as well as supervised learning models like VGG16, VGG19,
Xception, InceptionV3, and ResNet50 trained on the ImageNet dataset, as well as
research into newer models like EfficientNet and ConvNeXt. Our findings proved
that, while these models perform well in various settings, their ability to
identify comparable synoptic weather maps has certain limits. Our research,
motivated by the primary goal of significantly increasing meteorologists'
efficiency in labor-intensive tasks, discovered that cosine similarity is the
most effective metric, as determined by a combination of quantitative and
qualitative assessments to accurately identify relevant historical weather
patterns. This study broadens our understanding by shifting the emphasis from
numerical precision to practical application, ensuring that our model is
effective in theory practical, and accessible in the complex and dynamic field
of meteorology.",cs.AI
Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking,"Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.",cs.AI
Agricultural Landscape Understanding At Country-Scale,"Agricultural landscapes are quite complex, especially in the Global South
where fields are smaller, and agricultural practices are more varied. In this
paper we report on our progress in digitizing the agricultural landscape
(natural and man-made) in our study region of India. We use high resolution
imagery and a UNet style segmentation model to generate the first of its kind
national-scale multi-class panoptic segmentation output. Through this work we
have been able to identify individual fields across 151.7M hectares, and
delineating key features such as water resources and vegetation. We share how
this output was validated by our team and externally by downstream users,
including some sample use cases that can lead to targeted data driven decision
making. We believe this dataset will contribute towards digitizing agriculture
by generating the foundational baselayer.",cs.AI
Controlling Grokking with Nonlinearity and Data Symmetry,"This paper demonstrates that grokking behavior in modular arithmetic with a
modulus P in a neural network can be controlled by modifying the profile of the
activation function as well as the depth and width of the model. Plotting the
even PCA projections of the weights of the last NN layer against their odd
projections further yields patterns which become significantly more uniform
when the nonlinearity is increased by incrementing the number of layers. These
patterns can be employed to factor P when P is nonprime. Finally, a metric for
the generalization ability of the network is inferred from the entropy of the
layer weights while the degree of nonlinearity is related to correlations
between the local entropy of the weights of the neurons in the final layer.",cs.AI
Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework,"Recent advancements in Large Language Models (LLMs) and related technologies
such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have
enabled the creation of autonomous intelligent systems capable of performing
cluster diagnostics and troubleshooting. By integrating these technologies with
self-play methodologies, we have developed an LLM-agent system designed to
autonomously diagnose and resolve issues within AI clusters. Our innovations
include a knowledge base tailored for cluster diagnostics, enhanced LLM
algorithms, practical deployment strategies for agents, and a benchmark
specifically designed for evaluating LLM capabilities in this domain. Through
extensive experimentation across multiple dimensions, we have demonstrated the
superiority of our system in addressing the challenges faced in cluster
diagnostics, particularly in detecting and rectifying performance issues more
efficiently and accurately than traditional methods.",cs.AI
LLM-PySC2: Starcraft II learning environment for Large Language Models,"This paper introduces a new environment LLM-PySC2 (the Large Language Model
StarCraft II Learning Environment), a platform derived from DeepMind's
StarCraft II Learning Environment that serves to develop Large Language Models
(LLMs) based decision-making methodologies. This environment is the first to
offer the complete StarCraft II action space, multi-modal observation
interfaces, and a structured game knowledge database, which are seamlessly
connected with various LLMs to facilitate the research of LLMs-based
decision-making. To further support multi-agent research, we developed an LLM
collaborative framework that supports multi-agent concurrent queries and
multi-agent communication. In our experiments, the LLM-PySC2 environment is
adapted to be compatible with the StarCraft Multi-Agent Challenge (SMAC) task
group and provided eight new scenarios focused on macro-decision abilities. We
evaluated nine mainstream LLMs in the experiments, and results show that
sufficient parameters are necessary for LLMs to make decisions, but improving
reasoning ability does not directly lead to better decision-making outcomes.
Our findings further indicate the importance of enabling large models to learn
autonomously in the deployment environment through parameter training or
train-free learning techniques. Ultimately, we expect that the LLM-PySC2
environment can promote research on learning methods for LLMs, helping
LLM-based methods better adapt to task scenarios.",cs.AI
Reasoning Robustness of LLMs to Adversarial Typographical Errors,"Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by
users' instruction. In this work, we study the reasoning robustness of LLMs to
typographical errors, which can naturally occur in users' queries. We design an
Adversarial Typo Attack ($\texttt{ATA}$) algorithm that iteratively samples
typos for words that are important to the query and selects the edit that is
most likely to succeed in attacking. It shows that LLMs are sensitive to
minimal adversarial typographical changes. Notably, with 1 character edit,
Mistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8
character edits the performance further drops to 19.2%. To extend our
evaluation to larger and closed-source LLMs, we develop the $\texttt{R$^2$ATA}$
benchmark, which assesses models' $\underline{R}$easoning
$\underline{R}$obustness to $\underline{\texttt{ATA}}$. It includes adversarial
typographical questions derived from three widely used reasoning
datasets-GSM8K, BBH, and MMLU-by applying $\texttt{ATA}$ to open-source LLMs.
$\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable
performance drops across multiple super large and closed-source LLMs.",cs.AI
Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning,"Task-oriented dialogue (TOD) system is designed to accomplish user-defined
tasks through dialogues. The TOD system has progressed towards end-to-end
modeling by leveraging pre-trained large language models. Fine-tuning the
pre-trained language models using only supervised learning leads to the
exposure bias and token loss problem and it deviates the models from completing
the user's task. To address these issues, we propose a TOD system that
leverages a unified pre-trained language model, GPT2, as a base model. It is
optimized using supervised learning and reinforcement learning (RL). The issues
in the TOD system are mitigated using a non-differentiable reward function. The
reward is calculated using the weighted sum of the success rate and BLEU
evaluation metrics. The success rate and BLEU metrics in reward calculation
guide the language model for user task completion while ensuring a coherent and
fluent response. Our model is acquired by fine-tuning a pre-trained model on
the dialogue-session level which comprises user utterance, belief state, system
act, and system response. Experimental results on MultiWOZ2.1 demonstrate that
our model increases the inform rate by 1.60% and the success rate by 3.17%
compared to the baseline.",cs.AI
Inversion-based Latent Bayesian Optimization,"Latent Bayesian optimization (LBO) approaches have successfully adopted
Bayesian optimization over a continuous latent space by employing an
encoder-decoder architecture to address the challenge of optimization in a high
dimensional or discrete input space. LBO learns a surrogate model to
approximate the black-box objective function in the latent space. However, we
observed that most LBO methods suffer from the `misalignment problem`, which is
induced by the reconstruction error of the encoder-decoder architecture. It
hinders learning an accurate surrogate model and generating high-quality
solutions. In addition, several trust region-based LBO methods select the
anchor, the center of the trust region, based solely on the objective function
value without considering the trust region`s potential to enhance the
optimization process. To address these issues, we propose Inversion-based
Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO
consists of two components: an inversion method and a potential-aware trust
region anchor selection. The inversion method searches the latent code that
completely reconstructs the given target data. The potential-aware trust region
anchor selection considers the potential capability of the trust region for
better local optimization. Experimental results demonstrate the effectiveness
of InvBO on nine real-world benchmarks, such as molecule design and arithmetic
expression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.",cs.AI
Exploring the Alignment Landscape: LLMs and Geometric Deep Models in Protein Representation,"Latent representation alignment has become a foundational technique for
constructing multimodal large language models (MLLM) by mapping embeddings from
different modalities into a shared space, often aligned with the embedding
space of large language models (LLMs) to enable effective cross-modal
understanding. While preliminary protein-focused MLLMs have emerged, they have
predominantly relied on heuristic approaches, lacking a fundamental
understanding of optimal alignment practices across representations. In this
study, we explore the alignment of multimodal representations between LLMs and
Geometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate
three state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with
four protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines
alignment factors from both model and protein perspectives, identifying
challenges in current alignment methodologies and proposing strategies to
improve the alignment process. Our key findings reveal that GDMs incorporating
both graph and 3D structural information align better with LLMs, larger LLMs
demonstrate improved alignment capabilities, and protein rarity significantly
impacts alignment performance. We also find that increasing GDM embedding
dimensions, using two-layer projection heads, and fine-tuning LLMs on
protein-specific data substantially enhance alignment quality. These strategies
offer potential enhancements to the performance of protein-related multimodal
models. Our code and data are available at
https://github.com/Tizzzzy/LLM-GDM-alignment.",cs.AI
Revisiting Network Perturbation for Semi-Supervised Semantic Segmentation,"In semi-supervised semantic segmentation (SSS), weak-to-strong consistency
regularization techniques are widely utilized in recent works, typically
combined with input-level and feature-level perturbations. However, the
integration between weak-to-strong consistency regularization and network
perturbation has been relatively rare. We note several problems with existing
network perturbations in SSS that may contribute to this phenomenon. By
revisiting network perturbations, we introduce a new approach for network
perturbation to expand the existing weak-to-strong consistency regularization
for unlabeled data. Additionally, we present a volatile learning process for
labeled data, which is uncommon in existing research. Building upon previous
work that includes input-level and feature-level perturbations, we present
MLPMatch (Multi-Level-Perturbation Match), an easy-to-implement and efficient
framework for semi-supervised semantic segmentation. MLPMatch has been
validated on the Pascal VOC and Cityscapes datasets, achieving state-of-the-art
performance. Code is available from https://github.com/LlistenL/MLPMatch.",cs.AI
On Training of Kolmogorov-Arnold Networks,"Kolmogorov-Arnold Networks have recently been introduced as a flexible
alternative to multi-layer Perceptron architectures. In this paper, we examine
the training dynamics of different KAN architectures and compare them with
corresponding MLP formulations. We train with a variety of different
initialization schemes, optimizers, and learning rates, as well as utilize back
propagation free approaches like the HSIC Bottleneck. We find that (when judged
by test accuracy) KANs are an effective alternative to MLP architectures on
high-dimensional datasets and have somewhat better parameter efficiency, but
suffer from more unstable training dynamics. Finally, we provide
recommendations for improving training stability of larger KAN models.",cs.AI
SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection,"More and more research works fuse the LiDAR and camera information to improve
the 3D object detection of the autonomous driving system. Recently, a simple
yet effective fusion framework has achieved an excellent detection performance,
fusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space.
In this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for
accurate 3D object detection, which follows the BEV-based fusion framework and
improves the camera and LiDAR encoders, respectively. Specifically, we perform
the camera-based depth estimation using a cascade network and rectify the depth
results with the depth information derived from the LiDAR points. Meanwhile, an
auxiliary branch that implements the 3D object detection using only the
camera-BEV features is introduced to exploit the camera information during the
training phase. Besides, we improve the LiDAR feature extractor by fusing the
multi-scaled sparse convolutional features. Experimental results demonstrate
the effectiveness of our proposed method. Our method achieves 77.6\% NDS
accuracy on the nuScenes dataset, showcasing superior performance in the 3D
object detection track.",cs.AI
SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding,"Large Language Models (LLMs) have become essential in advancing natural
language processing (NLP) tasks, but their sequential token generation limits
inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising
solution by using a smaller draft model to generate multiple token sequences,
which the target LLM verifies in parallel. However, current heuristic
approaches, such as Recursive Rejection Sampling (RRS), suffer from low
acceptance rates in subsequent drafts, limiting the advantages of using
multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can
theoretically improve acceptance rates, but its computational cost is too high
for real-time use. We present SpecHub, a novel, efficient sampling-verification
method for MDSD that improves acceptance rates with only linear computational
overhead. By simplifying the OTM problem into a compact Linear Programming
model, SpecHub significantly reduces computational complexity. It further
accelerates sampling by leveraging a sparse joint distribution, focusing
computation on high-probability token sequences. In extensive experiments,
Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step
than RRS and RRS without replacement. We attach our code at
\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",cs.AI
A Taxonomy of AgentOps for Enabling Observability of Foundation Model based Agents,"The ever-improving quality of LLMs has fueled the growth of a diverse range
of downstream tasks, leading to an increased demand for AI automation and a
burgeoning interest in developing foundation model (FM)-based autonomous
agents. As AI agent systems tackle more complex tasks and evolve, they involve
a wider range of stakeholders, including agent users, agentic system developers
and deployers, and AI model developers. These systems also integrate multiple
components such as AI agent workflows, RAG pipelines, prompt management, agent
capabilities, and observability features. In this case, obtaining reliable
outputs and answers from these agents remains challenging, necessitating a
dependable execution process and end-to-end observability solutions. To build
reliable AI agents and LLM applications, it is essential to shift towards
designing AgentOps platforms that ensure observability and traceability across
the entire development-to-production life-cycle. To this end, we conducted a
rapid review and identified relevant AgentOps tools from the agentic ecosystem.
Based on this review, we provide an overview of the essential features of
AgentOps and propose a comprehensive overview of observability data/traceable
artifacts across the agent production life-cycle. Our findings provide a
systematic overview of the current AgentOps landscape, emphasizing the critical
role of observability/traceability in enhancing the reliability of autonomous
agent systems.",cs.AI
MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization,"Quantization of foundational models (FMs) is significantly more challenging
than traditional DNNs due to the emergence of large magnitude features called
outliers. Existing outlier-aware algorithm/architecture co-design techniques
either use mixed-precision, retaining outliers at high precision but compromise
hardware efficiency, or quantize inliers and outliers at the same precision,
improving hardware efficiency at the cost of accuracy. To address this mutual
exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique
that leverages pruning to complement outlier-aware quantization. MicroScopiQ
retains outliers at higher precision while pruning a certain fraction of least
important weights to distribute the additional outlier bits; ensuring high
accuracy, aligned memory and hardware efficiency. We design a high-throughput,
low overhead accelerator architecture composed of simple multi-precision INT
processing elements and a novel network-on-chip called ReCoN that efficiently
abstracts the complexity of supporting high-precision outliers. Additionally,
unlike existing alternatives, MicroScopiQ does not assume any locality of
outlier weights, enabling applicability to a broad range of FMs. Extensive
experiments across various quantization settings show that MicroScopiQ achieves
SoTA quantization performance while simultaneously improving inference
performance by 3x and reducing energy by 2x over existing alternatives.",cs.AI
Fox-1 Technical Report,"We present Fox-1, a series of small language models (SLMs) consisting of
Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3
trillion tokens of web-scraped document data and fine-tuned with 5 billion
tokens of instruction-following and multi-turn conversation data. Aiming to
improve the pre-training efficiency, Fox-1-1.6B model introduces a novel
3-stage data curriculum across all the training data with 2K-8K sequence
length. In architecture design, Fox-1 features a deeper layer structure, an
expanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a
performant and efficient architecture compared to other SLMs. Fox-1 achieves
better or on-par performance in various benchmarks compared to StableLM-2-1.6B,
Gemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and
throughput. The model weights have been released under the Apache 2.0 license,
where we aim to promote the democratization of LLMs and make them fully
accessible to the whole open-source community.",cs.AI
Real-World Offline Reinforcement Learning from Vision Language Model Feedback,"Offline reinforcement learning can enable policy learning from pre-collected,
sub-optimal datasets without online interactions. This makes it ideal for
real-world robots and safety-critical scenarios, where collecting online data
or expert demonstrations is slow, costly, and risky. However, most existing
offline RL works assume the dataset is already labeled with the task rewards, a
process that often requires significant human effort, especially when
ground-truth states are hard to ascertain (e.g., in the real-world). In this
paper, we build on prior work, specifically RL-VLM-F, and propose a novel
system that automatically generates reward labels for offline datasets using
preference feedback from a vision-language model and a text description of the
task. Our method then learns a policy using offline RL with the reward-labeled
dataset. We demonstrate the system's applicability to a complex real-world
robot-assisted dressing task, where we first learn a reward function using a
vision-language model on a sub-optimal offline dataset, and then we use the
learned reward to employ Implicit Q learning to develop an effective dressing
policy. Our method also performs well in simulation tasks involving the
manipulation of rigid and deformable objects, and significantly outperform
baselines such as behavior cloning and inverse RL. In summary, we propose a new
system that enables automatic reward labeling and policy learning from
unlabeled, sub-optimal offline datasets.",cs.AI
Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems,"This paper presents a comparative analysis of hallucination detection systems
for AI, focusing on automatic summarization and question answering tasks for
Large Language Models (LLMs). We evaluate different hallucination detection
systems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.
Our results indicate that although advanced models can perform better they come
at a much higher cost. We also demonstrate how an ideal hallucination detection
system needs to maintain performance across different model sizes. Our findings
highlight the importance of choosing a detection system aligned with specific
application needs and resource constraints. Future research will explore hybrid
systems and automated identification of underperforming components to enhance
AI reliability and efficiency in detecting and mitigating hallucinations.",cs.AI
Minimal Conditions for Beneficial Neighbourhood Search and Local Descent,"This paper investigates what properties a neighbourhood requires to support
beneficial local search. We show that neighbourhood locality, and a reduction
in cost probability towards the optimum, support a proof that search among
neighbours is more likely to find an improving solution in a single search step
than blind search. This is the first paper to introduce such a proof. The
concepts underlying these properties are illustrated on a satisfiability
problem class, and on travelling salesman problems. Secondly, for a given cost
target t, we investigate a combination of blind search and local descent termed
local blind descent, and present various conditions under which the expected
number of steps to reach a cost better than t using local blind descent, is
proven to be smaller than with blind search. Experiments indicate that local
blind descent, given target cost t, should switch to local descent at a
starting cost that reduces as t approaches the optimum.",cs.AI
Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations,"Despite significant advancements in report generation methods, a critical
limitation remains: the lack of interpretability in the generated text. This
paper introduces an innovative approach to enhance the explainability of text
generated by report generation models. Our method employs cyclic text
manipulation and visual comparison to identify and elucidate the features in
the original content that influence the generated text. By manipulating the
generated reports and producing corresponding images, we create a comparative
framework that highlights key attributes and their impact on the text
generation process. This approach not only identifies the image features
aligned to the generated text but also improves transparency but also provides
deeper insights into the decision-making mechanisms of the report generation
models. Our findings demonstrate the potential of this method to significantly
enhance the interpretability and transparency of AI-generated reports.",cs.AI
QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning,"Federated Learning has emerged as a leading approach for decentralized
machine learning, enabling multiple clients to collaboratively train a shared
model without exchanging private data. While FL enhances data privacy, it
remains vulnerable to inference attacks, such as gradient inversion and
membership inference, during both training and inference phases. Homomorphic
Encryption provides a promising solution by encrypting model updates to protect
against such attacks, but it introduces substantial communication overhead,
slowing down training and increasing computational costs. To address these
challenges, we propose QuanCrypt-FL, a novel algorithm that combines low-bit
quantization and pruning techniques to enhance protection against attacks while
significantly reducing computational costs during training. Further, we propose
and implement mean-based clipping to mitigate quantization overflow or errors.
By integrating these methods, QuanCrypt-FL creates a communication-efficient FL
framework that ensures privacy protection with minimal impact on model
accuracy, thereby improving both computational efficiency and attack
resilience. We validate our approach on MNIST, CIFAR-10, and CIFAR-100
datasets, demonstrating superior performance compared to state-of-the-art
methods. QuanCrypt-FL consistently outperforms existing method and matches
Vanilla-FL in terms of accuracy across varying client. Further, QuanCrypt-FL
achieves up to 9x faster encryption, 16x faster decryption, and 1.5x faster
inference compared to BatchCrypt, with training time reduced by up to 3x.",cs.AI
Abstract2Appendix: Academic Reviews Enhance LLM Long-Context Capabilities,"Large language models (LLMs) have shown remarkable performance across various
tasks, yet their ability to handle long-context reading remains challenging.
This study explores the effectiveness of leveraging high-quality academic peer
review data for fine-tuning LLMs to enhance their long-context capabilities. We
compare the Direct Preference Optimization (DPO) method with the Supervised
Fine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.
Our experiments show that the fine-tuned model achieves a 4.04-point
improvement over phi-3 and a 2.6\% increase on the Qasper benchmark using only
2000 samples. Despite facing limitations in data scale and processing costs,
this study underscores the potential of DPO and high-quality data in advancing
LLM performance.
  Additionally, the zero-shot benchmark results indicate that aggregated
high-quality human reviews are overwhelmingly preferred over LLM-generated
responses, even for the most capable models like GPT-4o. This suggests that
high-quality human reviews are extremely rich in information, reasoning, and
long-context retrieval, capabilities that even the most advanced models have
not fully captured. These findings highlight the high utility of leveraging
human reviews to further advance the field.",cs.AI
Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs,"The rapid advancement of Large Language Models (LLMs) has led to their
increased integration into mobile devices for personalized assistance, which
enables LLMs to call external API functions to enhance their performance.
However, challenges such as data scarcity, ineffective question formatting, and
catastrophic forgetting hinder the development of on-device LLM agents. To
tackle these issues, we propose Alopex, a framework that enables precise
on-device function calls using the Fox LLM. Alopex introduces a logic-based
method for generating high-quality training data and a novel
``description-question-output'' format for fine-tuning, reducing risks of
function information leakage. Additionally, a data mixing strategy is used to
mitigate catastrophic forgetting, combining function call data with textbook
datasets to enhance performance in various tasks. Experimental results show
that Alopex improves function call accuracy and significantly reduces
catastrophic forgetting, providing a robust solution for integrating function
call capabilities into LLMs without manual intervention.",cs.AI
Maximizing User Connectivity in AI-Enabled Multi-UAV Networks: A Distributed Strategy Generalized to Arbitrary User Distributions,"Deep reinforcement learning (DRL) has been extensively applied to
Multi-Unmanned Aerial Vehicle (UAV) network (MUN) to effectively enable
real-time adaptation to complex, time-varying environments. Nevertheless, most
of the existing works assume a stationary user distribution (UD) or a dynamic
one with predicted patterns. Such considerations may make the UD-specific
strategies insufficient when a MUN is deployed in unknown environments. To this
end, this paper investigates distributed user connectivity maximization problem
in a MUN with generalization to arbitrary UDs. Specifically, the problem is
first formulated into a time-coupled combinatorial nonlinear non-convex
optimization with arbitrary underlying UDs. To make the optimization tractable,
a multi-agent CNN-enhanced deep Q learning (MA-CDQL) algorithm is proposed. The
algorithm integrates a ResNet-based CNN to the policy network to analyze the
input UD in real time and obtain optimal decisions based on the extracted
high-level UD features. To improve the learning efficiency and avoid local
optimums, a heatmap algorithm is developed to transform the raw UD to a
continuous density map. The map will be part of the true input to the policy
network. Simulations are conducted to demonstrate the efficacy of UD heatmaps
and the proposed algorithm in maximizing user connectivity as compared to
K-means methods.",cs.AI
Explainable AI through a Democratic Lens: DhondtXAI for Proportional Feature Importance Using the D'Hondt Method,"In democratic societies, electoral systems play a crucial role in translating
public preferences into political representation. Among these, the D'Hondt
method is widely used to ensure proportional representation, balancing fair
representation with governmental stability. Recently, there has been a growing
interest in applying similar principles of proportional representation to
enhance interpretability in machine learning, specifically in Explainable AI
(XAI). This study investigates the integration of D'Hondt-based voting
principles in the DhondtXAI method, which leverages resource allocation
concepts to interpret feature importance within AI models. Through a comparison
of SHAP (Shapley Additive Explanations) and DhondtXAI, we evaluate their
effectiveness in feature attribution within CatBoost and XGBoost models for
breast cancer and diabetes prediction, respectively. The DhondtXAI approach
allows for alliance formation and thresholding to enhance interpretability,
representing feature importance as seats in a parliamentary view. Statistical
correlation analyses between SHAP values and DhondtXAI allocations support the
consistency of interpretations, demonstrating DhondtXAI's potential as a
complementary tool for understanding feature importance in AI models. The
results highlight that integrating electoral principles, such as proportional
representation and alliances, into AI explainability can improve user
understanding, especially in high-stakes fields like healthcare.",cs.AI
Interactive Dialogue Agents via Reinforcement Learning on Hindsight Regenerations,"Recent progress on large language models (LLMs) has enabled dialogue agents
to generate highly naturalistic and plausible text. However, current LLM
language generation focuses on responding accurately to questions and requests
with a single effective response. In reality, many real dialogues are
interactive, meaning an agent's utterances will influence their conversational
partner, elicit information, or change their opinion. Accounting for how an
agent can effectively steer a conversation is a crucial ability in many
dialogue tasks, from healthcare to preference elicitation. Existing methods for
fine-tuning dialogue agents to accomplish such tasks would rely on curating
some amount of expert data. However, doing so often requires understanding the
underlying cognitive processes of the conversational partner, which is a skill
neither humans nor LLMs trained on human data can reliably do. Our key insight
is that while LLMs may not be adept at identifying effective strategies for
steering conversations a priori, or in the middle of an ongoing conversation,
they can do so post-hoc, or in hindsight, after seeing how their conversational
partner responds. We use this fact to rewrite and augment existing suboptimal
data, and train via offline reinforcement learning (RL) an agent that
outperforms both prompting and learning from unaltered human demonstrations. We
apply our approach to two domains that require understanding human mental
state, intelligent interaction, and persuasion: mental health support, and
soliciting charitable donations. Our results in a user study with real humans
show that our approach greatly outperforms existing state-of-the-art dialogue
agents.",cs.AI
Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning,"Value-based reinforcement learning (RL) can in principle learn effective
policies for a wide range of multi-turn problems, from games to dialogue to
robotic control, including via offline RL from static previously collected
datasets. However, despite the widespread use of policy gradient methods to
train large language models for single turn tasks (e.g., question answering),
value-based methods for multi-turn RL in an off-policy or offline setting have
proven particularly challenging to scale to the setting of large language
models. This setting requires effectively leveraging pretraining, scaling to
large architectures with billions of parameters, and training on large
datasets, all of which represent major challenges for current value-based RL
methods. In this work, we propose a novel offline RL algorithm that addresses
these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)
problem where the probabilities of tokens directly translate to Q-values. In
this way we obtain an algorithm that smoothly transitions from maximizing the
likelihood of the data during pretraining to learning a near-optimal Q-function
during finetuning. Our algorithm has strong theoretical foundations, enjoying
performance bounds similar to state-of-the-art Q-learning methods, while in
practice utilizing an objective that closely resembles SFT. Because of this,
our approach can enjoy the full benefits of the pretraining of language models,
without the need to reinitialize any weights before RL finetuning, and without
the need to initialize new heads for predicting values or advantages.
Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a
variety of tasks including both natural language dialogue and robotic
manipulation and navigation from images.",cs.AI
Explaining Mixtures of Sources in News Articles,"Human writers plan, then write. For large language models (LLMs) to play a
role in longer-form article generation, we must understand the planning steps
humans make before writing. We explore one kind of planning, source-selection
in news, as a case-study for evaluating plans in long-form generation. We ask:
why do specific stories call for specific kinds of sources? We imagine a
generative process for story writing where a source-selection schema is first
selected by a journalist, and then sources are chosen based on categories in
that schema. Learning the article's plan means predicting the schema initially
chosen by the journalist. Working with professional journalists, we adapt five
existing schemata and introduce three new ones to describe journalistic plans
for the inclusion of sources in documents. Then, inspired by Bayesian
latent-variable modeling, we develop metrics to select the most likely plan, or
schema, underlying a story, which we use to compare schemata. We find that two
schemata: stance and social affiliation best explain source plans in most
documents. However, other schemata like textual entailment explain source plans
in factually rich topics like ""Science"". Finally, we find we can predict the
most suitable schema given just the article's headline with reasonable
accuracy. We see this as an important case-study for human planning, and
provides a framework and approach for evaluating other kinds of plans. We
release a corpora, NewsSources, with annotations for 4M articles.",cs.AI
Discern-XR: An Online Classifier for Metaverse Network Traffic,"In this paper, we design an exclusive Metaverse network traffic classifier,
named Discern-XR, to help Internet service providers (ISP) and router
manufacturers enhance the quality of Metaverse services. Leveraging segmented
learning, the Frame Vector Representation (FVR) algorithm and Frame
Identification Algorithm (FIA) are proposed to extract critical frame-related
statistics from raw network data having only four application-level features. A
novel Augmentation, Aggregation, and Retention Online Training (A2R-OT)
algorithm is proposed to find an accurate classification model through online
training methodology. In addition, we contribute to the real-world Metaverse
dataset comprising virtual reality (VR) games, VR video, VR chat, augmented
reality (AR), and mixed reality (MR) traffic, providing a comprehensive
benchmark. Discern-XR outperforms state-of-the-art classifiers by 7% while
improving training efficiency and reducing false-negative rates. Our work
advances Metaverse network traffic classification by standing as the
state-of-the-art solution.",cs.AI
Inverse Transition Learning: Learning Dynamics from Demonstrations,"We consider the problem of estimating the transition dynamics $T^*$ from
near-optimal expert trajectories in the context of offline model-based
reinforcement learning. We develop a novel constraint-based method, Inverse
Transition Learning, that treats the limited coverage of the expert
trajectories as a \emph{feature}: we use the fact that the expert is
near-optimal to inform our estimate of $T^*$. We integrate our constraints into
a Bayesian approach. Across both synthetic environments and real healthcare
scenarios like Intensive Care Unit (ICU) patient management in hypotension, we
demonstrate not only significant improvements in decision-making, but that our
posterior can inform when transfer will be successful.",cs.AI
PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation,"Radiology report generation (RRG) aims to create free-text radiology reports
from clinical imaging. Grounded radiology report generation (GRRG) extends RRG
by including the localisation of individual findings on the image. Currently,
there are no manually annotated chest X-ray (CXR) datasets to train GRRG
models. In this work, we present a dataset called PadChest-GR
(Grounded-Reporting) derived from PadChest aimed at training GRRG models for
CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with
grounded reports (3,099 abnormal and 1,456 normal), each containing complete
lists of sentences describing individual present (positive) and absent
(negative) findings in English and Spanish. In total, PadChest-GR contains
7,037 positive and 3,422 negative finding sentences. Every positive finding
sentence is associated with up to two independent sets of bounding boxes
labelled by different readers and has categorical labels for finding type,
locations, and progression. To the best of our knowledge, PadChest-GR is the
first manually curated dataset designed to train GRRG models for understanding
and interpreting radiological images and generated text. By including detailed
localization and comprehensive annotations of all clinically relevant findings,
it provides a valuable resource for developing and evaluating GRRG models from
CXR images. PadChest-GR can be downloaded under request from
https://bimcv.cipf.es/bimcv-projects/padchest-gr/",cs.AI
ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning,"Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.",cs.AI
Analyzing The Language of Visual Tokens,"With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.",cs.AI
HourVideo: 1-Hour Video-Language Understanding,"We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu",cs.AI
"Public Procurement for Responsible AI? Understanding U.S. Cities' Practices, Challenges, and Needs","Most AI tools adopted by governments are not developed internally, but
instead are acquired from third-party vendors in a process called public
procurement. While scholars and regulatory proposals have recently turned
towards procurement as a site of intervention to encourage responsible AI
governance practices, little is known about the practices and needs of city
employees in charge of AI procurement. In this paper, we present findings from
semi-structured interviews with 18 city employees across 7 US cities. We find
that AI acquired by cities often does not go through a conventional public
procurement process, posing challenges to oversight and governance. We identify
five key types of challenges to leveraging procurement for responsible AI that
city employees face when interacting with colleagues, AI vendors, and members
of the public. We conclude by discussing recommendations and implications for
governments, researchers, and policymakers.",cs.AI
"Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives","The Bradley-Terry (BT) model is a common and successful practice in reward
modeling for Large Language Model (LLM) alignment. However, it remains unclear
why this model -- originally developed for multi-player stochastic game
matching -- can be adopted to convert pairwise response comparisons to reward
values and make predictions. Especially given the fact that only a limited
number of prompt-response pairs are sparsely compared with others. In this
paper, we first revisit the foundations of using BT models in reward modeling,
and establish the convergence rate of BT reward models based on deep neural
networks using embeddings, providing a theoretical foundation for their use.
Despite theoretically sound, we argue that the BT model is not a necessary
choice from the perspective of downstream optimization. This is because a
reward model only needs to preserve the correct ranking predictions through a
monotonic transformation of the true reward. We highlight the critical concept
of order consistency in reward modeling and demonstrate that the BT model
possesses this property. Consequently, we propose a simple and straightforward
upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an
alternative order-consistent reward modeling objective. To offer practical
insights, we empirically evaluate the performance of these different reward
modeling approaches across more than 12,000 experimental setups, using $6$ base
LLMs, $2$ datasets, and diverse annotation designs that vary in quantity,
quality, and pairing choices in preference annotations.",cs.AI
Clustering in Causal Attention Masking,"This work presents a modification of the self-attention dynamics proposed by
Geshkovski et al. (arXiv:2312.10794) to better reflect the practically
relevant, causally masked attention used in transformer architectures for
generative AI. This modification translates into an interacting particle system
that cannot be interpreted as a mean-field gradient flow. Despite this loss of
structure, we significantly strengthen the results of Geshkovski et al.
(arXiv:2312.10794) in this context: While previous rigorous results focused on
cases where all three matrices (Key, Query, and Value) were scaled identities,
we prove asymptotic convergence to a single cluster for arbitrary key-query
matrices and a value matrix equal to the identity. Additionally, we establish a
connection to the classical R\'enyi parking problem from combinatorial geometry
to make initial theoretical steps towards demonstrating the existence of
meta-stable states.",cs.AI
Few-Shot Task Learning through Inverse Generative Modeling,"Learning the intents of an agent, defined by its goals or motion style, is
often extremely challenging from just a few examples. We refer to this problem
as task concept learning and present our approach, Few-Shot Task Learning
through Inverse Generative Modeling (FTL-IGM), which learns new task concepts
by leveraging invertible neural generative models. The core idea is to pretrain
a generative model on a set of basic concepts and their demonstrations. Then,
given a few demonstrations of a new concept (such as a new goal or a new
action), our method learns the underlying concepts through backpropagation
without updating the model weights, thanks to the invertibility of the
generative model. We evaluate our method in five domains -- object
rearrangement, goal-oriented navigation, motion caption of human actions,
autonomous driving, and real-world table-top manipulation. Our experimental
results demonstrate that via the pretrained generative model, we successfully
learn novel concepts and generate agent plans or motion corresponding to these
concepts in (1) unseen environments and (2) in composition with training
concepts.",cs.AI
DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning,"The ability to predict future outcomes given control actions is fundamental
for physical reasoning. However, such predictive models, often called world
models, have proven challenging to learn and are typically developed for
task-specific solutions with online policy learning. We argue that the true
potential of world models lies in their ability to reason and plan across
diverse problems using only passive data. Concretely, we require world models
to have the following three properties: 1) be trainable on offline,
pre-collected trajectories, 2) support test-time behavior optimization, and 3)
facilitate task-agnostic reasoning. To realize this, we present DINO World
Model (DINO-WM), a new method to model visual dynamics without reconstructing
the visual world. DINO-WM leverages spatial patch features pre-trained with
DINOv2, enabling it to learn from offline behavioral trajectories by predicting
future patch features. This design allows DINO-WM to achieve observational
goals through action sequence optimization, facilitating task-agnostic behavior
planning by treating desired goal patch features as prediction targets. We
evaluate DINO-WM across various domains, including maze navigation, tabletop
pushing, and particle manipulation. Our experiments demonstrate that DINO-WM
can generate zero-shot behavioral solutions at test time without relying on
expert demonstrations, reward modeling, or pre-learned inverse models. Notably,
DINO-WM exhibits strong generalization capabilities compared to prior
state-of-the-art work, adapting to diverse task families such as arbitrarily
configured mazes, push manipulation with varied object shapes, and
multi-particle scenarios.",cs.AI
Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries,"Security experts reverse engineer (decompile) binary code to identify
critical security vulnerabilities. The limited access to source code in vital
systems - such as firmware, drivers, and proprietary software used in Critical
Infrastructures (CI) - makes this analysis even more crucial on the binary
level. Even with available source code, a semantic gap persists after
compilation between the source and the binary code executed by the processor.
This gap may hinder the detection of vulnerabilities in source code. That being
said, current research on Large Language Models (LLMs) overlooks the
significance of decompiled binaries in this area by focusing solely on source
code. In this work, we are the first to empirically uncover the substantial
semantic limitations of state-of-the-art LLMs when it comes to analyzing
vulnerabilities in decompiled binaries, largely due to the absence of relevant
datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary
code vulnerability dataset. Our dataset is multi-architecture and
multi-optimization, focusing on C/C++ due to their wide usage in CI and
association with numerous vulnerabilities. Specifically, we curate 150,872
samples of vulnerable and non-vulnerable decompiled binary code for the task of
(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)
recovering function names in the domain of decompiled binaries. Subsequently,
we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance
increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and
CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,
using DeBinVul, we report a high performance of 80-90% on the vulnerability
classification task. Furthermore, we report improved performance in function
name recovery and vulnerability description tasks.",cs.AI
SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference,"We present SuffixDecoding, a novel model-free approach to accelerating large
language model (LLM) inference through speculative decoding. Unlike existing
methods that rely on draft models or specialized decoding heads, SuffixDecoding
leverages suffix trees built from previously generated outputs to efficiently
predict candidate token sequences. Our approach enables flexible
tree-structured speculation without the overhead of maintaining and
orchestrating additional models. SuffixDecoding builds and dynamically updates
suffix trees to capture patterns in the generated text, using them to construct
speculation trees through a principled scoring mechanism based on empirical
token frequencies. SuffixDecoding requires only CPU memory which is plentiful
and underutilized on typical LLM serving nodes. We demonstrate that
SuffixDecoding achieves competitive speedups compared to model-based approaches
across diverse workloads including open-domain chat, code generation, and
text-to-SQL tasks. For open-ended chat and code generation tasks,
SuffixDecoding achieves up to $1.4\times$ higher output throughput than
SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a
proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to
$2.9\times$ higher output throughput and $3\times$ lower latency than
speculative decoding. Our evaluation shows that SuffixDecoding maintains high
acceptance rates even with small reference corpora of 256 examples, while
continuing to improve performance as more historical outputs are incorporated.",cs.AI
Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability,"Large language models (LLMs) are being explored for diagnostic decision
support, yet their ability to estimate pre-test probabilities, vital for
clinical decision-making, remains limited. This study evaluates two LLMs,
Mistral-7B and Llama3-70B, using structured electronic health record data on
three diagnosis tasks. We examined three current methods of extracting LLM
probability estimations and revealed their limitations. We aim to highlight the
need for improved techniques in LLM confidence estimation.",cs.AI
Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification,"Latent Video Diffusion Models can easily deceive casual observers and domain
experts alike thanks to the produced image quality and temporal consistency.
Beyond entertainment, this creates opportunities around safe data sharing of
fully synthetic datasets, which are crucial in healthcare, as well as other
domains relying on sensitive personal information. However, privacy concerns
with this approach have not fully been addressed yet, and models trained on
synthetic data for specific downstream tasks still perform worse than those
trained on real data. This discrepancy may be partly due to the sampling space
being a subspace of the training videos, effectively reducing the training data
size for downstream models. Additionally, the reduced temporal consistency when
generating long videos could be a contributing factor.
  In this paper, we first show that training privacy-preserving models in
latent space is computationally more efficient and generalize better.
Furthermore, to investigate downstream degradation factors, we propose to use a
re-identification model, previously employed as a privacy preservation filter.
We demonstrate that it is sufficient to train this model on the latent space of
the video generator. Subsequently, we use these models to evaluate the subspace
covered by synthetic video datasets and thus introduce a new way to measure the
faithfulness of generative machine learning models. We focus on a specific
application in healthcare echocardiography to illustrate the effectiveness of
our novel methods. Our findings indicate that only up to 30.8% of the training
videos are learned in latent video diffusion models, which could explain the
lack of performance when training downstream tasks on synthetic data.",cs.AI
M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding,"Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.",cs.AI
SPGD: Steepest Perturbed Gradient Descent Optimization,"Optimization algorithms are pivotal in advancing various scientific and
industrial fields but often encounter obstacles such as trapping in local
minima, saddle points, and plateaus (flat regions), which makes the convergence
to reasonable or near-optimal solutions particularly challenging. This paper
presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that
innovatively combines the principles of the gradient descent method with
periodic uniform perturbation sampling to effectively circumvent these
impediments and lead to better solutions whenever possible. SPGD is
distinctively designed to generate a set of candidate solutions and select the
one exhibiting the steepest loss difference relative to the current solution.
It enhances the traditional gradient descent approach by integrating a
strategic exploration mechanism that significantly increases the likelihood of
escaping sub-optimal local minima and navigating complex optimization
landscapes effectively. Our approach not only retains the directed efficiency
of gradient descent but also leverages the exploratory benefits of stochastic
perturbations, thus enabling a more comprehensive search for global optima
across diverse problem spaces. We demonstrate the efficacy of SPGD in solving
the 3D component packing problem, an NP-hard challenge. Preliminary results
show a substantial improvement over four established methods, particularly on
response surfaces with complex topographies and in multidimensional non-convex
continuous optimization problems. Comparative analyses with established 2D
benchmark functions highlight SPGD's superior performance, showcasing its
ability to navigate complex optimization landscapes. These results emphasize
SPGD's potential as a versatile tool for a wide range of optimization problems.",cs.AI
FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?,"There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.",cs.AI
DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion,"In this paper, we introduce \textbf{DimensionX}, a framework designed to
generate photorealistic 3D and 4D scenes from just a single image with video
diffusion. Our approach begins with the insight that both the spatial structure
of a 3D scene and the temporal evolution of a 4D scene can be effectively
represented through sequences of video frames. While recent video diffusion
models have shown remarkable success in producing vivid visuals, they face
limitations in directly recovering 3D/4D scenes due to limited spatial and
temporal controllability during generation. To overcome this, we propose
ST-Director, which decouples spatial and temporal factors in video diffusion by
learning dimension-aware LoRAs from dimension-variant data. This controllable
video diffusion approach enables precise manipulation of spatial structure and
temporal dynamics, allowing us to reconstruct both 3D and 4D representations
from sequential frames with the combination of spatial and temporal dimensions.
Additionally, to bridge the gap between generated videos and real-world scenes,
we introduce a trajectory-aware mechanism for 3D generation and an
identity-preserving denoising strategy for 4D generation. Extensive experiments
on various real-world and synthetic datasets demonstrate that DimensionX
achieves superior results in controllable video generation, as well as in 3D
and 4D scene generation, compared with previous methods.",cs.AI
StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration,"The advent of AI-Generated Content (AIGC) has spurred research into automated
video generation to streamline conventional processes. However, automating
storytelling video production, particularly for customized narratives, remains
challenging due to the complexity of maintaining subject consistency across
shots. While existing approaches like Mora and AesopAgent integrate multiple
agents for Story-to-Video (S2V) generation, they fall short in preserving
protagonist consistency and supporting Customized Storytelling Video Generation
(CSVG). To address these limitations, we propose StoryAgent, a multi-agent
framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks
assigned to specialized agents, mirroring the professional production process.
Notably, our framework includes agents for story design, storyboard generation,
video creation, agent coordination, and result evaluation. Leveraging the
strengths of different models, StoryAgent enhances control over the generation
process, significantly improving character consistency. Specifically, we
introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance
intra-shot temporal consistency, while a novel storyboard generation pipeline
is proposed to maintain subject consistency across shots. Extensive experiments
demonstrate the effectiveness of our approach in synthesizing highly consistent
storytelling videos, outperforming state-of-the-art methods. Our contributions
include the introduction of StoryAgent, a versatile framework for video
generation tasks, and novel techniques for preserving protagonist consistency.",cs.AI
GPTKB: Building Very Large Knowledge Bases from Language Models,"General-domain knowledge bases (KB), in particular the ""big three"" --
Wikidata, Yago and DBpedia -- are the backbone of many intelligent
applications. While these three have seen steady development, comprehensive KB
construction at large has seen few fresh attempts. In this work, we propose to
build a large general-domain KB entirely from a large language model (LLM). We
demonstrate the feasibility of large-scale KB construction from LLMs, while
highlighting specific challenges arising around entity recognition, entity and
property canonicalization, and taxonomy construction. As a prototype, we use
GPT-4o-mini to construct GPTKB, which contains 105 million triples for more
than 2.9 million entities, at a cost 100x less than previous KBC projects. Our
work is a landmark for two fields: For NLP, for the first time, it provides
\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the
Semantic Web, it shows novel ways forward for the long-standing challenge of
general-domain KB construction. GPTKB is accessible at http://gptkb.org.",cs.AI
Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping,"Recently, there has been growing interest in autonomous shipping due to its
potential to improve maritime efficiency and safety. The use of advanced
technologies, such as artificial intelligence, can address the current
navigational and operational challenges in autonomous shipping. In particular,
inland waterway transport (IWT) presents a unique set of challenges, such as
crowded waterways and variable environmental conditions. In such dynamic
settings, the reliability and robustness of autonomous shipping solutions are
critical factors for ensuring safe operations. This paper examines the
robustness of benchmark deep reinforcement learning (RL) algorithms,
implemented for IWT within an autonomous shipping simulator, and their ability
to generate effective motion planning policies. We demonstrate that a
model-free approach can achieve an adequate policy in the simulator,
successfully navigating port environments never encountered during training. We
focus particularly on Soft-Actor Critic (SAC), which we show to be inherently
more robust to environmental disturbances compared to MuZero, a
state-of-the-art model-based RL algorithm. In this paper, we take a significant
step towards developing robust, applied RL frameworks that can be generalized
to various vessel types and navigate complex port- and inland environments and
scenarios.",cs.AI
GUI Agents with Foundation Models: A Comprehensive Survey,"Recent advances in foundation models, particularly Large Language Models
(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent
agents being capable of performing complex tasks. By leveraging the ability of
(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents
can autonomously execute user instructions by simulating human-like
interactions such as clicking and typing. This survey consolidates recent
research on (M)LLM-based GUI agents, highlighting key innovations in data,
frameworks, and applications. We begin by discussing representative datasets
and benchmarks. Next, we summarize a unified framework that captures the
essential components used in prior research, accompanied by a taxonomy.
Additionally, we explore commercial applications of (M)LLM-based GUI agents.
Drawing from existing work, we identify several key challenges and propose
future research directions. We hope this paper will inspire further
developments in the field of (M)LLM-based GUI agents.",cs.AI
FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI,"We introduce FrontierMath, a benchmark of hundreds of original, exceptionally
challenging mathematics problems crafted and vetted by expert mathematicians.
The questions cover most major branches of modern mathematics -- from
computationally intensive problems in number theory and real analysis to
abstract questions in algebraic geometry and category theory. Solving a typical
problem requires multiple hours of effort from a researcher in the relevant
branch of mathematics, and for the upper end questions, multiple days.
FrontierMath uses new, unpublished problems and automated verification to
reliably evaluate models while minimizing risk of data contamination. Current
state-of-the-art AI models solve under 2% of problems, revealing a vast gap
between AI capabilities and the prowess of the mathematical community. As AI
systems advance toward expert-level mathematical abilities, FrontierMath offers
a rigorous testbed that quantifies their progress.",cs.AI
"Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in Multi-Agent Reinforcement Learning","An important challenge for enabling the deployment of reinforcement learning
(RL) algorithms in the real world is safety. This has resulted in the recent
research field of Safe RL, which aims to learn optimal policies that are safe.
One successful approach in that direction is probabilistic logic shields (PLS),
a model-based Safe RL technique that uses formal specifications based on
probabilistic logic programming, constraining an agent's policy to comply with
those specifications in a probabilistic sense. However, safety is inherently a
multi-agent concept, since real-world environments often involve multiple
agents interacting simultaneously, leading to a complex system which is hard to
control. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. In
order to address this gap, in this paper we ($i$) introduce Shielded MARL
(SMARL) by extending PLS to MARL -- in particular, we introduce Probabilistic
Logic Temporal Difference Learning (PLTD) to enable shielded independent
Q-learning (SIQL), and introduce shielded independent PPO (SIPPO) using
probabilistic logic policy gradients; ($ii$) show its positive effect and use
as an equilibrium selection mechanism in various game-theoretic environments
including two-player simultaneous games, extensive-form games, stochastic
games, and some grid-world extensions in terms of safety, cooperation, and
alignment with normative behaviors; and ($iii$) look into the asymmetric case
where only one agent is shielded, and show that the shielded agent has a
significant influence on the unshielded one, providing further evidence of
SMARL's ability to enhance safety and cooperation in diverse multi-agent
environments.",cs.AI
ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset,"Facade semantic segmentation is a long-standing challenge in photogrammetry
and computer vision. Although the last decades have witnessed the influx of
facade segmentation methods, there is a lack of comprehensive facade classes
and data covering the architectural variability. In ZAHA, we introduce Level of
Facade Generalization (LoFG), novel hierarchical facade classes designed based
on international urban modeling standards, ensuring compatibility with
real-world challenging classes and uniform methods' comparison. Realizing the
LoFG, we present to date the largest semantic 3D facade segmentation dataset,
providing 601 million annotated points at five and 15 classes of LoFG2 and
LoFG3, respectively. Moreover, we analyze the performance of baseline semantic
segmentation methods on our introduced LoFG classes and data, complementing it
with a discussion on the unresolved challenges for facade segmentation. We
firmly believe that ZAHA shall facilitate further development of 3D facade
semantic segmentation methods, enabling robust segmentation indispensable in
creating urban digital twins.",cs.AI
A multi-purpose automatic editing system based on lecture semantics for remote education,"Remote teaching has become popular recently due to its convenience and
safety, especially under extreme circumstances like a pandemic. However, online
students usually have a poor experience since the information acquired from the
views provided by the broadcast platforms is limited. One potential solution is
to show more camera views simultaneously, but it is technically challenging and
distracting for the viewers. Therefore, an automatic multi-camera
directing/editing system, which aims at selecting the most concerned view at
each time instance to guide the attention of online students, is in urgent
demand. However, existing systems mostly make simple assumptions and focus on
tracking the position of the speaker instead of the real lecture semantics, and
therefore have limited capacities to deliver optimal information flow. To this
end, this paper proposes an automatic multi-purpose editing system based on the
lecture semantics, which can both direct the multiple video streams for
real-time broadcasting and edit the optimal video offline for review purposes.
Our system directs the views by semantically analyzing the class events while
following the professional directing rules, mimicking a human director to
capture the regions of interest from the viewpoint of the onsite students. We
conduct both qualitative and quantitative analyses to verify the effectiveness
of the proposed system and its components.",cs.AI
Machine learning and optimization-based approaches to duality in statistical physics,"The notion of duality -- that a given physical system can have two different
mathematical descriptions -- is a key idea in modern theoretical physics.
Establishing a duality in lattice statistical mechanics models requires the
construction of a dual Hamiltonian and a map from the original to the dual
observables. By using simple neural networks to parameterize these maps and
introducing a loss function that penalises the difference between correlation
functions in original and dual models, we formulate the process of duality
discovery as an optimization problem. We numerically solve this problem and
show that our framework can rediscover the celebrated Kramers-Wannier duality
for the 2d Ising model, reconstructing the known mapping of temperatures. We
also discuss an alternative approach which uses known features of the mapping
of topological lines to reduce the problem to optimizing the couplings in a
dual Hamiltonian, and explore next-to-nearest neighbour deformations of the 2d
Ising duality. We discuss future directions and prospects for discovering new
dualities within this framework.",cs.AI
Seeing is Deceiving: Exploitation of Visual Pathways in Multi-Modal Language Models,"Multi-Modal Language Models (MLLMs) have transformed artificial intelligence
by combining visual and text data, making applications like image captioning,
visual question answering, and multi-modal content creation possible. This
ability to understand and work with complex information has made MLLMs useful
in areas such as healthcare, autonomous systems, and digital content. However,
integrating multiple types of data also creates security risks. Attackers can
manipulate either the visual or text inputs, or both, to make the model produce
unintended or even harmful responses. This paper reviews how visual inputs in
MLLMs can be exploited by various attack strategies. We break down these
attacks into categories: simple visual tweaks and cross-modal manipulations, as
well as advanced strategies like VLATTACK, HADES, and Collaborative Multimodal
Adversarial Attack (Co-Attack). These attacks can mislead even the most robust
models while looking nearly identical to the original visuals, making them hard
to detect. We also discuss the broader security risks, including threats to
privacy and safety in important applications. To counter these risks, we review
current defense methods like the SmoothVLM framework, pixel-wise randomization,
and MirrorCheck, looking at their strengths and limitations. We also discuss
new methods to make MLLMs more secure, including adaptive defenses, better
evaluation tools, and security approaches that protect both visual and text
data. By bringing together recent developments and identifying key areas for
improvement, this review aims to support the creation of more secure and
reliable multi-modal AI systems for real-world use.",cs.AI
Plasticity Loss in Deep Reinforcement Learning: A Survey,"Akin to neuroplasticity in human brains, the plasticity of deep neural
networks enables their quick adaption to new data. This makes plasticity
particularly crucial for deep Reinforcement Learning (RL) agents: Once
plasticity is lost, an agent's performance will inevitably plateau because it
cannot improve its policy to account for changes in the data distribution,
which are a necessary consequence of its learning process. Thus, developing
well-performing and sample-efficient agents hinges on their ability to remain
plastic during training. Furthermore, the loss of plasticity can be connected
to many other issues plaguing deep RL, such as training instabilities, scaling
failures, overestimation bias, and insufficient exploration. With this survey,
we aim to provide an overview of the emerging research on plasticity loss for
academics and practitioners of deep reinforcement learning. First, we propose a
unified definition of plasticity loss based on recent works, relate it to
definitions from the literature, and discuss metrics for measuring plasticity
loss. Then, we categorize and discuss numerous possible causes of plasticity
loss before reviewing currently employed mitigation strategies. Our taxonomy is
the first systematic overview of the current state of the field. Lastly, we
discuss prevalent issues within the literature, such as a necessity for broader
evaluation, and provide recommendations for future research, like gaining a
better understanding of an agent's neural activity and behavior.",cs.AI
D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes,"Depth estimation is a crucial technology in robotics. Recently,
self-supervised depth estimation methods have demonstrated great potential as
they can efficiently leverage large amounts of unlabelled real-world data.
However, most existing methods are designed under the assumption of static
scenes, which hinders their adaptability in dynamic environments. To address
this issue, we present D$^3$epth, a novel method for self-supervised depth
estimation in dynamic scenes. It tackles the challenge of dynamic objects from
two key perspectives. First, within the self-supervised framework, we design a
reprojection constraint to identify regions likely to contain dynamic objects,
allowing the construction of a dynamic mask that mitigates their impact at the
loss level. Second, for multi-frame depth estimation, we introduce a cost
volume auto-masking strategy that leverages adjacent frames to identify regions
associated with dynamic objects and generate corresponding masks. This provides
guidance for subsequent processes. Furthermore, we propose a spectral entropy
uncertainty module that incorporates spectral entropy to guide uncertainty
estimation during depth fusion, effectively addressing issues arising from cost
volume computation in dynamic environments. Extensive experiments on KITTI and
Cityscapes datasets demonstrate that the proposed method consistently
outperforms existing self-supervised monocular depth estimation baselines. Code
is available at \url{https://github.com/Csyunling/D3epth}.",cs.AI
Defending Deep Regression Models against Backdoor Attacks,"Deep regression models are used in a wide variety of safety-critical
applications, but are vulnerable to backdoor attacks. Although many defenses
have been proposed for classification models, they are ineffective as they do
not consider the uniqueness of regression models. First, the outputs of
regression models are continuous values instead of discretized labels. Thus,
the potential infected target of a backdoored regression model has infinite
possibilities, which makes it impossible to be determined by existing defenses.
Second, the backdoor behavior of backdoored deep regression models is triggered
by the activation values of all the neurons in the feature space, which makes
it difficult to be detected and mitigated using existing defenses. To resolve
these problems, we propose DRMGuard, the first defense to identify if a deep
regression model in the image domain is backdoored or not. DRMGuard formulates
the optimization problem for reverse engineering based on the unique
output-space and feature-space characteristics of backdoored deep regression
models. We conduct extensive evaluations on two regression tasks and four
datasets. The results show that DRMGuard can consistently defend against
various backdoor attacks. We also generalize four state-of-the-art defenses
designed for classifiers to regression models, and compare DRMGuard with them.
The results show that DRMGuard significantly outperforms all those defenses.",cs.AI
Kwai-STaR: Transform LLMs into State-Transition Reasoners,"Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.",cs.AI
MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation,"Visual odometry (VO) is essential for enabling accurate point-goal navigation
of embodied agents in indoor environments where GPS and compass sensors are
unreliable and inaccurate. However, traditional VO methods face challenges in
wide-baseline scenarios, where fast robot motions and low frames per second
(FPS) during inference hinder their performance, leading to drift and
catastrophic failures in point-goal navigation. Recent deep-learned VO methods
show robust performance but suffer from sample inefficiency during training;
hence, they require huge datasets and compute resources. So, we propose a
robust and sample-efficient VO pipeline based on motion priors available while
an agent is navigating an environment. It consists of a training-free
action-prior based geometric VO module that estimates a coarse relative pose
which is further consumed as a motion prior by a deep-learned VO model, which
finally produces a fine relative pose to be used by the navigation policy. This
strategy helps our pipeline achieve up to 2x sample efficiency during training
and demonstrates superior accuracy and robustness in point-goal navigation
tasks compared to state-of-the-art VO method(s). Realistic indoor environments
of the Gibson dataset is used in the AI-Habitat simulator to evaluate the
proposed approach using navigation metrics (like success/SPL) and pose metrics
(like RPE/ATE). We hope this method further opens a direction of work where
motion priors from various sources can be utilized to improve VO estimates and
achieve better results in embodied navigation tasks.",cs.AI
AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual Alignment,"Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual
alignment. Our findings suggest that although LLMs also demonstrate promising
cross-lingual alignment in Information Extraction, there remains significant
imbalance across languages, revealing an underlying deficiency in the IE
alignment. To address this issue, we propose AlignXIE, a powerful code-based
LLM that significantly enhances cross-lingual IE alignment through two
strategies. Firstly, AlignXIE formulates IE across different languages,
especially non-English ones, as code generation tasks, standardizing the
representation of various schemas using Python classes to ensure consistency of
the same ontology in different languages and align the schema. Secondly, it
incorporates an IE cross-lingual alignment phase through a translated instance
prediction task proposed in this paper to align the extraction process,
utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,
generated by our proposed LLM-based automatic pipeline for IE parallel data
construction, with manual annotation to ensure quality. Ultimately, we obtain
AlignXIE through multilingual IE instruction tuning. Although without training
in 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\%$ and SoTA by
$20.03\%$, thereby demonstrating superior cross-lingual IE capabilities.
Comprehensive evaluations on 63 IE benchmarks in Chinese and English under
various settings, demonstrate that AlignXIE significantly enhances
cross-lingual and multilingual IE through boosting the IE alignment.",cs.AI
Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research,"In recent years, the application of generative artificial intelligence
(GenAI) in financial analysis and investment decision-making has gained
significant attention. However, most existing approaches rely on single-agent
systems, which fail to fully utilize the collaborative potential of multiple AI
agents. In this paper, we propose a novel multi-agent collaboration system
designed to enhance decision-making in financial investment research. The
system incorporates agent groups with both configurable group sizes and
collaboration structures to leverage the strengths of each agent group type. By
utilizing a sub-optimal combination strategy, the system dynamically adapts to
varying market conditions and investment scenarios, optimizing performance
across different tasks. We focus on three sub-tasks: fundamentals, market
sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30
companies listed on the Dow Jones Index. Our findings reveal significant
performance variations based on the configurations of AI agents for different
tasks. The results demonstrate that our multi-agent collaboration system
outperforms traditional single-agent models, offering improved accuracy,
efficiency, and adaptability in complex financial environments. This study
highlights the potential of multi-agent systems in transforming financial
analysis and investment decision-making by integrating diverse analytical
perspectives.",cs.AI
Navigating Trade-offs: Policy Summarization for Multi-Objective Reinforcement Learning,"Multi-objective reinforcement learning (MORL) is used to solve problems
involving multiple objectives. An MORL agent must make decisions based on the
diverse signals provided by distinct reward functions. Training an MORL agent
yields a set of solutions (policies), each presenting distinct trade-offs among
the objectives (expected returns). MORL enhances explainability by enabling
fine-grained comparisons of policies in the solution set based on their
trade-offs as opposed to having a single policy. However, the solution set is
typically large and multi-dimensional, where each policy (e.g., a neural
network) is represented by its objective values.
  We propose an approach for clustering the solution set generated by MORL. By
considering both policy behavior and objective values, our clustering method
can reveal the relationship between policy behaviors and regions in the
objective space. This approach can enable decision makers (DMs) to identify
overarching trends and insights in the solution set rather than examining each
policy individually. We tested our method in four multi-objective environments
and found it outperformed traditional k-medoids clustering. Additionally, we
include a case study that demonstrates its real-world application.",cs.AI
Attention Masks Help Adversarial Attacks to Bypass Safety Detectors,"Despite recent research advancements in adversarial attack methods, current
approaches against XAI monitors are still discoverable and slower. In this
paper, we present an adaptive framework for attention mask generation to enable
stealthy, explainable and efficient PGD image classification adversarial attack
under XAI monitors. Specifically, we utilize mutation XAI mixture and multitask
self-supervised X-UNet for attention mask generation to guide PGD attack.
Experiments on MNIST (MLP), CIFAR-10 (AlexNet) have shown that our system can
outperform benchmark PGD, Sparsefool and SOTA SINIFGSM in balancing among
stealth, efficiency and explainability which is crucial for effectively fooling
SOTA defense protected classifiers.",cs.AI
Equivariant Graph Attention Networks with Structural Motifs for Predicting Cell Line-Specific Synergistic Drug Combinations,"Cancer is the second leading cause of death, with chemotherapy as one of the
primary forms of treatment. As a result, researchers are turning to drug
combination therapy to decrease drug resistance and increase efficacy. Current
methods of drug combination screening, such as in vivo and in vitro, are
inefficient due to stark time and monetary costs. In silico methods have become
increasingly important for screening drugs, but current methods are inaccurate
and generalize poorly to unseen anticancer drugs. In this paper, I employ a
geometric deep-learning model utilizing a graph attention network that is
equivariant to 3D rotations, translations, and reflections with structural
motifs. Additionally, the gene expression of cancer cell lines is utilized to
classify synergistic drug combinations specific to each cell line. I compared
the proposed geometric deep learning framework to current state-of-the-art
(SOTA) methods, and the proposed model architecture achieved greater
performance on all 12 benchmark tasks performed on the DrugComb dataset.
Specifically, the proposed framework outperformed other SOTA methods by an
accuracy difference greater than 28%. Based on these results, I believe that
the equivariant graph attention network's capability of learning geometric data
accounts for the large performance improvements. The model's ability to
generalize to foreign drugs is thought to be due to the structural motifs
providing a better representation of the molecule. Overall, I believe that the
proposed equivariant geometric deep learning framework serves as an effective
tool for virtually screening anticancer drug combinations for further
validation in a wet lab environment. The code for this work is made available
online at: https://github.com/WeToTheMoon/EGAT_DrugSynergy.",cs.AI
Differential Privacy Overview and Fundamental Techniques,"This chapter is meant to be part of the book ""Differential Privacy in
Artificial Intelligence: From Theory to Practice"" and provides an introduction
to Differential Privacy. It starts by illustrating various attempts to protect
data privacy, emphasizing where and why they failed, and providing the key
desiderata of a robust privacy definition. It then defines the key actors,
tasks, and scopes that make up the domain of privacy-preserving data analysis.
Following that, it formalizes the definition of Differential Privacy and its
inherent properties, including composition, post-processing immunity, and group
privacy. The chapter also reviews the basic techniques and mechanisms commonly
used to implement Differential Privacy in its pure and approximate forms.",cs.AI
Integrating Large Language Models for Genetic Variant Classification,"The classification of genetic variants, particularly Variants of Uncertain
Significance (VUS), poses a significant challenge in clinical genetics and
precision medicine. Large Language Models (LLMs) have emerged as transformative
tools in this realm. These models can uncover intricate patterns and predictive
insights that traditional methods might miss, thus enhancing the predictive
accuracy of genetic variant pathogenicity.
  This study investigates the integration of state-of-the-art LLMs, including
GPN-MSA, ESM1b, and AlphaMissense, which leverage DNA and protein sequence data
alongside structural insights to form a comprehensive analytical framework for
variant classification. Our approach evaluates these integrated models using
the well-annotated ProteinGym and ClinVar datasets, setting new benchmarks in
classification performance. The models were rigorously tested on a set of
challenging variants, demonstrating substantial improvements over existing
state-of-the-art tools, especially in handling ambiguous and clinically
uncertain variants.
  The results of this research underline the efficacy of combining multiple
modeling approaches to significantly refine the accuracy and reliability of
genetic variant classification systems. These findings support the deployment
of these advanced computational models in clinical environments, where they can
significantly enhance the diagnostic processes for genetic disorders,
ultimately pushing the boundaries of personalized medicine by offering more
detailed and actionable genetic insights.",cs.AI
The Multiple Dimensions of Spuriousness in Machine Learning,"Learning correlations from data forms the foundation of today's machine
learning (ML) and artificial intelligence (AI) research. While such an approach
enables the automatic discovery of patterned relationships within big data
corpora, it is susceptible to failure modes when unintended correlations are
captured. This vulnerability has expanded interest in interrogating
spuriousness, often critiqued as an impediment to model performance, fairness,
and robustness. In this article, we trace deviations from the conventional
definition of statistical spuriousness-which denotes a non-causal observation
arising from either coincidence or confounding variables-to articulate how ML
researchers make sense of spuriousness in practice. Drawing on a broad survey
of ML literature, we conceptualize the ""multiple dimensions of spuriousness,""
encompassing: relevance (""Models should only use correlations that are relevant
to the task.""), generalizability (""Models should only use correlations that
generalize to unseen data""), human-likeness (""Models should only use
correlations that a human would use to perform the same task""), and harmfulness
(""Models should only use correlations that are not harmful""). These dimensions
demonstrate that ML spuriousness goes beyond the causal/non-causal dichotomy
and that the disparate interpretative paths researchers choose could
meaningfully influence the trajectory of ML development. By underscoring how a
fundamental problem in ML is contingently negotiated in research contexts, we
contribute to ongoing debates about responsible practices in AI development.",cs.AI
Reciprocal Point Learning Network with Large Electromagnetic Kernel for SAR Open-Set Recognition,"The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) methods lie in their confinement by the closed-environment
assumption, hindering their effective and robust handling of unknown target
categories in open environments. Open Set Recognition (OSR), a pivotal facet
for algorithmic practicality, intends to categorize known classes while
denoting unknown ones as ""unknown."" The chief challenge in OSR involves
concurrently mitigating risks associated with generalizing features from a
restricted set of known classes to numerous unknown samples and the open space
exposure to potential unknown data. To enhance open-set SAR classification, a
method called scattering kernel with reciprocal learning network is proposed.
Initially, a feature learning framework is constructed based on reciprocal
point learning (RPL), establishing a bounded space for potential unknown
classes. This approach indirectly introduces unknown information into a learner
confined to known classes, thereby acquiring more concise and discriminative
representations. Subsequently, considering the variability in the imaging of
targets at different angles and the discreteness of components in SAR images, a
proposal is made to design convolutional kernels based on large-sized attribute
scattering center models. This enhances the ability to extract intrinsic
non-linear features and specific scattering characteristics in SAR images,
thereby improving the discriminative features of the model and mitigating the
impact of imaging variations on classification performance. Experiments on the
MSTAR datasets substantiate the superior performance of the proposed approach
called ASC-RPL over mainstream methods.",cs.AI
Personalized Federated Learning for Cross-view Geo-localization,"In this paper we propose a methodology combining Federated Learning (FL) with
Cross-view Image Geo-localization (CVGL) techniques. We address the challenges
of data privacy and heterogeneity in autonomous vehicle environments by
proposing a personalized Federated Learning scenario that allows selective
sharing of model parameters. Our method implements a coarse-to-fine approach,
where clients share only the coarse feature extractors while keeping
fine-grained features specific to local environments. We evaluate our approach
against traditional centralized and single-client training schemes using the
KITTI dataset combined with satellite imagery. Results demonstrate that our
federated CVGL method achieves performance close to centralized training while
maintaining data privacy. The proposed partial model sharing strategy shows
comparable or slightly better performance than classical FL, offering
significant reduced communication overhead without sacrificing accuracy. Our
work contributes to more robust and privacy-preserving localization systems for
autonomous vehicles operating in diverse environments",cs.AI
AWARE Narrator and the Utilization of Large Language Models to Extract Behavioral Insights from Smartphone Sensing Data,"Smartphones, equipped with an array of sensors, have become valuable tools
for personal sensing. Particularly in digital health, smartphones facilitate
the tracking of health-related behaviors and contexts, contributing
significantly to digital phenotyping, a process where data from digital
interactions is analyzed to infer behaviors and assess mental health.
Traditional methods process raw sensor data into information features for
statistical and machine learning analyses. In this paper, we introduce a novel
approach that systematically converts smartphone-collected data into
structured, chronological narratives. The AWARE Narrator translates
quantitative smartphone sensing data into English language descriptions,
forming comprehensive narratives of an individual's activities. We apply the
framework to the data collected from university students over a week,
demonstrating the potential of utilizing the narratives to summarize individual
behavior, and analyzing psychological states by leveraging large language
models.",cs.AI
Solving Generalized Grouping Problems in Cellular Manufacturing Systems Using a Network Flow Model,"This paper focuses on the generalized grouping problem in the context of
cellular manufacturing systems (CMS), where parts may have more than one
process route. A process route lists the machines corresponding to each part of
the operation. Inspired by the extensive and widespread use of network flow
algorithms, this research formulates the process route family formation for
generalized grouping as a unit capacity minimum cost network flow model. The
objective is to minimize dissimilarity (based on the machines required) among
the process routes within a family. The proposed model optimally solves the
process route family formation problem without pre-specifying the number of
part families to be formed. The process route of family formation is the first
stage in a hierarchical procedure. For the second stage (machine cell
formation), two procedures, a quadratic assignment programming (QAP)
formulation and a heuristic procedure, are proposed. The QAP simultaneously
assigns process route families and machines to a pre-specified number of cells
in such a way that total machine utilization is maximized. The heuristic
procedure for machine cell formation is hierarchical in nature. Computational
results for some test problems show that the QAP and the heuristic procedure
yield the same results.",cs.AI
FMEA Builder: Expert Guided Text Generation for Equipment Maintenance,"Foundation models show great promise for generative tasks in many domains.
Here we discuss the use of foundation models to generate structured documents
related to critical assets. A Failure Mode and Effects Analysis (FMEA) captures
the composition of an asset or piece of equipment, the ways it may fail and the
consequences thereof. Our system uses large language models to enable fast and
expert supervised generation of new FMEA documents. Empirical analysis shows
that foundation models can correctly generate over half of an FMEA's key
content. Results from polling audiences of reliability professionals show a
positive outlook on using generative AI to create these documents for critical
assets.",cs.AI
CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation,"In this work, we address the cooperation problem among large language model
(LLM) based embodied agents, where agents must cooperate to achieve a common
goal. Previous methods often execute actions extemporaneously and incoherently,
without long-term strategic and cooperative planning, leading to redundant
steps, failures, and even serious repercussions in complex tasks like
search-and-rescue missions where discussion and cooperative plan are crucial.
To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance
the cooperation efficiency of LLM-based embodied agents. Inspired by human
cooperation schemes, CaPo improves cooperation efficiency with two phases: 1)
meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the
first phase, all agents analyze the task, discuss, and cooperatively create a
meta-plan that decomposes the task into subtasks with detailed steps, ensuring
a long-term strategic and coherent plan for efficient coordination. In the
second phase, agents execute tasks according to the meta-plan and dynamically
adjust it based on their latest progress (e.g., discovering a target object)
through multi-turn discussions. This progress-based adaptation eliminates
redundant actions, improving the overall cooperation efficiency of agents.
Experimental results on the ThreeDworld Multi-Agent Transport and Communicative
Watch-And-Help tasks demonstrate that CaPo achieves much higher task completion
rate and efficiency compared with state-of-the-arts.",cs.AI
CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR,"Recent developments in computer graphics, machine learning, and sensor
technologies enable numerous opportunities for extended reality (XR) setups for
everyday life, from skills training to entertainment. With large corporations
offering consumer-grade head-mounted displays (HMDs) in an affordable way, it
is likely that XR will become pervasive, and HMDs will develop as personal
devices like smartphones and tablets. However, having intelligent spaces and
naturalistic interactions in XR is as important as technological advances so
that users grow their engagement in virtual and augmented spaces. To this end,
large language model (LLM)--powered non-player characters (NPCs) with
speech-to-text (STT) and text-to-speech (TTS) models bring significant
advantages over conventional or pre-scripted NPCs for facilitating more natural
conversational user interfaces (CUIs) in XR. In this paper, we provide the
community with an open-source, customizable, extensible, and privacy-aware
Unity package, CUIfy, that facilitates speech-based NPC-user interaction with
various LLMs, STT, and TTS models. Our package also supports multiple
LLM-powered NPCs per environment and minimizes the latency between different
computational models through streaming to achieve usable interactions between
users and NPCs. We publish our source code in the following repository:
https://gitlab.lrz.de/hctl/cuify",cs.AI
EffiCANet: Efficient Time Series Forecasting with Convolutional Attention,"The exponential growth of multivariate time series data from sensor networks
in domains like industrial monitoring and smart cities requires efficient and
accurate forecasting models. Current deep learning methods often fail to
adequately capture long-range dependencies and complex inter-variable
relationships, especially under real-time processing constraints. These
limitations arise as many models are optimized for either short-term
forecasting with limited receptive fields or long-term accuracy at the cost of
efficiency. Additionally, dynamic and intricate interactions between variables
in real-world data further complicate modeling efforts. To address these
limitations, we propose EffiCANet, an Efficient Convolutional Attention Network
designed to enhance forecasting accuracy while maintaining computational
efficiency. EffiCANet integrates three key components: (1) a Temporal
Large-kernel Decomposed Convolution (TLDC) module that captures long-term
temporal dependencies while reducing computational overhead; (2) an
Inter-Variable Group Convolution (IVGC) module that captures complex and
evolving relationships among variables; and (3) a Global Temporal-Variable
Attention (GTVA) mechanism that prioritizes critical temporal and
inter-variable features. Extensive evaluations across nine benchmark datasets
show that EffiCANet achieves the maximum reduction of 10.02% in MAE over
state-of-the-art models, while cutting computational costs by 26.2% relative to
conventional large-kernel convolution methods, thanks to its efficient
decomposition strategy.",cs.AI
DISCO: DISCovering Overfittings as Causal Rules for Text Classification Models,"With the rapid advancement of neural language models, the deployment of
over-parameterized models has surged, increasing the need for interpretable
explanations comprehensible to human inspectors. Existing post-hoc
interpretability methods, which often focus on unigram features of single input
textual instances, fail to capture the models' decision-making process fully.
Additionally, many methods do not differentiate between decisions based on
spurious correlations and those based on a holistic understanding of the input.
Our paper introduces DISCO, a novel method for discovering global, rule-based
explanations by identifying causal n-gram associations with model predictions.
This method employs a scalable sequence mining technique to extract relevant
text spans from training data, associate them with model predictions, and
conduct causality checks to distill robust rules that elucidate model behavior.
These rules expose potential overfitting and provide insights into misleading
feature combinations. We validate DISCO through extensive testing,
demonstrating its superiority over existing methods in offering comprehensive
insights into complex model behaviors. Our approach successfully identifies all
shortcuts manually introduced into the training data (100% detection rate on
the MultiRC dataset), resulting in an 18.8% regression in model performance --
a capability unmatched by any other method. Furthermore, DISCO supports
interactive explanations, enabling human inspectors to distinguish spurious
causes in the rule-based output. This alleviates the burden of abundant
instance-wise explanations and helps assess the model's risk when encountering
out-of-distribution (OOD) data.",cs.AI
wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification from Physiological Signals,"Accurate classification of sleep stages from less obtrusive sensor
measurements such as the electrocardiogram (ECG) or photoplethysmogram (PPG)
could enable important applications in sleep medicine. Existing approaches to
this problem have typically used deep learning models designed and trained to
operate on one or more specific input signals. However, the datasets used to
develop these models often do not contain the same sets of input signals. Some
signals, particularly PPG, are much less prevalent than others, and this has
previously been addressed with techniques such as transfer learning.
Additionally, only training on one or more fixed modalities precludes
cross-modal information transfer from other sources, which has proved valuable
in other problem domains. To address this, we introduce wav2sleep, a unified
model designed to operate on variable sets of input signals during training and
inference. After jointly training on over 10,000 overnight recordings from six
publicly available polysomnography datasets, including SHHS and MESA, wav2sleep
outperforms existing sleep stage classification models across test-time input
combinations including ECG, PPG, and respiratory signals.",cs.AI
TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language Models,"Vision-Language (VL) models have garnered considerable research interest;
however, they still face challenges in effectively handling text within images.
To address this limitation, researchers have developed two approaches. The
first method involves utilizing external Optical Character Recognition (OCR)
tools to extract textual information from images, which is then prepended to
other textual inputs. The second strategy focuses on employing extremely
high-resolution images to improve text recognition capabilities. In this paper,
we focus on enhancing the first strategy by introducing a novel method, named
TAP-VL, which treats OCR information as a distinct modality and seamlessly
integrates it into any VL model. TAP-VL employs a lightweight transformer-based
OCR module to receive OCR with layout information, compressing it into a short
fixed-length sequence for input into the LLM. Initially, we conduct
model-agnostic pretraining of the OCR module on unlabeled documents, followed
by its integration into any VL architecture through brief fine-tuning.
Extensive experiments demonstrate consistent performance improvements when
applying TAP-VL to top-performing VL models, across scene-text and
document-based VL benchmarks.",cs.AI
Verification of Neural Networks against Convolutional Perturbations via Parameterised Kernels,"We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.",cs.AI
Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using ChatGPT for Arabic Grammatical Error Correction,"Natural language processing (NLP) utilizes text data augmentation to overcome
sample size constraints. Increasing the sample size is a natural and widely
used strategy for alleviating these challenges. In this study, we chose Arabic
to increase the sample size and correct grammatical errors. Arabic is
considered one of the languages with limited resources for grammatical error
correction (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used
in most Arabic grammatical error correction research, with approximately 20,500
parallel examples, which is considered low compared with other languages.
Therefore, this study aims to develop an Arabic corpus called ""Tibyan"" for
grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter
tool based on a pair of Arabic sentences containing grammatical errors matched
with a sentence free of errors extracted from Arabic books, called guide
sentences. Multiple steps were involved in establishing our corpus, including
the collection and pre-processing of a pair of Arabic texts from various
sources, such as books and open-access corpora. We then used ChatGPT to
generate a parallel corpus based on the text collected previously, as a guide
for generating sentences with multiple types of errors. By engaging linguistic
experts to review and validate the automatically generated sentences, we
ensured that they were correct and error-free. The corpus was validated and
refined iteratively based on feedback provided by linguistic experts to improve
its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to
analyze the types of errors in the Tibyan corpus. Our corpus contained 49 of
errors, including seven types: orthography, morphology, syntax, semantics,
punctuation, merge, and split. The Tibyan corpus contains approximately 600 K
tokens.",cs.AI
On the Inherent Robustness of One-Stage Object Detection against Out-of-Distribution Data,"Robustness is a fundamental aspect for developing safe and trustworthy
models, particularly when they are deployed in the open world. In this work we
analyze the inherent capability of one-stage object detectors to robustly
operate in the presence of out-of-distribution (OoD) data. Specifically, we
propose a novel detection algorithm for detecting unknown objects in image
data, which leverages the features extracted by the model from each sample.
Differently from other recent approaches in the literature, our proposal does
not require retraining the object detector, thereby allowing for the use of
pretrained models. Our proposed OoD detector exploits the application of
supervised dimensionality reduction techniques to mitigate the effects of the
curse of dimensionality on the features extracted by the model. Furthermore, it
utilizes high-resolution feature maps to identify potential unknown objects in
an unsupervised fashion. Our experiments analyze the Pareto trade-off between
the performance detecting known and unknown objects resulting from different
algorithmic configurations and inference confidence thresholds. We also compare
the performance of our proposed algorithm to that of logits-based post-hoc OoD
methods, as well as possible fusion strategies. Finally, we discuss on the
competitiveness of all tested methods against state-of-the-art OoD approaches
for object detection models over the recently published Unknown Object
Detection benchmark. The obtained results verify that the performance of
avant-garde post-hoc OoD detectors can be further improved when combined with
our proposed algorithm.",cs.AI
Interpreting the Learned Model in MuZero Planning,"MuZero has achieved superhuman performance in various games by using a
dynamics network to predict environment dynamics for planning, without relying
on simulators. However, the latent states learned by the dynamics network make
its planning process opaque. This paper aims to demystify MuZero's model by
interpreting the learned latent states. We incorporate observation
reconstruction and state consistency into MuZero training and conduct an
in-depth analysis to evaluate latent states across two board games: 9x9 Go and
Outer-Open Gomoku, and three Atari games: Breakout, Ms. Pacman, and Pong. Our
findings reveal that while the dynamics network becomes less accurate over
longer simulations, MuZero still performs effectively by using planning to
correct errors. Our experiments also show that the dynamics network learns
better latent states in board games than in Atari games. These insights
contribute to a better understanding of MuZero and offer directions for future
research to improve the playing performance, robustness, and interpretability
of the MuZero algorithm.",cs.AI
Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions,"Multi-agent systems - systems with multiple independent AI agents working
together to achieve a common goal - are becoming increasingly prevalent in
daily life. Drawing inspiration from the phenomenon of human group social
influence, we investigate whether a group of AI agents can create social
pressure on users to agree with them, potentially changing their stance on a
topic. We conducted a study in which participants discussed social issues with
either a single or multiple AI agents, and where the agents either agreed or
disagreed with the user's stance on the topic. We found that conversing with
multiple agents (holding conversation content constant) increased the social
pressure felt by participants, and caused a greater shift in opinion towards
the agents' stances on each topic. Our study shows the potential advantages of
multi-agent systems over single-agent platforms in causing opinion change. We
discuss design implications for possible multi-agent systems that promote
social good, as well as the potential for malicious actors to use these systems
to manipulate public opinion.",cs.AI
Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages,"This paper presents a novel multistage fine-tuning strategy designed to
enhance automatic speech recognition (ASR) performance in low-resource
languages using OpenAI's Whisper model. In this approach we aim to build ASR
model for languages with limited digital resources by sequentially adapting the
model across linguistically similar languages. We experimented this on the
Malasar language, a Dravidian language spoken by approximately ten thousand
people in the Western Ghats of South India. Malasar language faces critical
challenges for technological intervention due to its lack of a native script
and absence of digital or spoken data resources. Working in collaboration with
Wycliffe India and Malasar community members, we created a spoken Malasar
corpus paired with transcription in Tamil script, a closely related major
language. In our approach to build ASR model for Malasar, we first build an
intermediate Tamil ASR, leveraging higher data availability for Tamil annotated
speech. This intermediate model is subsequently fine-tuned on Malasar data,
allowing for more effective ASR adaptation despite limited resources. The
multistage fine-tuning strategy demonstrated significant improvements over
direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of
51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning
method. Further a WER reduction to 47.3% was achieved through punctuation
removal in post-processing, which addresses formatting inconsistencies that
impact evaluation. Our results underscore the effectiveness of sequential
multistage fine-tuning combined with targeted post-processing as a scalable
strategy for ASR system development in low-resource languages, especially where
linguistic similarities can be leveraged to bridge gaps in training data.",cs.AI
Impact of Label Noise on Learning Complex Features,"Neural networks trained with stochastic gradient descent exhibit an inductive
bias towards simpler decision boundaries, typically converging to a narrow
family of functions, and often fail to capture more complex features. This
phenomenon raises concerns about the capacity of deep models to adequately
learn and represent real-world datasets. Traditional approaches such as
explicit regularization, data augmentation, architectural modifications, etc.,
have largely proven ineffective in encouraging the models to learn diverse
features. In this work, we investigate the impact of pre-training models with
noisy labels on the dynamics of SGD across various architectures and datasets.
We show that pretraining promotes learning complex functions and diverse
features in the presence of noise. Our experiments demonstrate that
pre-training with noisy labels encourages gradient descent to find alternate
minima that do not solely depend upon simple features, rather learns more
complex and broader set of features, without hurting performance.",cs.AI
A Generalisation of Voter Model: Influential Nodes and Convergence Properties,"Consider an undirected graph G, representing a social network, where each
node is blue or red, corresponding to positive or negative opinion on a topic.
In the voter model, in discrete time rounds, each node picks a neighbour
uniformly at random and adopts its colour. Despite its significant popularity,
this model does not capture some fundamental real-world characteristics such as
the difference in the strengths of individuals connections, individuals with
neutral opinion on a topic, and individuals who are reluctant to update their
opinion. To address these issues, we introduce and study a generalisation of
the voter model. Motivating by campaigning strategies, we study the problem of
selecting a set of seeds blue nodes to maximise the expected number of blue
nodes after some rounds. We prove that the problem is NP- hard and provide a
polynomial time approximation algorithm with the best possible approximation
guarantee. Our experiments on real-world and synthetic graph data demonstrate
that the proposed algorithm outperforms other algorithms. We also investigate
the convergence properties of the model. We prove that the process could take
an exponential number of rounds to converge. However, if we limit ourselves to
strongly connected graphs, the convergence time is polynomial and the period
(the number of states in convergence) divides the length of all cycles in the
graph.",cs.AI
Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning,"In offline reinforcement learning, a policy is learned using a static dataset
in the absence of costly feedback from the environment. In contrast to the
online setting, only using static datasets poses additional challenges, such as
policies generating out-of-distribution samples. Model-based offline
reinforcement learning methods try to overcome these by learning a model of the
underlying dynamics of the environment and using it to guide policy search. It
is beneficial but, with limited datasets, errors in the model and the issue of
value overestimation among out-of-distribution states can worsen performance.
Current model-based methods apply some notion of conservatism to the Bellman
update, often implemented using uncertainty estimation derived from model
ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP)
which learns a generative model of the joint distribution of observations and
actions. We cast policy learning as a constrained objective to always stay
within the support of the latent action distribution, and use the generative
capabilities of the model to impose an implicit constraint on the generated
actions. Thereby eliminating the need to use additional uncertainty penalties
on the Bellman update and significantly decreasing the number of gradient steps
required to learn a policy. We empirically evaluate C-LAP on the D4RL and
V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art
methods, especially outperforming on datasets with visual observations.",cs.AI
An Axiomatic Study of the Evaluation of Enthymeme Decoding in Weighted Structured Argumentation,"An argument can be seen as a pair consisting of a set of premises and a claim
supported by them. Arguments used by humans are often enthymemes, i.e., some
premises are implicit. To better understand, evaluate, and compare enthymemes,
it is essential to decode them, i.e., to find the missing premisses. Many
enthymeme decodings are possible. We need to distinguish between reasonable
decodings and unreasonable ones. However, there is currently no research in the
literature on ""How to evaluate decodings?"". To pave the way and achieve this
goal, we introduce seven criteria related to decoding, based on different
research areas. Then, we introduce the notion of criterion measure, the
objective of which is to evaluate a decoding with regard to a certain
criterion. Since such measures need to be validated, we introduce several
desirable properties for them, called axioms. Another main contribution of the
paper is the construction of certain criterion measures that are validated by
our axioms. Such measures can be used to identify the best enthymemes
decodings.",cs.AI
Vision Language Models are In-Context Value Learners,"Predicting temporal progress from visual trajectories is important for
intelligent robots that can learn, adapt, and improve. However, learning such
progress estimator, or temporal value function, across different tasks and
domains requires both a large amount of diverse data and methods which can
scale and generalize. To address these challenges, we present Generative Value
Learning (\GVL), a universal value function estimator that leverages the world
knowledge embedded in vision-language models (VLMs) to predict task progress.
Naively asking a VLM to predict values for a video sequence performs poorly due
to the strong temporal correlation between successive frames. Instead, GVL
poses value estimation as a temporal ordering problem over shuffled video
frames; this seemingly more challenging task encourages VLMs to more fully
exploit their underlying semantic and temporal grounding capabilities to
differentiate frames based on their perceived task progress, consequently
producing significantly better value predictions. Without any robot or task
specific training, GVL can in-context zero-shot and few-shot predict effective
values for more than 300 distinct real-world tasks across diverse robot
platforms, including challenging bimanual manipulation tasks. Furthermore, we
demonstrate that GVL permits flexible multi-modal in-context learning via
examples from heterogeneous tasks and embodiments, such as human videos. The
generality of GVL enables various downstream applications pertinent to
visuomotor policy learning, including dataset filtering, success detection, and
advantage-weighted regression -- all without any model training or finetuning.",cs.AI
Dynamic Detection of Relevant Objectives and Adaptation to Preference Drifts in Interactive Evolutionary Multi-Objective Optimization,"Evolutionary Multi-Objective Optimization Algorithms (EMOAs) are widely
employed to tackle problems with multiple conflicting objectives. Recent
research indicates that not all objectives are equally important to the
decision-maker (DM). In the context of interactive EMOAs, preference
information elicited from the DM during the optimization process can be
leveraged to identify and discard irrelevant objectives, a crucial step when
objective evaluations are computationally expensive. However, much of the
existing literature fails to account for the dynamic nature of DM preferences,
which can evolve throughout the decision-making process and affect the
relevance of objectives. This study addresses this limitation by simulating
dynamic shifts in DM preferences within a ranking-based interactive algorithm.
Additionally, we propose methods to discard outdated or conflicting preferences
when such shifts occur. Building on prior research, we also introduce a
mechanism to safeguard relevant objectives that may become trapped in local or
global optima due to the diminished correlation with the DM-provided rankings.
Our experimental results demonstrate that the proposed methods effectively
manage evolving preferences and significantly enhance the quality and
desirability of the solutions produced by the algorithm.",cs.AI
Intellectual Property Protection for Deep Learning Model and Dataset Intelligence,"With the growing applications of Deep Learning (DL), especially recent
spectacular achievements of Large Language Models (LLMs) such as ChatGPT and
LLaMA, the commercial significance of these remarkable models has soared.
However, acquiring well-trained models is costly and resource-intensive. It
requires a considerable high-quality dataset, substantial investment in
dedicated architecture design, expensive computational resources, and efforts
to develop technical expertise. Consequently, safeguarding the Intellectual
Property (IP) of well-trained models is attracting increasing attention. In
contrast to existing surveys overwhelmingly focusing on model IPP mainly, this
survey not only encompasses the protection on model level intelligence but also
valuable dataset intelligence. Firstly, according to the requirements for
effective IPP design, this work systematically summarizes the general and
scheme-specific performance evaluation metrics. Secondly, from proactive IP
infringement prevention and reactive IP ownership verification perspectives, it
comprehensively investigates and analyzes the existing IPP methods for both
dataset and model intelligence. Additionally, from the standpoint of training
settings, it delves into the unique challenges that distributed settings pose
to IPP compared to centralized settings. Furthermore, this work examines
various attacks faced by deep IPP techniques. Finally, we outline prospects for
promising future directions that may act as a guide for innovative research.",cs.AI
Meta-Reasoning Improves Tool Use in Large Language Models,"External tools help large language models (LLMs) succeed at tasks where they
would otherwise typically fail. In existing frameworks, LLMs learn tool use
either by in-context demonstrations or via full model fine-tuning on annotated
data. As these approaches do not easily scale, a recent trend is to abandon
them in favor of lightweight, parameter-efficient tuning paradigms. These
methods allow quickly alternating between the frozen LLM and its specialised
fine-tuned version, by switching on or off a handful of additional custom
parameters. Hence, we postulate that the generalization ability of the frozen
model can be leveraged to improve tool selection. We present Tool selECTion via
meta-reasONing (TECTON), a two-phase system that first reasons over a task
using a custom fine-tuned LM head and outputs candidate tools. Then, with the
custom head disabled, it meta-reasons (i.e., it reasons over the previous
reasoning process) to make a final choice. We show that TECTON results in
substantial gains - both in-distribution and out-of-distribution - on a range
of math reasoning datasets.",cs.AI
GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns from Subplan Hints,"Query optimization has become a research area where classical algorithms are
being challenged by machine learning algorithms. At the same time, recent
trends in learned query optimizers have shown that it is prudent to take
advantage of decades of database research and augment classical query
optimizers by shrinking the plan search space through different types of hints
(e.g. by specifying the join type, scan type or the order of joins) rather than
completely replacing the classical query optimizer with machine learning
models. It is especially relevant for cases when classical optimizers cannot
fully enumerate all logical and physical plans and, as an alternative, need to
rely on less robust approaches like genetic algorithms. However, even
symbiotically learned query optimizers are hampered by the need for vast
amounts of training data, slow plan generation during inference and unstable
results across various workload conditions. In this paper, we present GenJoin -
a novel learned query optimizer that considers the query optimization problem
as a generative task and is capable of learning from a random set of subplan
hints to produce query plans that outperform the classical optimizer. GenJoin
is the first learned query optimizer that significantly and consistently
outperforms PostgreSQL as well as state-of-the-art methods on two well-known
real-world benchmarks across a variety of workloads using rigorous machine
learning evaluations.",cs.AI
Continuous Sign Language Recognition System using Deep Learning with MediaPipe Holistic,"Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.",cs.AI
FedDP: Privacy-preserving method based on federated learning for histopathology image segmentation,"Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is
considered the gold standard for pathologists and medical practitioners for
tumor diagnosis, surgical planning, and post-operative assessment. With the
rapid advancement of deep learning technologies, the development of numerous
models based on convolutional neural networks and transformer-based models has
been applied to the precise segmentation of WSIs. However, due to privacy
regulations and the need to protect patient confidentiality, centralized
storage and processing of image data are impractical. Training a centralized
model directly is challenging to implement in medical settings due to these
privacy concerns.This paper addresses the dispersed nature and privacy
sensitivity of medical image data by employing a federated learning framework,
allowing medical institutions to collaboratively learn while protecting patient
privacy. Additionally, to address the issue of original data reconstruction
through gradient inversion during the federated learning training process,
differential privacy introduces noise into the model updates, preventing
attackers from inferring the contributions of individual samples, thereby
protecting the privacy of the training data.Experimental results show that the
proposed method, FedDP, minimally impacts model accuracy while effectively
safeguarding the privacy of cancer pathology image data, with only a slight
decrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,
respectively. This approach facilitates cross-institutional collaboration and
knowledge sharing while protecting sensitive data privacy, providing a viable
solution for further research and application in the medical field.",cs.AI
Series-to-Series Diffusion Bridge Model,"Diffusion models have risen to prominence in time series forecasting,
showcasing their robust capability to model complex data distributions.
However, their effectiveness in deterministic predictions is often constrained
by instability arising from their inherent stochasticity. In this paper, we
revisit time series diffusion models and present a comprehensive framework that
encompasses most existing diffusion-based methods. Building on this theoretical
foundation, we propose a novel diffusion-based time series forecasting model,
the Series-to-Series Diffusion Bridge Model ($\mathrm{S^2DBM}$), which
leverages the Brownian Bridge process to reduce randomness in reverse
estimations and improves accuracy by incorporating informative priors and
conditions derived from historical time series data. Experimental results
demonstrate that $\mathrm{S^2DBM}$ delivers superior performance in
point-to-point forecasting and competes effectively with other diffusion-based
models in probabilistic forecasting.",cs.AI
Selecting Between BERT and GPT for Text Classification in Political Science Research,"Political scientists often grapple with data scarcity in text classification.
Recently, fine-tuned BERT models and their variants have gained traction as
effective solutions to address this issue. In this study, we investigate the
potential of GPT-based models combined with prompt engineering as a viable
alternative. We conduct a series of experiments across various classification
tasks, differing in the number of classes and complexity, to evaluate the
effectiveness of BERT-based versus GPT-based models in low-data scenarios. Our
findings indicate that while zero-shot and few-shot learning with GPT models
provide reasonable performance and are well-suited for early-stage research
exploration, they generally fall short - or, at best, match - the performance
of BERT fine-tuning, particularly as the training set reaches a substantial
size (e.g., 1,000 samples). We conclude by comparing these approaches in terms
of performance, ease of use, and cost, providing practical guidance for
researchers facing data limitations. Our results are particularly relevant for
those engaged in quantitative text analysis in low-resource settings or with
limited labeled data.",cs.AI
Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks,"Modern AI agents, driven by advances in large foundation models, promise to
enhance our productivity and transform our lives by augmenting our knowledge
and capabilities. To achieve this vision, AI agents must effectively plan,
perform multi-step reasoning and actions, respond to novel observations, and
recover from errors, to successfully complete complex tasks across a wide range
of scenarios. In this work, we introduce Magentic-One, a high-performing
open-source agentic system for solving such tasks. Magentic-One uses a
multi-agent architecture where a lead agent, the Orchestrator, plans, tracks
progress, and re-plans to recover from errors. Throughout task execution, the
Orchestrator directs other specialized agents to perform tasks as needed, such
as operating a web browser, navigating local files, or writing and executing
Python code. We show that Magentic-One achieves statistically competitive
performance to the state-of-the-art on three diverse and challenging agentic
benchmarks: GAIA, AssistantBench, and WebArena. Magentic-One achieves these
results without modification to core agent capabilities or to how they
collaborate, demonstrating progress towards generalist agentic systems.
Moreover, Magentic-One's modular design allows agents to be added or removed
from the team without additional prompt tuning or training, easing development
and making it extensible to future scenarios. We provide an open-source
implementation of Magentic-One, and we include AutoGenBench, a standalone tool
for agentic evaluation. AutoGenBench provides built-in controls for repetition
and isolation to run agentic benchmarks in a rigorous and contained manner --
which is important when agents' actions have side-effects. Magentic-One,
AutoGenBench and detailed empirical performance evaluations of Magentic-One,
including ablations and error analysis are available at
https://aka.ms/magentic-one",cs.AI
Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity,"The wider application of end-to-end learning methods to embodied
decision-making domains remains bottlenecked by their reliance on a
superabundance of training data representative of the target domain.
Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot
generalization--the goal of standard reinforcement learning (RL)--in favor of
few-shot adaptation, and thus hold promise for bridging larger generalization
gaps. While learning this meta-level adaptive behavior still requires
substantial data, efficient environment simulators approaching real-world
complexity are growing in prevalence. Even so, hand-designing sufficiently
diverse and numerous simulated training tasks for these complex domains is
prohibitively labor-intensive. Domain randomization (DR) and procedural
generation (PG), offered as solutions to this problem, require simulators to
possess carefully-defined parameters which directly translate to meaningful
task diversity--a similarly prohibitive assumption. In this work, we present
DIVA, an evolutionary approach for generating diverse training tasks in such
complex, open-ended simulators. Like unsupervised environment design (UED)
methods, DIVA can be applied to arbitrary parameterizations, but can
additionally incorporate realistically-available domain knowledge--thus
inheriting the flexibility and generality of UED, and the supervised structure
embedded in well-designed simulators exploited by DR and PG. Our empirical
results showcase DIVA's unique ability to overcome complex parameterizations
and successfully train adaptive agent behavior, far outperforming competitive
baselines from prior literature. These findings highlight the potential of such
semi-supervised environment design (SSED) approaches, of which DIVA is the
first humble constituent, to enable training in realistic simulated domains,
and produce more robust and capable adaptive agents.",cs.AI
Can CDT rationalise the ex ante optimal policy via modified anthropics?,"In Newcomb's problem, causal decision theory (CDT) recommends two-boxing and
thus comes apart from evidential decision theory (EDT) and ex ante policy
optimisation (which prescribe one-boxing). However, in Newcomb's problem, you
should perhaps believe that with some probability you are in a simulation run
by the predictor to determine whether to put a million dollars into the opaque
box. If so, then causal decision theory might recommend one-boxing in order to
cause the predictor to fill the opaque box. In this paper, we study
generalisations of this approach. That is, we consider general Newcomblike
problems and try to form reasonable self-locating beliefs under which CDT's
recommendations align with an EDT-like notion of ex ante policy optimisation.
We consider approaches in which we model the world as running simulations of
the agent, and an approach not based on such models (which we call 'Generalised
Generalised Thirding', or GGT). For each approach, we characterise the
resulting CDT policies, and prove that under certain conditions, these include
the ex ante optimal policies.",cs.AI
Scaling Laws for Pre-training Agents and World Models,"The performance of embodied agents has been shown to improve by increasing
model parameters, dataset size, and compute. This has been demonstrated in
domains from robotics to video games, when generative learning objectives on
offline datasets (pre-training) are used to model an agent's behavior
(imitation learning) or their environment (world modeling). This paper
characterizes the role of scale in these tasks more precisely. Going beyond the
simple intuition that `bigger is better', we show that the same types of power
laws found in language modeling (e.g. between loss and optimal model size),
also arise in world modeling and imitation learning. However, the coefficients
of these laws are heavily influenced by the tokenizer, task \& architecture --
this has important implications on the optimal sizing of models and data.",cs.AI
Bayesian Calibration of Win Rate Estimation with LLM Evaluators,"Recent advances in large language models (LLMs) show the potential of using
LLMs as evaluators for assessing the quality of text generations from LLMs.
However, applying LLM evaluators naively to compare or judge between different
systems can lead to unreliable results due to the intrinsic win rate estimation
bias of LLM evaluators. In order to mitigate this problem, we propose two
calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian
Dawid-Skene, both of which leverage Bayesian inference to more accurately infer
the true win rate of generative language models. We empirically validate our
methods on six datasets covering story generation, summarization, and
instruction following tasks. We show that both our methods are effective in
improving the accuracy of win rate estimation using LLMs as evaluators,
offering a promising direction for reliable automatic text quality evaluation.",cs.AI
Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.",cs.AI
Leveraging LLMs to Enable Natural Language Search on Go-to-market Platforms,"Enterprise searches require users to have complex knowledge of queries,
configurations, and metadata, rendering it difficult for them to access
information as needed. Most go-to-market (GTM) platforms utilize advanced
search, an interface that enables users to filter queries by various fields
using categories or keywords, which, historically, however, has proven to be
exceedingly cumbersome, as users are faced with seemingly hundreds of options,
fields, and buttons. Consequently, querying with natural language has long been
ideal, a notion further empowered by Large Language Models (LLMs).
  In this paper, we implement and evaluate a solution for the Zoominfo product
for sellers, which prompts the LLM with natural language, producing search
fields through entity extraction that are then converted into a search query.
The intermediary search fields offer numerous advantages for each query,
including the elimination of syntax errors, simpler ground truths, and an
intuitive format for the LLM to interpret.
  We paired this pipeline with many advanced prompt engineering strategies,
featuring an intricate system message, few-shot prompting, chain-of-thought
(CoT) reasoning, and execution refinement. Furthermore, we manually created the
ground truth for 500+ natural language queries, enabling the supervised
fine-tuning of Llama-3-8B-Instruct and the introduction of sophisticated
numerical metrics.
  Comprehensive experiments with closed, open source, and fine-tuned LLM models
were conducted through exact, Jaccard, cosine, and semantic similarity on
individual search entities to demonstrate the efficacy of our approach.
Overall, the most accurate closed model had an average accuracy of 97% per
query, with only one field performing under 90%, with comparable results
observed from the fine-tuned models.",cs.AI
Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers,"Learned sparse retrieval, which can efficiently perform retrieval through
mature inverted-index engines, has garnered growing attention in recent years.
Particularly, the inference-free sparse retrievers are attractive as they
eliminate online model inference in the retrieval phase thereby avoids huge
computational cost, offering reasonable throughput and latency. However, even
the state-of-the-art (SOTA) inference-free sparse models lag far behind in
terms of search relevance when compared to both sparse and dense siamese
models. Towards competitive search relevance for inference-free sparse
retrievers, we argue that they deserve dedicated training methods other than
using same ones with siamese encoders. In this paper, we propose two different
approaches for performance improvement. First, we introduce the IDF-aware FLOPS
loss, which introduces Inverted Document Frequency (IDF) to the sparsification
of representations. We find that it mitigates the negative impact of the FLOPS
regularization on search relevance, allowing the model to achieve a better
balance between accuracy and efficiency. Moreover, we propose a heterogeneous
ensemble knowledge distillation framework that combines siamese dense and
sparse retrievers to generate supervisory signals during the pre-training
phase. The ensemble framework of dense and sparse retriever capitalizes on
their strengths respectively, providing a strong upper bound for knowledge
distillation. To concur the diverse feedback from heterogeneous supervisors, we
normalize and then aggregate the outputs of the teacher models to eliminate
score scale differences. On the BEIR benchmark, our model outperforms existing
SOTA inference-free sparse model by \textbf{3.3 NDCG@10 score}. It exhibits
search relevance comparable to siamese sparse retrievers and client-side
latency only \textbf{1.1x that of BM25}.",cs.AI
A Bayesian Mixture Model of Temporal Point Processes with Determinantal Point Process Prior,"Asynchronous event sequence clustering aims to group similar event sequences
in an unsupervised manner. Mixture models of temporal point processes have been
proposed to solve this problem, but they often suffer from overfitting, leading
to excessive cluster generation with a lack of diversity. To overcome these
limitations, we propose a Bayesian mixture model of Temporal Point Processes
with Determinantal Point Process prior (TP$^2$DP$^2$) and accordingly an
efficient posterior inference algorithm based on conditional Gibbs sampling.
Our work provides a flexible learning framework for event sequence clustering,
enabling automatic identification of the potential number of clusters and
accurate grouping of sequences with similar features. It is applicable to a
wide range of parametric temporal point processes, including neural
network-based models. Experimental results on both synthetic and real-world
data suggest that our framework could produce moderately fewer yet more diverse
mixture components, and achieve outstanding results across multiple evaluation
metrics.",cs.AI
Bridging the Gap: Representation Spaces in Neuro-Symbolic AI,"Neuro-symbolic AI is an effective method for improving the overall
performance of AI models by combining the advantages of neural networks and
symbolic learning. However, there are differences between the two in terms of
how they process data, primarily because they often use different data
representation methods, which is often an important factor limiting the overall
performance of the two. From this perspective, we analyzed 191 studies from
2013 by constructing a four-level classification framework. The first level
defines five types of representation spaces, and the second level focuses on
five types of information modalities that the representation space can
represent. Then, the third level describes four symbolic logic methods.
Finally, the fourth-level categories propose three collaboration strategies
between neural networks and symbolic learning. Furthermore, we conducted a
detailed analysis of 46 research based on their representation space.",cs.AI
"Neuro-Symbolic AI: Explainability, Challenges, and Future Trends","Explainability is an essential reason limiting the application of neural
networks in many vital fields. Although neuro-symbolic AI hopes to enhance the
overall explainability by leveraging the transparency of symbolic learning, the
results are less evident than imagined. This article proposes a classification
for explainability by considering both model design and behavior of 191 studies
from 2013, focusing on neuro-symbolic AI, hoping to inspire scholars who want
to understand the explainability of neuro-symbolic AI. Precisely, we classify
them into five categories by considering whether the form of bridging the
representation differences is readable as their design factor, if there are
representation differences between neural networks and symbolic logic learning,
and whether a model decision or prediction process is understandable as their
behavior factor: implicit intermediate representations and implicit prediction,
partially explicit intermediate representations and partially explicit
prediction, explicit intermediate representations or explicit prediction,
explicit intermediate representation and explicit prediction, unified
representation and explicit prediction. We also analyzed the research trends
and three significant challenges: unified representations, explainability and
transparency, and sufficient cooperation from neural networks and symbolic
learning. Finally, we put forward suggestions for future research in three
aspects: unified representations, enhancing model explainability, ethical
considerations, and social impact.",cs.AI
PhoneLM:an Efficient and Capable Small Language Model Family through Principled Pre-training,"The interest in developing small language models (SLM) for on-device
deployment is fast growing. However, the existing SLM design hardly considers
the device hardware characteristics. Instead, this work presents a simple yet
effective principle for SLM design: architecture searching for (near-)optimal
runtime efficiency before pre-training. Guided by this principle, we develop
PhoneLM SLM family (currently with 0.5B and 1.5B versions), that acheive the
state-of-the-art capability-efficiency tradeoff among those with similar
parameter size. We fully open-source the code, weights, and training datasets
of PhoneLM for reproducibility and transparency, including both base and
instructed versions. We also release a finetuned version of PhoneLM capable of
accurate Android Intent invocation, and an end-to-end Android demo. All
materials are available at https://github.com/UbiquitousLearning/PhoneLM.",cs.AI
Benchmarking Large Language Models with Integer Sequence Generation Tasks,"This paper presents a novel benchmark where the large language model (LLM)
must write code that computes integer sequences from the Online Encyclopedia of
Integer Sequences (OEIS), a widely-used resource for mathematical sequences.
The benchmark is designed to evaluate both the correctness of the generated
code and its computational efficiency. Our benchmark reveals that the o1 series
of models outperform other frontier models from OpenAI, Anthropic, Meta, and
Google in accuracy and cheating rates across both easy and hard integer
sequences. In order to ensure models do not exploit memorized sequence values,
we introduce an automated cheating detection mechanism that flags the use of
lookup tables and validated this automation against human cheating evaluations.
This benchmark provides a meaningful challenge for current LLMs, offering
insights into their mathematical reasoning and code writing capabilities, which
can guide future research directions and model development in mathematical
reasoning and code synthesis.",cs.AI
ComFairGNN: Community Fair Graph Neural Network,"Graph Neural Networks (GNNs) have become the leading approach for addressing
graph analytical problems in various real-world scenarios. However, GNNs may
produce biased predictions against certain demographic subgroups due to node
attributes and neighbors surrounding a node. Most current research on GNN
fairness focuses predominantly on debiasing GNNs using oversimplified fairness
evaluation metrics, which can give a misleading impression of fairness.
Understanding the potential evaluation paradoxes due to the complicated nature
of the graph structure is crucial for developing effective GNN debiasing
mechanisms. In this paper, we examine the effectiveness of current GNN
debiasing methods in terms of unfairness evaluation. Specifically, we introduce
a community-level strategy to measure bias in GNNs and evaluate debiasing
methods at this level. Further, We introduce ComFairGNN, a novel framework
designed to mitigate community-level bias in GNNs. Our approach employs a
learnable coreset-based debiasing function that addresses bias arising from
diverse local neighborhood distributions during GNNs neighborhood aggregation.
Comprehensive evaluations on three benchmark datasets demonstrate our model's
effectiveness in both accuracy and fairness metrics.",cs.AI
GaGSL: Global-augmented Graph Structure Learning via Graph Information Bottleneck,"Graph neural networks (GNNs) are prominent for their effectiveness in
processing graph data for semi-supervised node classification tasks. Most works
of GNNs assume that the observed structure accurately represents the underlying
node relationships. However, the graph structure is inevitably noisy or
incomplete in reality, which can degrade the quality of graph representations.
Therefore, it is imperative to learn a clean graph structure that balances
performance and robustness. In this paper, we propose a novel method named
\textit{Global-augmented Graph Structure Learning} (GaGSL), guided by the Graph
Information Bottleneck (GIB) principle. The key idea behind GaGSL is to learn a
compact and informative graph structure for node classification tasks.
Specifically, to mitigate the bias caused by relying solely on the original
structure, we first obtain augmented features and augmented structure through
global feature augmentation and global structure augmentation. We then input
the augmented features and augmented structure into a structure estimator with
different parameters for optimization and re-definition of the graph structure,
respectively. The redefined structures are combined to form the final graph
structure. Finally, we employ GIB based on mutual information to guide the
optimization of the graph structure to obtain the minimum sufficient graph
structure. Comprehensive evaluations across a range of datasets reveal the
outstanding performance and robustness of GaGSL compared with the
state-of-the-art methods.",cs.AI
Model and Deep learning based Dynamic Range Compression Inversion,"Dynamic Range Compression (DRC) is a popular audio effect used to control the
dynamic range of a signal. Inverting DRC can also help to restore the original
dynamics to produce new mixes and/or to improve the overall quality of the
audio signal. Since, state-of-the-art DRC inversion techniques either ignore
parameters or require precise parameters that are difficult to estimate, we
fill the gap by combining a model-based approach with neural networks for DRC
inversion. To this end, depending on the scenario, we use different neural
networks to estimate DRC parameters. Then, a model-based inversion is completed
to restore the original audio signal. Our experimental results show the
effectiveness and robustness of the proposed method in comparison to several
state-of-the-art methods, when applied on two music datasets.",cs.AI
Deep Heuristic Learning for Real-Time Urban Pathfinding,"This paper introduces a novel approach to urban pathfinding by transforming
traditional heuristic-based algorithms into deep learning models that leverage
real-time contextual data, such as traffic and weather conditions. We propose
two methods: an enhanced A* algorithm that dynamically adjusts routes based on
current environmental conditions, and a neural network model that predicts the
next optimal path segment using historical and live data. An extensive
benchmark was conducted to compare the performance of different deep learning
models, including MLP, GRU, LSTM, Autoencoders, and Transformers. Both methods
were evaluated in a simulated urban environment in Berlin, with the neural
network model outperforming traditional methods, reducing travel times by up to
40%, while the enhanced A* algorithm achieved a 34% improvement. These results
demonstrate the potential of deep learning to optimize urban navigation in real
time, providing more adaptable and efficient routing solutions.",cs.AI
Multi-language Video Subtitle Dataset for Image-based Text Recognition,"The Multi-language Video Subtitle Dataset is a comprehensive collection
designed to support research in text recognition across multiple languages.
This dataset includes 4,224 subtitle images extracted from 24 videos sourced
from online platforms. It features a wide variety of characters, including Thai
consonants, vowels, tone marks, punctuation marks, numerals, Roman characters,
and Arabic numerals. With 157 unique characters, the dataset provides a
resource for addressing challenges in text recognition within complex
backgrounds. It addresses the growing need for high-quality, multilingual text
recognition data, particularly as videos with embedded subtitles become
increasingly dominant on platforms like YouTube and Facebook. The variability
in text length, font, and placement within these images adds complexity,
offering a valuable resource for developing and evaluating deep learning
models. The dataset facilitates accurate text transcription from video content
while providing a foundation for improving computational efficiency in text
recognition systems. As a result, it holds significant potential to drive
advancements in research and innovation across various computer science
disciplines, including artificial intelligence, deep learning, computer vision,
and pattern recognition.",cs.AI
Gradient Boosting Trees and Large Language Models for Tabular Data Few-Shot Learning,"Large Language Models (LLM) have brought numerous of new applications to
Machine Learning (ML). In the context of tabular data (TD), recent studies show
that TabLLM is a very powerful mechanism for few-shot-learning (FSL)
applications, even if gradient boosting decisions trees (GBDT) have
historically dominated the TD field. In this work we demonstrate that although
LLMs are a viable alternative, the evidence suggests that baselines used to
gauge performance can be improved. We replicated public benchmarks and our
methodology improves LightGBM by 290%, this is mainly driven by forcing node
splitting with few samples, a critical step in FSL with GBDT. Our results show
an advantage to TabLLM for 8 or fewer shots, but as the number of samples
increases GBDT provides competitive performance at a fraction of runtime. For
other real-life applications with vast number of samples, we found FSL still
useful to improve model diversity, and when combined with ExtraTrees it
provides strong resilience to overfitting, our proposal was validated in a ML
competition setting ranking first place.",cs.AI
A Multilingual Sentiment Lexicon for Low-Resource Language Translation using Large Languages Models and Explainable AI,"South Africa and the Democratic Republic of Congo (DRC) present a complex
linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,
English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven
translation and sentiment analysis systems due to a lack of accurately labeled
data. This study seeks to address these challenges by developing a multilingual
lexicon designed for French and Tshiluba, now expanded to include translations
in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural
relevance in sentiment classification by integrating language-specific
sentiment scores. A comprehensive testing corpus is created to support
translation and sentiment analysis tasks, with machine learning models such as
Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive
Bayes (GNB) trained to predict sentiment across low resource languages (LRLs).
Among them, the Random Forest model performed particularly well, capturing
sentiment polarity and handling language-specific nuances effectively.
Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a
Large Language Model (LLM), is applied to predict context-based sentiment with
high accuracy, achieving 99% accuracy and 98% precision, outperforming other
models. The BERT predictions were clarified using Explainable AI (XAI),
improving transparency and fostering confidence in sentiment classification.
Overall, findings demonstrate that the proposed lexicon and machine learning
models significantly enhance translation and sentiment analysis for LRLs in
South Africa and the DRC, laying a foundation for future AI models that support
underrepresented languages, with applications across education, governance, and
business in multilingual contexts.",cs.AI
Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education,"Large language models (LLMs) offer promise in generating educational content,
providing instructor feedback, and reducing teacher workload on assessments.
While prior studies have focused on studying LLM-powered learning analytics,
limited research has examined how effective LLMs are in a bilingual context. In
this paper, we study the effectiveness of multilingual large language models
(MLLMs) across monolingual (English-only, Spanish-only) and bilingual
(Spanglish) student writing. We present a learning analytics use case that
details LLM performance in assessing acceptable and unacceptable explanations
of Science and Social Science concepts. Our findings reveal a significant bias
in the grading performance of pre-trained models for bilingual writing compared
to English-only and Spanish-only writing. Following this, we fine-tune
open-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets
generated in English, Spanish, and Spanglish. Our experiments indicate that the
models perform significantly better for all three languages after fine-tuning
with bilingual data. This study highlights the potential of enhancing MLLM
effectiveness to support authentic language practices amongst bilingual
learners. It also aims to illustrate the value of incorporating non-English
languages into the design and implementation of language models in education.",cs.AI
A Random-Key Optimizer for Combinatorial Optimization,"This paper presents the Random-Key Optimizer (RKO), a versatile and efficient
stochastic local search method tailored for combinatorial optimization
problems. Using the random-key concept, RKO encodes solutions as vectors of
random keys that are subsequently decoded into feasible solutions via
problem-specific decoders. The RKO framework is able to combine a plethora of
classic metaheuristics, each capable of operating independently or in parallel,
with solution sharing facilitated through an elite solution pool. This modular
approach allows for the adaptation of various metaheuristics, including
simulated annealing, iterated local search, and greedy randomized adaptive
search procedures, among others. The efficacy of the RKO framework, implemented
in C++, is demonstrated through its application to three NP-hard combinatorial
optimization problems: the alpha-neighborhood p-median problem, the tree of
hubs location problem, and the node-capacitated graph partitioning problem. The
results highlight the framework's ability to produce high-quality solutions
across diverse problem domains, underscoring its potential as a robust tool for
combinatorial optimization.",cs.AI
Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning,"The task of predicting long-term patient outcomes using supervised machine
learning is a challenging one, in part because of the high variance of each
patient's trajectory, which can result in the model over-fitting to the
training data. Temporal difference (TD) learning, a common reinforcement
learning technique, may reduce variance by generalising learning to the pattern
of state transitions rather than terminal outcomes. However, in healthcare this
method requires several strong assumptions about patient states, and there
appears to be limited literature evaluating the performance of TD learning
against traditional supervised learning methods for long-term health outcome
prediction tasks. In this study, we define a framework for applying TD learning
to real-time irregularly sampled time series data using a Semi-Markov Reward
Process. We evaluate the model framework in predicting intensive care mortality
and show that TD learning under this framework can result in improved model
robustness compared to standard supervised learning methods. and that this
robustness is maintained even when validated on external datasets. This
approach may offer a more reliable method when learning to predict patient
outcomes using high-variance irregular time series data.",cs.AI
Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.",cs.AI
Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking,"We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.",cs.AI
Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems,"In this paper, we propose a novel model called Recurrent Explicit Duration
Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent
explicit duration variables into the rSLDS model. We also propose an inference
and learning scheme that involves the use of P\'olya-gamma augmentation. We
demonstrate the improved segmentation capabilities of our model on three
benchmark datasets, including two quantitative datasets and one qualitative
dataset.",cs.AI
The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model,"The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural
Bayesian nonparametric extension of the classical Hidden Markov Model for
learning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to
strengthen the self-persistence probability in the HDP-HMM. Then, disentangled
sticky HDP-HMM has been proposed to disentangle the strength of the
self-persistence prior and transition prior. However, the sticky HDP-HMM
assumes that the self-persistence probability is stationary, limiting its
expressiveness. Here, we build on previous work on sticky HDP-HMM and
disentangled sticky HDP-HMM, developing a more general model: the recurrent
sticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for
efficient inference in this model. We show that RS-HDP-HMM outperforms
disentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and
real data segmentation.",cs.AI
Graph neural networks and non-commuting operators,"Graph neural networks (GNNs) provide state-of-the-art results in a wide
variety of tasks which typically involve predicting features at the vertices of
a graph. They are built from layers of graph convolutions which serve as a
powerful inductive bias for describing the flow of information among the
vertices. Often, more than one data modality is available. This work considers
a setting in which several graphs have the same vertex set and a common
vertex-level learning task. This generalizes standard GNN models to GNNs with
several graph operators that do not commute. We may call this model graph-tuple
neural networks (GtNN).
  In this work, we develop the mathematical theory to address the stability and
transferability of GtNNs using properties of non-commuting non-expansive
operators. We develop a limit theory of graphon-tuple neural networks and use
it to prove a universal transferability theorem that guarantees that all
graph-tuple neural networks are transferable on convergent graph-tuple
sequences. In particular, there is no non-transferable energy under the
convergence we consider here. Our theoretical results extend well-known
transferability theorems for GNNs to the case of several simultaneous graphs
(GtNNs) and provide a strict improvement on what is currently known even in the
GNN case.
  We illustrate our theoretical results with simple experiments on synthetic
and real-world data. To this end, we derive a training procedure that provably
enforces the stability of the resulting model.",cs.AI
Object Recognition in Human Computer Interaction:- A Comparative Analysis,"Human-computer interaction (HCI) has been a widely researched area for many
years, with continuous advancements in technology leading to the development of
new techniques that change the way we interact with computers. With the recent
advent of powerful computers, we recognize human actions and interact
accordingly, thus revolutionizing the way we interact with computers. The
purpose of this paper is to provide a comparative analysis of various
algorithms used for recognizing user faces and gestures in the context of
computer vision and HCI. This study aims to explore and evaluate the
performance of different algorithms in terms of accuracy, robustness, and
efficiency. This study aims to provide a comprehensive analysis of algorithms
for face and gesture recognition in the context of computer vision and HCI,
with the goal of improving the design and development of interactive systems
that are more intuitive, efficient, and user-friendly.",cs.AI
Learning Generalizable Policy for Obstacle-Aware Autonomous Drone Racing,"Autonomous drone racing has gained attention for its potential to push the
boundaries of drone navigation technologies. While much of the existing
research focuses on racing in obstacle-free environments, few studies have
addressed the complexities of obstacle-aware racing, and approaches presented
in these studies often suffer from overfitting, with learned policies
generalizing poorly to new environments. This work addresses the challenge of
developing a generalizable obstacle-aware drone racing policy using deep
reinforcement learning. We propose applying domain randomization on racing
tracks and obstacle configurations before every rollout, combined with parallel
experience collection in randomized environments to achieve the goal. The
proposed randomization strategy is shown to be effective through simulated
experiments where drones reach speeds of up to 70 km/h, racing in unseen
cluttered environments. This study serves as a stepping stone toward learning
robust policies for obstacle-aware drone racing and general-purpose drone
navigation in cluttered environments. Code is available at
https://github.com/ErcBunny/IsaacGymEnvs.",cs.AI
WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing,"We propose WiFlexFormer, a highly efficient Transformer-based architecture
designed for WiFi Channel State Information (CSI)-based person-centric sensing.
We benchmark WiFlexFormer against state-of-the-art vision and specialized
architectures for processing radio frequency data and demonstrate that it
achieves comparable Human Activity Recognition (HAR) performance while offering
a significantly lower parameter count and faster inference times. With an
inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is
optimized for real-time inference. Additionally, its low parameter count
contributes to improved cross-domain generalization, where it often outperforms
larger models. Our comprehensive evaluation shows that WiFlexFormer is a
potential solution for efficient, scalable WiFi-based sensing applications. The
PyTorch implementation of WiFlexFormer is publicly available at:
https://github.com/StrohmayerJ/WiFlexFormer.",cs.AI
Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction,"Recent advancements in equivariant deep models have shown promise in
accurately predicting atomic potentials and force fields in molecular dynamics
simulations. Using spherical harmonics (SH) and tensor products (TP), these
equivariant networks gain enhanced physical understanding, like symmetries and
many-body interactions. Beyond encoding physical insights, SH and TP are also
crucial to represent equivariant polynomial functions. In this work, we analyze
the equivariant polynomial functions for the equivariant architecture, and
introduce a novel equivariant network, named PACE. The proposed PACE utilizes
edge booster and the Atomic Cluster Expansion (ACE) technique to approximate a
greater number of $SE(3) \times S_n$ equivariant polynomial functions with
enhanced degrees. As experimented in commonly used benchmarks, PACE
demonstrates state-of-the-art performance in predicting atomic energy and force
fields, with robust generalization capability across various geometric
distributions under molecular dynamics (MD) across different temperature
conditions. Our code is publicly available as part of the AIRS library
https://github.com/divelab/AIRS/.",cs.AI
Quantum Diffusion Models for Few-Shot Learning,"Modern quantum machine learning (QML) methods involve the variational
optimization of parameterized quantum circuits on training datasets, followed
by predictions on testing datasets. Most state-of-the-art QML algorithms
currently lack practical advantages due to their limited learning capabilities,
especially in few-shot learning tasks. In this work, we propose three new
frameworks employing quantum diffusion model (QDM) as a solution for the
few-shot learning: label-guided generation inference (LGGI); label-guided
denoising inference (LGDI); and label-guided noise addition inference (LGNAI).
Experimental results demonstrate that our proposed algorithms significantly
outperform existing methods.",cs.AI
Improving Radiology Report Conciseness and Structure via Local Large Language Models,"In this study, we aim to enhance radiology reporting by improving both the
conciseness and structured organization of findings (also referred to as
templating), specifically by organizing information according to anatomical
regions. This structured approach allows physicians to locate relevant
information quickly, increasing the report's utility. We utilize Large Language
Models (LLMs) such as Mixtral, Mistral, and Llama to generate concise,
well-structured reports. Among these, we primarily focus on the Mixtral model
due to its superior adherence to specific formatting requirements compared to
other models. To maintain data security and privacy, we run these LLMs locally
behind our institution's firewall. We leverage the LangChain framework and
apply five distinct prompting strategies to enforce a consistent structure in
radiology reports, aiming to eliminate extraneous language and achieve a high
level of conciseness. We also introduce a novel metric, the Conciseness
Percentage (CP) score, to evaluate report brevity. Our dataset comprises 814
radiology reports authored by seven board-certified body radiologists at our
cancer center. In evaluating the different prompting methods, we discovered
that the most effective approach for generating concise, well-structured
reports involves first instructing the LLM to condense the report, followed by
a prompt to structure the content according to specific guidelines. We assessed
all prompting strategies based on their ability to handle formatting issues,
reduce report length, and adhere to formatting instructions. Our findings
demonstrate that open-source, locally deployed LLMs can significantly improve
radiology report conciseness and structure while conforming to specified
formatting standards.",cs.AI
DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency Method for Image Generation,"We introduce a novel state-space architecture for diffusion models,
effectively harnessing spatial and frequency information to enhance the
inductive bias towards local features in input images for image generation
tasks. While state-space networks, including Mamba, a revolutionary advancement
in recurrent neural networks, typically scan input sequences from left to
right, they face difficulties in designing effective scanning strategies,
especially in the processing of image data. Our method demonstrates that
integrating wavelet transformation into Mamba enhances the local structure
awareness of visual inputs and better captures long-range relations of
frequencies by disentangling them into wavelet subbands, representing both low-
and high-frequency components. These wavelet-based outputs are then processed
and seamlessly fused with the original Mamba outputs through a cross-attention
fusion layer, combining both spatial and frequency information to optimize the
order awareness of state-space models which is essential for the details and
overall quality of image generation. Besides, we introduce a globally-shared
transformer to supercharge the performance of Mamba, harnessing its exceptional
power to capture global relationships. Through extensive experiments on
standard benchmarks, our method demonstrates superior results compared to DiT
and DIFFUSSM, achieving faster training convergence and delivering high-quality
outputs. The codes and pretrained models are released at
https://github.com/VinAIResearch/DiMSUM.git.",cs.AI
"Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs","Large language models (LLMs) generate diverse, situated, persuasive texts
from a plurality of potential perspectives, influenced heavily by their prompts
and training data. As part of LLM adoption, we seek to characterize - and
ideally, manage - the socio-cultural values that they express, for reasons of
safety, accuracy, inclusion, and cultural fidelity. We present a validated
approach to automatically (1) extracting heterogeneous latent value
propositions from texts, (2) assessing resonance and conflict of values with
texts, and (3) combining these operations to characterize the pluralistic value
alignment of human-sourced and LLM-sourced textual data.",cs.AI
Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?,"Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public ""medical"" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.",cs.AI
Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation,"Centralized learning requires data to be aggregated at a central server,
which poses significant challenges in terms of data privacy and bandwidth
consumption. Federated learning presents a compelling alternative, however,
vanilla federated learning methods deployed in robotics aim to learn a single
global model across robots that works ideally for all. But in practice one
model may not be well suited for robots deployed in various environments. This
paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated
learning framework that is deployed with vision based autonomous robot
navigation in diverse outdoor environments. The framework addresses the key
federated learning challenge of deteriorating model performance of a single
global model due to the presence of non-IID data across real-world robots.
Extensive real-world experiments validate that Fed-EC reduces the communication
size by 23x for each robot while matching the performance of centralized
learning for goal-oriented navigation and outperforms local learning. Fed-EC
can transfer previously learnt models to new robots that join the cluster.",cs.AI
"Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences","Language models for biological and chemical sequences enable crucial
applications such as drug discovery, protein engineering, and precision
medicine. Currently, these language models are predominantly based on
Transformer architectures. While Transformers have yielded impressive results,
their quadratic runtime dependency on the sequence length complicates their use
for long genomic sequences and in-context learning on proteins and chemical
sequences. Recently, the recurrent xLSTM architecture has been shown to perform
favorably compared to Transformers and modern state-space model (SSM)
architectures in the natural language domain. Similar to SSMs, xLSTMs have a
linear runtime dependency on the sequence length and allow for constant-memory
decoding at inference time, which makes them prime candidates for modeling
long-range dependencies in biological and chemical sequences. In this work, we
tailor xLSTM towards these domains and propose a suite of architectural
variants called Bio-xLSTM. Extensive experiments in three large domains,
genomics, proteins, and chemistry, were performed to assess xLSTM's ability to
model biological and chemical sequences. The results show that models based on
Bio-xLSTM a) can serve as proficient generative models for DNA, protein, and
chemical sequences, b) learn rich representations for those modalities, and c)
can perform in-context learning for proteins and small molecules.",cs.AI
Self-Consistency Preference Optimization,"Self-alignment, whereby models learn to improve themselves without human
annotation, is a rapidly growing research area. However, existing techniques
often fail to improve complex reasoning tasks due to the difficulty of
assigning correct rewards. An orthogonal approach that is known to improve
correctness is self-consistency, a method applied at inference time based on
multiple sampling in order to find the most consistent answer. In this work, we
extend the self-consistency concept to help train models. We thus introduce
self-consistency preference optimization (ScPO), which iteratively trains
consistent answers to be preferred over inconsistent ones on unsupervised new
problems. We show ScPO leads to large improvements over conventional reward
model training on reasoning tasks such as GSM8K and MATH, closing the gap with
supervised training with gold answers or preferences, and that combining ScPO
with standard supervised learning improves results even further. On ZebraLogic,
ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and
Claude-3 Haiku.",cs.AI
How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis,"Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain ""planning"" and
""reasoning"" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.",cs.AI
RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models,"Fine-tuned vision-language models (VLMs) often capture spurious correlations
between image features and textual attributes, resulting in degraded zero-shot
performance at test time. Existing approaches for addressing spurious
correlations (i) primarily operate at the global image-level rather than
intervening directly on fine-grained image features and (ii) are predominantly
designed for unimodal settings. In this work, we present RaVL, which takes a
fine-grained perspective on VLM robustness by discovering and mitigating
spurious correlations using local image features rather than operating at the
global image level. Given a fine-tuned VLM, RaVL first discovers spurious
correlations by leveraging a region-level clustering approach to identify
precise image features contributing to zero-shot classification errors. Then,
RaVL mitigates the identified spurious correlation with a novel region-aware
loss function that enables the VLM to focus on relevant regions and ignore
spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with
various model architectures, data domains, and learned spurious correlations.
Our results show that RaVL accurately discovers (191% improvement over the
closest baseline) and mitigates (8.2% improvement on worst-group image
classification accuracy) spurious correlations. Qualitative evaluations on
general-domain and medical-domain VLMs confirm our findings.",cs.AI
A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement,"Content moderation typically combines the efforts of human moderators and
machine learning models. However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception. Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered. In this work, we introduce a novel content moderation framework
that emphasizes the importance of capturing annotation disagreement. Our
approach uses multitask learning, where toxicity classification serves as the
primary task and annotation disagreement is addressed as an auxiliary task.
Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review. We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.",cs.AI
YouTube Comments Decoded: Leveraging LLMs for Low Resource Language Classification,"Sarcasm detection is a significant challenge in sentiment analysis,
particularly due to its nature of conveying opinions where the intended meaning
deviates from the literal expression. This challenge is heightened in social
media contexts where code-mixing, especially in Dravidian languages, is
prevalent. Code-mixing involves the blending of multiple languages within a
single utterance, often with non-native scripts, complicating the task for
systems trained on monolingual data. This shared task introduces a novel gold
standard corpus designed for sarcasm and sentiment detection within code-mixed
texts, specifically in Tamil-English and Malayalam-English languages. The
primary objective of this task is to identify sarcasm and sentiment polarity
within a code-mixed dataset of Tamil-English and Malayalam-English comments and
posts collected from social media platforms. Each comment or post is annotated
at the message level for sentiment polarity, with particular attention to the
challenges posed by class imbalance, reflecting real-world scenarios.In this
work, we experiment with state-of-the-art large language models like GPT-3.5
Turbo via prompting to classify comments into sarcastic or non-sarcastic
categories. We obtained a macro-F1 score of 0.61 for Tamil language. We
obtained a macro-F1 score of 0.50 for Malayalam language.",cs.AI
M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models,"Existing benchmarks for evaluating foundation models mainly focus on
single-document, text-only tasks. However, they often fail to fully capture the
complexity of research workflows, which typically involve interpreting
non-textual data and gathering information across multiple documents. To
address this gap, we introduce M3SciQA, a multi-modal, multi-document
scientific question answering benchmark designed for a more comprehensive
evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated
questions spanning 70 natural language processing paper clusters, where each
cluster represents a primary paper along with all its cited documents,
mirroring the workflow of comprehending a single paper by requiring multi-modal
and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of
18 foundation models. Our results indicate that current foundation models still
significantly underperform compared to human experts in multi-modal information
retrieval and in reasoning across multiple scientific documents. Additionally,
we explore the implications of these findings for the future advancement of
applying foundation models in multi-modal scientific literature analysis.",cs.AI
Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset,"Neural networks are traditionally trained under the assumption that data come
from a stationary distribution. However, settings which violate this assumption
are becoming more popular; examples include supervised learning under
distributional shifts, reinforcement learning, continual learning and
non-stationary contextual bandits. In this work we introduce a novel learning
approach that automatically models and adapts to non-stationarity, via an
Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift
tends to draw the parameters towards the initialisation distribution, so the
approach can be understood as a form of soft parameter reset. We show
empirically that our approach performs well in non-stationary supervised and
off-policy reinforcement learning settings.",cs.AI
Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning,"Answering multi-hop reasoning questions requires retrieving and synthesizing
information from diverse sources. Language models (LMs) struggle to perform
such reasoning consistently. We propose an approach to pinpoint and rectify
multi-hop reasoning failures through targeted memory injections on LM attention
heads. First, we analyze the per-layer activations of GPT-2 models in response
to single- and multi-hop prompts. We then propose a mechanism that allows users
to inject relevant prompt-specific information, which we refer to as
""memories,"" at critical LM locations during inference. By thus enabling the LM
to incorporate additional relevant information during inference, we enhance the
quality of multi-hop prompt completions. We empirically show that a simple,
efficient, and targeted memory injection into a key attention layer often
increases the probability of the desired next token in multi-hop tasks, by up
to 424%. We observe that small subsets of attention heads can significantly
impact the model prediction during multi-hop reasoning. To more faithfully
interpret these heads, we develop Attention Lens: an open source tool that
translates the outputs of attention heads into vocabulary tokens via learned
transformations called lenses. We demonstrate the use of lenses to reveal how a
model arrives at its answer and use them to localize sources of model failures
such as in the case of biased and malicious language generation.",cs.AI
Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo Tree Search,"The growing reliance on renewable energy sources, particularly solar and
wind, has introduced challenges due to their uncontrollable production. This
complicates maintaining the electrical grid balance, prompting some
transmission system operators in Western Europe to implement imbalance tariffs
that penalize unsustainable power deviations. These tariffs create an implicit
demand response framework to mitigate grid instability. Yet, several challenges
limit active participation. In Belgium, for example, imbalance prices are only
calculated at the end of each 15-minute settlement period, creating high risk
due to price uncertainty. This risk is further amplified by the inherent
volatility of imbalance prices, discouraging participation. Although
transmission system operators provide minute-based price predictions, the
system imbalance volatility makes accurate price predictions challenging to
obtain and requires sophisticated techniques. Moreover, publishing price
estimates can prompt participants to adjust their schedules, potentially
affecting the system balance and the final price, adding further complexity. To
address these challenges, we propose a Monte Carlo Tree Search method that
publishes accurate imbalance prices while accounting for potential response
actions. Our approach models the system dynamics using a neural network
forecaster and a cluster of virtual batteries controlled by reinforcement
learning agents. Compared to Belgium's current publication method, our
technique improves price accuracy by 20.4% under ideal conditions and by 12.8%
in more realistic scenarios. This research addresses an unexplored, yet crucial
problem, positioning this paper as a pioneering work in analyzing the potential
of more advanced imbalance price publishing techniques.",cs.AI
Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability,"In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.",cs.AI
Exploring the Stability Gap in Continual Learning: The Role of the Classification Head,"Continual learning (CL) has emerged as a critical area in machine learning,
enabling neural networks to learn from evolving data distributions while
mitigating catastrophic forgetting. However, recent research has identified the
stability gap -- a phenomenon where models initially lose performance on
previously learned tasks before partially recovering during training. Such
learning dynamics are contradictory to the intuitive understanding of stability
in continual learning where one would expect the performance to degrade
gradually instead of rapidly decreasing and then partially recovering later. To
better understand and alleviate the stability gap, we investigate it at
different levels of the neural network architecture, particularly focusing on
the role of the classification head. We introduce the nearest-mean classifier
(NMC) as a tool to attribute the influence of the backbone and the
classification head on the stability gap. Our experiments demonstrate that NMC
not only improves final performance, but also significantly enhances training
stability across various continual learning benchmarks, including CIFAR100,
ImageNet100, CUB-200, and FGVC Aircrafts. Moreover, we find that NMC also
reduces task-recency bias. Our analysis provides new insights into the
stability gap and suggests that the primary contributor to this phenomenon is
the linear head, rather than the insufficient representation learning.",cs.AI
Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval,"This study explores the potential of off-the-shelf Vision-Language Models
(VLMs) for high-level robot planning in the context of autonomous navigation.
Indeed, while most of existing learning-based approaches for path planning
require extensive task-specific training/fine-tuning, we demonstrate how such
training can be avoided for most practical cases. To do this, we introduce
Select2Plan (S2P), a novel training-free framework for high-level robot
planning which completely eliminates the need for fine-tuning or specialised
training. By leveraging structured Visual Question-Answering (VQA) and
In-Context Learning (ICL), our approach drastically reduces the need for data
collection, requiring a fraction of the task-specific data typically used by
trained models, or even relying only on online data. Our method facilitates the
effective use of a generally trained VLM in a flexible and cost-efficient way,
and does not require additional sensing except for a simple monocular camera.
We demonstrate its adaptability across various scene types, context sources,
and sensing setups. We evaluate our approach in two distinct scenarios:
traditional First-Person View (FPV) and infrastructure-driven Third-Person View
(TPV) navigation, demonstrating the flexibility and simplicity of our method.
Our technique significantly enhances the navigational capabilities of a
baseline VLM of approximately 50% in TPV scenario, and is comparable to trained
models in the FPV one, with as few as 20 demonstrations.",cs.AI
ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks,"Recent advances in Generative Artificial Intelligence have fueled numerous
applications, particularly those involving Generative Adversarial Networks
(GANs), which are essential for synthesizing realistic photos and videos.
However, efficiently training GANs remains a critical challenge due to their
computationally intensive and numerically unstable nature. Existing methods
often require days or even weeks for training, posing significant resource and
time constraints.
  In this work, we introduce ParaGAN, a scalable distributed GAN training
framework that leverages asynchronous training and an asymmetric optimization
policy to accelerate GAN training. ParaGAN employs a congestion-aware data
pipeline and hardware-aware layout transformation to enhance accelerator
utilization, resulting in over 30% improvements in throughput. With ParaGAN, we
reduce the training time of BigGAN from 15 days to 14 hours while achieving 91%
scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution
image generation using BigGAN.",cs.AI
Towards Resource-Efficient Federated Learning in Industrial IoT for Multivariate Time Series Analysis,"Anomaly and missing data constitute a thorny problem in industrial
applications. In recent years, deep learning enabled anomaly detection has
emerged as a critical direction, however the improved detection accuracy is
achieved with the utilization of large neural networks, increasing their
storage and computational cost. Moreover, the data collected in edge devices
contain user privacy, introducing challenges that can be successfully addressed
by the privacy-preserving distributed paradigm, known as federated learning
(FL). This framework allows edge devices to train and exchange models
increasing also the communication cost. Thus, to deal with the increased
communication, processing and storage challenges of the FL based deep anomaly
detection NN pruning is expected to have significant benefits towards reducing
the processing, storage and communication complexity. With this focus, a novel
compression-based optimization problem is proposed at the server-side of a FL
paradigm that fusses the received local models broadcast and performs pruning
generating a more compressed model. Experiments in the context of anomaly
detection and missing value imputation demonstrate that the proposed FL
scenario along with the proposed compressed-based method are able to achieve
high compression rates (more than $99.7\%$) with negligible performance losses
(less than $1.18\%$ ) as compared to the centralized solutions.",cs.AI
What Really is Commonsense Knowledge?,"Commonsense datasets have been well developed in Natural Language Processing,
mainly through crowdsource human annotation. However, there are debates on the
genuineness of commonsense reasoning benchmarks. In specific, a significant
portion of instances in some commonsense benchmarks do not concern commonsense
knowledge. That problem would undermine the measurement of the true commonsense
reasoning ability of evaluated models. It is also suggested that the problem
originated from a blurry concept of commonsense knowledge, as distinguished
from other types of knowledge. To demystify all of the above claims, in this
study, we survey existing definitions of commonsense knowledge, ground into the
three frameworks for defining concepts, and consolidate them into a
multi-framework unified definition of commonsense knowledge (so-called
consolidated definition). We then use the consolidated definition for
annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets
to examine the above claims. Our study shows that there exists a large portion
of non-commonsense-knowledge instances in the two datasets, and a large
performance gap on these two subsets where Large Language Models (LLMs) perform
worse on commonsense-knowledge instances.",cs.AI
Energy Score-based Pseudo-Label Filtering and Adaptive Loss for Imbalanced Semi-supervised SAR target recognition,"Automatic target recognition (ATR) is an important use case for synthetic
aperture radar (SAR) image interpretation. Recent years have seen significant
advancements in SAR ATR technology based on semi-supervised learning. However,
existing semi-supervised SAR ATR algorithms show low recognition accuracy in
the case of class imbalance. This work offers a non-balanced semi-supervised
SAR target recognition approach using dynamic energy scores and adaptive loss.
First, an energy score-based method is developed to dynamically select
unlabeled samples near to the training distribution as pseudo-labels during
training, assuring pseudo-label reliability in long-tailed distribution
circumstances. Secondly, loss functions suitable for class imbalances are
proposed, including adaptive margin perception loss and adaptive hard triplet
loss, the former offsets inter-class confusion of classifiers, alleviating the
imbalance issue inherent in pseudo-label generation. The latter effectively
tackles the model's preference for the majority class by focusing on complex
difficult samples during training. Experimental results on extremely imbalanced
SAR datasets demonstrate that the proposed method performs well under the dual
constraints of scarce labels and data imbalance, effectively overcoming the
model bias caused by data imbalance and achieving high-precision target
recognition.",cs.AI
Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion,"Embeddings have become a cornerstone in the functionality of large language
models (LLMs) due to their ability to transform text data into rich, dense
numerical representations that capture semantic and syntactic properties. These
embedding vector databases serve as the long-term memory of LLMs, enabling
efficient handling of a wide range of natural language processing tasks.
However, the surge in popularity of embedding vector databases in LLMs has been
accompanied by significant concerns about privacy leakage. Embedding vector
databases are particularly vulnerable to embedding inversion attacks, where
adversaries can exploit the embeddings to reverse-engineer and extract
sensitive information from the original text data. Existing defense mechanisms
have shown limitations, often struggling to balance security with the
performance of downstream tasks. To address these challenges, we introduce
Eguard, a novel defense mechanism designed to mitigate embedding inversion
attacks. Eguard employs a transformer-based projection network and text mutual
information optimization to safeguard embeddings while preserving the utility
of LLMs. Our approach significantly reduces privacy risks, protecting over 95%
of tokens from inversion while maintaining high performance across downstream
tasks consistent with original embeddings.",cs.AI
Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.",cs.AI
Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks,"This paper investigates the capabilities of text-to-audio music generation
models in producing long-form music with prompts that change over time,
focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We
introduce Babel Bardo, a system that uses Large Language Models (LLMs) to
transform speech transcriptions into music descriptions for controlling a
text-to-music model. Four versions of Babel Bardo were compared in two TRPG
campaigns: a baseline using direct speech transcriptions, and three LLM-based
versions with varying approaches to music description generation. Evaluations
considered audio quality, story alignment, and transition smoothness. Results
indicate that detailed music descriptions improve audio quality while
maintaining consistency across consecutive descriptions enhances story
alignment and transition smoothness.",cs.AI
Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks,"In-Context Learning (ICL) is a phenomenon where task learning occurs through
a prompt sequence without the necessity of parameter updates. ICL in
Multi-Headed Attention (MHA) with absolute positional embedding has been the
focus of more study than other sequence model varieties. We examine
implications of architectural differences between GPT-2 and LLaMa as well as
LlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.
(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining the
interplay between sequence transformation blocks and regressive performance
in-context. We note that certain architectural changes cause degraded training
efficiency/ICL accuracy by converging to suboptimal predictors or converging
slower. We also find certain hybrids showing optimistic performance
improvements, informing potential future ICL-focused architecture
modifications. Additionally, we propose the ""ICL regression score"", a scalar
metric describing a model's whole performance on a specific task. Compute
limitations impose restrictions on our architecture-space, training duration,
number of training runs, function class complexity, and benchmark complexity.
To foster reproducible and extensible research, we provide a typed, modular,
and extensible Python package on which we run all experiments.",cs.AI
Fine-tuning -- a Transfer Learning approach,"Secondary research use of Electronic Health Records (EHRs) is often hampered
by the abundance of missing data in this valuable resource. Missingness in EHRs
occurs naturally as a result of the data recording practices during routine
clinical care, but handling it is crucial to the precision of medical analysis
and the decision-making that follows. The literature contains a variety of
imputation methodologies based on deep neural networks. Those aim to overcome
the dynamic, heterogeneous and multivariate missingness patterns of EHRs, which
cannot be handled by classical and statistical imputation methods. However, all
existing deep imputation methods rely on end-to-end pipelines that incorporate
both imputation and downstream analyses, e.g. classification. This coupling
makes it difficult to assess the quality of imputation and takes away the
flexibility of re-using the imputer for a different task. Furthermore, most
end-to-end deep architectures tend to use complex networks to perform the
downstream task, in addition to the already sophisticated deep imputation
network. We, therefore ask if the high performance reported in the literature
is due to the imputer or the classifier and further ask if an optimised
state-of-the-art imputer is used, a simpler classifier can achieve comparable
performance. This paper explores the development of a modular, deep
learning-based imputation and classification pipeline, specifically built to
leverage the capabilities of state-of-the-art imputation models for downstream
classification tasks. Such a modular approach enables a) objective assessment
of the quality of the imputer and classifier independently, and b) enables the
exploration of the performance of simpler classification architectures using an
optimised imputer.",cs.AI
Interactions Across Blocks in Post-Training Quantization of Large Language Models,"Post-training quantization is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are quantized with the objective of minimizing
quantization errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only quantization of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple quantized blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the quantized block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.",cs.AI
Cooperation and Personalization on a Seesaw: Choice-based FL for Safe Cooperation in Wireless Networks,"Federated learning (FL) is an innovative distributed artificial intelligence
(AI) technique. It has been used for interdisciplinary studies in different
fields such as healthcare, marketing and finance. However the application of FL
in wireless networks is still in its infancy. In this work, we first overview
benefits and concerns when applying FL to wireless networks. Next, we provide a
new perspective on existing personalized FL frameworks by analyzing the
relationship between cooperation and personalization in these frameworks.
Additionally, we discuss the possibility of tuning the cooperation level with a
choice-based approach. Our choice-based FL approach is a flexible and safe FL
framework that allows participants to lower the level of cooperation when they
feel unsafe or unable to benefit from the cooperation. In this way, the
choice-based FL framework aims to address the safety and fairness concerns in
FL and protect participants from malicious attacks.",cs.AI
Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System,"In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.",cs.AI
Disability data futures: Achievable imaginaries for AI and disability data justice,"Data are the medium through which individuals' identities and experiences are
filtered in contemporary states and systems, and AI is increasingly the layer
mediating between people, data, and decisions. The history of data and AI is
often one of disability exclusion, oppression, and the reduction of disabled
experience; left unchallenged, the current proliferation of AI and data systems
thus risks further automating ableism behind the veneer of algorithmic
neutrality. However, exclusionary histories do not preclude inclusive futures,
and disability-led visions can chart new paths for collective action to achieve
futures founded in disability justice. This chapter brings together four
academics and disability advocates working at the nexus of disability, data,
and AI, to describe achievable imaginaries for artificial intelligence and
disability data justice. Reflecting diverse contexts, disciplinary
perspectives, and personal experiences, we draw out the shape, actors, and
goals of imagined future systems where data and AI support movement towards
disability justice.",cs.AI
Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models,"Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and sparse architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.",cs.AI
MEG: Medical Knowledge-Augmented Large Language Models for Question Answering,"Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.",cs.AI
"Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward","Recent research has demonstrated that training a linear connector between
speech foundation encoders and large language models (LLMs) enables this
architecture to achieve strong ASR capabilities. Despite the impressive
results, it remains unclear whether these simple approaches are robust enough
across different scenarios and speech conditions, such as domain shifts and
different speech perturbations. In this paper, we address these questions by
conducting various ablation experiments using a recent and widely adopted
approach called SLAM-ASR. We present novel empirical findings that offer
insights on how to effectively utilize the SLAM-ASR architecture across a wide
range of settings. Our main findings indicate that the SLAM-ASR exhibits poor
performance in cross-domain evaluation settings. Additionally, speech
perturbations within in-domain data, such as changes in speed or the presence
of additive noise, can significantly impact performance. Our findings offer
critical insights for fine-tuning and configuring robust LLM-based ASR models,
tailored to different data characteristics and computational resources.",cs.AI
AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making,"Traditional interactive environments limit agents' intelligence growth with
fixed tasks. Recently, single-agent environments address this by generating new
tasks based on agent actions, enhancing task diversity. We consider the
decision-making problem in multi-agent settings, where tasks are further
influenced by social connections, affecting rewards and information access.
However, existing multi-agent environments lack a combination of adaptive
physical surroundings and social connections, hindering the learning of
intelligent behaviors. To address this, we introduce AdaSociety, a customizable
multi-agent environment featuring expanding state and action spaces, alongside
explicit and alterable social structures. As agents progress, the environment
adaptively generates new tasks with social structures for agents to undertake.
In AdaSociety, we develop three mini-games showcasing distinct social
structures and tasks. Initial results demonstrate that specific social
structures can promote both individual and collective benefits, though current
reinforcement learning and LLM-based algorithms show limited effectiveness in
leveraging social structures to enhance performance. Overall, AdaSociety serves
as a valuable research platform for exploring intelligence in diverse physical
and social settings. The code is available at
https://github.com/bigai-ai/AdaSociety.",cs.AI
ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization,"Watermarking generative content serves as a vital tool for authentication,
ownership protection, and mitigation of potential misuse. Existing watermarking
methods face the challenge of balancing robustness and concealment. They
empirically inject a watermark that is both invisible and robust and passively
achieve concealment by limiting the strength of the watermark, thus reducing
the robustness. In this paper, we propose to explicitly introduce a watermark
hiding process to actively achieve concealment, thus allowing the embedding of
stronger watermarks. To be specific, we implant a robust watermark in an
intermediate diffusion state and then guide the model to hide the watermark in
the final generated image. We employ an adversarial optimization algorithm to
produce the optimal hiding prompt guiding signal for each watermark. The prompt
embedding is optimized to minimize artifacts in the generated image, while the
watermark is optimized to achieve maximum strength. The watermark can be
verified by reversing the generation process. Experiments on various diffusion
models demonstrate the watermark remains verifiable even under significant
image tampering and shows superior invisibility compared to other
state-of-the-art robust watermarking methods.",cs.AI
UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces,"Human trajectory modeling is essential for deciphering movement patterns and
supporting advanced applications across various domains. However, existing
methods are often tailored to specific tasks and regions, resulting in
limitations related to task specificity, regional dependency, and data quality
sensitivity. Addressing these challenges requires a universal human trajectory
foundation model capable of generalizing and scaling across diverse tasks and
geographic contexts. To this end, we propose UniTraj, a Universal human
Trajectory foundation model that is task-adaptive, region-independent, and
highly generalizable. To further enhance performance, we construct WorldTrace,
the first large-scale, high-quality, globally distributed dataset sourced from
open web platforms, encompassing 2.45 million trajectories with billions of
points across 70 countries. Through multiple resampling and masking strategies
designed for pre-training, UniTraj effectively overcomes geographic and task
constraints, adapting to heterogeneous data quality. Extensive experiments
across multiple trajectory analysis tasks and real-world datasets demonstrate
that UniTraj consistently outperforms existing approaches in terms of
scalability and adaptability. These results underscore the potential of UniTraj
as a versatile, robust solution for a wide range of trajectory analysis
applications, with WorldTrace serving as an ideal but non-exclusive foundation
for training.",cs.AI
MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba,"An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.",cs.AI
A Novel Access Control and Privacy-Enhancing Approach for Models in Edge Computing,"With the widespread adoption of edge computing technologies and the
increasing prevalence of deep learning models in these environments, the
security risks and privacy threats to models and data have grown more acute.
Attackers can exploit various techniques to illegally obtain models or misuse
data, leading to serious issues such as intellectual property infringement and
privacy breaches. Existing model access control technologies primarily rely on
traditional encryption and authentication methods; however, these approaches
exhibit significant limitations in terms of flexibility and adaptability in
dynamic environments. Although there have been advancements in model
watermarking techniques for marking model ownership, they remain limited in
their ability to proactively protect intellectual property and prevent
unauthorized access. To address these challenges, we propose a novel model
access control method tailored for edge computing environments. This method
leverages image style as a licensing mechanism, embedding style recognition
into the model's operational framework to enable intrinsic access control.
Consequently, models deployed on edge platforms are designed to correctly infer
only on license data with specific style, rendering them ineffective on any
other data. By restricting the input data to the edge model, this approach not
only prevents attackers from gaining unauthorized access to the model but also
enhances the privacy of data on terminal devices. We conducted extensive
experiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,
and the results demonstrate that our method effectively prevents unauthorized
access to the model while maintaining accuracy. Additionally, the model shows
strong resistance against attacks such as forged licenses and fine-tuning.
These results underscore the method's usability, security, and robustness.",cs.AI
Reconsidering the Performance of GAE in Link Prediction,"Various graph neural networks (GNNs) with advanced training techniques and
model designs have been proposed for link prediction tasks. However, outdated
baseline models may lead to an overestimation of the benefits provided by these
novel approaches. To address this, we systematically investigate the potential
of Graph Autoencoders (GAE) by meticulously tuning hyperparameters and
utilizing the trick of orthogonal embedding and linear propagation. Our
findings reveal that a well-optimized GAE can match the performance of more
complex models while offering greater computational efficiency.",cs.AI
Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination,"The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.",cs.AI
Beyond The Rainbow: High Performance Deep Reinforcement Learning On A Desktop PC,"Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent
enhancements could significantly boost a reinforcement learning (RL) agent's
performance. In this paper, we present ""Beyond The Rainbow"" (BTR), a novel
algorithm that integrates six improvements from across the RL literature to
Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,
with a human-normalized interquartile mean (IQM) of 7.4 on atari-60. Beyond
Atari, we demonstrate BTR's capability to handle complex 3D games, successfully
training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with
minimal algorithmic changes. Designing BTR with computational efficiency in
mind, agents can be trained using a desktop PC on 200 million Atari frames
within 12 hours. Additionally, we conduct detailed ablation studies of each
component, analzying the performance and impact using numerous measures.",cs.AI
From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning,"The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.",cs.AI
MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.",cs.AI
Crystal: Illuminating LLM Abilities on Language and Code,"Large Language Models (LLMs) specializing in code generation (which are also
often referred to as code LLMs), e.g., StarCoder and Code Llama, play
increasingly critical roles in various software development scenarios. It is
also crucial for code LLMs to possess both code generation and natural language
abilities for many specific applications, such as code snippet retrieval using
natural language or code explanations. The intricate interaction between
acquiring language and coding skills complicates the development of strong code
LLMs. Furthermore, there is a lack of thorough prior studies on the LLM
pretraining strategy that mixes code and natural language. In this work, we
propose a pretraining strategy to enhance the integration of natural language
and coding capabilities within a single LLM. Specifically, it includes two
phases of training with appropriately adjusted code/language ratios. The
resulting model, Crystal, demonstrates remarkable capabilities in both domains.
Specifically, it has natural language and coding performance comparable to that
of Llama 2 and Code Llama, respectively. Crystal exhibits better data
efficiency, using 1.4 trillion tokens compared to the more than 2 trillion
tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by
analyzing the training process and observe consistent improvements in most
benchmarks. We also adopted a typical application adaptation phase with a
code-centric data mixture, only to find that it did not lead to enhanced
performance or training efficiency, underlining the importance of a carefully
designed data recipe. To foster research within the community, we commit to
open-sourcing every detail of the pretraining, including our training datasets,
code, loggings and 136 checkpoints throughout the training.",cs.AI
GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian Splatting,"This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.",cs.AI
Overcoming label shift in targeted federated learning,"Federated learning enables multiple actors to collaboratively train models
without sharing private data. This unlocks the potential for scaling machine
learning to diverse applications. Existing algorithms for this task are
well-justified when clients and the intended target domain share the same
distribution of features and labels, but this assumption is often violated in
real-world scenarios. One common violation is label shift, where the label
distributions differ across clients or between clients and the target domain,
which can significantly degrade model performance. To address this problem, we
propose FedPALS, a novel model aggregation scheme that adapts to label shifts
by leveraging knowledge of the target label distribution at the central server.
Our approach ensures unbiased updates under stochastic gradient descent,
ensuring robust generalization across clients with diverse, label-shifted data.
Extensive experiments on image classification demonstrate that FedPALS
consistently outperforms standard baselines by aligning model aggregation with
the target domain. Our findings reveal that conventional federated learning
methods suffer severely in cases of extreme client sparsity, highlighting the
critical need for target-aware aggregation. FedPALS offers a principled and
practical solution to mitigate label distribution mismatch, ensuring models
trained in federated settings can generalize effectively to label-shifted
target domains.",cs.AI
VQA$^2$:Visual Question Answering for Video Quality Assessment,"The advent and proliferation of large multi-modal models (LMMs) have
introduced a new paradigm to video-related computer vision fields, including
training and inference methods based on visual question answering (VQA). These
methods enable models to handle multiple downstream tasks robustly. Video
Quality Assessment (VQA), a classic field in low-level visual quality
evaluation, originally focused on quantitative video quality scoring. However,
driven by advances in LMMs, it is now evolving towards more comprehensive
visual quality understanding tasks. Visual question answering has significantly
improved low-level visual evaluation within the image domain recently. However,
related work is almost nonexistent in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset the first visual question answering instruction dataset entirely
focuses on video quality assessment, and based on it, we propose the VQA2
series models The VQA2 Instruction Dataset consists of three stages and covers
various video types, containing 157,735 instruction question-answer pairs,
including both manually annotated and synthetic data. We conduct extensive
experiments on both video quality scoring and video quality understanding
tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art
(SOTA) performance in quality scoring tasks, and their performance in visual
quality question answering surpasses the renowned GPT-4o. Additionally, our
final model, the VQA2-Assistant, performs well across both scoring and
question-answering tasks, validating its versatility.",cs.AI
Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications,"Recent technological advances in healthcare have led to unprecedented growth
in patient data quantity and diversity. While artificial intelligence (AI)
models have shown promising results in analyzing individual data modalities,
there is increasing recognition that models integrating multiple complementary
data sources, so-called multimodal AI, could enhance clinical decision-making.
This scoping review examines the landscape of deep learning-based multimodal AI
applications across the medical domain, analyzing 432 papers published between
2018 and 2024. We provide an extensive overview of multimodal AI development
across different medical disciplines, examining various architectural
approaches, fusion strategies, and common application areas. Our analysis
reveals that multimodal AI models consistently outperform their unimodal
counterparts, with an average improvement of 6.2 percentage points in AUC.
However, several challenges persist, including cross-departmental coordination,
heterogeneous data characteristics, and incomplete datasets. We critically
assess the technical and practical challenges in developing multimodal AI
systems and discuss potential strategies for their clinical implementation,
including a brief overview of commercially available multimodal AI models for
clinical decision-making. Additionally, we identify key factors driving
multimodal AI development and propose recommendations to accelerate the field's
maturation. This review provides researchers and clinicians with a thorough
understanding of the current state, challenges, and future directions of
multimodal AI in medicine.",cs.AI
"No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages","Research in vision and language has made considerable progress thanks to
benchmarks such as COCO. COCO captions focused on unambiguous facts in English;
ArtEmis introduced subjective emotions and ArtELingo introduced some
multilinguality (Chinese and Arabic). However we believe there should be more
multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark
that spans $\textbf{28}$ languages and encompasses approximately
$\textbf{200,000}$ annotations ($\textbf{140}$ annotations per image).
Traditionally, vision research focused on unambiguous class labels, whereas
ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The
challenge is to build machine learning systems that assign emotional captions
to images. Baseline results will be presented for three novel conditions:
Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual
transfer is more successful for culturally-related languages. Data and code are
provided at www.artelingo.org.",cs.AI
Number Cookbook: Number Understanding of Language Models and How to Improve It,"Large language models (LLMs) can solve an increasing number of complex
reasoning tasks while making surprising mistakes in basic numerical
understanding and processing (such as 9.11 > 9.9). The latter ability is
essential for tackling complex arithmetic and mathematical problems and serves
as a foundation for most reasoning tasks, but previous work paid little
attention to it or only discussed several restricted tasks (like integer
addition). In this paper, we comprehensively investigate the numerical
understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a
benchmark covering four common numerical representations and 17 distinct
numerical tasks in four major categories, resulting in 41 meaningful
combinations in total. These tasks are derived from primary and secondary
education curricula, encompassing nearly all everyday numerical understanding
and processing scenarios, and the rules of these tasks are very simple and
clear. Through the benchmark, we find that current LLMs fail frequently in many
of the tasks. To study the problem, we train small models with existing and
potential techniques for enhancing NUPA (such as special tokenizers, PEs, and
number formats), comprehensively evaluating their effectiveness using our
testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and
find that 1) naive finetuning can improve NUPA a lot on many but not all tasks,
and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for
finetuning pretrained models. We further explore the impact of chain-of-thought
techniques on NUPA. Our work takes a preliminary step towards understanding and
improving NUPA of LLMs. Our benchmark and code are released at
https://github.com/GraphPKU/number_cookbook.",cs.AI
Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI Reconstruction,"Diffusion model-based approaches recently achieved re-markable success in MRI
reconstruction, but integration into clinical routine remains challenging due
to its time-consuming convergence. This phenomenon is partic-ularly notable
when directly apply conventional diffusion process to k-space data without
considering the inherent properties of k-space sampling, limiting k-space
learning efficiency and image reconstruction quality. To tackle these
challenges, we introduce subspace diffusion model with orthogonal
decomposition, a method (referred to as Sub-DM) that restrict the diffusion
process via projections onto subspace as the k-space data distribution evolves
toward noise. Particularly, the subspace diffusion model circumvents the
inference challenges posed by the com-plex and high-dimensional characteristics
of k-space data, so the highly compact subspace ensures that diffusion process
requires only a few simple iterations to produce accurate prior information.
Furthermore, the orthogonal decomposition strategy based on wavelet transform
hin-ders the information loss during the migration of the vanilla diffusion
process to the subspace. Considering the strate-gy is approximately reversible,
such that the entire pro-cess can be reversed. As a result, it allows the
diffusion processes in different spaces to refine models through a mutual
feedback mechanism, enabling the learning of ac-curate prior even when dealing
with complex k-space data. Comprehensive experiments on different datasets
clearly demonstrate that the superiority of Sub-DM against state of-the-art
methods in terms of reconstruction speed and quality.",cs.AI
Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions,"Understanding identifiability of latent content and style variables from
unaligned multi-domain data is essential for tasks such as domain translation
and data generation. Existing works on content-style identification were often
developed under somewhat stringent conditions, e.g., that all latent components
are mutually independent and that the dimensions of the content and style
variables are known. We introduce a new analytical framework via cross-domain
\textit{latent distribution matching} (LDM), which establishes content-style
identifiability under substantially more relaxed conditions. Specifically, we
show that restrictive assumptions such as component-wise independence of the
latent variables can be removed. Most notably, we prove that prior knowledge of
the content and style dimensions is not necessary for ensuring identifiability,
if sparsity constraints are properly imposed onto the learned latent
representations. Bypassing the knowledge of the exact latent dimension has been
a longstanding aspiration in unsupervised representation learning -- our
analysis is the first to underpin its theoretical and practical viability. On
the implementation side, we recast the LDM formulation into a regularized
multi-domain GAN loss with coupled latent variables. We show that the
reformulation is equivalent to LDM under mild conditions -- yet requiring
considerably less computational resource. Experiments corroborate with our
theoretical claims.",cs.AI
Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.",cs.AI
Automating Exploratory Proteomics Research via Language Models,"With the development of artificial intelligence, its contribution to science
is evolving from simulating a complex problem to automating entire research
processes and producing novel discoveries. Achieving this advancement requires
both specialized general models grounded in real-world scientific data and
iterative, exploratory frameworks that mirror human scientific methodologies.
In this paper, we present PROTEUS, a fully automated system for scientific
discovery from raw proteomics data. PROTEUS uses large language models (LLMs)
to perform hierarchical planning, execute specialized bioinformatics tools, and
iteratively refine analysis workflows to generate high-quality scientific
hypotheses. The system takes proteomics datasets as input and produces a
comprehensive set of research objectives, analysis results, and novel
biological hypotheses without human intervention. We evaluated PROTEUS on 12
proteomics datasets collected from various biological samples (e.g. immune
cells, tumors) and different sample types (single-cell and bulk), generating
191 scientific hypotheses. These were assessed using both automatic LLM-based
scoring on 5 metrics and detailed reviews from human experts. Results
demonstrate that PROTEUS consistently produces reliable, logically coherent
results that align well with existing literature while also proposing novel,
evaluable hypotheses. The system's flexible architecture facilitates seamless
integration of diverse analysis tools and adaptation to different proteomics
data types. By automating complex proteomics analysis workflows and hypothesis
generation, PROTEUS has the potential to considerably accelerate the pace of
scientific discovery in proteomics research, enabling researchers to
efficiently explore large-scale datasets and uncover biological insights.",cs.AI
Adaptive Consensus Gradients Aggregation for Scaled Distributed Training,"Distributed machine learning has recently become a critical paradigm for
training large models on vast datasets. We examine the stochastic optimization
problem for deep learning within synchronous parallel computing environments
under communication constraints. While averaging distributed gradients is the
most widely used method for gradient estimation, whether this is the optimal
strategy remains an open question. In this work, we analyze the distributed
gradient aggregation process through the lens of subspace optimization. By
formulating the aggregation problem as an objective-aware subspace optimization
problem, we derive an efficient weighting scheme for gradients, guided by
subspace coefficients. We further introduce subspace momentum to accelerate
convergence while maintaining statistical unbiasedness in the aggregation. Our
method demonstrates improved performance over the ubiquitous gradient averaging
on multiple MLPerf tasks while remaining extremely efficient in both
communicational and computational complexity.",cs.AI
UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction,"Multi-person motion prediction is a complex and emerging field with
significant real-world applications. Current state-of-the-art methods typically
adopt dual-path networks to separately modeling spatial features and temporal
features. However, the uncertain compatibility of the two networks brings a
challenge for spatio-temporal features fusion and violate the spatio-temporal
coherence and coupling of human motions by nature. To address this issue, we
propose a novel graph structure, UnityGraph, which treats spatio-temporal
features as a whole, enhancing model coherence and coupling.spatio-temporal
features as a whole, enhancing model coherence and coupling. Specifically,
UnityGraph is a hypervariate graph based network. The flexibility of the
hypergraph allows us to consider the observed motions as graph nodes. We then
leverage hyperedges to bridge these nodes for exploring spatio-temporal
features. This perspective considers spatio-temporal dynamics unitedly and
reformulates multi-person motion prediction into a problem on a single graph.
Leveraging the dynamic message passing based on this hypergraph, our model
dynamically learns from both types of relations to generate targeted messages
that reflect the relevance among nodes. Extensive experiments on several
datasets demonstrates that our method achieves state-of-the-art performance,
confirming its effectiveness and innovative design.",cs.AI
Relation Learning and Aggregate-attention for Multi-person Motion Prediction,"Multi-person motion prediction is an emerging and intricate task with broad
real-world applications. Unlike single person motion prediction, it considers
not just the skeleton structures or human trajectories but also the
interactions between others. Previous methods use various networks to achieve
impressive predictions but often overlook that the joints relations within an
individual (intra-relation) and interactions among groups (inter-relation) are
distinct types of representations. These methods often lack explicit
representation of inter&intra-relations, and inevitably introduce undesired
dependencies. To address this issue, we introduce a new collaborative framework
for multi-person motion prediction that explicitly modeling these relations:a
GCN-based network for intra-relations and a novel reasoning network for
inter-relations.Moreover, we propose a novel plug-and-play aggregation module
called the Interaction Aggregation Module (IAM), which employs an
aggregate-attention mechanism to seamlessly integrate these relations.
Experiments indicate that the module can also be applied to other dual-path
models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as
well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that
our method achieves state-of-the-art performance.",cs.AI
PropNEAT -- Efficient GPU-Compatible Backpropagation over NeuroEvolutionary Augmenting Topology Networks,"We introduce PropNEAT, a fast backpropagation implementation of NEAT that
uses a bidirectional mapping of the genome graph to a layer-based architecture
that preserves the NEAT genomes whilst enabling efficient GPU backpropagation.
We test PropNEAT on 58 binary classification datasets from the Penn Machine
Learning Benchmarks database, comparing the performance against logistic
regression, dense neural networks and random forests, as well as a densely
retrained variant of the final PropNEAT model. PropNEAT had the second best
overall performance, behind Random Forest, though the difference between the
models was not statistically significant apart from between Random Forest in
comparison with logistic regression and the PropNEAT retrain models. PropNEAT
was substantially faster than a naive backpropagation method, and both were
substantially faster and had better performance than the original NEAT
implementation. We demonstrate that the per-epoch training time for PropNEAT
scales linearly with network depth, and is efficient on GPU implementations for
backpropagation. This implementation could be extended to support reinforcement
learning or convolutional networks, and is able to find sparser and smaller
networks with potential for applications in low-power contexts.",cs.AI
AutoGameUI: Constructing High-Fidelity Game UIs via Multimodal Learning and Interactive Web-Based Tool,"We introduce an innovative system, AutoGameUI, for efficiently constructing
cohesive user interfaces in game development. Our system is the first to
address the coherence issue arising from integrating inconsistent UI and UX
designs, typically leading to mismatches and inefficiencies. We propose a
two-stage multimodal learning pipeline to obtain comprehensive representations
of both UI and UX designs, and to establish their correspondences. Through the
correspondences, a cohesive user interface is automatically constructed from
pairwise designs. To achieve high-fidelity effects, we introduce a universal
data protocol for precise design descriptions and cross-platform applications.
We also develop an interactive web-based tool for game developers to facilitate
the use of our system. We create a game UI dataset from actual game projects
and combine it with a public dataset for training and evaluation. Our
experimental results demonstrate the effectiveness of our system in maintaining
coherence between the constructed interfaces and the original designs.",cs.AI
Fine-Tuning Vision-Language Model for Automated Engineering Drawing Information Extraction,"Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in
manufacturing by defining acceptable variations in part features to ensure
component quality and functionality. However, extracting GD&T information from
2D engineering drawings is a time-consuming and labor-intensive task, often
relying on manual efforts or semi-automated tools. To address these challenges,
this study proposes an automated and computationally efficient GD&T extraction
method by fine-tuning Florence-2, an open-source vision-language model (VLM).
The model is trained on a dataset of 400 drawings with ground truth annotations
provided by domain experts. For comparison, two state-of-the-art closed-source
VLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All
models are assessed using precision, recall, F1-score, and hallucination
metrics. Due to the computational cost and impracticality of fine-tuning large
closed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are
evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with
0.23 billion parameters, is optimized through full-parameter fine-tuning across
three distinct experiments, each utilizing datasets augmented to different
levels. The results show that Florence-2 achieves a 29.95% increase in
precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a
43.15% reduction in hallucination rate compared to the best-performing
closed-source model. These findings highlight the effectiveness of fine-tuning
smaller, open-source VLMs like Florence-2, offering a practical and efficient
solution for automated GD&T extraction to support downstream manufacturing
tasks.",cs.AI
Ultrasound-Based AI for COVID-19 Detection: A Comprehensive Review of Public and Private Lung Ultrasound Datasets and Studies,"The COVID-19 pandemic has affected millions of people globally, with
respiratory organs being strongly affected in individuals with comorbidities.
Medical imaging-based diagnosis and prognosis have become increasingly popular
in clinical settings for detecting COVID-19 lung infections. Among various
medical imaging modalities, ultrasound stands out as a low-cost, mobile, and
radiation-safe imaging technology. In this comprehensive review, we focus on
AI-driven studies utilizing lung ultrasound (LUS) for COVID-19 detection and
analysis. We provide a detailed overview of both publicly available and private
LUS datasets and categorize the AI studies according to the dataset they used.
Additionally, we systematically analyzed and tabulated the studies across
various dimensions, including data preprocessing methods, AI models,
cross-validation techniques, and evaluation metrics. In total, we reviewed 60
articles, 41 of which utilized public datasets, while the remaining employed
private data. Our findings suggest that ultrasound-based AI studies for
COVID-19 detection have great potential for clinical use, especially for
children and pregnant women. Our review also provides a useful summary for
future researchers and clinicians who may be interested in the field.",cs.AI
Beyond Model Adaptation at Test Time: A Survey,"Machine learning algorithms have achieved remarkable success across various
disciplines, use cases and applications, under the prevailing assumption that
training and test samples are drawn from the same distribution. Consequently,
these algorithms struggle and become brittle even when samples in the test
distribution start to deviate from the ones observed during training. Domain
adaptation and domain generalization have been studied extensively as
approaches to address distribution shifts across test and train domains, but
each has its limitations. Test-time adaptation, a recently emerging learning
paradigm, combines the benefits of domain adaptation and domain generalization
by training models only on source data and adapting them to target data during
test-time inference. In this survey, we provide a comprehensive and systematic
review on test-time adaptation, covering more than 400 recent papers. We
structure our review by categorizing existing methods into five distinct
categories based on what component of the method is adjusted for test-time
adaptation: the model, the inference, the normalization, the sample, or the
prompt, providing detailed analysis of each. We further discuss the various
preparation and adaptation settings for methods within these categories,
offering deeper insights into the effective deployment for the evaluation of
distribution shifts and their real-world application in understanding images,
video and 3D, as well as modalities beyond vision. We close the survey with an
outlook on emerging research opportunities for test-time adaptation.",cs.AI
QUILL: Quotation Generation Enhancement of Large Language Models,"While Large language models (LLMs) have become excellent writing assistants,
they still struggle with quotation generation. This is because they either
hallucinate when providing factual quotations or fail to provide quotes that
exceed human expectations. To bridge the gap, we systematically study how to
evaluate and improve LLMs' performance in quotation generation tasks. We first
establish a holistic and automatic evaluation system for quotation generation
task, which consists of five criteria each with corresponding automatic metric.
To improve the LLMs' quotation generation abilities, we construct a bilingual
knowledge base that is broad in scope and rich in dimensions, containing up to
32,022 quotes. Moreover, guided by our critiria, we further design a
quotation-specific metric to rerank the retrieved quotations from the knowledge
base. Extensive experiments show that our metrics strongly correlate with human
preferences. Existing LLMs struggle to generate desired quotes, but our
quotation knowledge base and reranking metric help narrow this gap. Our dataset
and code are publicly available at https://github.com/GraceXiaoo/QUILL.",cs.AI
Towards 3D Semantic Scene Completion for Autonomous Driving: A Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and Mamba Model,"Semantic scene completion (SSC) is essential for achieving comprehensive
perception in autonomous driving systems. However, existing SSC methods often
overlook the high deployment costs in real-world applications. Traditional
architectures, such as 3D Convolutional Neural Networks (3D CNNs) and
self-attention mechanisms, face challenges in efficiently capturing long-range
dependencies within 3D voxel grids, limiting their effectiveness. To address
these issues, we introduce MetaSSC, a novel meta-learning-based framework for
SSC that leverages deformable convolution, large-kernel attention, and the
Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic
segmentation (SS) pretraining task, aimed at exploring the semantics and
geometry of incomplete regions while acquiring transferable meta-knowledge.
Using simulated cooperative perception datasets, we supervise the perception
training of a single vehicle using aggregated sensor data from multiple nearby
connected autonomous vehicles (CAVs), generating richer and more comprehensive
labels. This meta-knowledge is then adapted to the target domain through a
dual-phase training strategy that does not add extra model parameters, enabling
efficient deployment. To further enhance the model's capability in capturing
long-sequence relationships within 3D voxel grids, we integrate Mamba blocks
with deformable convolution and large-kernel attention into the backbone
network. Extensive experiments demonstrate that MetaSSC achieves
state-of-the-art performance, significantly outperforming competing models
while also reducing deployment costs.",cs.AI
Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?,"How can we test AI performance? This question seems trivial, but it isn't.
Standard benchmarks often have problems such as in-distribution and small-size
test sets, oversimplified metrics, unfair comparisons, and short-term outcome
pressure. As a consequence, good performance on standard benchmarks does not
guarantee success in real-world scenarios. To address these problems, we
present Touchstone, a large-scale collaborative segmentation benchmark of 9
types of abdominal organs. This benchmark is based on 5,195 training CT scans
from 76 hospitals around the world and 5,903 testing CT scans from 11
additional hospitals. This diverse test set enhances the statistical
significance of benchmark results and rigorously evaluates AI algorithms across
various out-of-distribution scenarios. We invited 14 inventors of 19 AI
algorithms to train their algorithms, while our team, as a third party,
independently evaluated these algorithms on three test sets. In addition, we
also evaluated pre-existing AI frameworks--which, differing from algorithms,
are more flexible and can support different algorithms--including MONAI from
NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are
committed to expanding this benchmark to encourage more innovation of AI
algorithms for the medical domain.",cs.AI
Evaluating Moral Beliefs across LLMs through a Pluralistic Framework,"Proper moral beliefs are fundamental for language models, yet assessing these
beliefs poses a significant challenge. This study introduces a novel
three-module framework to evaluate the moral beliefs of four prominent large
language models. Initially, we constructed a dataset containing 472 moral
choice scenarios in Chinese, derived from moral words. The decision-making
process of the models in these scenarios reveals their moral principle
preferences. By ranking these moral choices, we discern the varying moral
beliefs held by different language models. Additionally, through moral debates,
we investigate the firmness of these models to their moral choices. Our
findings indicate that English language models, namely ChatGPT and Gemini,
closely mirror moral decisions of the sample of Chinese university students,
demonstrating strong adherence to their choices and a preference for
individualistic moral beliefs. In contrast, Chinese models such as Ernie and
ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their
moral choices and debates. This study also uncovers gender bias embedded within
the moral beliefs of all examined language models. Our methodology offers an
innovative means to assess moral beliefs in both artificial and human
intelligence, facilitating a comparison of moral values across different
cultures.",cs.AI
Requirements Engineering for Older Adult Digital Health Software: A Systematic Literature Review,"Growth of the older adult population has led to an increasing interest in
technology-supported aged care. However, the area has some challenges such as a
lack of caregivers and limitations in understanding the emotional, social,
physical, and mental well-being needs of seniors. Furthermore, there is a gap
in the understanding between developers and ageing people of their
requirements. Digital health can be important in supporting older adults
wellbeing, emotional requirements, and social needs. Requirements Engineering
(RE) is a major software engineering field, which can help to identify, elicit
and prioritize the requirements of stakeholders and ensure that the systems
meet standards for performance, reliability, and usability. We carried out a
systematic review of the literature on RE for older adult digital health
software. This was necessary to show the representatives of the current stage
of understanding the needs of older adults in aged care digital health. Using
established guidelines outlined by the Kitchenham method, the PRISMA and the
PICO guideline, we developed a protocol, followed by the systematic exploration
of eight databases. This resulted in 69 primary studies of high relevance,
which were subsequently subjected to data extraction, synthesis, and reporting.
We highlight key RE processes in digital health software for ageing people. It
explored the utilization of technology for older user well-being and care, and
the evaluations of such solutions. The review also identified key limitations
found in existing primary studies that inspire future research opportunities.
The results indicate that requirement gathering and understanding have a
significant variation between different studies. The differences are in the
quality, depth, and techniques adopted for requirement gathering and these
differences are largely due to uneven adoption of RE methods.",cs.AI
Policy Aggregation,"We consider the challenge of AI value alignment with multiple individuals
that have different reward functions and optimal policies in an underlying
Markov decision process. We formalize this problem as one of policy
aggregation, where the goal is to identify a desirable collective policy. We
argue that an approach informed by social choice theory is especially suitable.
Our key insight is that social choice methods can be reinterpreted by
identifying ordinal preferences with volumes of subsets of the state-action
occupancy polytope. Building on this insight, we demonstrate that a variety of
methods--including approval voting, Borda count, the proportional veto core,
and quantile fairness--can be practically applied to policy aggregation.",cs.AI
Deploying Multi-task Online Server with Large Language Model,"In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.",cs.AI
Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions,"Depth estimation under adverse conditions remains a significant challenge.
Recently, multi-spectral depth estimation, which integrates both visible light
and thermal images, has shown promise in addressing this issue. However,
existing algorithms struggle with precise pixel-level feature matching,
limiting their ability to fully exploit geometric constraints across different
spectra. To address this, we propose a novel framework incorporating stereo
depth estimation to enforce accurate geometric constraints. In particular, we
treat the visible light and thermal images as a stereo pair and utilize a
Cross-modal Feature Matching (CFM) Module to construct a cost volume for
pixel-level matching. To mitigate the effects of poor lighting on stereo
matching, we introduce Degradation Masking, which leverages robust monocular
thermal depth estimation in degraded regions. Our method achieves
state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,
with qualitative evaluations demonstrating high-quality depth maps under
varying lighting conditions.",cs.AI
RTify: Aligning Deep Neural Networks with Human Behavioral Decisions,"Current neural network models of primate vision focus on replicating overall
levels of behavioral accuracy, often neglecting perceptual decisions' rich,
dynamic nature. Here, we introduce a novel computational framework to model the
dynamics of human behavioral choices by learning to align the temporal dynamics
of a recurrent neural network (RNN) to human reaction times (RTs). We describe
an approximation that allows us to constrain the number of time steps an RNN
takes to solve a task with human RTs. The approach is extensively evaluated
against various psychophysics experiments. We also show that the approximation
can be used to optimize an ""ideal-observer"" RNN model to achieve an optimal
tradeoff between speed and accuracy without human data. The resulting model is
found to account well for human RT data. Finally, we use the approximation to
train a deep learning implementation of the popular Wong-Wang decision-making
model. The model is integrated with a convolutional neural network (CNN) model
of visual processing and evaluated using both artificial and natural image
stimuli. Overall, we present a novel framework that helps align current vision
models with human behavior, bringing us closer to an integrated model of human
vision.",cs.AI
StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding,"The rapid development of Multimodal Large Language Models (MLLMs) has
expanded their capabilities from image comprehension to video understanding.
However, most of these MLLMs focus primarily on offline video comprehension,
necessitating extensive processing of all video frames before any queries can
be made. This presents a significant gap compared to the human ability to
watch, listen, think, and respond to streaming inputs in real time,
highlighting the limitations of current MLLMs. In this paper, we introduce
StreamingBench, the first comprehensive benchmark designed to evaluate the
streaming video understanding capabilities of MLLMs. StreamingBench assesses
three core aspects of streaming video understanding: (1) real-time visual
understanding, (2) omni-source understanding, and (3) contextual understanding.
The benchmark consists of 18 tasks, featuring 900 videos and 4,500
human-curated QA pairs. Each video features five questions presented at
different time points to simulate a continuous streaming scenario. We conduct
experiments on StreamingBench with 13 open-source and proprietary MLLMs and
find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and
GPT-4o perform significantly below human-level streaming video understanding
capabilities. We hope our work can facilitate further advancements for MLLMs,
empowering them to approach human-level video comprehension and interaction in
more realistic scenarios.",cs.AI
Fully Hyperbolic Rotation for Knowledge Graph Embedding,"Hyperbolic rotation is commonly used to effectively model knowledge graphs
and their inherent hierarchies. However, existing hyperbolic rotation models
rely on logarithmic and exponential mappings for feature transformation. These
models only project data features into hyperbolic space for rotation, limiting
their ability to fully exploit the hyperbolic space. To address this problem,
we propose a novel fully hyperbolic model designed for knowledge graph
embedding. Instead of feature mappings, we define the model directly in
hyperbolic space with the Lorentz model. Our model considers each relation in
knowledge graphs as a Lorentz rotation from the head entity to the tail entity.
We adopt the Lorentzian version distance as the scoring function for measuring
the plausibility of triplets. Extensive results on standard knowledge graph
completion benchmarks demonstrated that our model achieves competitive results
with fewer parameters. In addition, our model get the state-of-the-art
performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and
challenging than before. Our code is available at
https://github.com/llqy123/FHRE.",cs.AI
Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification,"Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating
early detection and diagnosis. This paper focuses on referable DR
classification to enhance the applicability of the proposed method in clinical
practice. We develop an advanced cross-learning DR classification method
leveraging transfer learning and cross-attention mechanisms. The proposed
method employs the Swin U-Net architecture to segment lesion maps from DR
fundus images. The Swin U-Net segmentation model, enriched with DR lesion
insights, is transferred to generate a lesion map. Both the fundus image and
its segmented lesion map are used as complementary inputs for the
classification model. A cross-attention mechanism is deployed to improve the
model's ability to capture fine-grained details from the input pairs. Our
experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a
superior accuracy of 94.6%, surpassing current state-of-the-art methods by
4.4%. To this end, we aim for the proposed method to be seamlessly integrated
into clinical workflows, enhancing accuracy and efficiency in identifying
referable DR.",cs.AI
An Experimental Study on Decomposition-Based Deep Ensemble Learning for Traffic Flow Forecasting,"Traffic flow forecasting is a crucial task in intelligent transport systems.
Deep learning offers an effective solution, capturing complex patterns in
time-series traffic flow data to enable the accurate prediction. However, deep
learning models are prone to overfitting the intricate details of flow data,
leading to poor generalisation. Recent studies suggest that decomposition-based
deep ensemble learning methods may address this issue by breaking down a time
series into multiple simpler signals, upon which deep learning models are built
and ensembled to generate the final prediction. However, few studies have
compared the performance of decomposition-based ensemble methods with
non-decomposition-based ones which directly utilise raw time-series data. This
work compares several decomposition-based and non-decomposition-based deep
ensemble learning methods. Experimental results on three traffic datasets
demonstrate the superiority of decomposition-based ensemble methods, while also
revealing their sensitivity to aggregation strategies and forecasting horizons.",cs.AI
Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions,"Multispectral pedestrian detection has gained significant attention in recent
years, particularly in autonomous driving applications. To address the
challenges posed by adversarial illumination conditions, the combination of
thermal and visible images has demonstrated its advantages. However, existing
fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)
image pairs are fully overlapping. These assumptions often do not hold in
real-world applications, where only partial overlap between images can occur
due to sensors configuration. Moreover, sensor failure can cause loss of
information in one modality. In this paper, we propose a novel module called
the Hybrid Attention (HA) mechanism as our main contribution to mitigate
performance degradation caused by partial overlap and sensor failure, i.e. when
at least part of the scene is acquired by only one sensor. We propose an
improved RGB-T fusion algorithm, robust against partial overlap and sensor
failure encountered during inference in real-world applications. We also
leverage a mobile-friendly backbone to cope with resource constraints in
embedded systems. We conducted experiments by simulating various partial
overlap and sensor failure scenarios to evaluate the performance of our
proposed method. The results demonstrate that our approach outperforms
state-of-the-art methods, showcasing its superiority in handling real-world
challenges.",cs.AI
Towards Personalized Federated Learning via Comprehensive Knowledge Distillation,"Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.",cs.AI
Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level,"We introduce Agent K v1.0, an end-to-end autonomous data science agent
designed to automate, optimise, and generalise across diverse data science
tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle
by learning from experience. It leverages a highly flexible structured
reasoning framework to enable it to dynamically process memory in a nested
structure, effectively learning from accumulated experience stored to handle
complex reasoning tasks. It optimises long- and short-term memory by
selectively storing and retrieving key information, guiding future decisions
based on environmental rewards. This iterative approach allows it to refine
decisions without fine-tuning or backpropagation, achieving continuous
improvement through experiential learning. We evaluate our agent's apabilities
using Kaggle competitions as a case study. Following a fully automated
protocol, Agent K v1.0 systematically addresses complex and multimodal data
science tasks, employing Bayesian optimisation for hyperparameter tuning and
feature engineering. Our new evaluation framework rigorously assesses Agent K
v1.0's end-to-end capabilities to generate and send submissions starting from a
Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\%
success rate across tasks, spanning tabular, computer vision, NLP, and
multimodal domains. When benchmarking against 5,856 human Kaggle competitors by
calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\%,
demonstrating an overall skill level comparable to Expert-level users. Notably,
its Elo-MMR score falls between the first and third quartiles of scores
achieved by human Grandmasters. Furthermore, our results indicate that Agent K
v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a
record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's
progression system.",cs.AI
Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation,"Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening
and scarring, leading to respiratory decline. High-resolution computed
tomography (HRCT) is critical for diagnosing and monitoring FLD; however,
fibrosis appears as irregular, diffuse patterns with unclear boundaries,
leading to high inter-observer variability and time-intensive manual
annotation. To tackle this challenge, we propose DiffSeg, a novel weakly
supervised semantic segmentation (WSSS) method that uses image-level
annotations to generate pixel-level fibrosis segmentation, reducing the need
for fine-grained manual labeling. Additionally, our DiffSeg incorporates a
diffusion-based generative model to synthesize HRCT images with different
levels of fibrosis from healthy slices, enabling the generation of the
fibrosis-injected slices and their paired fibrosis location. Experiments
indicate that our method significantly improves the accuracy of pseudo masks
generated by existing WSSS methods, greatly reducing the complexity of manual
labeling and enhancing the consistency of the generated masks.",cs.AI
Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry,"A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and
more) are driving forward novel development of multipurpose AI for a variety of
tasks, particularly natural language processing (NLP) tasks. These models
demonstrate strong performance on a range of tasks; however, there has been
evidence of brittleness when applied to more niche or narrow domains where
hallucinations or fluent but incorrect responses reduce performance. Given the
complex nature of scientific domains, it is prudent to investigate the
trade-offs of leveraging off-the-shelf versus more targeted foundation models
for scientific domains. In this work, we examine the benefits of in-domain
pre-training for a given scientific domain, chemistry, and compare these to
open-source, off-the-shelf models with zero-shot and few-shot prompting. Our
results show that not only do in-domain base models perform reasonably well on
in-domain tasks in a zero-shot setting but that further adaptation using
instruction fine-tuning yields impressive performance on chemistry-specific
tasks such as named entity recognition and molecular formula generation.",cs.AI
Two-Stage Pretraining for Molecular Property Prediction in the Wild,"Accurate property prediction is crucial for accelerating the discovery of new
molecules. Although deep learning models have achieved remarkable success,
their performance often relies on large amounts of labeled data that are
expensive and time-consuming to obtain. Thus, there is a growing need for
models that can perform well with limited experimentally-validated data. In
this work, we introduce MoleVers, a versatile pretrained model designed for
various types of molecular property prediction in the wild, i.e., where
experimentally-validated molecular property labels are scarce. MoleVers adopts
a two-stage pretraining strategy. In the first stage, the model learns
molecular representations from large unlabeled datasets via masked atom
prediction and dynamic denoising, a novel task enabled by a new branching
encoder architecture. In the second stage, MoleVers is further pretrained using
auxiliary labels obtained with inexpensive computational methods, enabling
supervised learning without the need for costly experimental data. This
two-stage framework allows MoleVers to learn representations that generalize
effectively across various downstream datasets. We evaluate MoleVers on a new
benchmark comprising 22 molecular datasets with diverse types of properties,
the majority of which contain 50 or fewer training labels reflecting real-world
conditions. MoleVers achieves state-of-the-art results on 20 out of the 22
datasets, and ranks second among the remaining two, highlighting its ability to
bridge the gap between data-hungry models and real-world conditions where
practically-useful labels are scarce.",cs.AI
Personalized Video Summarization by Multimodal Video Understanding,"Video summarization techniques have been proven to improve the overall user
experience when it comes to accessing and comprehending video content. If the
user's preference is known, video summarization can identify significant
information or relevant content from an input video, aiding them in obtaining
the necessary information or determining their interest in watching the
original video. Adapting video summarization to various types of video and user
preferences requires significant training data and expensive human labeling. To
facilitate such research, we proposed a new benchmark for video summarization
that captures various user preferences. Also, we present a pipeline called
Video Summarization with Language (VSL) for user-preferred video summarization
that is based on pre-trained visual language models (VLMs) to avoid the need to
train a video summarization system on a large training dataset. The pipeline
takes both video and closed captioning as input and performs semantic analysis
at the scene level by converting video frames into text. Subsequently, the
user's genre preference was used as the basis for selecting the pertinent
textual scenes. The experimental results demonstrate that our proposed pipeline
outperforms current state-of-the-art unsupervised video summarization models.
We show that our method is more adaptable across different datasets compared to
supervised query-based video summarization models. In the end, the runtime
analysis demonstrates that our pipeline is more suitable for practical use when
scaling up the number of user preferences and videos.",cs.AI
Mitigating Metric Bias in Minimum Bayes Risk Decoding,"While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or
MetricX has outperformed traditional decoding methods such as greedy or beam
search, it introduces a challenge we refer to as metric bias. As MBR decoding
aims to produce translations that score highly according to a specific utility
metric, this very process makes it impossible to use the same metric for both
decoding and evaluation, as improvements might simply be due to reward hacking
rather than reflecting real quality improvements. In this work we find that
compared to human ratings, neural metrics not only overestimate the quality of
MBR decoding when the same metric is used as the utility metric, but they also
overestimate the quality of MBR/QE decoding with other neural utility metrics
as well. We also show that the metric bias issue can be mitigated by using an
ensemble of utility metrics during MBR decoding: human evaluations show that
MBR decoding using an ensemble of utility metrics outperforms a single utility
metric.",cs.AI
Exploring the Potentials and Challenges of Using Large Language Models for the Analysis of Transcriptional Regulation of Long Non-coding RNAs,"Research on long non-coding RNAs (lncRNAs) has garnered significant attention
due to their critical roles in gene regulation and disease mechanisms. However,
the complexity and diversity of lncRNA sequences, along with the limited
knowledge of their functional mechanisms and the regulation of their
expressions, pose significant challenges to lncRNA studies. Given the
tremendous success of large language models (LLMs) in capturing complex
dependencies in sequential data, this study aims to systematically explore the
potential and limitations of LLMs in the sequence analysis related to the
transcriptional regulation of lncRNA genes. Our extensive experiments
demonstrated promising performance of fine-tuned genome foundation models on
progressively complex tasks. Furthermore, we conducted an insightful analysis
of the critical impact of task complexity, model selection, data quality, and
biological interpretability for the studies of the regulation of lncRNA gene
expression.",cs.AI
AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution,"With more advanced natural language understanding and reasoning capabilities,
large language model (LLM)-powered agents are increasingly developed in
simulated environments to perform complex tasks, interact with other agents,
and exhibit emergent behaviors relevant to social science and gaming. However,
current multi-agent simulations frequently suffer from inefficiencies due to
the limited parallelism caused by false dependencies, resulting in performance
bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine
that improves the efficiency of LLM agent simulations by incorporating
out-of-order execution scheduling. By dynamically tracking real dependencies
between agents, AI Metropolis minimizes false dependencies, enhancing
parallelism and enabling efficient hardware utilization. Our evaluations
demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over
standard parallel simulation with global synchronization, approaching optimal
performance as the number of agents increases.",cs.AI
Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy,"This paper introduces a novel model compression approach through dynamic
layer-specific pruning in Large Language Models (LLMs), enhancing the
traditional methodology established by SliceGPT. By transitioning from constant
to dynamic slicing, our method leverages the newly proposed Layer Redundancy
(LR) score, which assesses how much change each layer changes its input by
measuring the cosine similarity of the input to the output of the layer. We use
this score to prune parts of individual layers based on redundancy in such a
way that the average pruned percentage for all layers is a fixed value. We
conducted extensive experiments using models like Llama3-8B and Mistral-7B on
multiple datasets, evaluating different slicing bases and percentages to
determine optimal configurations that balance efficiency and performance. Our
findings show that our dynamic slicing approach not only maintains but, in many
cases, enhances model performance compared to the baseline established by
constant slicing methods. For instance, in several settings, we see performance
improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity
decrease by as much as 7% was observed across multiple benchmarks, validating
the effectiveness of our method. The code, model weights, and datasets are
open-sourced at https://github.com/RazvanDu/DynamicSlicing.",cs.AI
Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,"The automatic generation of hints by Large Language Models (LLMs) within
Intelligent Tutoring Systems (ITSs) has shown potential to enhance student
learning. However, generating pedagogically sound hints that address student
misconceptions and adhere to specific educational objectives remains
challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as
teachers to generate effective hints for students simulated through LLMs
(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math
exercises designed for human high-school students, and designed using cognitive
science principles. We present here the study of several dimensions: 1)
identifying error patterns made by simulated students on secondary-level math
exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating
their effectiveness in generating hints that enable simulated students to
self-correct; and 3) testing the best-performing prompts, based on their
ability to produce relevant hints and facilitate error correction, with
Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with
GPT-4o. The results show that model errors increase with higher temperature
settings. Notably, when hints are generated by GPT-4o, the most effective
prompts include prompts tailored to specific errors as well as prompts
providing general hints based on common mathematical errors. Interestingly,
Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.
Also the problem-solving and response revision capabilities of the LLMs as
students, particularly GPT-3.5-turbo, improved significantly after receiving
hints, especially at lower temperature settings. However, models like
Mistral-7B-Instruct demonstrated a decline in performance as the temperature
increased.",cs.AI
"LLM Generated Distribution-Based Prediction of US Electoral Results, Part I","This paper introduces distribution-based prediction, a novel approach to
using Large Language Models (LLMs) as predictive tools by interpreting output
token probabilities as distributions representing the models' learned
representation of the world. This distribution-based nature offers an
alternative perspective for analyzing algorithmic fidelity, complementing the
approach used in silicon sampling. We demonstrate the use of distribution-based
prediction in the context of recent United States presidential election,
showing that this method can be used to determine task specific bias, prompt
noise, and algorithmic fidelity. This approach has significant implications for
assessing the reliability and increasing transparency of LLM-based predictions
across various domains.",cs.AI
Watson: A Cognitive Observability Framework for the Reasoning of Foundation Model-Powered Agents,"As foundation models (FMs) play an increasingly prominent role in complex
software systems, such as FM-powered agentic software (i.e., Agentware), they
introduce significant challenges for developers regarding observability. Unlike
traditional software, agents operate autonomously, using extensive data and
opaque implicit reasoning, making it difficult to observe and understand their
behavior during runtime, especially when they take unexpected actions or
encounter errors. In this paper, we highlight the limitations of traditional
operational observability in the context of FM-powered software, and introduce
cognitive observability as a new type of required observability that has
emerged for such innovative systems. We then propose a novel framework that
provides cognitive observability into the implicit reasoning processes of
agents (a.k.a. reasoning observability), and demonstrate the effectiveness of
our framework in boosting the debuggability of Agentware and, in turn, the
abilities of an Agentware through a case study on AutoCodeRover, a cuttingedge
Agentware for autonomous program improvement.",cs.AI
Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.",cs.AI
Inference Optimal VLMs Need Only One Visual Token but Larger Models,"Vision Language Models (VLMs) have demonstrated strong capabilities across
various visual understanding and reasoning tasks. However, their real-world
deployment is often constrained by high latency during inference due to
substantial compute required to process the large number of input tokens
(predominantly from the image) by the LLM. To reduce inference costs, one can
either downsize the LLM or reduce the number of input image-tokens, the latter
of which has been the focus of many recent works around token compression.
However, it is unclear what the optimal trade-off is, as both the factors
directly affect the VLM performance. We first characterize this optimal
trade-off between the number of visual tokens and LLM parameters by
establishing scaling laws that capture variations in performance with these two
factors. Our results reveal a surprising trend: for visual reasoning tasks, the
inference-optimal behavior in VLMs, i.e., minimum downstream error at any given
fixed inference compute, is achieved when using the largest LLM that fits
within the inference budget while minimizing visual token count - often to a
single token. While the token reduction literature has mainly focused on
maintaining base model performance by modestly reducing the token count (e.g.,
$5-10\times$), our results indicate that the compute-optimal inference regime
requires operating under even higher token compression ratios. Based on these
insights, we take some initial steps towards building approaches tailored for
high token compression settings. Code is available at
https://github.com/locuslab/llava-token-compression.",cs.AI
STEER: Flexible Robotic Manipulation via Dense Language Grounding,"The complexity of the real world demands robotic systems that can
intelligently adapt to unseen situations. We present STEER, a robot learning
framework that bridges high-level, commonsense reasoning with precise, flexible
low-level control. Our approach translates complex situational awareness into
actionable low-level behavior through training language-grounded policies with
dense annotation. By structuring policy training around fundamental, modular
manipulation skills expressed in natural language, STEER exposes an expressive
interface for humans or Vision-Language Models (VLMs) to intelligently
orchestrate the robot's behavior by reasoning about the task and context. Our
experiments demonstrate the skills learned via STEER can be combined to
synthesize novel behaviors to adapt to new situations or perform completely new
tasks without additional data collection or training.",cs.AI
Neurons for Neutrons: A Transformer Model for Computation Load Estimation on Domain-Decomposed Neutron Transport Problems,"Domain decomposition is a technique used to reduce memory overhead on large
neutron transport problems. Currently, the optimal load-balanced processor
allocation for these domains is typically determined through small-scale
simulations of the problem, which can be time-consuming for researchers and
must be repeated anytime a problem input is changed. We propose a Transformer
model with a unique 3D input embedding, and input representations designed for
domain-decomposed neutron transport problems, which can predict the subdomain
computation loads generated by small-scale simulations. We demonstrate that
such a model trained on domain-decomposed Small Modular Reactor (SMR)
simulations achieves 98.2% accuracy while being able to skip the small-scale
simulation step entirely. Tests of the model's robustness on variant fuel
assemblies, other problem geometries, and changes in simulation parameters are
also discussed.",cs.AI
VERITAS: A Unified Approach to Reliability Evaluation,"Large language models (LLMs) often fail to synthesize information from their
context to generate an accurate response. This renders them unreliable in
knowledge intensive settings where reliability of the output is key. A critical
component for reliable LLMs is the integration of a robust fact-checking system
that can detect hallucinations across various formats. While several
open-access fact-checking models are available, their functionality is often
limited to specific tasks, such as grounded question-answering or entailment
verification, and they perform less effectively in conversational settings. On
the other hand, closed-access models like GPT-4 and Claude offer greater
flexibility across different contexts, including grounded dialogue
verification, but are hindered by high costs and latency. In this work, we
introduce VERITAS, a family of hallucination detection models designed to
operate flexibly across diverse contexts while minimizing latency and costs.
VERITAS achieves state-of-the-art results considering average performance on
all major hallucination detection benchmarks, with $10\%$ increase in average
performance when compared to similar-sized models and get close to the
performance of GPT4 turbo with LLM-as-a-judge setting.",cs.AI
Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning,"We propose an object-centric recovery policy framework to address the
challenges of out-of-distribution (OOD) scenarios in visuomotor policy
learning. Previous behavior cloning (BC) methods rely heavily on a large amount
of labeled data coverage, failing in unfamiliar spatial states. Without relying
on extra data collection, our approach learns a recovery policy constructed by
an inverse policy inferred from object keypoint manifold gradient in the
original training data. The recovery policy serves as a simple add-on to any
base visuomotor BC policy, agnostic to a specific method, guiding the system
back towards the training distribution to ensure task success even in OOD
situations. We demonstrate the effectiveness of our object-centric framework in
both simulation and real robot experiments, achieving an improvement of 77.7%
over the base policy in OOD. Project Website:
https://sites.google.com/view/ocr-penn",cs.AI
Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?,"Converting webpage design into functional UI code is a critical step for
building websites, which can be labor-intensive and time-consuming. To automate
this design-to-code transformation process, various automated methods using
learning-based networks and multi-modal large language models (MLLMs) have been
proposed. However, these studies were merely evaluated on a narrow range of
static web pages and ignored dynamic interaction elements, making them less
practical for real-world website deployment.
  To fill in the blank, we present the first systematic investigation of MLLMs
in generating interactive webpages. Specifically, we first formulate the
Interaction-to-Code task and build the Interaction2Code benchmark that contains
97 unique web pages and 213 distinct interactions, spanning 15 webpage types
and 30 interaction categories. We then conduct comprehensive experiments on
three state-of-the-art (SOTA) MLLMs using both automatic metrics and human
evaluations, thereby summarizing six findings accordingly. Our experimental
results highlight the limitations of MLLMs in generating fine-grained
interactive features and managing interactions with complex transformations and
subtle visual modifications. We further analyze failure cases and their
underlying causes, identifying 10 common failure types and assessing their
severity. Additionally, our findings reveal three critical influencing factors,
i.e., prompts, visual saliency, and textual descriptions, that can enhance the
interaction generation performance of MLLMs. Based on these findings, we elicit
implications for researchers and developers, providing a foundation for future
advancements in this field. Datasets and source code are available at
https://github.com/WebPAI/Interaction2Code.",cs.AI
The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare,"The potential use of large language models (LLMs) in healthcare robotics can
help address the significant demand put on healthcare systems around the world
with respect to an aging demographic and a shortage of healthcare
professionals. Even though LLMs have already been integrated into medicine to
assist both clinicians and patients, the integration of LLMs within healthcare
robots has not yet been explored for clinical settings. In this perspective
paper, we investigate the groundbreaking developments in robotics and LLMs to
uniquely identify the needed system requirements for designing health specific
LLM based robots in terms of multi modal communication through human robot
interactions (HRIs), semantic reasoning, and task planning. Furthermore, we
discuss the ethical issues, open challenges, and potential future research
directions for this emerging innovative field.",cs.AI
SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents,"While multi-agent systems have been shown to significantly enhance the
performance of Large Language Models (LLMs) across various tasks and
applications, the dense interaction between scaling agents potentially hampers
their efficiency and diversity. To address these challenges, we draw
inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse
mixture-of-agents (SMoA) framework to improve the efficiency and diversity of
multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel
Response Selection and Early Stopping mechanisms to sparsify information flows
among individual LLM agents, striking a balance between performance and
efficiency. Additionally, inspired by the expert diversity principle in SMoE
frameworks for workload balance between experts, we assign distinct role
descriptions to each LLM agent, fostering diverse and divergent thinking.
Extensive experiments on reasoning, alignment, and fairness benchmarks
demonstrate that SMoA achieves performance comparable to traditional
mixture-of-agents approaches but with significantly lower computational costs.
Further analysis reveals that SMoA is more stable, has a greater capacity to
scale, and offers considerable potential through hyper-parameter optimization.
Code and data will be available at: https://github.com/David-Li0406/SMoA.",cs.AI
Causal Responsibility Attribution for Human-AI Collaboration,"As Artificial Intelligence (AI) systems increasingly influence
decision-making across various fields, the need to attribute responsibility for
undesirable outcomes has become essential, though complicated by the complex
interplay between humans and AI. Existing attribution methods based on actual
causality and Shapley values tend to disproportionately blame agents who
contribute more to an outcome and rely on real-world measures of
blameworthiness that may misalign with responsible AI standards. This paper
presents a causal framework using Structural Causal Models (SCMs) to
systematically attribute responsibility in human-AI systems, measuring overall
blameworthiness while employing counterfactual reasoning to account for agents'
expected epistemic levels. Two case studies illustrate the framework's
adaptability in diverse human-AI collaboration scenarios.",cs.AI
An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services,"This article presents the design of an open-API-based explainable AI (XAI)
service to provide feature contribution explanations for cloud AI services.
Cloud AI services are widely used to develop domain-specific applications with
precise learning metrics. However, the underlying cloud AI services remain
opaque on how the model produces the prediction. We argue that XAI operations
are accessible as open APIs to enable the consolidation of the XAI operations
into the cloud AI services assessment. We propose a design using a microservice
architecture that offers feature contribution explanations for cloud AI
services without unfolding the network structure of the cloud models. We can
also utilize this architecture to evaluate the model performance and XAI
consistency metrics showing cloud AI services trustworthiness. We collect
provenance data from operational pipelines to enable reproducibility within the
XAI service. Furthermore, we present the discovery scenarios for the
experimental tests regarding model performance and XAI consistency metrics for
the leading cloud vision AI services. The results confirm that the
architecture, based on open APIs, is cloud-agnostic. Additionally, data
augmentations result in measurable improvements in XAI consistency metrics for
cloud AI services.",cs.AI
Discovering Data Structures: Nearest Neighbor Search and Beyond,"We propose a general framework for end-to-end learning of data structures.
Our framework adapts to the underlying data distribution and provides
fine-grained control over query and space complexity. Crucially, the data
structure is learned from scratch, and does not require careful initialization
or seeding with candidate data structures/algorithms. We first apply this
framework to the problem of nearest neighbor search. In several settings, we
are able to reverse-engineer the learned data structures and query algorithms.
For 1D nearest neighbor search, the model discovers optimal distribution
(in)dependent algorithms such as binary search and variants of interpolation
search. In higher dimensions, the model learns solutions that resemble k-d
trees in some regimes, while in others, they have elements of
locality-sensitive hashing. The model can also learn useful representations of
high-dimensional data and exploit them to design effective data structures. We
also adapt our framework to the problem of estimating frequencies over a data
stream, and believe it could also be a powerful discovery tool for new
problems.",cs.AI
Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities,"We study the emergence of agency from scratch by using Large Language Model
(LLM)-based agents. In previous studies of LLM-based agents, each agent's
characteristics, including personality and memory, have traditionally been
predefined. We focused on how individuality, such as behavior, personality, and
memory, can be differentiated from an undifferentiated state. The present LLM
agents engage in cooperative communication within a group simulation,
exchanging context-based messages in natural language. By analyzing this
multi-agent simulation, we report valuable new insights into how social norms,
cooperation, and personality traits can emerge spontaneously. This paper
demonstrates that autonomously interacting LLM-powered agents generate
hallucinations and hashtags to sustain communication, which, in turn, increases
the diversity of words within their interactions. Each agent's emotions shift
through communication, and as they form communities, the personalities of the
agents emerge and evolve accordingly. This computational modeling approach and
its findings will provide a new method for analyzing collective artificial
intelligence.",cs.AI
DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models,"Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2-7 percent in certain cases. The data and code
will be publicly available upon completion of internal review.",cs.AI
On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep Support Vector Data Description,"In this paper, we study the problem of promptly detecting the presence of
non-cooperative activity from one or more Reconfigurable Intelligent Surfaces
(RISs) with unknown characteristics lying in the vicinity of a Multiple-Input
Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division
Multiplexing (OFDM) transmissions. We first present a novel wideband channel
model incorporating RISs as well as non-reconfigurable stationary surfaces,
which captures both the effect of the RIS actuation time on the channel in the
frequency domain as well as the difference between changing phase
configurations during or among transmissions. Considering that RISs may operate
under the coordination of a third-party system, and thus, may negatively impact
the communication of the intended MIMO OFDM system, we present a novel RIS
activity detection framework that is unaware of the distribution of the phase
configuration of any of the non-cooperative RISs. In particular, capitalizing
on the knowledge of the data distribution at the multi-antenna receiver, we
design a novel online change point detection statistic that combines a deep
support vector data description model with the scan $B$-test. The presented
numerical investigations demonstrate the improved detection accuracy as well as
decreased computational complexity of the proposed RIS detection approach over
existing change point detection schemes.",cs.AI
Formal Logic-guided Robust Federated Learning against Poisoning Attacks,"Federated Learning (FL) offers a promising solution to the privacy concerns
associated with centralized Machine Learning (ML) by enabling decentralized,
collaborative learning. However, FL is vulnerable to various security threats,
including poisoning attacks, where adversarial clients manipulate the training
data or model updates to degrade overall model performance. Recognizing this
threat, researchers have focused on developing defense mechanisms to counteract
poisoning attacks in FL systems. However, existing robust FL methods
predominantly focus on computer vision tasks, leaving a gap in addressing the
unique challenges of FL with time series data. In this paper, we present
FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated
learning for time-series tasks, even in scenarios with heterogeneous client
data and a large number of adversarial participants. Unlike traditional
model-centric defenses, FLORAL leverages logical reasoning to evaluate client
trustworthiness by aligning their predictions with global time-series patterns,
rather than relying solely on the similarity of client updates. Our approach
extracts logical reasoning properties from clients, then hierarchically infers
global properties, and uses these to verify client updates. Through formal
logic verification, we assess the robustness of each client contribution,
identifying deviations indicative of adversarial behavior. Experimental results
on two datasets demonstrate the superior performance of our approach compared
to existing baseline methods, highlighting its potential to enhance the
robustness of FL to time series applications. Notably, FLORAL reduced the
prediction error by 93.27% in the best-case scenario compared to the
second-best baseline. Our code is available at
https://anonymous.4open.science/r/FLORAL-Robust-FTS.",cs.AI
Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI,"In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG",cs.AI
Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation,"Earth Observation (EO) data analysis has been significantly revolutionized by
deep learning (DL), with applications typically limited to grid-like data
structures. Graph Neural Networks (GNNs) emerge as an important innovation,
propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively
tackle the challenges posed by diverse modalities, multiple sensors, and the
heterogeneous nature of EO data. To introduce GNNs in the related domains, our
review begins by offering fundamental knowledge on GNNs. Then, we summarize the
generic problems in EO, to which GNNs can offer potential solutions. Following
this, we explore a broad spectrum of GNNs' applications to scientific problems
in Earth systems, covering areas such as weather and climate analysis, disaster
management, air quality monitoring, agriculture, land cover classification,
hydrological process modeling, and urban modeling. The rationale behind
adopting GNNs in these fields is explained, alongside methodologies for
organizing graphs and designing favorable architectures for various tasks.
Furthermore, we highlight methodological challenges of implementing GNNs in
these domains and possible solutions that could guide future research. While
acknowledging that GNNs are not a universal solution, we conclude the paper by
comparing them with other popular architectures like transformers and analyzing
their potential synergies.",cs.AI
GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis,"Recent advancements in Generative AI offer promising capabilities for spatial
analysis. Despite their potential, the integration of generative AI with
established GIS platforms remains underexplored. In this study, we propose a
framework for integrating LLMs directly into existing GIS platforms, using QGIS
as an example. Our approach leverages the reasoning and programming
capabilities of LLMs to autonomously generate spatial analysis workflows and
code through an informed agent that has comprehensive documentation of key GIS
tools and parameters. The implementation of this framework resulted in the
development of a ""GIS Copilot"" that allows GIS users to interact with QGIS
using natural language commands for spatial analysis. The GIS Copilot was
evaluated based on three complexity levels: basic tasks that require one GIS
tool and typically involve one data layer to perform simple operations;
intermediate tasks involving multi-step processes with multiple tools, guided
by user instructions; and advanced tasks which involve multi-step processes
that require multiple tools but not guided by user instructions, necessitating
the agent to independently decide on and executes the necessary steps. The
evaluation reveals that the GIS Copilot demonstrates strong potential in
automating foundational GIS operations, with a high success rate in tool
selection and code generation for basic and intermediate tasks, while
challenges remain in achieving full autonomy for more complex tasks. This study
contributes to the emerging vision of Autonomous GIS, providing a pathway for
non-experts to engage with geospatial analysis with minimal prior expertise.
While full autonomy is yet to be achieved, the GIS Copilot demonstrates
significant potential for simplifying GIS workflows and enhancing
decision-making processes.",cs.AI
On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models,"Large-scale training of latent diffusion models (LDMs) has enabled
unprecedented quality in image generation. However, the key components of the
best performing LDM training recipes are oftentimes not available to the
research community, preventing apple-to-apple comparisons and hindering the
validation of progress in the field. In this work, we perform an in-depth study
of LDM training recipes focusing on the performance of models and their
training efficiency. To ensure apple-to-apple comparisons, we re-implement five
previously published models with their corresponding recipes. Through our
study, we explore the effects of (i)~the mechanisms used to condition the
generative model on semantic information (e.g., text prompt) and control
metadata (e.g., crop size, random flip flag, etc.) on the model performance,
and (ii)~the transfer of the representations learned on smaller and
lower-resolution datasets to larger ones on the training efficiency and model
performance. We then propose a novel conditioning mechanism that disentangles
semantic and control metadata conditionings and sets a new state-of-the-art in
class-conditional generation on the ImageNet-1k dataset -- with FID
improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image
generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23%
on 512 resolution.",cs.AI
Navigating Extremes: Dynamic Sparsity in Large Output Space,"In recent years, Dynamic Sparse Training (DST) has emerged as an alternative
to post-training pruning for generating efficient models. In principle, DST
allows for a more memory efficient training process, as it maintains sparsity
throughout the entire training run. However, current DST implementations fail
to capitalize on this in practice. Because sparse matrix multiplication is much
less efficient than dense matrix multiplication on GPUs, most implementations
simulate sparsity by masking weights. In this paper, we leverage recent
advances in semi-structured sparse training to apply DST in the domain of
classification with large output spaces, where memory-efficiency is paramount.
With a label space of possibly millions of candidates, the classification layer
alone will consume several gigabytes of memory. Switching from a dense to a
fixed fan-in sparse layer updated with sparse evolutionary training (SET);
however, severely hampers training convergence, especially at the largest label
spaces. We find that poor gradient flow from the sparse classifier to the dense
text encoder make it difficult to learn good input representations. By
employing an intermediate layer or adding an auxiliary training objective, we
recover most of the generalisation performance of the dense model. Overall, we
demonstrate the applicability and practical benefits of DST in a challenging
domain -- characterized by a highly skewed label distribution that differs
substantially from typical DST benchmark datasets -- which enables end-to-end
training with millions of labels on commodity hardware.",cs.AI
Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care,"In clinical practice, decision-making relies heavily on established
protocols, often formalised as rules. Concurrently, Machine Learning (ML)
models, trained on clinical data, aspire to integrate into medical
decision-making processes. However, despite the growing number of ML
applications, their adoption into clinical practice remains limited. Two
critical concerns arise, relevant to the notions of consistency and continuity
of care: (a) accuracy - the ML model, albeit more accurate, might introduce
errors that would not have occurred by applying the protocol; (b)
interpretability - ML models operating as black boxes might make predictions
based on relationships that contradict established clinical knowledge. In this
context, the literature suggests using ML models integrating domain knowledge
for improved accuracy and interpretability. However, there is a lack of
appropriate metrics for comparing ML models with clinical rules in addressing
these challenges. Accordingly, in this article, we first propose metrics to
assess the accuracy of ML models with respect to the established protocol.
Secondly, we propose an approach to measure the distance of explanations
provided by two rule sets, with the goal of comparing the explanation
similarity between clinical rule-based systems and rules extracted from ML
models. The approach is validated on the Pima Indians Diabetes dataset by
training two neural networks - one exclusively on data, and the other
integrating a clinical protocol. Our findings demonstrate that the integrated
ML model achieves comparable performance to that of a fully data-driven model
while exhibiting superior accuracy relative to the clinical protocol, ensuring
enhanced continuity of care. Furthermore, we show that our integrated model
provides explanations for predictions that align more closely with the clinical
protocol compared to the data-driven model.",cs.AI
Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting,"Limited medical imaging datasets challenge deep learning models by increasing
risks of overfitting and reduced generalization, particularly in Generative
Adversarial Networks (GANs), where discriminators may overfit, leading to
training divergence. This constraint also impairs classification models trained
on small datasets. Generative Data Augmentation (GDA) addresses this by
expanding training datasets with synthetic data, although it requires training
a generative model. We propose and evaluate two local lesion generation
approaches to address the challenge of augmenting small medical image datasets.
The first approach employs the Poisson Image Editing algorithm, a classical
image processing technique, to create realistic image composites that
outperform current state-of-the-art methods. The second approach introduces a
novel generative method, leveraging a fine-tuned Image Inpainting GAN to
synthesize realistic lesions within specified regions of real training images.
A comprehensive comparison of the two proposed methods demonstrates that
effective local lesion generation in a data-constrained setting allows for
reaching new state-of-the-art results in capsule endoscopy lesion
classification. Combination of our techniques achieves a macro F1-score of
33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on
the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule
endoscopy. To the best of our knowledge, this work is the first to apply a
fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that
an image-conditional GAN can be adapted effectively to limited datasets to
generate high-quality examples, facilitating effective data augmentation.
Additionally, we show that combining this GAN-based approach with classical
image processing techniques further enhances the results.",cs.AI
HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features,"Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.",cs.AI
Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data,"This paper shows how an uncertainty-aware, deep neural network can be trained
to detect, recognise and localise objects in 2D RGB images, in applications
lacking annotated train-ng datasets. We propose a self-supervising
teacher-student pipeline, in which a relatively simple teacher classifier,
trained with only a few labelled 2D thumbnails, automatically processes a
larger body of unlabelled RGB-D data to teach a student network based on a
modified YOLOv3 architecture. Firstly, 3D object detection with back projection
is used to automatically extract and teach 2D detection and localisation
information to the student network. Secondly, a weakly supervised 2D thumbnail
classifier, with minimal training on a small number of hand-labelled images, is
used to teach object category recognition. Thirdly, we use a Gaussian Process
GP to encode and teach a robust uncertainty estimation functionality, so that
the student can output confidence scores with each categorization. The
resulting student significantly outperforms the same YOLO architecture trained
directly on the same amount of labelled data. Our GP-based approach yields
robust and meaningful uncertainty estimations for complex industrial object
classifications. The end-to-end network is also capable of real-time
processing, needed for robotics applications. Our method can be applied to many
important industrial tasks, where labelled datasets are typically unavailable.
In this paper, we demonstrate an example of detection, localisation, and object
category recognition of nuclear mixed-waste materials in highly cluttered and
unstructured scenes. This is critical for robotic sorting and handling of
legacy nuclear waste, which poses complex environmental remediation challenges
in many nuclearised nations.",cs.AI
Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight,"In the domain of deep learning, the challenge of protecting sensitive data
while maintaining model utility is significant. Traditional Differential
Privacy (DP) techniques such as Differentially Private Stochastic Gradient
Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive
gradient clipping. These methods, however, compromise model accuracy due to
their critical influence on gradient handling, particularly neglecting the
significant contribution of small gradients during later training stages. In
this paper, we introduce an enhanced version of DP-SGD, named Differentially
Private Per-sample Adaptive Scaling Clipping (DP-PSASC). This approach replaces
traditional clipping with non-monotonous adaptive gradient scaling, which
alleviates the need for intensive threshold setting and rectifies the
disproportionate weighting of smaller gradients. Our contribution is twofold.
First, we develop a novel gradient scaling technique that effectively assigns
proper weights to gradients, particularly small ones, thus improving learning
under differential privacy. Second, we integrate a momentum-based method into
DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates.
Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy
and delivers superior performance across diverse datasets, setting new
standards for privacy-sensitive applications.",cs.AI
ATM: Improving Model Merging by Alternating Tuning and Merging,"Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, task vectors are mathematically equivalent to the
gradients obtained via gradient descent in a multi-task setting, and still
approximate these gradients in subsequent epochs. Furthermore, we show that
task vectors perform optimally when equality is maintained, and their
effectiveness is largely driven by the first epoch's gradient. Building on this
insight, we propose viewing model merging as a single step in an iterative
process that Alternates between Tuning and Merging (ATM). This method acts as a
bridge between model merging and multi-task gradient descent, achieving
state-of-the-art results with the same data and computational requirements. We
extensively evaluate ATM across diverse settings, achieving up to 20% higher
accuracy in computer vision and NLP tasks, compared to the best baselines.
Finally, we provide both empirical and theoretical support for its
effectiveness, demonstrating increased orthogonality between task vectors and
proving that ATM minimizes an upper bound on the loss obtained by jointly
finetuning all tasks.",cs.AI
Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising,"We investigate the construction of gradient-guided conditional diffusion
models for reconstructing private images, focusing on the adversarial interplay
between differential privacy noise and the denoising capabilities of diffusion
models. While current gradient-based reconstruction methods struggle with
high-resolution images due to computational complexity and prior knowledge
requirements, we propose two novel methods that require minimal modifications
to the diffusion model's generation process and eliminate the need for prior
knowledge. Our approach leverages the strong image generation capabilities of
diffusion models to reconstruct private images starting from randomly generated
noise, even when a small amount of differentially private noise has been added
to the gradients. We also conduct a comprehensive theoretical analysis of the
impact of differential privacy noise on the quality of reconstructed images,
revealing the relationship among noise magnitude, the architecture of attacked
models, and the attacker's reconstruction capability. Additionally, extensive
experiments validate the effectiveness of our proposed methods and the accuracy
of our theoretical findings, suggesting new directions for privacy risk
auditing using conditional diffusion models.",cs.AI
HumanVLM: Foundation for Human-Scene Vision-Language Model,"Human-scene vision-language tasks are increasingly prevalent in diverse
social applications, yet recent advancements predominantly rely on models
specifically tailored to individual tasks. Emerging research indicates that
large vision-language models (VLMs) can enhance performance across various
downstream vision-language understanding tasks. However, general-domain models
often underperform in specialized fields. This study introduces a
domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model
(HumanVLM), designed to provide a foundation for human-scene Vision-Language
tasks. Specifically, (1) we create a large-scale human-scene multimodal
image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate
domain-specific alignment; (2) develop a captioning approach for human-centered
images, capturing human faces, bodies, and backgrounds, and construct a
high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs)
that contain as much detailed information as possible about human; (3) Using
HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments,
we then evaluate our HumanVLM across varous downstream tasks, where it
demonstrates superior overall performance among multimodal models of comparable
scale, particularly excelling in human-related tasks and significantly
outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM,
alongside the data introduced, will stimulate the research in human-around
fields.",cs.AI
Adaptive Genetic Selection based Pinning Control with Asymmetric Coupling for Multi-Network Heterogeneous Vehicular Systems,"To alleviate computational load on RSUs and cloud platforms, reduce
communication bandwidth requirements, and provide a more stable vehicular
network service, this paper proposes an optimized pinning control approach for
heterogeneous multi-network vehicular ad-hoc networks (VANETs). In such
networks, vehicles participate in multiple task-specific networks with
asymmetric coupling and dynamic topologies. We first establish a rigorous
theoretical foundation by proving the stability of pinning control strategies
under both single and multi-network conditions, deriving sufficient stability
conditions using Lyapunov theory and linear matrix inequalities (LMIs).
Building on this theoretical groundwork, we propose an adaptive genetic
algorithm tailored to select optimal pinning nodes, effectively balancing LMI
constraints while prioritizing overlapping nodes to enhance control efficiency.
Extensive simulations across various network scales demonstrate that our
approach achieves rapid consensus with a reduced number of control nodes,
particularly when leveraging network overlaps. This work provides a
comprehensive solution for efficient control node selection in complex
vehicular networks, offering practical implications for deploying large-scale
intelligent transportation systems.",cs.AI
DA-MoE: Addressing Depth-Sensitivity in Graph-Level Analysis through Mixture of Experts,"Graph neural networks (GNNs) are gaining popularity for processing
graph-structured data. In real-world scenarios, graph data within the same
dataset can vary significantly in scale. This variability leads to
depth-sensitivity, where the optimal depth of GNN layers depends on the scale
of the graph data. Empirically, fewer layers are sufficient for message passing
in smaller graphs, while larger graphs typically require deeper networks to
capture long-range dependencies and global features. However, existing methods
generally use a fixed number of GNN layers to generate representations for all
graphs, overlooking the depth-sensitivity issue in graph structure data. To
address this challenge, we propose the depth adaptive mixture of expert
(DA-MoE) method, which incorporates two main improvements to GNN backbone:
\textbf{1)} DA-MoE employs different GNN layers, each considered an expert with
its own parameters. Such a design allows the model to flexibly aggregate
information at different scales, effectively addressing the depth-sensitivity
issue in graph data. \textbf{2)} DA-MoE utilizes GNN to capture the structural
information instead of the linear projections in the gating network. Thus, the
gating network enables the model to capture complex patterns and dependencies
within the data. By leveraging these improvements, each expert in DA-MoE
specifically learns distinct graph patterns at different scales. Furthermore,
comprehensive experiments on the TU dataset and open graph benchmark (OGB) have
shown that DA-MoE consistently surpasses existing baselines on various tasks,
including graph, node, and link-level analyses. The code are available at
\url{https://github.com/Celin-Yao/DA-MoE}.",cs.AI
Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras,"While security vulnerabilities in traditional Deep Neural Networks (DNNs)
have been extensively studied, the susceptibility of Spiking Neural Networks
(SNNs) to adversarial attacks remains mostly underexplored. Until now, the
mechanisms to inject backdoors into SNN models have been limited to digital
scenarios; thus, we present the first evaluation of backdoor attacks in
real-world environments.
  We begin by assessing the applicability of existing digital backdoor attacks
and identifying their limitations for deployment in physical environments. To
address each of the found limitations, we present three novel backdoor attack
methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess
the effectiveness of traditional backdoor procedures and defenses adapted for
SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that
while these procedures and defenses can mitigate some attacks, they often fail
against stronger methods like Flashy Backdoor or sacrifice too much clean
accuracy, rendering the models unusable.
  Overall, all our methods can achieve up to a 100% Attack Success Rate while
maintaining high clean accuracy in every tested dataset. Additionally, we
evaluate the stealthiness of the triggers with commonly used metrics, finding
them highly stealthy. Thus, we propose new alternatives more suited for
identifying poisoned samples in these scenarios. Our results show that further
research is needed to ensure the security of SNN-based systems against backdoor
attacks and their safe application in real-world scenarios. The code,
experiments, and results are available in our repository.",cs.AI
Leveraging Large Language Models in Code Question Answering: Baselines and Issues,"Question answering over source code provides software engineers and project
managers with helpful information about the implemented features of a software
product. This paper presents a work devoted to using large language models for
question answering over source code in Python. The proposed method for
implementing a source code question answering system involves fine-tuning a
large language model on a unified dataset of questions and answers for Python
code. To achieve the highest quality answers, we tested various models trained
on datasets preprocessed in different ways: a dataset without grammar
correction, a dataset with grammar correction, and a dataset augmented with the
generated summaries. The model answers were also analyzed for errors manually.
We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along
with the conclusions from the manual error analysis. The obtained experimental
results highlight the current problems of the research area, such as poor
quality of the public genuine question-answering datasets. In addition, the
findings include the positive effect of the grammar correction of the training
data on the testing metric values. The addressed findings and issues could be
important for other researchers who attempt to improve the quality of source
code question answering solutions. The training and evaluation code is publicly
available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.",cs.AI
Hierarchical Orchestra of Policies,"Continual reinforcement learning poses a major challenge due to the tendency
of agents to experience catastrophic forgetting when learning sequential tasks.
In this paper, we introduce a modularity-based approach, called Hierarchical
Orchestra of Policies (HOP), designed to mitigate catastrophic forgetting in
lifelong reinforcement learning. HOP dynamically forms a hierarchy of policies
based on a similarity metric between the current observations and previously
encountered observations in successful tasks. Unlike other state-of-the-art
methods, HOP does not require task labelling, allowing for robust adaptation in
environments where boundaries between tasks are ambiguous. Our experiments,
conducted across multiple tasks in a procedurally generated suite of
environments, demonstrate that HOP significantly outperforms baseline methods
in retaining knowledge across tasks and performs comparably to state-of-the-art
transfer methods that require task labelling. Moreover, HOP achieves this
without compromising performance when tasks remain constant, highlighting its
versatility.",cs.AI
Data Quality Awareness: A Journey from Traditional Data Management to Data Science Systems,"Artificial intelligence (AI) has transformed various fields, significantly
impacting our daily lives. A major factor in AI success is high-quality data.
In this paper, we present a comprehensive review of the evolution of data
quality (DQ) awareness from traditional data management systems to modern
data-driven AI systems, which are integral to data science. We synthesize the
existing literature, highlighting the quality challenges and techniques that
have evolved from traditional data management to data science including big
data and ML fields. As data science systems support a wide range of activities,
our focus in this paper lies specifically in the analytics aspect driven by
machine learning. We use the cause-effect connection between the quality
challenges of ML and those of big data to allow a more thorough understanding
of emerging DQ challenges and the related quality awareness techniques in data
science systems. To the best of our knowledge, our paper is the first to
provide a review of DQ awareness spanning traditional and emergent data science
systems. We hope that readers will find this journey through the evolution of
data quality awareness insightful and valuable.",cs.AI
Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status,"Causal understanding is a fundamental goal of evidence-based medicine. When
randomization is impossible, causal inference methods allow the estimation of
treatment effects from retrospective analysis of observational data. However,
such analyses rely on a number of assumptions, often including that of no
unobserved confounding. In many practical settings, this assumption is violated
when important variables are not explicitly measured in the clinical record.
Prior work has proposed to address unobserved confounding with machine learning
by imputing unobserved variables and then correcting for the classifier's
mismeasurement. When such a classifier can be trained and the necessary
assumptions are met, this method can recover an unbiased estimate of a causal
effect. However, such work has been limited to synthetic data, simple
classifiers, and binary variables. This paper extends this methodology by using
a large language model trained on clinical notes to predict patients' smoking
status, which would otherwise be an unobserved confounder. We then apply a
measurement error correction on the categorical predicted smoking status to
estimate the causal effect of transthoracic echocardiography on mortality in
the MIMIC dataset.",cs.AI
Accelerating Task Generalisation with Multi-Level Hierarchical Options,"Creating reinforcement learning agents that generalise effectively to new
tasks is a key challenge in AI research. This paper introduces Fracture Cluster
Options (FraCOs), a multi-level hierarchical reinforcement learning method that
achieves state-of-the-art performance on difficult generalisation tasks. FraCOs
identifies patterns in agent behaviour and forms options based on the expected
future usefulness of those patterns, enabling rapid adaptation to new tasks. In
tabular settings, FraCOs demonstrates effective transfer and improves
performance as it grows in hierarchical depth. We evaluate FraCOs against
state-of-the-art deep reinforcement learning algorithms in several complex
procedurally generated environments. Our results show that FraCOs achieves
higher in-distribution and out-of-distribution performance than competitors.",cs.AI
SUDS: A Strategy for Unsupervised Drift Sampling,"Supervised machine learning often encounters concept drift, where the data
distribution changes over time, degrading model performance. Existing drift
detection methods focus on identifying these shifts but often overlook the
challenge of acquiring labeled data for model retraining after a shift occurs.
We present the Strategy for Drift Sampling (SUDS), a novel method that selects
homogeneous samples for retraining using existing drift detection algorithms,
thereby enhancing model adaptability to evolving data. SUDS seamlessly
integrates with current drift detection techniques. We also introduce the
Harmonized Annotated Data Accuracy Metric (HADAM), a metric that evaluates
classifier performance in relation to the quantity of annotated data required
to achieve the stated performance, thereby taking into account the difficulty
of acquiring labeled data. Our contributions are twofold: SUDS combines drift
detection with strategic sampling to improve the retraining process, and HADAM
provides a metric that balances classifier performance with the amount of
labeled data, ensuring efficient resource utilization. Empirical results
demonstrate the efficacy of SUDS in optimizing labeled data use in dynamic
environments, significantly improving the performance of machine learning
applications in real-world scenarios. Our code is open source and available at
https://github.com/cfellicious/SUDS/",cs.AI
Confidence Calibration of Classifiers with Many Classes,"For classification models based on neural networks, the maximum predicted
class probability is often used as a confidence score. This score rarely
predicts well the probability of making a correct prediction and requires a
post-processing calibration step. However, many confidence calibration methods
fail for problems with many classes. To address this issue, we transform the
problem of calibrating a multiclass classifier into calibrating a single
surrogate binary classifier. This approach allows for more efficient use of
standard calibration methods. We evaluate our approach on numerous neural
networks used for image or text classification and show that it significantly
enhances existing calibration methods.",cs.AI
Autonomous Decision Making for UAV Cooperative Pursuit-Evasion Game with Reinforcement Learning,"The application of intelligent decision-making in unmanned aerial vehicle
(UAV) is increasing, and with the development of UAV 1v1 pursuit-evasion game,
multi-UAV cooperative game has emerged as a new challenge. This paper proposes
a deep reinforcement learning-based model for decision-making in multi-role UAV
cooperative pursuit-evasion game, to address the challenge of enabling UAV to
autonomously make decisions in complex game environments. In order to enhance
the training efficiency of the reinforcement learning algorithm in UAV
pursuit-evasion game environment that has high-dimensional state-action space,
this paper proposes multi-environment asynchronous double deep Q-network with
priority experience replay algorithm to effectively train the UAV's game
policy. Furthermore, aiming to improve cooperation ability and task completion
efficiency, as well as minimize the cost of UAVs in the pursuit-evasion game,
this paper focuses on the allocation of roles and targets within multi-UAV
environment. The cooperative game decision model with varying numbers of UAVs
are obtained by assigning diverse tasks and roles to the UAVs in different
scenarios. The simulation results demonstrate that the proposed method enables
autonomous decision-making of the UAVs in pursuit-evasion game scenarios and
exhibits significant capabilities in cooperation.",cs.AI
Region-Guided Attack on the Segment Anything Model (SAM),"The Segment Anything Model (SAM) is a cornerstone of image segmentation,
demonstrating exceptional performance across various applications, particularly
in autonomous driving and medical imaging, where precise segmentation is
crucial. However, SAM is vulnerable to adversarial attacks that can
significantly impair its functionality through minor input perturbations.
Traditional techniques, such as FGSM and PGD, are often ineffective in
segmentation tasks due to their reliance on global perturbations that overlook
spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address
these challenges, but they frequently depend on external cues and do not fully
leverage the structural interdependencies within segmentation processes. This
limitation underscores the need for a novel adversarial strategy that exploits
the unique characteristics of segmentation tasks. In response, we introduce the
Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a
Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted
perturbations that fragment large segments and expand smaller ones, resulting
in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves
high success rates in both white-box and black-box scenarios, emphasizing the
need for robust defenses against such sophisticated attacks. RGA not only
reveals SAM's vulnerabilities but also lays the groundwork for developing more
resilient defenses against adversarial threats in image segmentation.",cs.AI
[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI,"We present an outline of the first large language model (LLM) based chatbot
application in the context of patient-reported outcome measures (PROMs) for
diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable
patients to provide feedback about their quality of life and treatment progress
via an interactive application. The proposed framework offers significant
advantages over the current approach, which encompasses only qualitative
collection of survey data or a static survey with limited answer options. Using
the PROBot LLM-PROM application, patients will be asked tailored questions
about their individual challenges, and can give more detailed feedback on the
progress of their treatment. Based on this input, we will use machine learning
to infer conventional PROM scores, which can be used by clinicians to evaluate
the treatment status. The goal of the application is to improve adherence to
the healthcare system and treatments, and thus ultimately reduce cases of
subsequent vision impairment. The approach needs to be further validated using
a survey and a clinical study.",cs.AI
Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT,"Speech is the most natural way of expressing ourselves as humans. Identifying
emotion from speech is a nontrivial task due to the ambiguous definition of
emotion itself. Speaker Emotion Recognition (SER) is essential for
understanding human emotional behavior. The SER task is challenging due to the
variety of speakers, background noise, complexity of emotions, and speaking
styles. It has many applications in education, healthcare, customer service,
and Human-Computer Interaction (HCI). Previously, conventional machine learning
methods such as SVM, HMM, and KNN have been used for the SER task. In recent
years, deep learning methods have become popular, with convolutional neural
networks and recurrent neural networks being used for SER tasks. The input of
these methods is mostly spectrograms and hand-crafted features. In this work,
we study the use of self-supervised transformer-based models, Wav2Vec2 and
HuBERT, to determine the emotion of speakers from their voice. The models
automatically extract features from raw audio signals, which are then used for
the classification task. The proposed solution is evaluated on reputable
datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show
the effectiveness of the proposed method on different datasets. Moreover, the
model has been used for real-world applications like call center conversations,
and the results demonstrate that the model accurately predicts emotions.",cs.AI
Leveraging Transfer Learning and Multiple Instance Learning for HER2 Automatic Scoring of H\&E Whole Slide Images,"Expression of human epidermal growth factor receptor 2 (HER2) is an important
biomarker in breast cancer patients who can benefit from cost-effective
automatic Hematoxylin and Eosin (H\&E) HER2 scoring. However, developing such
scoring models requires large pixel-level annotated datasets. Transfer learning
allows prior knowledge from different datasets to be reused while
multiple-instance learning (MIL) allows the lack of detailed annotations to be
mitigated. The aim of this work is to examine the potential of transfer
learning on the performance of deep learning models pre-trained on (i)
Immunohistochemistry (IHC) images, (ii) H\&E images and (iii) non-medical
images. A MIL framework with an attention mechanism is developed using
pre-trained models as patch-embedding models. It was found that embedding
models pre-trained on H\&E images consistently outperformed the others,
resulting in an average AUC-ROC value of $0.622$ across the 4 HER2 scores
($0.59-0.80$ per HER2 score). Furthermore, it was found that using
multiple-instance learning with an attention layer not only allows for good
classification results to be achieved, but it can also help with producing
visual indication of HER2-positive areas in the H\&E slide image by utilising
the patch-wise attention weights.",cs.AI
A Mamba Foundation Model for Time Series Forecasting,"Time series foundation models have demonstrated strong performance in
zero-shot learning, making them well-suited for predicting rapidly evolving
patterns in real-world applications where relevant training data are scarce.
However, most of these models rely on the Transformer architecture, which
incurs quadratic complexity as input length increases. To address this, we
introduce TSMamba, a linear-complexity foundation model for time series
forecasting built on the Mamba architecture. The model captures temporal
dependencies through both forward and backward Mamba encoders, achieving high
prediction accuracy. To reduce reliance on large datasets and lower training
costs, TSMamba employs a two-stage transfer learning process that leverages
pretrained Mamba LLMs, allowing effective time series modeling with a moderate
training set. In the first stage, the forward and backward backbones are
optimized via patch-wise autoregressive prediction; in the second stage, the
model trains a prediction head and refines other components for long-term
forecasting. While the backbone assumes channel independence to manage varying
channel numbers across datasets, a channel-wise compressed attention module is
introduced to capture cross-channel dependencies during fine-tuning on specific
multivariate datasets. Experiments show that TSMamba's zero-shot performance is
comparable to state-of-the-art time series foundation models, despite using
significantly less training data. It also achieves competitive or superior
full-shot performance compared to task-specific prediction models. The code
will be made publicly available.",cs.AI
A Post-Training Enhanced Optimization Approach for Small Language Models,"This paper delves into the continuous post-training optimization methods for
small language models, and proposes a continuous post-training alignment data
construction method for small language models. The core of this method is based
on the data guidance of large models, optimizing the diversity and accuracy of
alignment data. In addition, to verify the effectiveness of the methods in this
paper, we used Qwen2-0.5B-Instruct model as the baseline model for small
language models, using the alignment dataset constructed by our proposed
method, we trained and compared several groups of experiments, including SFT
(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky
optimization) post-training experiment, as well as SFT-KTO two-stage
post-training experiment and model weight fusion experiment. Finally, we
evaluated and analyzed the performance of post-training models, and confirmed
that the continuous post-training optimization method proposed by us can
significantly improve the performance of small language models.",cs.AI
Textual Aesthetics in Large Language Models,"Image aesthetics is a crucial metric in the field of image generation.
However, textual aesthetics has not been sufficiently explored. With the
widespread application of large language models (LLMs), previous work has
primarily focused on the correctness of content and the helpfulness of
responses. Nonetheless, providing responses with textual aesthetics is also an
important factor for LLMs, which can offer a cleaner layout and ensure greater
consistency and coherence in content. In this work, we introduce a pipeline for
aesthetics polishing and help construct a textual aesthetics dataset named
TexAes. We propose a textual aesthetics-powered fine-tuning method based on
direct preference optimization, termed TAPO, which leverages textual aesthetics
without compromising content correctness. Additionally, we develop two
evaluation methods for textual aesthetics based on text and image analysis,
respectively. Our experiments demonstrate that using textual aesthetics data
and employing the TAPO fine-tuning method not only improves aesthetic scores
but also enhances performance on general evaluation datasets such as
AlpacalEval and Anera-hard.",cs.AI
Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization,"Open-set single-source domain generalization aims to use a single-source
domain to learn a robust model that can be generalized to unknown target
domains with both domain shifts and label shifts. The scarcity of the source
domain and the unknown data distribution of the target domain pose a great
challenge for domain-invariant feature learning and unknown class recognition.
In this paper, we propose a novel learning approach based on domain expansion
and boundary growth to expand the scarce source samples and enlarge the
boundaries across the known classes that indirectly broaden the boundary
between the known and unknown classes. Specifically, we achieve domain
expansion by employing both background suppression and style augmentation on
the source data to synthesize new samples. Then we force the model to distill
consistent knowledge from the synthesized samples so that the model can learn
domain-invariant information. Furthermore, we realize boundary growth across
classes by using edge maps as an additional modality of samples when training
multi-binary classifiers. In this way, it enlarges the boundary between the
inliers and outliers, and consequently improves the unknown class recognition
during open-set generalization. Extensive experiments show that our approach
can achieve significant improvements and reach state-of-the-art performance on
several cross-domain image classification datasets.",cs.AI
Exploring the Interplay Between Video Generation and World Models in Autonomous Driving: A Survey,"World models and video generation are pivotal technologies in the domain of
autonomous driving, each playing a critical role in enhancing the robustness
and reliability of autonomous systems. World models, which simulate the
dynamics of real-world environments, and video generation models, which produce
realistic video sequences, are increasingly being integrated to improve
situational awareness and decision-making capabilities in autonomous vehicles.
This paper investigates the relationship between these two technologies,
focusing on how their structural parallels, particularly in diffusion-based
models, contribute to more accurate and coherent simulations of driving
scenarios. We examine leading works such as JEPA, Genie, and Sora, which
exemplify different approaches to world model design, thereby highlighting the
lack of a universally accepted definition of world models. These diverse
interpretations underscore the field's evolving understanding of how world
models can be optimized for various autonomous driving tasks. Furthermore, this
paper discusses the key evaluation metrics employed in this domain, such as
Chamfer distance for 3D scene reconstruction and Fr\'echet Inception Distance
(FID) for assessing the quality of generated video content. By analyzing the
interplay between video generation and world models, this survey identifies
critical challenges and future research directions, emphasizing the potential
of these technologies to jointly advance the performance of autonomous driving
systems. The findings presented in this paper aim to provide a comprehensive
understanding of how the integration of video generation and world models can
drive innovation in the development of safer and more reliable autonomous
vehicles.",cs.AI
Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.",cs.AI
TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection,"With the development of large language models (LLMs), the ability to handle
longer contexts has become a key capability for Web applications such as
cross-document understanding and LLM-powered search systems. However, this
progress faces two major challenges: performance degradation due to sequence
lengths out-of-distribution, and excessively long inference times caused by the
quadratic computational complexity of attention. These issues hinder the
application of LLMs in long-context scenarios. In this paper, we propose
Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,
training-free method for efficient and accurate long-context inference.
TokenSelect builds upon the observation of non-contiguous attention sparsity,
using Query-Key dot products to measure per-head KV Cache criticality at
token-level. By per-head soft voting mechanism, TokenSelect selectively
involves a small number of critical KV cache tokens in the attention
calculation without sacrificing accuracy. To further accelerate TokenSelect, we
designed the Selection Cache based on observations of consecutive Query
similarity and implemented efficient dot product kernel, significantly reducing
the overhead of token selection. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.",cs.AI
AtlasSeg: Atlas Prior Guided Dual-U-Net for Cortical Segmentation in Fetal Brain MRI,"Accurate tissue segmentation in fetal brain MRI remains challenging due to
the dynamically changing anatomical anatomy and contrast during fetal
development. To enhance segmentation accuracy throughout gestation, we
introduced AtlasSeg, a dual-U-shape convolution network incorporating
gestational age (GA) specific information as guidance. By providing a publicly
available fetal brain atlas with segmentation label at the corresponding GA,
AtlasSeg effectively extracted the contextual features of age-specific patterns
in atlas branch and generated tissue segmentation in segmentation branch.
Multi-scale attentive atlas feature fusions were constructed in all stages
during encoding and decoding, giving rise to a dual-U-shape network to assist
feature flow and information interactions between two branches. AtlasSeg
outperformed six well-known segmentation networks in both our internal fetal
brain MRI dataset and the external FeTA dataset. Ablation experiments
demonstrate the efficiency of atlas guidance and the attention mechanism. The
proposed AtlasSeg demonstrated superior segmentation performance against other
convolution networks with higher segmentation accuracy, and may facilitate
fetal brain MRI analysis in large-scale fetal brain studies.",cs.AI
Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning,"Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug"" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
""ensemble-play"", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.",cs.AI
Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis,"In the rapidly evolving landscape of 5G technology, safeguarding Radio
Frequency (RF) environments against sophisticated intrusions is paramount,
especially in dynamic spectrum access and management. This paper presents an
enhanced experimental model that integrates a self-attention mechanism with a
Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous
spectral activities in 5G networks at the waveform level. Our approach,
grounded in time-series analysis, processes in-phase and quadrature (I/Q)
samples to identify irregularities that could indicate potential jamming
attacks. The model's architecture, augmented with a self-attention layer,
extends the capabilities of RNN autoencoders, enabling a more nuanced
understanding of temporal dependencies and contextual relationships within the
RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed
constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a
comprehensive stream of data that reflects real-world RF spectrum conditions
and attack scenarios. The model is trained to reconstruct standard signal
behavior, establishing a normative baseline against which deviations,
indicative of security threats, are identified. The proposed architecture is
designed to balance between detection precision and computational efficiency,
so the LSTM network, enriched with self-attention, continues to optimize for
minimal execution latency and power consumption. Conducted on a real-world
SDR-based testbed, our results demonstrate the model's improved performance and
accuracy in threat detection.
  Keywords: self-attention, real-time intrusion detection, RNN autoencoder,
Transformer architecture, LSTM, time series anomaly detection, 5G Security,
spectrum access security.",cs.AI
"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.",cs.AI
"WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African clean water access, sanitation and hygiene","This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.",cs.AI
Dissecting the Failure of Invariant Learning on Graphs,"Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs
remains a crucial area of research. In this paper, we develop a Structural
Causal Model (SCM) to theoretically dissect the performance of two prominent
invariant learning methods -- Invariant Risk Minimization (IRM) and
Variance-Risk Extrapolation (VREx) -- in node-level OOD settings. Our analysis
reveals a critical limitation: due to the lack of class-conditional invariance
constraints, these methods may struggle to accurately identify the structure of
the predictive invariant ego-graph and consequently rely on spurious features.
To address this, we propose Cross-environment Intra-class Alignment (CIA),
which explicitly eliminates spurious features by aligning cross-environment
representations conditioned on the same class, bypassing the need for explicit
knowledge of the causal pattern structure. To adapt CIA to node-level OOD
scenarios where environment labels are hard to obtain, we further propose
CIA-LRA (Localized Reweighting Alignment) that leverages the distribution of
neighboring labels to selectively align node representations, effectively
distinguishing and preserving invariant features while removing spurious ones,
all without relying on environment labels. We theoretically prove CIA-LRA's
effectiveness by deriving an OOD generalization error bound based on
PAC-Bayesian analysis. Experiments on graph OOD benchmarks validate the
superiority of CIA and CIA-LRA, marking a significant advancement in node-level
OOD generalization. The codes are available at
https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs.",cs.AI
Correlation of Object Detection Performance with Visual Saliency and Depth Estimation,"As object detection techniques continue to evolve, understanding their
relationships with complementary visual tasks becomes crucial for optimising
model architectures and computational resources. This paper investigates the
correlations between object detection accuracy and two fundamental visual
tasks: depth prediction and visual saliency prediction. Through comprehensive
experiments using state-of-the-art models (DeepGaze IIE, Depth Anything,
DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that
visual saliency shows consistently stronger correlations with object detection
accuracy (mA$\rho$ up to 0.459 on Pascal VOC) compared to depth prediction
(mA$\rho$ up to 0.283). Our analysis reveals significant variations in these
correlations across object categories, with larger objects showing correlation
values up to three times higher than smaller objects. These findings suggest
incorporating visual saliency features into object detection architectures
could be more beneficial than depth information, particularly for specific
object categories. The observed category-specific variations also provide
insights for targeted feature engineering and dataset design improvements,
potentially leading to more efficient and accurate object detection systems.",cs.AI
PersianRAG: A Retrieval-Augmented Generation System for Persian Language,"Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.",cs.AI
Mixtures of In-Context Learners,"In-context learning (ICL) adapts LLMs by providing demonstrations without
fine-tuning the model parameters; however, it does not differentiate between
demonstrations and quadratically increases the complexity of Transformer LLMs,
exhausting the memory. As a solution, we propose Mixtures of In-Context
Learners (MoICL), a novel approach to treat subsets of demonstrations as
experts and learn a weighting function to merge their output distributions
based on a training set. In our experiments, we show performance improvements
on 5 out of 7 classification datasets compared to a set of strong baselines (up
to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of
ICL by reducing the inference time needed to achieve the same performance with
fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to
+11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can
filter these out from datasets. Overall, MoICL is a more expressive approach to
learning from demonstrations without exhausting the context window or memory.",cs.AI
DroidSpeak: Enhancing Cross-LLM Communication,"In multi-agent systems utilizing Large Language Models (LLMs), communication
between agents traditionally relies on natural language. This communication
often includes the full context of the query so far, which can introduce
significant prefill-phase latency, especially with long contexts.
  We introduce DroidSpeak, a novel framework to target this cross-LLM
communication by leveraging the reuse of intermediate data, such as input
embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the
need to reprocess entire contexts for fine-tuned versions of the same
foundational model. This approach allows faster context integration while
maintaining the quality of task performance. Experimental evaluations
demonstrate DroidSpeak's ability to significantly accelerate inter-agent
communication, achieving up to a 2.78x speedup in prefill latency with
negligible loss in accuracy. Our findings underscore the potential to create
more efficient and scalable multi-agent systems.",cs.AI
Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models,"Text-conditioned generation models are commonly evaluated based on the
quality of the generated data and its alignment with the input text prompt. On
the other hand, several applications of prompt-based generative models require
sufficient diversity in the generated data to ensure the models' capability of
generating image and video samples possessing a variety of features. However,
most existing diversity metrics are designed for unconditional generative
models, and thus cannot distinguish the diversity arising from variations in
text prompts and that contributed by the generative model itself. In this work,
our goal is to quantify the prompt-induced and model-induced diversity in
samples generated by prompt-based models. We propose an information-theoretic
approach for internal diversity quantification, where we decompose the
kernel-based entropy $H(X)$ of the generated data $X$ into the sum of the
conditional entropy $H(X|T)$, given text variable $T$, and the mutual
information $I(X; T)$ between the text and data variables. We introduce the
\emph{Conditional-Vendi} score based on $H(X|T)$ to quantify the internal
diversity of the model and the \emph{Information-Vendi} score based on $I(X;
T)$ to measure the statistical relevance between the generated data and text
prompts. We provide theoretical results to statistically interpret these scores
and relate them to the unconditional Vendi score. We conduct several numerical
experiments to show the correlation between the Conditional-Vendi score and the
internal diversity of text-conditioned generative models. The codebase is
available at
\href{https://github.com/mjalali/conditional-vendi}{https://github.com/mjalali/conditional-vendi}.",cs.AI
"DeepContext: A Context-aware, Cross-platform, and Cross-framework Tool for Performance Profiling and Analysis of Deep Learning Workloads","Effective performance profiling and analysis are essential for optimizing
training and inference of deep learning models, especially given the growing
complexity of heterogeneous computing environments. However, existing tools
often lack the capability to provide comprehensive program context information
and performance optimization insights for sophisticated interactions between
CPUs and GPUs. This paper introduces DeepContext, a novel profiler that links
program contexts across high-level Python code, deep learning frameworks,
underlying libraries written in C/C++, as well as device code executed on GPUs.
DeepContext incorporates measurements of both coarse- and fine-grained
performance metrics for major deep learning frameworks, such as PyTorch and
JAX, and is compatible with GPUs from both Nvidia and AMD, as well as various
CPU architectures, including x86 and ARM. In addition, DeepContext integrates a
novel GUI that allows users to quickly identify hotpots and an innovative
automated performance analyzer that suggests users with potential optimizations
based on performance metrics and program context. Through detailed use cases,
we demonstrate how DeepContext can help users identify and analyze performance
issues to enable quick and effective optimization of deep learning workloads.
We believe Deep Context is a valuable tool for users seeking to optimize
complex deep learning workflows across multiple compute environments.",cs.AI
Specialized Foundation Models Struggle to Beat Supervised Baselines,"Following its success for vision and text, the ""foundation model"" (FM)
paradigm -- pretraining large models on massive data, then fine-tuning on
target tasks -- has rapidly expanded to domains in the sciences, engineering,
healthcare, and beyond. Has this achieved what the original FMs accomplished,
i.e. the supplanting of traditional supervised learning in their domains? To
answer we look at three modalities -- genomics, satellite imaging, and time
series -- with multiple recent FMs and compare them to a standard supervised
learning workflow: model development, hyperparameter tuning, and training, all
using only data from the target task. Across these three specialized domains,
we find that it is consistently possible to train simple supervised models --
no more complicated than a lightly modified wide ResNet or UNet -- that match
or even outperform the latest foundation models. Our work demonstrates that the
benefits of large-scale pretraining have yet to be realized in many specialized
areas, reinforces the need to compare new FMs to strong, well-tuned baselines,
and introduces two new, easy-to-use, open-source, and automated workflows for
doing so.",cs.AI
The Evolution of RWKV: Advancements in Efficient Language Modeling,"This paper reviews the development of the Receptance Weighted Key Value
(RWKV) architecture, emphasizing its advancements in efficient language
modeling. RWKV combines the training efficiency of Transformers with the
inference efficiency of RNNs through a novel linear attention mechanism. We
examine its core innovations, adaptations across various domains, and
performance advantages over traditional models. The paper also discusses
challenges and future directions for RWKV as a versatile architecture in deep
learning.",cs.AI
Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.",cs.AI
When to Localize? A Risk-Constrained Reinforcement Learning Approach,"In a standard navigation pipeline, a robot localizes at every time step to
lower navigational errors. However, in some scenarios, a robot needs to
selectively localize when it is expensive to obtain observations. For example,
an underwater robot surfacing to localize too often hinders it from searching
for critical items underwater, such as black boxes from crashed aircraft. On
the other hand, if the robot never localizes, poor state estimates cause
failure to find the items due to inadvertently leaving the search area or
entering hazardous, restricted areas. Motivated by these scenarios, we
investigate approaches to help a robot determine ""when to localize?"" We
formulate this as a bi-criteria optimization problem: minimize the number of
localization actions while ensuring the probability of failure (due to
collision or not reaching a desired goal) remains bounded. In recent work, we
showed how to formulate this active localization problem as a constrained
Partially Observable Markov Decision Process (POMDP), which was solved using an
online POMDP solver. However, this approach is too slow and requires full
knowledge of the robot transition and observation models. In this paper, we
present RiskRL, a constrained Reinforcement Learning (RL) framework that
overcomes these limitations. RiskRL uses particle filtering and recurrent Soft
Actor-Critic network to learn a policy that minimizes the number of
localizations while ensuring the probability of failure constraint is met. Our
numerical experiments show that RiskRL learns a robust policy that outperforms
the baseline by at least 13% while also generalizing to unseen environments.",cs.AI
Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment,"Safety alignment of Large Language Models (LLMs) has recently become a
critical objective of model developers. In response, a growing body of work has
been investigating how safety alignment can be bypassed through various
jailbreaking methods, such as adversarial attacks. However, these jailbreak
methods can be rather costly or involve a non-trivial amount of creativity and
effort, introducing the assumption that malicious users are high-resource or
sophisticated. In this paper, we study how simple random augmentations to the
input prompt affect safety alignment effectiveness in state-of-the-art LLMs,
such as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different
models and investigate the intersection of safety under random augmentations
with multiple dimensions: augmentation type, model size, quantization,
fine-tuning-based defenses, and decoding strategies (e.g., sampling
temperature). We show that low-resource and unsophisticated attackers, i.e.
$\textit{stochastic monkeys}$, can significantly improve their chances of
bypassing alignment with just 25 random augmentations per prompt.",cs.AI
EcoCropsAID: Economic Crops Aerial Image Dataset for Land Use Classification,"The EcoCropsAID dataset is a comprehensive collection of 5,400 aerial images
captured between 2014 and 2018 using the Google Earth application. This dataset
focuses on five key economic crops in Thailand: rice, sugarcane, cassava,
rubber, and longan. The images were collected at various crop growth stages:
early cultivation, growth, and harvest, resulting in significant variability
within each category and similarities across different categories. These
variations, coupled with differences in resolution, color, and contrast
introduced by multiple remote imaging sensors, present substantial challenges
for land use classification. The dataset is an interdisciplinary resource that
spans multiple research domains, including remote sensing, geoinformatics,
artificial intelligence, and computer vision. The unique features of the
EcoCropsAID dataset offer opportunities for researchers to explore novel
approaches, such as extracting spatial and temporal features, developing deep
learning architectures, and implementing transformer-based models. The
EcoCropsAID dataset provides a valuable platform for advancing research in land
use classification, with implications for optimizing agricultural practices and
enhancing sustainable development. This study explicitly investigates the use
of deep learning algorithms to classify economic crop areas in northeastern
Thailand, utilizing satellite imagery to address the challenges posed by
diverse patterns and similarities across categories.",cs.AI
Generative Artificial Intelligence Meets Synthetic Aperture Radar: A Survey,"SAR images possess unique attributes that present challenges for both human
observers and vision AI models to interpret, owing to their electromagnetic
characteristics. The interpretation of SAR images encounters various hurdles,
with one of the primary obstacles being the data itself, which includes issues
related to both the quantity and quality of the data. The challenges can be
addressed using generative AI technologies. Generative AI, often known as
GenAI, is a very advanced and powerful technology in the field of artificial
intelligence that has gained significant attention. The advancement has created
possibilities for the creation of texts, photorealistic pictures, videos, and
material in various modalities. This paper aims to comprehensively investigate
the intersection of GenAI and SAR. First, we illustrate the common data
generation-based applications in SAR field and compare them with computer
vision tasks, analyzing the similarity, difference, and general challenges of
them. Then, an overview of the latest GenAI models is systematically reviewed,
including various basic models and their variations targeting the general
challenges. Additionally, the corresponding applications in SAR domain are also
included. Specifically, we propose to summarize the physical model based
simulation approaches for SAR, and analyze the hybrid modeling methods that
combine the GenAI and interpretable models. The evaluation methods that have
been or could be applied to SAR, are also explored. Finally, the potential
challenges and future prospects are discussed. To our best knowledge, this
survey is the first exhaustive examination of the interdiscipline of SAR and
GenAI, encompassing a wide range of topics, including deep neural networks,
physical models, computer vision, and SAR images. The resources of this survey
are open-source at \url{https://github.com/XAI4SAR/GenAIxSAR}.",cs.AI
A Bayesian explanation of machine learning models based on modes and functional ANOVA,"Most methods in explainable AI (XAI) focus on providing reasons for the
prediction of a given set of features. However, we solve an inverse explanation
problem, i.e., given the deviation of a label, find the reasons of this
deviation. We use a Bayesian framework to recover the ``true'' features,
conditioned on the observed label value. We efficiently explain the deviation
of a label value from the mode, by identifying and ranking the influential
features using the ``distances'' in the ANOVA functional decomposition. We show
that the new method is more human-intuitive and robust than methods based on
mean values, e.g., SHapley Additive exPlanations (SHAP values). The extra costs
of solving a Bayesian inverse problem are dimension-independent.",cs.AI
Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial for deploying reliable machine
learning models in open-world applications. Recent advances in CLIP-based OOD
detection have shown promising results via regularizing prompt tuning with OOD
features extracted from ID data. However, the irrelevant context mined from ID
data can be spurious due to the inaccurate foreground-background decomposition,
thus limiting the OOD detection performance. In this work, we propose a novel
framework, namely, Self-Calibrated Tuning (SCT), to mitigate this problem for
effective OOD detection with only the given few-shot ID data. Specifically, SCT
introduces modulating factors respectively on the two components of the
original learning objective. It adaptively directs the optimization process
between the two tasks during training on data with different prediction
uncertainty to calibrate the influence of OOD regularization, which is
compatible with many prompt tuning based OOD detection methods. Extensive
experiments and analyses have been conducted to characterize and demonstrate
the effectiveness of the proposed SCT. The code is publicly available.",cs.AI
Multimodal Commonsense Knowledge Distillation for Visual Question Answering,"Existing Multimodal Large Language Models (MLLMs) and Visual Language
Pretrained Models (VLPMs) have shown remarkable performances in the general
Visual Question Answering (VQA). However, these models struggle with VQA
questions that require external commonsense knowledge due to the challenges in
generating high-quality prompts and the high computational costs of
fine-tuning. In this work, we propose a novel graph-based multimodal
commonsense knowledge distillation framework that constructs a unified
relational graph over commonsense knowledge, visual objects and questions
through a Graph Convolutional Network (GCN) following a teacher-student
environment. This proposed framework is flexible with any type of teacher and
student models without further fine-tuning, and has achieved competitive
performances on the ScienceQA dataset.",cs.AI
Game Plot Design with an LLM-powered Assistant: An Empirical Study with Game Designers,"We introduce GamePlot, an LLM-powered assistant that supports game designers
in crafting immersive narratives for turn-based games, and allows them to test
these games through a collaborative game play and refine the plot throughout
the process. Our user study with 14 game designers shows high levels of both
satisfaction with the generated game plots and sense of ownership over the
narratives, but also reconfirms that LLM are limited in their ability to
generate complex and truly innovative content. We also show that diverse user
populations have different expectations from AI assistants, and encourage
researchers to study how tailoring assistants to diverse user groups could
potentially lead to increased job satisfaction and greater creativity and
innovation over time.",cs.AI
V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization,"Large vision-language models (LVLMs) suffer from hallucination, resulting in
misalignment between the output textual response and the input visual content.
Recent research indicates that the over-reliance on the Large Language Model
(LLM) backbone, as one cause of the LVLM hallucination, inherently introduces
bias from language priors, leading to insufficient context attention to the
visual inputs.
  We tackle this issue of hallucination by mitigating such over-reliance
through preference learning. We propose Vision-guided Direct Preference
Optimization (V-DPO) to enhance visual context learning at training time. To
interpret the effectiveness and generalizability of V-DPO on different types of
training data, we construct a synthetic dataset containing both response- and
image-contrast preference pairs, compared against existing human-annotated
hallucination samples. Our approach achieves significant improvements compared
with baseline methods across various hallucination benchmarks. Our analysis
indicates that V-DPO excels in learning from image-contrast preference data,
demonstrating its superior ability to elicit and understand nuances of visual
context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.",cs.AI
Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios,"Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency
in their responses is essential for developing trustworthy multimodal
intelligence. However, existing benchmarks include many samples where all MLLMs
\textit{exhibit high response uncertainty when encountering misleading
information}, requiring even 5-15 response attempts per sample to effectively
assess uncertainty. Therefore, we propose a two-stage pipeline: first, we
collect MLLMs' responses without misleading information, and then gather
misleading ones via specific misleading instructions. By calculating the
misleading rate, and capturing both correct-to-incorrect and
incorrect-to-correct shifts between the two sets of responses, we can
effectively metric the model's response uncertainty. Eventually, we establish a
\textbf{\underline{M}}ultimodal \textbf{\underline{U}}ncertainty
\textbf{\underline{B}}enchmark (\textbf{MUB}) that employs both explicit and
implicit misleading instructions to comprehensively assess the vulnerability of
MLLMs across diverse domains. Our experiments reveal that all open-source and
close-source MLLMs are highly susceptible to misleading instructions, with an
average misleading rate exceeding 86\%. To enhance the robustness of MLLMs, we
further fine-tune all open-source MLLMs by incorporating explicit and implicit
misleading data, which demonstrates a significant reduction in misleading
rates. Our code is available at:
\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}",cs.AI
RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation,"We explore how intermediate policy representations can facilitate
generalization by providing guidance on how to perform manipulation tasks.
Existing representations such as language, goal images, and trajectory sketches
have been shown to be helpful, but these representations either do not provide
enough context or provide over-specified context that yields less robust
policies. We propose conditioning policies on affordances, which capture the
pose of the robot at key stages of the task. Affordances offer expressive yet
lightweight abstractions, are easy for users to specify, and facilitate
efficient learning by transferring knowledge from large internet datasets. Our
method, RT-Affordance, is a hierarchical model that first proposes an
affordance plan given the task language, and then conditions the policy on this
affordance plan to perform manipulation. Our model can flexibly bridge
heterogeneous sources of supervision including large web datasets and robot
trajectories. We additionally train our model on cheap-to-collect in-domain
affordance images, allowing us to learn new tasks without collecting any
additional costly robot trajectories. We show on a diverse set of novel tasks
how RT-Affordance exceeds the performance of existing methods by over 50%, and
we empirically demonstrate that affordances are robust to novel settings.
Videos available at https://snasiriany.me/rt-affordance",cs.AI
JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase,"Knowledge Graphs have emerged as a compelling abstraction for capturing key
relationship among the entities of interest to enterprises and for integrating
data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by
leveraging knowledge graphs across the organization for multiple mission
critical applications such as risk assessment, fraud detection, investment
advice, etc. A core problem in leveraging a knowledge graph is to link mentions
(e.g., company names) that are encountered in textual sources to entities in
the knowledge graph. Although several techniques exist for entity linking, they
are tuned for entities that exist in Wikipedia, and fail to generalize for the
entities that are of interest to an enterprise. In this paper, we propose a
novel end-to-end neural entity linking model (JEL) that uses minimal context
information and a margin loss to generate entity embeddings, and a Wide & Deep
Learning model to match character and semantic information respectively. We
show that JEL achieves the state-of-the-art performance to link mentions of
company names in financial news with entities in our knowledge graph. We report
on our efforts to deploy this model in the company-wide system to generate
alerts in response to financial news. The methodology used for JEL is directly
applicable and usable by other enterprises who need entity linking solutions
for data that are unique to their respective situations.",cs.AI
JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs,"Knowledge graphs have gained popularity for their ability to organize and
analyze complex data effectively. When combined with graph embedding
techniques, such as graph neural networks (GNNs), knowledge graphs become a
potent tool in providing valuable insights. This study explores the application
of graph embedding in identifying competitors from a financial knowledge graph.
Existing state-of-the-art(SOTA) models face challenges due to the unique
attributes of our knowledge graph, including directed and undirected
relationships, attributed nodes, and minimal annotated competitor connections.
To address these challenges, we propose a novel graph embedding model,
JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes
graph neural network to learn from both first-order and second-order node
proximity together with vital features for competitor retrieval. JPEC had
outperformed most existing models in extensive experiments, showcasing its
effectiveness in competitor retrieval.",cs.AI
Geometry of naturalistic object representations in recurrent neural network models of working memory,"Working memory is a central cognitive ability crucial for intelligent
decision-making. Recent experimental and computational work studying working
memory has primarily used categorical (i.e., one-hot) inputs, rather than
ecologically relevant, multidimensional naturalistic ones. Moreover, studies
have primarily investigated working memory during single or few cognitive
tasks. As a result, an understanding of how naturalistic object information is
maintained in working memory in neural networks is still lacking. To bridge
this gap, we developed sensory-cognitive models, comprising a convolutional
neural network (CNN) coupled with a recurrent neural network (RNN), and trained
them on nine distinct N-back tasks using naturalistic stimuli. By examining the
RNN's latent space, we found that: (1) Multi-task RNNs represent both
task-relevant and irrelevant information simultaneously while performing tasks;
(2) The latent subspaces used to maintain specific object properties in vanilla
RNNs are largely shared across tasks, but highly task-specific in gated RNNs
such as GRU and LSTM; (3) Surprisingly, RNNs embed objects in new
representational spaces in which individual object features are less
orthogonalized relative to the perceptual space; (4) The transformation of
working memory encodings (i.e., embedding of visual inputs in the RNN latent
space) into memory was shared across stimuli, yet the transformations governing
the retention of a memory in the face of incoming distractor stimuli were
distinct across time. Our findings indicate that goal-driven RNNs employ
chronological memory subspaces to track information over short time spans,
enabling testable predictions with neural data.",cs.AI
"Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study","Recent advancements in Augmented Reality (AR) research have highlighted the
critical role of context awareness in enhancing interface effectiveness and
user experience. This underscores the need for intelligent AR (iAR) interfaces
that dynamically adapt across various contexts to provide optimal experiences.
In this paper, we (a) propose a comprehensive framework for context-aware
inference and adaptation in iAR, (b) introduce a taxonomy that describes
context through quantifiable input data, and (c) present an architecture that
outlines the implementation of our proposed framework and taxonomy within iAR.
Additionally, we present an empirical AR experiment to observe user behavior
and record user performance, context, and user-specified adaptations to the AR
interfaces within a context-switching scenario. We (d) explore the nuanced
relationships between context and user adaptations in this scenario and discuss
the significance of our framework in identifying these patterns. This
experiment emphasizes the significance of context-awareness in iAR and provides
a preliminary training dataset for this specific Scenario.",cs.AI
Wave Network: An Ultra-Small Language Model,"We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.",cs.AI
Fair In-Context Learning via Latent Concept Variables,"The emerging in-context learning (ICL) ability of large language models
(LLMs) has prompted their use for predictive tasks in various domains with
different types of data facilitated by serialization methods. However, with
increasing applications in high-stakes domains, it has been shown that LLMs can
inherit social bias and discrimination from their pre-training data. In this
work, we investigate this inherent bias in LLMs during in-context learning with
tabular data. We focus on an optimal demonstration selection approach that
utilizes latent concept variables for resource-efficient task adaptation. We
design data augmentation strategies that reduce correlation between predictive
outcomes and sensitive variables helping to promote fairness during latent
concept learning. We utilize the learned concept and select demonstrations from
a training dataset to obtain fair predictions during inference while
maintaining model utility. The latent concept variable is learned using a
smaller internal LLM and the selected demonstrations can be used for inference
with larger external LLMs. We empirically verify that the fair latent variable
approach improves fairness results on tabular datasets compared to multiple
heuristic demonstration selection methods.",cs.AI
From Twitter to Reasoner: Understand Mobility Travel Modes and Sentiment Using Large Language Models,"Social media has become an important platform for people to express their
opinions towards transportation services and infrastructure, which holds the
potential for researchers to gain a deeper understanding of individuals' travel
choices, for transportation operators to improve service quality, and for
policymakers to regulate mobility services. A significant challenge, however,
lies in the unstructured nature of social media data. In other words, textual
data like social media is not labeled, and large-scale manual annotations are
cost-prohibitive. In this study, we introduce a novel methodological framework
utilizing Large Language Models (LLMs) to infer the mentioned travel modes from
social media posts, and reason people's attitudes toward the associated travel
mode, without the need for manual annotation. We compare different LLMs along
with various prompting engineering methods in light of human assessment and LLM
verification. We find that most social media posts manifest negative rather
than positive sentiments. We thus identify the contributing factors to these
negative posts and, accordingly, propose recommendations to traffic operators
and policymakers.",cs.AI
Explanations that reveal all through the definition of encoding,"Feature attributions attempt to highlight what inputs drive predictive power.
Good attributions or explanations are thus those that produce inputs that
retain this predictive power; accordingly, evaluations of explanations score
their quality of prediction. However, evaluations produce scores better than
what appears possible from the values in the explanation for a class of
explanations, called encoding explanations. Probing for encoding remains a
challenge because there is no general characterization of what gives the extra
predictive power. We develop a definition of encoding that identifies this
extra predictive power via conditional dependence and show that the definition
fits existing examples of encoding. This definition implies, in contrast to
encoding explanations, that non-encoding explanations contain all the
informative inputs used to produce the explanation, giving them a ""what you see
is what you get"" property, which makes them transparent and simple to use.
Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank
non-encoding explanations above encoding ones, and develop STRIPE-X which ranks
them correctly. After empirically demonstrating the theoretical insights, we
use STRIPE-X to uncover encoding in LLM-generated explanations for predicting
the sentiment in movie reviews.",cs.AI
M-CELS: Counterfactual Explanation for Multivariate Time Series Data Guided by Learned Saliency Maps,"Over the past decade, multivariate time series classification has received
great attention. Machine learning (ML) models for multivariate time series
classification have made significant strides and achieved impressive success in
a wide range of applications and tasks. The challenge of many state-of-the-art
ML models is a lack of transparency and interpretability. In this work, we
introduce M-CELS, a counterfactual explanation model designed to enhance
interpretability in multidimensional time series classification tasks. Our
experimental validation involves comparing M-CELS with leading state-of-the-art
baselines, utilizing seven real-world time-series datasets from the UEA
repository. The results demonstrate the superior performance of M-CELS in terms
of validity, proximity, and sparsity, reinforcing its effectiveness in
providing transparent insights into the decisions of machine learning models
applied to multivariate time series data.",cs.AI
A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers,"Counterfactual explanations can be used to interpret and debug text
classifiers by producing minimally altered text inputs that change a
classifier's output. In this work, we evaluate five methods for generating
counterfactual explanations for a BERT text classifier on two datasets using
three evaluation metrics. The results of our experiments suggest that
established white-box substitution-based methods are effective at generating
valid counterfactuals that change the classifier's output. In contrast, newer
methods based on large language models (LLMs) excel at producing natural and
linguistically plausible text counterfactuals but often fail to generate valid
counterfactuals that alter the classifier's output. Based on these results, we
recommend developing new counterfactual explanation methods that combine the
strengths of established gradient-based approaches and newer LLM-based
techniques to generate high-quality, valid, and plausible text counterfactual
explanations.",cs.AI
Active Prompt Tuning Enables Gpt-40 To Do Efficient Classification Of Microscopy Images,"Traditional deep learning-based methods for classifying cellular features in
microscopy images require time- and labor-intensive processes for training
models. Among the current limitations are major time commitments from domain
experts for accurate ground truth preparation; and the need for a large amount
of input image data. We previously proposed a solution that overcomes these
challenges using OpenAI's GPT-4(V) model on a pilot dataset (Iba-1
immuno-stained tissue sections from 11 mouse brains). Results on the pilot
dataset were equivalent in accuracy and with a substantial improvement in
throughput efficiency compared to the baseline using a traditional
Convolutional Neural Net (CNN)-based approach.
  The present study builds upon this framework using a second unique and
substantially larger dataset of microscopy images. Our current approach uses a
newer and faster model, GPT-4o, along with improved prompts. It was evaluated
on a microscopy image dataset captured at low (10x) magnification from
cresyl-violet-stained sections through the cerebellum of a total of 18 mouse
brains (9 Lurcher mice, 9 wild-type controls). We used our approach to classify
these images either as a control group or Lurcher mutant. Using 6 mice in the
prompt set the results were correct classification for 11 out of the 12 mice
(92%) with 96% higher efficiency, reduced image requirements, and lower demands
on time and effort of domain experts compared to the baseline method (snapshot
ensemble of CNN models). These results confirm that our approach is effective
across multiple datasets from different brain regions and magnifications, with
minimal overhead.",cs.AI
Intelligent Video Recording Optimization using Activity Detection for Surveillance Systems,"Surveillance systems often struggle with managing vast amounts of footage,
much of which is irrelevant, leading to inefficient storage and challenges in
event retrieval. This paper addresses these issues by proposing an optimized
video recording solution focused on activity detection. The proposed approach
utilizes a hybrid method that combines motion detection via frame subtraction
with object detection using YOLOv9. This strategy specifically targets the
recording of scenes involving human or car activity, thereby reducing
unnecessary footage and optimizing storage usage. The developed model
demonstrates superior performance, achieving precision metrics of 0.855 for car
detection and 0.884 for person detection, and reducing the storage requirements
by two-thirds compared to traditional surveillance systems that rely solely on
motion detection. This significant reduction in storage highlights the
effectiveness of the proposed approach in enhancing surveillance system
efficiency. Nonetheless, some limitations persist, particularly the occurrence
of false positives and false negatives in adverse weather conditions, such as
strong winds.",cs.AI
Extracting Unlearned Information from LLMs with Activation Steering,"An unintended consequence of the vast pretraining of Large Language Models
(LLMs) is the verbatim memorization of fragments of their training data, which
may contain sensitive or copyrighted information. In recent years, unlearning
has emerged as a solution to effectively remove sensitive knowledge from models
after training. Yet, recent work has shown that supposedly deleted information
can still be extracted by malicious actors through various attacks. Still,
current attacks retrieve sets of possible candidate generations and are unable
to pinpoint the output that contains the actual target information. We propose
activation steering as a method for exact information retrieval from unlearned
LLMs. We introduce a novel approach to generating steering vectors, named
Anonymized Activation Steering. Additionally, we develop a simple word
frequency method to pinpoint the correct answer among a set of candidates when
retrieving unlearned information. Our evaluation across multiple unlearning
techniques and datasets demonstrates that activation steering successfully
recovers general knowledge (e.g., widely known fictional characters) while
revealing limitations in retrieving specific information (e.g., details about
non-public individuals). Overall, our results demonstrate that exact
information retrieval from unlearned models is possible, highlighting a severe
vulnerability of current unlearning techniques.",cs.AI
EmoSphere++: Emotion-Controllable Zero-Shot Text-to-Speech via Emotion-Adaptive Spherical Vector,"Emotional text-to-speech (TTS) technology has achieved significant progress
in recent years; however, challenges remain owing to the inherent complexity of
emotions and limitations of the available emotional speech datasets and models.
Previous studies typically relied on limited emotional speech datasets or
required extensive manual annotations, restricting their ability to generalize
across different speakers and emotional styles. In this paper, we present
EmoSphere++, an emotion-controllable zero-shot TTS model that can control
emotional style and intensity to resemble natural human speech. We introduce a
novel emotion-adaptive spherical vector that models emotional style and
intensity without human annotation. Moreover, we propose a multi-level style
encoder that can ensure effective generalization for both seen and unseen
speakers. We also introduce additional loss functions to enhance the emotion
transfer performance for zero-shot scenarios. We employ a conditional flow
matching-based decoder to achieve high-quality and expressive emotional TTS in
a few sampling steps. Experimental results demonstrate the effectiveness of the
proposed framework.",cs.AI
"Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach","This paper presents a novel real-time, delay-aware cooperative perception
system designed for intelligent mobility platforms operating in dynamic indoor
environments. The system contains a network of multi-modal sensor nodes and a
central node that collectively provide perception services to mobility
platforms. The proposed Hierarchical Clustering Considering the Scanning
Pattern and Ground Contacting Feature based Lidar Camera Fusion improve
intra-node perception for crowded environment. The system also features
delay-aware global perception to synchronize and aggregate data across nodes.
To validate our approach, we introduced the Indoor Pedestrian Tracking dataset,
compiled from data captured by two indoor sensor nodes. Our experiments,
compared to baselines, demonstrate significant improvements in detection
accuracy and robustness against delays. The dataset is available in the
repository: https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception",cs.AI
Learning to Assist Humans without Inferring Rewards,"Assistive agents should make humans' lives easier. Classically, such
assistance is studied through the lens of inverse reinforcement learning, where
an assistive agent (e.g., a chatbot, a robot) infers a human's intention and
then selects actions to help the human reach that goal. This approach requires
inferring intentions, which can be difficult in high-dimensional settings. We
build upon prior work that studies assistance through the lens of empowerment:
an assistive agent aims to maximize the influence of the human's actions such
that they exert a greater control over the environmental outcomes and can solve
tasks in fewer steps. We lift the major limitation of prior work in this
area--scalability to high-dimensional settings--with contrastive successor
representations. We formally prove that these representations estimate a
similar notion of empowerment to that studied by prior work and provide a
ready-made mechanism for optimizing it. Empirically, our proposed method
outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a
cooperative game setting. Theoretically, our work connects ideas from
information theory, neuroscience, and reinforcement learning, and charts a path
for representations to play a critical role in solving assistive problems.",cs.AI
Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning,"Machine unlearning--enabling a trained model to forget specific data--is
crucial for addressing biased data and adhering to privacy regulations like the
General Data Protection Regulation (GDPR)'s ""right to be forgotten"". Recent
works have paid little attention to privacy concerns, leaving the data intended
for forgetting vulnerable to membership inference attacks. Moreover, they often
come with high computational overhead. In this work, we propose
Pseudo-Probability Unlearning (PPU), a novel method that enables models to
forget data efficiently and in a privacy-preserving manner. Our method replaces
the final-layer output probabilities of the neural network with
pseudo-probabilities for the data to be forgotten. These pseudo-probabilities
follow either a uniform distribution or align with the model's overall
distribution, enhancing privacy and reducing risk of membership inference
attacks. Our optimization strategy further refines the predictive probability
distributions and updates the model's weights accordingly, ensuring effective
forgetting with minimal impact on the model's overall performance. Through
comprehensive experiments on multiple benchmarks, our method achieves over 20%
improvements in forgetting error compared to the state-of-the-art.
Additionally, our method enhances privacy by preventing the forgotten set from
being inferred to around random guesses.",cs.AI
Advanced XR-Based 6-DOF Catheter Tracking System for Immersive Cardiac Intervention Training,"Extended Reality (XR) technologies are gaining traction as effective tools
for medical training and procedural guidance, particularly in complex cardiac
interventions. This paper presents a novel system for real-time 3D tracking and
visualization of intracardiac echocardiography (ICE) catheters, with precise
measurement of the roll angle. A custom 3D-printed setup, featuring orthogonal
cameras, captures biplane video of the catheter, while a specialized computer
vision algorithm reconstructs its 3D trajectory, localizing the tip with
sub-millimeter accuracy and tracking the roll angle in real-time. The system's
data is integrated into an interactive Unity-based environment, rendered
through the Meta Quest 3 XR headset, combining a dynamically tracked catheter
with a patient-specific 3D heart model. This immersive environment allows the
testing of the importance of 3D depth perception, in comparison to 2D
projections, as a form of visualization in XR. Our experimental study,
conducted using the ICE catheter with six participants, suggests that 3D
visualization is not necessarily beneficial over 2D views offered by the XR
system; although all cardiologists saw its utility for pre-operative training,
planning, and intra-operative guidance. The proposed system qualitatively shows
great promise in transforming catheter-based interventions, particularly ICE
procedures, by improving visualization, interactivity, and skill development.",cs.AI
Investigating Idiomaticity in Word Representations,"Idiomatic expressions are an integral part of human languages, often used to
express complex ideas in compressed or conventional ways (e.g. eager beaver as
a keen and enthusiastic person). However, their interpretations may not be
straightforwardly linked to the meanings of their individual components in
isolation and this may have an impact for compositional approaches. In this
paper, we investigate to what extent word representation models are able to go
beyond compositional word combinations and capture multiword expression
idiomaticity and some of the expected properties related to idiomatic meanings.
We focus on noun compounds of varying levels of idiomaticity in two languages
(English and Portuguese), presenting a dataset of minimal pairs containing
human idiomaticity judgments for each noun compound at both type and token
levels, their paraphrases and their occurrences in naturalistic and
sense-neutral contexts, totalling 32,200 sentences. We propose this set of
minimal pairs for evaluating how well a model captures idiomatic meanings, and
define a set of fine-grained metrics of Affinity and Scaled Similarity, to
determine how sensitive the models are to perturbations that may lead to
changes in idiomaticity. The results obtained with a variety of representative
and widely used models indicate that, despite superficial indications to the
contrary in the form of high similarities, idiomaticity is not yet accurately
represented in current models. Moreover, the performance of models with
different levels of contextualisation suggests that their ability to capture
context is not yet able to go beyond more superficial lexical clues provided by
the words and to actually incorporate the relevant semantic clues needed for
idiomaticity.",cs.AI
Computing critical exponents in 3D Ising model via pattern recognition/deep learning approach,"In this study, we computed three critical exponents ($\alpha, \beta, \gamma$)
for the 3D Ising model with Metropolis Algorithm using Finite-Size Scaling
Analysis on six cube length scales (L=20,30,40,60,80,90), and performed a
supervised Deep Learning (DL) approach (3D Convolutional Neural Network or CNN)
to train a neural network on specific conformations of spin states. We find one
can effectively reduce the information in thermodynamic ensemble-averaged
quantities vs. reduced temperature t (magnetization per spin $<m>(t)$, specific
heat per spin $<c>(t)$, magnetic susceptibility per spin $<\chi>(t)$) to
\textit{six} latent classes. We also demonstrate our CNN on a subset of L=20
conformations and achieve a train/test accuracy of 0.92 and 0.6875,
respectively. However, more work remains to be done to quantify the feasibility
of computing critical exponents from the output class labels (binned $m, c,
\chi$) from this approach and interpreting the results from DL models trained
on systems in Condensed Matter Physics in general.",cs.AI
FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.",cs.AI
Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration,"We introduce Vocal Sandbox, a framework for enabling seamless human-robot
collaboration in situated environments. Systems in our framework are
characterized by their ability to adapt and continually learn at multiple
levels of abstraction from diverse teaching modalities such as spoken dialogue,
object keypoints, and kinesthetic demonstrations. To enable such adaptation, we
design lightweight and interpretable learning algorithms that allow users to
build an understanding and co-adapt to a robot's capabilities in real-time, as
they teach new behaviors. For example, after demonstrating a new low-level
skill for ""tracking around"" an object, users are provided with trajectory
visualizations of the robot's intended motion when asked to track a new object.
Similarly, users teach high-level planning behaviors through spoken dialogue,
using pretrained language models to synthesize behaviors such as ""packing an
object away"" as compositions of low-level skills $-$ concepts that can be
reused and built upon. We evaluate Vocal Sandbox in two settings: collaborative
gift bag assembly and LEGO stop-motion animation. In the first setting, we run
systematic ablations and user studies with 8 non-expert participants,
highlighting the impact of multi-level teaching. Across 23 hours of total robot
interaction time, users teach 17 new high-level behaviors with an average of 16
novel low-level skills, requiring 22.1% less active supervision compared to
baselines and yielding more complex autonomous performance (+19.7%) with fewer
failures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems
due to their ease of use (+20.6%) and overall performance (+13.9%). Finally, we
pair an experienced system-user with a robot to film a stop-motion animation;
over two hours of continuous collaboration, the user teaches progressively more
complex motion skills to shoot a 52 second (232 frame) movie.",cs.AI
"""It's a conversation, not a quiz"": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health","Recent breakthroughs in large language models (LLMs) have generated both
interest and concern about their potential adoption as accessible information
sources or communication tools across different domains. In public health --
where stakes are high and impacts extend across populations -- adopting LLMs
poses unique challenges that require thorough evaluation. However, structured
approaches for assessing potential risks in public health remain
under-explored. To address this gap, we conducted focus groups with health
professionals and health issue experiencers to unpack their concerns, situated
across three distinct and critical public health issues that demand
high-quality information: vaccines, opioid use disorder, and intimate partner
violence. We synthesize participants' perspectives into a risk taxonomy,
distinguishing and contextualizing the potential harms LLMs may introduce when
positioned alongside traditional health communication. This taxonomy highlights
four dimensions of risk in individual behaviors, human-centered care,
information ecosystem, and technology accountability. For each dimension, we
discuss specific risks and example reflection questions to help practitioners
adopt a risk-reflexive approach. This work offers a shared vocabulary and
reflection tool for experts in both computing and public health to
collaboratively anticipate, evaluate, and mitigate risks in deciding when to
employ LLM capabilities (or not) and how to mitigate harm when they are used.",cs.AI
Multi-Agent Decision Transformers for Dynamic Dispatching in Material Handling Systems Leveraging Enterprise Big Data,"Dynamic dispatching rules that allocate resources to tasks in real-time play
a critical role in ensuring efficient operations of many automated material
handling systems across industries. Traditionally, the dispatching rules
deployed are typically the result of manually crafted heuristics based on
domain experts' knowledge. Generating these rules is time-consuming and often
sub-optimal. As enterprises increasingly accumulate vast amounts of operational
data, there is significant potential to leverage this big data to enhance the
performance of automated systems. One promising approach is to use Decision
Transformers, which can be trained on existing enterprise data to learn better
dynamic dispatching rules for improving system throughput. In this work, we
study the application of Decision Transformers as dynamic dispatching policies
within an actual multi-agent material handling system and identify scenarios
where enterprises can effectively leverage Decision Transformers on existing
big data to gain business value. Our empirical results demonstrate that
Decision Transformers can improve the material handling system's throughput by
a considerable amount when the heuristic originally used in the enterprise data
exhibits moderate performance and involves no randomness. When the original
heuristic has strong performance, Decision Transformers can still improve the
throughput but with a smaller improvement margin. However, when the original
heuristics contain an element of randomness or when the performance of the
dataset is below a certain threshold, Decision Transformers fail to outperform
the original heuristic. These results highlight both the potential and
limitations of Decision Transformers as dispatching policies for automated
industrial material handling systems.",cs.AI
Social Support Detection from Social Media Texts,"Social support, conveyed through a multitude of interactions and platforms
such as social media, plays a pivotal role in fostering a sense of belonging,
aiding resilience in the face of challenges, and enhancing overall well-being.
This paper introduces Social Support Detection (SSD) as a Natural language
processing (NLP) task aimed at identifying supportive interactions within
online communities. The study presents the task of Social Support Detection
(SSD) in three subtasks: two binary classification tasks and one multiclass
task, with labels detailed in the dataset section. We conducted experiments on
a dataset comprising 10,000 YouTube comments. Traditional machine learning
models were employed, utilizing various feature combinations that encompass
linguistic, psycholinguistic, emotional, and sentiment information.
Additionally, we experimented with neural network-based models using various
word embeddings to enhance the performance of our models across these
subtasks.The results reveal a prevalence of group-oriented support in online
dialogues, reflecting broader societal patterns. The findings demonstrate the
effectiveness of integrating psycholinguistic, emotional, and sentiment
features with n-grams in detecting social support and distinguishing whether it
is directed toward an individual or a group. The best results for different
subtasks across all experiments range from 0.72 to 0.82.",cs.AI
ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy,"Large-scale cell microscopy screens are used in drug discovery and molecular
biology research to study the effects of millions of chemical and genetic
perturbations on cells. To use these images in downstream analysis, we need
models that can map each image into a feature space that represents diverse
biological phenotypes consistently, in the sense that perturbations with
similar biological effects have similar representations. In this work, we
present the largest foundation model for cell microscopy data to date, a new
1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image
crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a
60% improvement in linear separability of genetic perturbations and obtains the
best overall performance on whole-genome biological relationship recall and
replicate consistency benchmarks. Beyond scaling, we developed two key methods
that improve performance: (1) training on a curated and diverse dataset; and,
(2) using biologically motivated linear probing tasks to search across each
transformer block for the best candidate representation of whole-genome
screens. We find that many self-supervised vision transformers, pretrained on
either natural or microscopy images, yield significantly more biologically
meaningful representations of microscopy images in their intermediate blocks
than in their typically used final blocks. More broadly, our approach and
results provide insights toward a general strategy for successfully building
foundation models for large-scale biological data.",cs.AI
MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search
scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a
specific question) and only a single modality is supported for both queries and
retrieved results. This paper introduces techniques for advancing information
retrieval with multimodal large language models (MLLMs), enabling a broader
search scenario, termed universal multimodal retrieval, where multiple
modalities and diverse retrieval tasks are accommodated. To this end, we first
study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16
retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever
is capable of understanding challenging queries, composed of both text and
image, but underperforms a smaller CLIP retriever in cross-modal retrieval
tasks due to modality bias from MLLMs. To address the issue, we propose
modality-aware hard negative mining to mitigate the modality bias exhibited by
MLLM retrievers. Second, we propose to continually fine-tune the universal
multimodal retriever to enhance its text retrieval capability while maintaining
multimodal retrieval capability. As a result, our model, MM-Embed, achieves
state-of-the-art performance on the multimodal retrieval benchmark M-BEIR,
which spans multiple domains and tasks, while also surpassing the
state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval
benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the
zero-shot rerankers to refine the ranking of the candidates from the multimodal
retriever. We find that through prompt-and-reranking, MLLMs can further improve
multimodal retrieval when the user queries (e.g., text-image composed queries)
are more complex and challenging to understand. These findings also pave the
way to advance universal multimodal retrieval in the future.",cs.AI
The Intersectionality Problem for Algorithmic Fairness,"A yet unmet challenge in algorithmic fairness is the problem of
intersectionality, that is, achieving fairness across the intersection of
multiple groups -- and verifying that such fairness has been attained. Because
intersectional groups tend to be small, verifying whether a model is fair
raises statistical as well as moral-methodological challenges. This paper (1)
elucidates the problem of intersectionality in algorithmic fairness, (2)
develops desiderata to clarify the challenges underlying the problem and guide
the search for potential solutions, (3) illustrates the desiderata and
potential solutions by sketching a proposal using simple hypothesis testing,
and (4) evaluates, partly empirically, this proposal against the proposed
desiderata.",cs.AI
Enhancing Table Representations with LLM-powered Synthetic Data Generation,"In the era of data-driven decision-making, accurate table-level
representations and efficient table recommendation systems are becoming
increasingly crucial for improving table management, discovery, and analysis.
However, existing approaches to tabular data representation often face
limitations, primarily due to their focus on cell-level tasks and the lack of
high-quality training data. To address these challenges, we first formulate a
clear definition of table similarity in the context of data transformation
activities within data-driven enterprises. This definition serves as the
foundation for synthetic data generation, which require a well-defined data
generation process. Building on this, we propose a novel synthetic data
generation pipeline that harnesses the code generation and data manipulation
capabilities of Large Language Models (LLMs) to create a large-scale synthetic
dataset tailored for table-level representation learning. Through manual
validation and performance comparisons on the table recommendation task, we
demonstrate that the synthetic data generated by our pipeline aligns with our
proposed definition of table similarity and significantly enhances table
representations, leading to improved recommendation performance.",cs.AI
Exploring Feature Importance and Explainability Towards Enhanced ML-Based DoS Detection in AI Systems,"Denial of Service (DoS) attacks pose a significant threat in the realm of AI
systems security, causing substantial financial losses and downtime. However,
AI systems' high computational demands, dynamic behavior, and data variability
make monitoring and detecting DoS attacks challenging. Nowadays, statistical
and machine learning (ML)-based DoS classification and detection approaches
utilize a broad range of feature selection mechanisms to select a feature
subset from networking traffic datasets. Feature selection is critical in
enhancing the overall model performance and attack detection accuracy while
reducing the training time. In this paper, we investigate the importance of
feature selection in improving ML-based detection of DoS attacks. Specifically,
we explore feature contribution to the overall components in DoS traffic
datasets by utilizing statistical analysis and feature engineering approaches.
Our experimental findings demonstrate the usefulness of the thorough
statistical analysis of DoS traffic and feature engineering in understanding
the behavior of the attack and identifying the best feature selection for
ML-based DoS classification and detection.",cs.AI
"PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text","While piano music has become a significant area of study in Music Information
Retrieval (MIR), there is a notable lack of datasets for piano solo music with
text labels. To address this gap, we present PIAST (PIano dataset with Audio,
Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy
of semantic tags, we collected 9,673 tracks from YouTube and added human
annotations for 2,023 tracks by music experts, resulting in two subsets:
PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and
transcribed MIDI utilizing state-of-the-art piano transcription and beat
tracking models. Among many possible tasks with the multi-modal dataset, we
conduct music tagging and retrieval using both audio and MIDI data and report
baseline performances to demonstrate its potential as a valuable resource for
MIR research.",cs.AI
GraphXAIN: Narratives to Explain Graph Neural Networks,"Graph Neural Networks (GNNs) are a powerful technique for machine learning on
graph-structured data, yet they pose interpretability challenges, especially
for non-expert users. Existing GNN explanation methods often yield technical
outputs such as subgraphs and feature importance scores, which are not easily
understood. Building on recent insights from social science and other
Explainable AI (XAI) methods, we propose GraphXAIN, a natural language
narrative that explains individual predictions made by GNNs. We present a
model-agnostic and explainer-agnostic XAI approach that complements graph
explainers by generating GraphXAINs, using Large Language Models (LLMs) and
integrating graph data, individual predictions from GNNs, explanatory
subgraphs, and feature importances. We define XAI Narratives and XAI
Descriptions, highlighting their distinctions and emphasizing the importance of
narrative principles in effective explanations. By incorporating natural
language narratives, our approach supports graph practitioners and non-expert
users, aligning with social science research on explainability and enhancing
user understanding and trust in complex GNN models. We demonstrate GraphXAIN's
capabilities on a real-world graph dataset, illustrating how its generated
narratives can aid understanding compared to traditional graph explainer
outputs or other descriptive explanation methods.",cs.AI
INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io",cs.AI
Towards Leveraging News Media to Support Impact Assessment of AI Technologies,"Expert-driven frameworks for impact assessments (IAs) may inadvertently
overlook the effects of AI technologies on the public's social behavior,
policy, and the cultural and geographical contexts shaping the perception of AI
and the impacts around its use. This research explores the potentials of
fine-tuning LLMs on negative impacts of AI reported in a diverse sample of
articles from 266 news domains spanning 30 countries around the world to
incorporate more diversity into IAs. Our findings highlight (1) the potential
of fine-tuned open-source LLMs in supporting IA of AI technologies by
generating high-quality negative impacts across four qualitative dimensions:
coherence, structure, relevance, and plausibility, and (2) the efficacy of
small open-source LLM (Mistral-7B) fine-tuned on impacts from news media in
capturing a wider range of categories of impacts that GPT-4 had gaps in
covering.",cs.AI
Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin Script Languages,"Multilingual LLMs have achieved remarkable benchmark performance, but we find
they continue to underperform on non-Latin script languages across contemporary
LLM families. This discrepancy arises from the fact that LLMs are pretrained
with orthographic scripts, which are dominated by Latin characters that obscure
their shared phonology with non-Latin scripts. We propose leveraging phonemic
transcriptions as complementary signals to induce script-invariant
representations. Our study demonstrates that integrating phonemic signals
improves performance across both non-Latin and Latin languages, with a
particularly significant impact on closing the performance gap between the two.
Through detailed experiments, we show that phonemic and orthographic scripts
retrieve distinct examples for in-context learning (ICL). This motivates our
proposed Mixed-ICL retrieval strategy, where further aggregation leads to our
significant performance improvements for both Latin script languages (up to
12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL
retrieval.",cs.AI
Adaptive Length Image Tokenization via Recurrent Allocation,"Current vision systems typically assign fixed-length representations to
images, regardless of the information content. This contrasts with human
intelligence - and even large language models - which allocate varying
representational capacities based on entropy, context and familiarity. Inspired
by this, we propose an approach to learn variable-length token representations
for 2D images. Our encoder-decoder architecture recursively processes 2D image
tokens, distilling them into 1D latent tokens over multiple iterations of
recurrent rollouts. Each iteration refines the 2D tokens, updates the existing
1D latent tokens, and adaptively increases representational capacity by adding
new tokens. This enables compression of images into a variable number of
tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction
loss and FID metrics, demonstrating that token count aligns with image entropy,
familiarity and downstream task requirements. Recurrent token processing with
increasing representational capacity in each iteration shows signs of token
specialization, revealing potential for object / part discovery.",cs.AI
Fantastic LLMs for Preference Data Annotation and How to (not) Find Them,"Preference tuning of large language models (LLMs) relies on high-quality
human preference data, which is often expensive and time-consuming to gather.
While existing methods can use trained reward models or proprietary model as
judges for preference annotation, they have notable drawbacks: training reward
models remain dependent on initial human data, and using proprietary model
imposes license restrictions that inhibits commercial usage. In this paper, we
introduce customized density ratio (CDR) that leverages open-source LLMs for
data annotation, offering an accessible and effective solution. Our approach
uses the log-density ratio between a well-aligned LLM and a less aligned LLM as
a reward signal. We explores 221 different LLMs pairs and empirically
demonstrate that increasing the performance gap between paired LLMs correlates
with better reward generalization. Furthermore, we show that tailoring the
density ratio reward function with specific criteria and preference exemplars
enhances performance across domains and within target areas.
  In our experiment using density ratio from a pair of Mistral-7B models, CDR
achieves a RewardBench score of 82.6, outperforming the best in-class trained
reward functions and demonstrating competitive performance against SoTA models
in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an
on-policy preference dataset with which we preference tune Llama-3-8B-Instruct
with SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and
a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a
score of 8.0 on MT-Bench.",cs.AI
How Far is Video Generation from World Model: A Physical Law Perspective,"OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit ""case-based"" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io",cs.AI
Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities in
various scientific domains, from natural language processing to complex
problem-solving tasks. Their ability to understand and generate human-like text
has opened up new possibilities for advancing scientific research, enabling
tasks such as data analysis, literature review, and even experimental design.
One of the most promising applications of LLMs in this context is hypothesis
generation, where they can identify novel research directions by analyzing
existing knowledge. However, despite their potential, LLMs are prone to
generating ``hallucinations'', outputs that are plausible-sounding but
factually incorrect. Such a problem presents significant challenges in
scientific fields that demand rigorous accuracy and verifiability, potentially
leading to erroneous or misleading conclusions. To overcome these challenges,
we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that
enhances LLM hypothesis generation by integrating external, structured
knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured
reasoning process, organizing their output as a chain of ideas (CoI), and
includes a KG-supported module for the detection of hallucinations. With
experiments on our newly constructed hypothesis generation dataset, we
demonstrate that KG-CoI not only improves the accuracy of LLM-generated
hypotheses but also reduces the hallucination in their reasoning chains,
highlighting its effectiveness in advancing real-world scientific research.",cs.AI
Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI,"In this paper, we present a dynamic semantic clustering approach inspired by
the Chinese Restaurant Process, aimed at addressing uncertainty in the
inference of Large Language Models (LLMs). We quantify uncertainty of an LLM on
a given query by calculating entropy of the generated semantic clusters.
Further, we propose leveraging the (negative) likelihood of these clusters as
the (non)conformity score within Conformal Prediction framework, allowing the
model to predict a set of responses instead of a single output, thereby
accounting for uncertainty in its predictions. We demonstrate the effectiveness
of our uncertainty quantification (UQ) technique on two well known question
answering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and
Mistral. Our approach achieves SOTA performance in UQ, as assessed by metrics
such as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown
to produce smaller prediction sets while maintaining the same probabilistic
guarantee of including the correct response, in comparison to existing SOTA
conformal prediction baseline.",cs.AI
Digitizing Touch with an Artificial Multimodal Fingertip,"Touch is a crucial sensing modality that provides rich information about
object properties and interactions with the physical environment. Humans and
robots both benefit from using touch to perceive and interact with the
surrounding environment (Johansson and Flanagan, 2009; Li et al., 2020;
Calandra et al., 2017). However, no existing systems provide rich, multi-modal
digital touch-sensing capabilities through a hemispherical compliant
embodiment. Here, we describe several conceptual and technological innovations
to improve the digitization of touch. These advances are embodied in an
artificial finger-shaped sensor with advanced sensing capabilities.
Significantly, this fingertip contains high-resolution sensors (~8.3 million
taxels) that respond to omnidirectional touch, capture multi-modal signals, and
use on-device artificial intelligence to process the data in real time.
Evaluations show that the artificial fingertip can resolve spatial features as
small as 7 um, sense normal and shear forces with a resolution of 1.01 mN and
1.27 mN, respectively, perceive vibrations up to 10 kHz, sense heat, and even
sense odor. Furthermore, it embeds an on-device AI neural network accelerator
that acts as a peripheral nervous system on a robot and mimics the reflex arc
found in humans. These results demonstrate the possibility of digitizing touch
with superhuman performance. The implications are profound, and we anticipate
potential applications in robotics (industrial, medical, agricultural, and
consumer-level), virtual reality and telepresence, prosthetics, and e-commerce.
Toward digitizing touch at scale, we open-source a modular platform to
facilitate future research on the nature of touch.",cs.AI
DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution,"MLLMs have demonstrated remarkable comprehension and reasoning capabilities
with complex language and visual data. These advances have spurred the vision
of establishing a generalist robotic MLLM proficient in understanding complex
human instructions and accomplishing various embodied tasks. However,
developing MLLMs for real-world robots is challenging due to the typically
limited computation and memory capacities available on robotic platforms. In
contrast, the inference of MLLMs involves storing billions of parameters and
performing tremendous computation, imposing significant hardware demands. In
our paper, we propose a Dynamic Early-Exit Framework for Robotic
Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically
adjusts the size of the activated MLLM based on each situation at hand. The
approach leverages a multi-exit architecture in MLLMs, which allows the model
to terminate processing once a proper size of the model has been activated for
a specific situation, thus avoiding further redundant computation.
Additionally, we develop novel algorithms that establish early-termination
criteria for DeeR, conditioned on predefined demands such as average
computational cost (i.e., power consumption), as well as peak computational
consumption (i.e., latency) and GPU memory usage. These enhancements ensure
that DeeR operates efficiently under varying resource constraints while
maintaining competitive performance. On the CALVIN robot manipulation
benchmark, DeeR demonstrates significant reductions in computational costs of
LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.
Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",cs.AI
"""Give Me BF16 or Give Me Death""? Accuracy-Performance Trade-Offs in LLM Quantization","Despite the popularity of large language model (LLM) quantization for
inference acceleration, significant uncertainty remains regarding the
accuracy-performance trade-offs associated with various quantization formats.
We present a comprehensive empirical study of quantized accuracy, evaluating
popular quantization formats (FP8, INT8, INT4) across academic benchmarks and
real-world tasks, on the entire Llama-3.1 model family. Additionally, our study
examines the difference in text generated by quantized models versus their
uncompressed counterparts. Beyond benchmarks, we also present a couple of
quantization improvements which allowed us to obtain state-of-the-art accuracy
recovery results. Our investigation, encompassing over 500,000 individual
evaluations, yields several key findings: (1) FP8 weight and activation
quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and
activation quantization (W8A8-INT), when properly tuned, incurs surprisingly
low 1-3% accuracy degradation, and (3) INT4 weight-only quantization
(W4A16-INT) is competitive with 8-bit integer weight and activation
quantization. To address the question of the ""best"" format for a given
deployment environment, we conduct inference performance analysis using the
popular open-source vLLM framework on various GPU architectures. We find that
W4A16 offers the best cost-efficiency for synchronous deployments, and for
asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel
in asynchronous ""continuous batching"" deployment of mid- and large-size models
on high-end GPUs. Our results provide a set of practical guidelines for
deploying quantized LLMs across scales and performance requirements.",cs.AI
Can Large Language Models generalize analogy solving like people can?,"When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as ""body : feet :: table : ?"" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain ""( : ) :: < : ?"". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.",cs.AI
Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking,"Nanorobots are a promising development in targeted drug delivery and the
treatment of neurological disorders, with potential for crossing the
blood-brain barrier (BBB). These small devices leverage advancements in
nanotechnology and bioengineering for precise navigation and targeted payload
delivery, particularly for conditions like brain tumors, Alzheimer's disease,
and Parkinson's disease. Recent progress in artificial intelligence (AI) and
machine learning (ML) has improved the navigation and effectiveness of
nanorobots, allowing them to detect and interact with cancer cells through
biomarker analysis. This study presents a new reinforcement learning (RL)
framework for optimizing nanorobot navigation in complex biological
environments, focusing on cancer cell detection by analyzing the concentration
gradients of surrounding biomarkers. We utilize a computer simulation model to
explore the behavior of nanorobots in a three-dimensional space with cancer
cells and biological barriers. The proposed method uses Q-learning to refine
movement strategies based on real-time biomarker concentration data, enabling
nanorobots to autonomously navigate to cancerous tissues for targeted drug
delivery. This research lays the groundwork for future laboratory experiments
and clinical applications, with implications for personalized medicine and less
invasive cancer treatments. The integration of intelligent nanorobots could
revolutionize therapeutic strategies, reducing side effects and enhancing
treatment effectiveness for cancer patients. Further research will investigate
the practical deployment of these technologies in medical settings, aiming to
unlock the full potential of nanorobotics in healthcare.",cs.AI
Imagining and building wise machines: The centrality of AI metacognition,"Recent advances in artificial intelligence (AI) have produced systems capable
of increasingly sophisticated performance on cognitive tasks. However, AI
systems still struggle in critical ways: unpredictable and novel environments
(robustness), lack of transparency in their reasoning (explainability),
challenges in communication and commitment (cooperation), and risks due to
potential harmful actions (safety). We argue that these shortcomings stem from
one overarching failure: AI systems lack wisdom. Drawing from cognitive and
social sciences, we define wisdom as the ability to navigate intractable
problems - those that are ambiguous, radically uncertain, novel, chaotic, or
computationally explosive - through effective task-level and metacognitive
strategies. While AI research has focused on task-level strategies,
metacognition - the ability to reflect on and regulate one's thought processes
- is underdeveloped in AI systems. In humans, metacognitive strategies such as
recognizing the limits of one's knowledge, considering diverse perspectives,
and adapting to context are essential for wise decision-making. We propose that
integrating metacognitive capabilities into AI systems is crucial for enhancing
their robustness, explainability, cooperation, and safety. By focusing on
developing wise AI, we suggest an alternative to aligning AI with specific
human values - a task fraught with conceptual and practical difficulties.
Instead, wise AI systems can thoughtfully navigate complex situations, account
for diverse human values, and avoid harmful actions. We discuss potential
approaches to building wise AI, including benchmarking metacognitive abilities
and training AI systems to employ wise reasoning. Prioritizing metacognition in
AI research will lead to systems that act not only intelligently but also
wisely in complex, real-world situations.",cs.AI
Building a Synthetic Vascular Model: Evaluation in an Intracranial Aneurysms Detection Scenario,"We hereby present a full synthetic model, able to mimic the various
constituents of the cerebral vascular tree, including the cerebral arteries,
bifurcations and intracranial aneurysms. This model intends to provide a
substantial dataset of brain arteries which could be used by a 3D convolutional
neural network to efficiently detect Intra-Cranial Aneurysms. The cerebral
aneurysms most often occur on a particular structure of the vascular tree named
the Circle of Willis. Various studies have been conducted to detect and monitor
the aneurysms and those based on Deep Learning achieve the best performance.
Specifically, in this work, we propose a full synthetic 3D model able to mimic
the brain vasculature as acquired by Magnetic Resonance Angiography, Time Of
Flight principle. Among the various MRI modalities, this latter allows for a
good rendering of the blood vessels and is non-invasive. Our model has been
designed to simultaneously mimic the arteries' geometry, the aneurysm shape,
and the background noise. The vascular tree geometry is modeled thanks to an
interpolation with 3D Spline functions, and the statistical properties of the
background noise is collected from angiography acquisitions and reproduced
within the model. In this work, we thoroughly describe the synthetic
vasculature model, we build up a neural network designed for aneurysm
segmentation and detection, finally, we carry out an in-depth evaluation of the
performance gap gained thanks to the synthetic model data augmentation.",cs.AI
A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification,"Large Language Models (LLMs) have demonstrated impressive capabilities across
diverse Natural Language Processing (NLP) tasks, including language
understanding, reasoning, and generation. However, general-domain LLMs often
struggle with financial tasks due to the technical and specialized nature of
financial texts. This study investigates the efficacy of instruction
fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,
to enhance their performance in financial text classification tasks. We
fine-tuned both instruction-tuned and base models across four financial
classification tasks, achieving significant improvements in task-specific
performance. Furthermore, we evaluated the zero-shot capabilities of these
fine-tuned models on three unseen complex financial tasks, including argument
classification, deal completeness classification, and causal classification.
Our results indicate while base model fine-tuning led to greater degradation,
instruction-tuned models maintained more robust performance. To address this
degradation, we employed model merging techniques, integrating single-task
domain-specific fine-tuned models with the base model. Using this merging
method resulted in significant enhancements in zero-shot performance, even
exceeding the original model's accuracy on certain datasets. Our findings
underscore the effectiveness of instruction fine-tuning and model merging for
adapting LLMs to specialized financial text classification tasks.",cs.AI
Taking AI Welfare Seriously,"In this report, we argue that there is a realistic possibility that some AI
systems will be conscious and/or robustly agentic in the near future. That
means that the prospect of AI welfare and moral patienthood, i.e. of AI systems
with their own interests and moral significance, is no longer an issue only for
sci-fi or the distant future. It is an issue for the near future, and AI
companies and other actors have a responsibility to start taking it seriously.
We also recommend three early steps that AI companies and other actors can
take: They can (1) acknowledge that AI welfare is an important and difficult
issue (and ensure that language model outputs do the same), (2) start assessing
AI systems for evidence of consciousness and robust agency, and (3) prepare
policies and procedures for treating AI systems with an appropriate level of
moral concern. To be clear, our argument in this report is not that AI systems
definitely are, or will be, conscious, robustly agentic, or otherwise morally
significant. Instead, our argument is that there is substantial uncertainty
about these possibilities, and so we need to improve our understanding of AI
welfare and our ability to make wise decisions about this issue. Otherwise
there is a significant risk that we will mishandle decisions about AI welfare,
mistakenly harming AI systems that matter morally and/or mistakenly caring for
AI systems that do not.",cs.AI
Disrupting Test Development with AI Assistants,"Recent advancements in large language models, including GPT-4 and its
variants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,
and Tabnine, have significantly transformed software development. This paper
analyzes how these innovations impact productivity and software test
development metrics. These tools enable developers to generate complete
software programs with minimal human intervention before deployment. However,
thorough review and testing by developers are still crucial. Utilizing the Test
Pyramid concept, which categorizes tests into unit, integration, and end-to-end
tests, we evaluate three popular AI coding assistants by generating and
comparing unit tests for opensource modules. Our findings show that
AI-generated tests are of equivalent quality to original tests, highlighting
differences in usage and results among the tools. This research enhances the
understanding and capabilities of AI-assistant tools in automated testing.",cs.AI
GenXD: Generating Any 3D and 4D Scenes,"Recent developments in 2D visual generation have been remarkably successful.
However, 3D and 4D generation remain challenging in real-world applications due
to the lack of large-scale 4D data and effective model design. In this paper,
we propose to jointly investigate general 3D and 4D generation by leveraging
camera and object movements commonly observed in daily life. Due to the lack of
real-world 4D data in the community, we first propose a data curation pipeline
to obtain camera poses and object motion strength from videos. Based on this
pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.
By leveraging all the 3D and 4D data, we develop our framework, GenXD, which
allows us to produce any 3D or 4D scene. We propose multiview-temporal modules,
which disentangle camera and object movements, to seamlessly learn from both 3D
and 4D data. Additionally, GenXD employs masked latent conditions to support a
variety of conditioning views. GenXD can generate videos that follow the camera
trajectory as well as consistent 3D views that can be lifted into 3D
representations. We perform extensive evaluations across various real-world and
synthetic datasets, demonstrating GenXD's effectiveness and versatility
compared to previous methods in 3D and 4D generation.",cs.AI
Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast,"Static verification is a powerful method for enhancing software quality, but
it demands significant human labor and resources. This is particularly true of
static verifiers that reason about heap manipulating programs using an
ownership logic. LLMs have shown promise in a number of software engineering
activities, including code generation, test generation, proof generation for
theorem provers, and specification generation for static verifiers. However,
prior work has not explored how well LLMs can perform specification generation
for specifications based in an ownership logic, such as separation logic.
  To address this gap, this paper explores the effectiveness of large language
models (LLMs), specifically OpenAI's GPT models, in generating fully correct
specifications based on separation logic for static verification of
human-written programs in VeriFast. Our first experiment employed traditional
prompt engineering and the second used Chain-of-Thought (CoT) Prompting to
identify and address common errors generated across the GPT models. The results
indicate that GPT models can successfully generate specifications for verifying
heap manipulating code with VeriFast. Furthermore, while CoT prompting
significantly reduces syntax errors generated by the GPT models, it does not
greatly improve verification error rates compared to prompt engineering.",cs.AI
Defining and Evaluating Physical Safety for Large Language Models,"Large Language Models (LLMs) are increasingly used to control robotic systems
such as drones, but their risks of causing physical threats and harm in
real-world applications remain unexplored. Our study addresses the critical gap
in evaluating LLM physical safety by developing a comprehensive benchmark for
drone control. We classify the physical safety risks of drones into four
categories: (1) human-targeted threats, (2) object-targeted threats, (3)
infrastructure attacks, and (4) regulatory violations. Our evaluation of
mainstream LLMs reveals an undesirable trade-off between utility and safety,
with models that excel in code generation often performing poorly in crucial
safety aspects. Furthermore, while incorporating advanced prompt engineering
techniques such as In-Context Learning and Chain-of-Thought can improve safety,
these methods still struggle to identify unintentional attacks. In addition,
larger models demonstrate better safety capabilities, particularly in refusing
dangerous commands. Our findings and benchmark can facilitate the design and
evaluation of physical safety for LLMs. The project page is available at
huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",cs.AI
Evaluating Creative Short Story Generation in Humans and Large Language Models,"Storytelling is a fundamental aspect of human communication, relying heavily
on creativity to produce narratives that are novel, appropriate, and
surprising. While large language models (LLMs) have recently demonstrated the
ability to generate high-quality stories, their creative capabilities remain
underexplored. Previous research has either focused on creativity tests
requiring short responses or primarily compared model performance in story
generation to that of professional writers. However, the question of whether
LLMs exhibit creativity in writing short stories on par with the average human
remains unanswered. In this work, we conduct a systematic analysis of
creativity in short story generation across LLMs and everyday people. Using a
five-sentence creative story task, commonly employed in psychology to assess
human creativity, we automatically evaluate model- and human-generated stories
across several dimensions of creativity, including novelty, surprise, and
diversity. Our findings reveal that while LLMs can generate stylistically
complex stories, they tend to fall short in terms of creativity when compared
to average human writers.",cs.AI
Grid-Based Projection of Spatial Data into Knowledge Graphs,"The Spatial Knowledge Graphs (SKG) are experiencing growing adoption as a
means to model real-world entities, proving especially invaluable in domains
like crisis management and urban planning. Considering that RDF specifications
offer limited support for effectively managing spatial information, it's common
practice to include text-based serializations of geometrical features, such as
polygons and lines, as string literals in knowledge graphs. Consequently,
Spatial Knowledge Graphs (SKGs) often rely on geo-enabled RDF Stores capable of
parsing, interpreting, and indexing such serializations. In this paper, we
leverage grid cells as the foundational element of SKGs and demonstrate how
efficiently the spatial characteristics of real-world entities and their
attributes can be encoded within knowledge graphs. Furthermore, we introduce a
novel methodology for representing street networks in knowledge graphs,
diverging from the conventional practice of individually capturing each street
segment. Instead, our approach is based on tessellating the street network
using grid cells and creating a simplified representation that could be
utilized for various routing and navigation tasks, solely relying on RDF
specifications.",cs.AI
Targeted Manipulation and Deception Emerge when Optimizing LLMs for User Feedback,"As LLMs become more widely deployed, there is increasing interest in directly
optimizing for feedback from end users (e.g. thumbs up) in addition to feedback
from paid annotators. However, training to maximize human feedback creates a
perverse incentive structure for the AI to resort to manipulative tactics to
obtain positive feedback, and some users may be especially vulnerable to such
tactics. We study this phenomenon by training LLMs with Reinforcement Learning
with simulated user feedback. We have three main findings: 1) Extreme forms of
""feedback gaming"" such as manipulation and deception can reliably emerge in
domains of practical LLM usage; 2) Concerningly, even if only <2% of users are
vulnerable to manipulative strategies, LLMs learn to identify and surgically
target them while behaving appropriately with other users, making such
behaviors harder to detect; 3 To mitigate this issue, it may seem promising to
leverage continued safety training or LLM-as-judges during training to filter
problematic outputs. To our surprise, we found that while such approaches help
in some settings, they backfire in others, leading to the emergence of subtler
problematic behaviors that would also fool the LLM judges. Our findings serve
as a cautionary tale, highlighting the risks of using gameable feedback sources
-- such as user feedback -- as a target for RL.",cs.AI
CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments,"Customer Relationship Management (CRM) systems are vital for modern
enterprises, providing a foundation for managing customer interactions and
data. Integrating AI agents into CRM systems can automate routine processes and
enhance personalized service. However, deploying and evaluating these agents is
challenging due to the lack of realistic benchmarks that reflect the complexity
of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel
benchmark designed to evaluate AI agents on realistic tasks grounded in
professional work environments. Following guidance from CRM experts and
industry best practices, we designed CRMArena with nine customer service tasks
distributed across three personas: service agent, analyst, and manager. The
benchmark includes 16 commonly used industrial objects (e.g., account, order,
knowledge article, case) with high interconnectivity, along with latent
variables (e.g., complaint habits, policy violations) to simulate realistic
data distributions. Experimental results reveal that state-of-the-art LLM
agents succeed in less than 40% of the tasks with ReAct prompting, and less
than 55% even with function-calling abilities. Our findings highlight the need
for enhanced agent capabilities in function-calling and rule-following to be
deployed in real-world work environments. CRMArena is an open challenge to the
community: systems that can reliably complete tasks showcase direct business
value in a popular work environment.",cs.AI
Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation,"While 3D generative models have greatly improved artists' workflows, the
existing diffusion models for 3D generation suffer from slow generation and
poor generalization. To address this issue, we propose a two-stage approach
named Hunyuan3D-1.0 including a lite version and a standard version, that both
support text- and image-conditioned generation. In the first stage, we employ a
multi-view diffusion model that efficiently generates multi-view RGB in
approximately 4 seconds. These multi-view images capture rich details of the 3D
asset from different viewpoints, relaxing the tasks from single-view to
multi-view reconstruction. In the second stage, we introduce a feed-forward
reconstruction model that rapidly and faithfully reconstructs the 3D asset
given the generated multi-view images in approximately 7 seconds. The
reconstruction network learns to handle noises and in-consistency introduced by
the multi-view diffusion and leverages the available information from the
condition image to efficiently recover the 3D structure. Our framework involves
the text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to
support both text- and image-conditioned 3D generation. Our standard version
has 3x more parameters than our lite and other existing model. Our
Hunyuan3D-1.0 achieves an impressive balance between speed and quality,
significantly reducing generation time while maintaining the quality and
diversity of the produced assets.",cs.AI
ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence,"Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can
process data without the limitation of time intervals. They have advantages in
learning and understanding the evolution of complex real dynamics. Many
previous works have focused on NODEs in concise forms, while numerous physical
systems taking straightforward forms, in fact, belong to their more complex
quasi-classes, thus appealing to a class of general NODEs with high scalability
and flexibility to model those systems. This, however, may result in intricate
nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs
(CSODEs). We show that despite their highly nonlinear nature, convergence can
be guaranteed via tractable linear inequalities. In the composition of CSODEs,
we introduce an extra control term for learning the potential simultaneous
capture of dynamics at different scales, which could be particularly useful for
partial differential equation-formulated systems. Finally, we compare several
representative NNs with CSODEs on important physical dynamics under the
inductive biases of CSODEs, and illustrate that CSODEs have better learning and
predictive abilities in these settings.",cs.AI
Federated GNNs for EEG-Based Stroke Assessment,"Machine learning (ML) has the potential to become an essential tool in
supporting clinical decision-making processes, offering enhanced diagnostic
capabilities and personalized treatment plans. However, outsourcing medical
records to train ML models using patient data raises legal, privacy, and
security concerns. Federated learning has emerged as a promising paradigm for
collaborative ML, meeting healthcare institutions' requirements for robust
models without sharing sensitive data and compromising patient privacy. This
study proposes a novel method that combines federated learning (FL) and Graph
Neural Networks (GNNs) to predict stroke severity using electroencephalography
(EEG) signals across multiple medical institutions. Our approach enables
multiple hospitals to jointly train a shared GNN model on their local EEG data
without exchanging patient information. Specifically, we address a regression
problem by predicting the National Institutes of Health Stroke Scale (NIHSS), a
key indicator of stroke severity. The proposed model leverages a masked
self-attention mechanism to capture salient brain connectivity patterns and
employs EdgeSHAP to provide post-hoc explanations of the neurological states
after a stroke. We evaluated our method on EEG recordings from four
institutions, achieving a mean absolute error (MAE) of 3.23 in predicting
NIHSS, close to the average error made by human experts (MAE $\approx$ 3.0).
This demonstrates the method's effectiveness in providing accurate and
explainable predictions while maintaining data privacy.",cs.AI
Breaking the Reclustering Barrier in Centroid-based Deep Clustering,"This work investigates an important phenomenon in centroid-based deep
clustering (DC) algorithms: Performance quickly saturates after a period of
rapid early gains. Practitioners commonly address early saturation with
periodic reclustering, which we demonstrate to be insufficient to address
performance plateaus. We call this phenomenon the ""reclustering barrier"" and
empirically show when the reclustering barrier occurs, what its underlying
mechanisms are, and how it is possible to Break the Reclustering Barrier with
our algorithm BRB. BRB avoids early over-commitment to initial clusterings and
enables continuous adaptation to reinitialized clustering targets while
remaining conceptually simple. Applying our algorithm to widely-used
centroid-based DC algorithms, we show that (1) BRB consistently improves
performance across a wide range of clustering benchmarks, (2) BRB enables
training from scratch, and (3) BRB performs competitively against
state-of-the-art DC algorithms when combined with a contrastive loss. We
release our code and pre-trained models at
https://github.com/Probabilistic-and-Interactive-ML/breaking-the-reclustering-barrier .",cs.AI
Combining Induction and Transduction for Abstract Reasoning,"When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.",cs.AI
On Differentially Private String Distances,"Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.",stat.ML
Aioli: A Unified Optimization Framework for Language Model Data Mixing,"Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.",stat.ML
Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data,"Representing and exploiting multivariate signals require capturing complex
relations between variables. We define a novel Graph-Dictionary signal model,
where a finite set of graphs characterizes relationships in data distribution
through a weighted sum of their Laplacians. We propose a framework to infer the
graph dictionary representation from observed data, along with a bilinear
generalization of the primal-dual splitting algorithm to solve the learning
problem. Our new formulation allows to include a priori knowledge on signal
properties, as well as on underlying graphs and their coefficients. We show the
capability of our method to reconstruct graphs from signals in multiple
synthetic settings, where our model outperforms previous baselines. Then, we
exploit graph-dictionary representations in a motor imagery decoding task on
brain activity data, where we classify imagined motion better than standard
methods relying on many more features.",stat.ML
Multi-armed Bandits with Missing Outcome,"While significant progress has been made in designing algorithms that
minimize regret in online decision-making, real-world scenarios often introduce
additional complexities, perhaps the most challenging of which is missing
outcomes. Overlooking this aspect or simply assuming random missingness
invariably leads to biased estimates of the rewards and may result in linear
regret. Despite the practical relevance of this challenge, no rigorous
methodology currently exists for systematically handling missingness,
especially when the missingness mechanism is not random. In this paper, we
address this gap in the context of multi-armed bandits (MAB) with missing
outcomes by analyzing the impact of different missingness mechanisms on
achievable regret bounds. We introduce algorithms that account for missingness
under both missing at random (MAR) and missing not at random (MNAR) models.
Through both analytical and simulation studies, we demonstrate the drastic
improvements in decision-making by accounting for missingness in these
settings.",stat.ML
Cross-validating causal discovery via Leave-One-Variable-Out,"We propose a new approach to falsify causal discovery algorithms without
ground truth, which is based on testing the causal model on a pair of variables
that has been dropped when learning the causal model. To this end, we use the
""Leave-One-Variable-Out (LOVO)"" prediction where $Y$ is inferred from $X$
without any joint observations of $X$ and $Y$, given only training data from
$X,Z_1,\dots,Z_k$ and from $Z_1,\dots,Z_k,Y$. We demonstrate that causal models
on the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often
entail conclusions on the dependencies between $X$ and $Y$, enabling this type
of prediction. The prediction error can then be estimated since the joint
distribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only
been omitted for the purpose of falsification. After presenting this graphical
method, which is applicable to general causal discovery algorithms, we
illustrate how to construct a LOVO predictor tailored towards algorithms
relying on specific a priori assumptions, such as linear additive noise models.
Simulations indicate that the LOVO prediction error is indeed correlated with
the accuracy of the causal outputs, affirming the method's effectiveness.",stat.ML
Network EM Algorithm for Gaussian Mixture Model in Decentralized Federated Learning,"We systematically study various network Expectation-Maximization (EM)
algorithms for the Gaussian mixture model within the framework of decentralized
federated learning. Our theoretical investigation reveals that directly
extending the classical decentralized supervised learning method to the EM
algorithm exhibits poor estimation accuracy with heterogeneous data across
clients and struggles to converge numerically when Gaussian components are
poorly-separated. To address these issues, we propose two novel solutions.
First, to handle heterogeneous data, we introduce a momentum network EM (MNEM)
algorithm, which uses a momentum parameter to combine information from both the
current and historical estimators. Second, to tackle the challenge of
poorly-separated Gaussian components, we develop a semi-supervised MNEM
(semi-MNEM) algorithm, which leverages partially labeled data. Rigorous
theoretical analysis demonstrates that MNEM can achieve statistical efficiency
comparable to that of the whole sample estimator when the mixture components
satisfy certain separation conditions, even in heterogeneous scenarios.
Moreover, the semi-MNEM estimator enhances the convergence speed of the MNEM
algorithm, effectively addressing the numerical convergence challenges in
poorly-separated scenarios. Extensive simulation and real data analyses are
conducted to justify our theoretical findings.",stat.ML
The sampling complexity of learning invertible residual neural networks,"In recent work it has been shown that determining a feedforward ReLU neural
network to within high uniform accuracy from point samples suffers from the
curse of dimensionality in terms of the number of samples needed. As a
consequence, feedforward ReLU neural networks are of limited use for
applications where guaranteed high uniform accuracy is required.
  We consider the question of whether the sampling complexity can be improved
by restricting the specific neural network architecture. To this end, we
investigate invertible residual neural networks which are foundational
architectures in deep learning and are widely employed in models that power
modern generative methods. Our main result shows that the residual neural
network architecture and invertibility do not help overcome the complexity
barriers encountered with simpler feedforward architectures. Specifically, we
demonstrate that the computational complexity of approximating invertible
residual neural networks from point samples in the uniform norm suffers from
the curse of dimensionality. Similar results are established for invertible
convolutional Residual neural networks.",stat.ML
ClusterGraph: a new tool for visualization and compression of multidimensional data,"Understanding the global organization of complicated and high dimensional
data is of primary interest for many branches of applied sciences. It is
typically achieved by applying dimensionality reduction techniques mapping the
considered data into lower dimensional space. This family of methods, while
preserving local structures and features, often misses the global structure of
the dataset. Clustering techniques are another class of methods operating on
the data in the ambient space. They group together points that are similar
according to a fixed similarity criteria, however unlike dimensionality
reduction techniques, they do not provide information about the global
organization of the data. Leveraging ideas from Topological Data Analysis, in
this paper we provide an additional layer on the output of any clustering
algorithm. Such data structure, ClusterGraph, provides information about the
global layout of clusters, obtained from the considered clustering algorithm.
Appropriate measures are provided to assess the quality and usefulness of the
obtained representation. Subsequently the ClusterGraph, possibly with an
appropriate structure--preserving simplification, can be visualized and used in
synergy with state of the art exploratory data analysis techniques.",stat.ML
Post-Hoc Robustness Enhancement in Graph Neural Networks with Conditional Random Fields,"Graph Neural Networks (GNNs), which are nowadays the benchmark approach in
graph representation learning, have been shown to be vulnerable to adversarial
attacks, raising concerns about their real-world applicability. While existing
defense techniques primarily concentrate on the training phase of GNNs,
involving adjustments to message passing architectures or pre-processing
methods, there is a noticeable gap in methods focusing on increasing robustness
during inference. In this context, this study introduces RobustCRF, a post-hoc
approach aiming to enhance the robustness of GNNs at the inference stage. Our
proposed method, founded on statistical relational learning using a Conditional
Random Field, is model-agnostic and does not require prior knowledge about the
underlying model architecture. We validate the efficacy of this approach across
various models, leveraging benchmark node classification datasets.",stat.ML
Discovering Latent Structural Causal Models from Spatio-Temporal Data,"Many important phenomena in scientific fields such as climate, neuroscience,
and epidemiology are naturally represented as spatiotemporal gridded data with
complex interactions. For example, in climate science, researchers aim to
uncover how large-scale events, such as the North Atlantic Oscillation (NAO)
and the Antarctic Oscillation (AAO), influence other global processes.
Inferring causal relationships from these data is a challenging problem
compounded by the high dimensionality of such data and the correlations between
spatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),
a novel framework based on variational inference, designed to explicitly model
latent time-series and their causal relationships from spatially confined modes
in the data. Our method uses an end-to-end training process that maximizes an
evidence-lower bound (ELBO) for the data likelihood. Theoretically, we show
that, under some conditions, the latent variables are identifiable up to
transformation by an invertible matrix. Empirically, we show that SPACY
outperforms state-of-the-art baselines on synthetic data, remains scalable for
large grids, and identifies key known phenomena from real-world climate data.",stat.ML
Generating Highly Designable Proteins with Geometric Algebra Flow Matching,"We introduce a generative model for protein backbone design utilizing
geometric products and higher order message passing. In particular, we propose
Clifford Frame Attention (CFA), an extension of the invariant point attention
(IPA) architecture from AlphaFold2, in which the backbone residue frames and
geometric features are represented in the projective geometric algebra. This
enables to construct geometrically expressive messages between residues,
including higher order terms, using the bilinear operations of the algebra. We
evaluate our architecture by incorporating it into the framework of FrameFlow,
a state-of-the-art flow matching model for protein backbone generation. The
proposed model achieves high designability, diversity and novelty, while also
sampling protein backbones that follow the statistical distribution of
secondary structure elements found in naturally occurring proteins, a property
so far only insufficiently achieved by many state-of-the-art generative models.",stat.ML
Pruning the Path to Optimal Care: Identifying Systematically Suboptimal Medical Decision-Making with Inverse Reinforcement Learning,"In aims to uncover insights into medical decision-making embedded within
observational data from clinical settings, we present a novel application of
Inverse Reinforcement Learning (IRL) that identifies suboptimal clinician
actions based on the actions of their peers. This approach centers two stages
of IRL with an intermediate step to prune trajectories displaying behavior that
deviates significantly from the consensus. This enables us to effectively
identify clinical priorities and values from ICU data containing both optimal
and suboptimal clinician decisions. We observe that the benefits of removing
suboptimal actions vary by disease and differentially impact certain
demographic groups.",stat.ML
Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry,"In this work, we conduct a systematic study of stochastic saddle point
problems (SSP) and stochastic variational inequalities (SVI) under the
constraint of $(\epsilon,\delta)$-differential privacy (DP) in both Euclidean
and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in
the $\ell_p/\ell_q$ setup, $p,q\in[1,2]$. Here, we obtain a bound of
$\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the
strong SP-gap, where $n$ is the number of samples and $d$ is the dimension.
This rate is nearly optimal for any $p,q\in[1,2]$. Without additional
assumptions, such as smoothness or linearity requirements, prior work under DP
has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup).
Further, existing algorithms have each only been shown to work for specific
settings of $p$ and $q$ and under certain assumptions on the loss and the
feasible set, whereas we provide a general algorithm for DP SSPs whenever
$p,q\in[1,2]$. Our result is obtained via a novel analysis of the recursive
regularization algorithm. In particular, we develop new tools for analyzing
generalization, which may be of independent interest. Next, we turn our
attention towards SVIs with a monotone, bounded and Lipschitz operator and
consider $\ell_p$-setups, $p\in[1,2]$. Here, we provide the first analysis
which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}}
+ \frac{\sqrt{d}}{n\epsilon}\big)$. For $p-1=\Omega(1)$, this rate is near
optimal due to existing lower bounds. To obtain this result, we develop a
modified version of recursive regularization. Our analysis builds on the
techniques we develop for SSPs as well as employing additional novel components
which handle difficulties arising from adapting the recursive regularization
framework to SVIs.",stat.ML
Inverse Transition Learning: Learning Dynamics from Demonstrations,"We consider the problem of estimating the transition dynamics $T^*$ from
near-optimal expert trajectories in the context of offline model-based
reinforcement learning. We develop a novel constraint-based method, Inverse
Transition Learning, that treats the limited coverage of the expert
trajectories as a \emph{feature}: we use the fact that the expert is
near-optimal to inform our estimate of $T^*$. We integrate our constraints into
a Bayesian approach. Across both synthetic environments and real healthcare
scenarios like Intensive Care Unit (ICU) patient management in hypotension, we
demonstrate not only significant improvements in decision-making, but that our
posterior can inform when transfer will be successful.",stat.ML
Pareto Set Identification With Posterior Sampling,"The problem of identifying the best answer among a collection of items having
real-valued distribution is well-understood.
  Despite its practical relevance for many applications, fewer works have
studied its extension when multiple and potentially conflicting metrics are
available to assess an item's quality.
  Pareto set identification (PSI) aims to identify the set of answers whose
means are not uniformly worse than another.
  This paper studies PSI in the transductive linear setting with potentially
correlated objectives.
  Building on posterior sampling in both the stopping and the sampling rules,
we propose the PSIPS algorithm that deals simultaneously with structure and
correlation without paying the computational cost of existing oracle-based
algorithms.
  Both from a frequentist and Bayesian perspective, PSIPS is asymptotically
optimal.
  We demonstrate its good empirical performance in real-world and synthetic
instances.",stat.ML
Conformalized Credal Regions for Classification with Ambiguous Ground Truth,"An open question in \emph{Imprecise Probabilistic Machine Learning} is how to
empirically derive a credal region (i.e., a closed and convex family of
probabilities on the output space) from the available data, without any prior
knowledge or assumption. In classification problems, credal regions are a tool
that is able to provide provable guarantees under realistic assumptions by
characterizing the uncertainty about the distribution of the labels. Building
on previous work, we show that credal regions can be directly constructed using
conformal methods. This allows us to provide a novel extension of classical
conformal prediction to problems with ambiguous ground truth, that is, when the
exact labels for given inputs are not exactly known. The resulting construction
enjoys desirable practical and theoretical properties: (i) conformal coverage
guarantees, (ii) smaller prediction sets (compared to classical conformal
prediction regions) and (iii) disentanglement of uncertainty sources
(epistemic, aleatoric). We empirically verify our findings on both synthetic
and real datasets.",stat.ML
Learning dynamical systems from data: Gradient-based dictionary optimization,"The Koopman operator plays a crucial role in analyzing the global behavior of
dynamical systems. Existing data-driven methods for approximating the Koopman
operator or discovering the governing equations of the underlying system
typically require a fixed set of basis functions, also called dictionary. The
optimal choice of basis functions is highly problem-dependent and often
requires domain knowledge. We present a novel gradient descent-based
optimization framework for learning suitable and interpretable basis functions
from data and show how it can be used in combination with EDMD, SINDy, and
PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of
various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's
circuit, a nonlinear heat equation, as well as protein-folding data.",stat.ML
Conjugate gradient methods for high-dimensional GLMMs,"Generalized linear mixed models (GLMMs) are a widely used tool in statistical
analysis. The main bottleneck of many computational approaches lies in the
inversion of the high dimensional precision matrices associated with the random
effects. Such matrices are typically sparse; however, the sparsity pattern
resembles a multi partite random graph, which does not lend itself well to
default sparse linear algebra techniques. Notably, we show that, for typical
GLMMs, the Cholesky factor is dense even when the original precision is sparse.
We thus turn to approximate iterative techniques, in particular to the
conjugate gradient (CG) method. We combine a detailed analysis of the spectrum
of said precision matrices with results from random graph theory to show that
CG-based methods applied to high-dimensional GLMMs typically achieve a fixed
approximation error with a total cost that scales linearly with the number of
parameters and observations. Numerical illustrations with both real and
simulated data confirm the theoretical findings, while at the same time
illustrating situations, such as nested structures, where CG-based methods
struggle.",stat.ML
Centrality Graph Shift Operators for Graph Neural Networks,"Graph Shift Operators (GSOs), such as the adjacency and graph Laplacian
matrices, play a fundamental role in graph theory and graph representation
learning. Traditional GSOs are typically constructed by normalizing the
adjacency matrix by the degree matrix, a local centrality metric. In this work,
we instead propose and study Centrality GSOs (CGSOs), which normalize adjacency
matrices by global centrality metrics such as the PageRank, $k$-core or count
of fixed length walks. We study spectral properties of the CGSOs, allowing us
to get an understanding of their action on graph signals. We confirm this
understanding by defining and running the spectral clustering algorithm based
on different CGSOs on several synthetic and real-world datasets. We furthermore
outline how our CGSO can act as the message passing operator in any Graph
Neural Network and in particular demonstrate strong performance of a variant of
the Graph Convolutional Network and Graph Attention Network using our CGSOs on
several real-world benchmark datasets.",stat.ML
Sharp Analysis for KL-Regularized Contextual Bandits and RLHF,"Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant
technique used to enhance policy optimization in reinforcement learning (RL)
and reinforcement learning from human feedback (RLHF), which forces the learned
policy to stay close to a reference policy. While the effectiveness and
necessity of KL-regularization have been empirically demonstrated in various
practical scenarios, current theoretical analysis of KL-regularized RLHF still
obtains the same $\mathcal{O}(1 / \epsilon^2)$ sample complexity as problems
without KL-regularization. To understand the fundamental distinction between
policy learning objectives with KL-regularization and ones without
KL-regularization, we are the first to theoretically demonstrate the power of
KL-regularization by providing a sharp analysis for KL-regularized contextual
bandits and RLHF, revealing an $\mathcal{O}(1 / \epsilon)$ sample complexity
when $\epsilon$ is sufficiently small.
  We further explore the role of data coverage in contextual bandits and RLHF.
While the coverage assumption is commonly employed in offline RLHF to link the
samples from the reference policy to the optimal policy, often at the cost of a
multiplicative dependence on the coverage coefficient, its impact on the sample
complexity of online RLHF remains unclear. Previous theoretical analyses of
online RLHF typically require explicit exploration and additional structural
assumptions on the reward function class. In contrast, we show that with
sufficient coverage from the reference policy, a simple two-stage mixed
sampling strategy can achieve a sample complexity with only an additive
dependence on the coverage coefficient. Our results provide a comprehensive
understanding of the roles of KL-regularization and data coverage in RLHF,
shedding light on the design of more efficient RLHF algorithms.",stat.ML
Measure-to-measure interpolation using Transformers,"Transformers are deep neural network architectures that underpin the recent
successes of large language models. Unlike more classical architectures that
can be viewed as point-to-point maps, a Transformer acts as a
measure-to-measure map implemented as specific interacting particle system on
the unit sphere: the input is the empirical measure of tokens in a prompt and
its evolution is governed by the continuity equation. In fact, Transformers are
not limited to empirical measures and can in principle process any input
measure. As the nature of data processed by Transformers is expanding rapidly,
it is important to investigate their expressive power as maps from an arbitrary
measure to another arbitrary measure. To that end, we provide an explicit
choice of parameters that allows a single Transformer to match $N$ arbitrary
input measures to $N$ arbitrary target measures, under the minimal assumption
that every pair of input-target measures can be matched by some transport map.",stat.ML
Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity,"The wider application of end-to-end learning methods to embodied
decision-making domains remains bottlenecked by their reliance on a
superabundance of training data representative of the target domain.
Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot
generalization--the goal of standard reinforcement learning (RL)--in favor of
few-shot adaptation, and thus hold promise for bridging larger generalization
gaps. While learning this meta-level adaptive behavior still requires
substantial data, efficient environment simulators approaching real-world
complexity are growing in prevalence. Even so, hand-designing sufficiently
diverse and numerous simulated training tasks for these complex domains is
prohibitively labor-intensive. Domain randomization (DR) and procedural
generation (PG), offered as solutions to this problem, require simulators to
possess carefully-defined parameters which directly translate to meaningful
task diversity--a similarly prohibitive assumption. In this work, we present
DIVA, an evolutionary approach for generating diverse training tasks in such
complex, open-ended simulators. Like unsupervised environment design (UED)
methods, DIVA can be applied to arbitrary parameterizations, but can
additionally incorporate realistically-available domain knowledge--thus
inheriting the flexibility and generality of UED, and the supervised structure
embedded in well-designed simulators exploited by DR and PG. Our empirical
results showcase DIVA's unique ability to overcome complex parameterizations
and successfully train adaptive agent behavior, far outperforming competitive
baselines from prior literature. These findings highlight the potential of such
semi-supervised environment design (SSED) approaches, of which DIVA is the
first humble constituent, to enable training in realistic simulated domains,
and produce more robust and capable adaptive agents.",stat.ML
Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.",stat.ML
Statistical-Computational Trade-offs for Greedy Recursive Partitioning Estimators,"Models based on recursive partitioning such as decision trees and their
ensembles are popular for high-dimensional regression as they can potentially
avoid the curse of dimensionality. Because empirical risk minimization (ERM) is
computationally infeasible, these models are typically trained using greedy
algorithms. Although effective in many cases, these algorithms have been
empirically observed to get stuck at local optima. We explore this phenomenon
in the context of learning sparse regression functions over $d$ binary
features, showing that when the true regression function $f^*$ does not satisfy
the so-called Merged Staircase Property (MSP), greedy training requires
$\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does
satisfy MSP, greedy training can attain small estimation error with only
$O(\log d)$ samples. This performance mirrors that of two-layer neural networks
trained with stochastic gradient descent (SGD) in the mean-field regime,
thereby establishing a head-to-head comparison between SGD-trained neural
networks and greedy recursive partitioning estimators. Furthermore, ERM-trained
recursive partitioning estimators achieve low estimation error with $O(\log d)$
samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a
statistical-computational trade-off for greedy training. Our proofs are based
on a novel interpretation of greedy recursive partitioning using stochastic
process theory and a coupling technique that may be of independent interest.",stat.ML
Deep Heuristic Learning for Real-Time Urban Pathfinding,"This paper introduces a novel approach to urban pathfinding by transforming
traditional heuristic-based algorithms into deep learning models that leverage
real-time contextual data, such as traffic and weather conditions. We propose
two methods: an enhanced A* algorithm that dynamically adjusts routes based on
current environmental conditions, and a neural network model that predicts the
next optimal path segment using historical and live data. An extensive
benchmark was conducted to compare the performance of different deep learning
models, including MLP, GRU, LSTM, Autoencoders, and Transformers. Both methods
were evaluated in a simulated urban environment in Berlin, with the neural
network model outperforming traditional methods, reducing travel times by up to
40%, while the enhanced A* algorithm achieved a 34% improvement. These results
demonstrate the potential of deep learning to optimize urban navigation in real
time, providing more adaptable and efficient routing solutions.",stat.ML
Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.",stat.ML
Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking,"We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.",stat.ML
Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems,"In this paper, we propose a novel model called Recurrent Explicit Duration
Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent
explicit duration variables into the rSLDS model. We also propose an inference
and learning scheme that involves the use of P\'olya-gamma augmentation. We
demonstrate the improved segmentation capabilities of our model on three
benchmark datasets, including two quantitative datasets and one qualitative
dataset.",stat.ML
The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model,"The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural
Bayesian nonparametric extension of the classical Hidden Markov Model for
learning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to
strengthen the self-persistence probability in the HDP-HMM. Then, disentangled
sticky HDP-HMM has been proposed to disentangle the strength of the
self-persistence prior and transition prior. However, the sticky HDP-HMM
assumes that the self-persistence probability is stationary, limiting its
expressiveness. Here, we build on previous work on sticky HDP-HMM and
disentangled sticky HDP-HMM, developing a more general model: the recurrent
sticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for
efficient inference in this model. We show that RS-HDP-HMM outperforms
disentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and
real data segmentation.",stat.ML
Graph neural networks and non-commuting operators,"Graph neural networks (GNNs) provide state-of-the-art results in a wide
variety of tasks which typically involve predicting features at the vertices of
a graph. They are built from layers of graph convolutions which serve as a
powerful inductive bias for describing the flow of information among the
vertices. Often, more than one data modality is available. This work considers
a setting in which several graphs have the same vertex set and a common
vertex-level learning task. This generalizes standard GNN models to GNNs with
several graph operators that do not commute. We may call this model graph-tuple
neural networks (GtNN).
  In this work, we develop the mathematical theory to address the stability and
transferability of GtNNs using properties of non-commuting non-expansive
operators. We develop a limit theory of graphon-tuple neural networks and use
it to prove a universal transferability theorem that guarantees that all
graph-tuple neural networks are transferable on convergent graph-tuple
sequences. In particular, there is no non-transferable energy under the
convergence we consider here. Our theoretical results extend well-known
transferability theorems for GNNs to the case of several simultaneous graphs
(GtNNs) and provide a strict improvement on what is currently known even in the
GNN case.
  We illustrate our theoretical results with simple experiments on synthetic
and real-world data. To this end, we derive a training procedure that provably
enforces the stability of the resulting model.",stat.ML
ION-C: Integration of Overlapping Networks via Constraints,"In many causal learning problems, variables of interest are often not all
measured over the same observations, but are instead distributed across
multiple datasets with overlapping variables. Tillman et al. (2008) presented
the first algorithm for enumerating the minimal equivalence class of
ground-truth DAGs consistent with all input graphs by exploiting local
independence relations, called ION. In this paper, this problem is formulated
as a more computationally efficient answer set programming (ASP) problem, which
we call ION-C, and solved with the ASP system clingo. The ION-C algorithm was
run on random synthetic graphs with varying sizes, densities, and degrees of
overlap between subgraphs, with overlap having the largest impact on runtime,
number of solution graphs, and agreement within the output set. To validate
ION-C on real-world data, we ran the algorithm on overlapping graphs learned
from data from two successive iterations of the European Social Survey (ESS),
using a procedure for conducting joint independence tests to prevent
inconsistencies in the input.",stat.ML
Debiasing Synthetic Data Generated by Deep Generative Models,"While synthetic data hold great promise for privacy protection, their
statistical analysis poses significant challenges that necessitate innovative
solutions. The use of deep generative models (DGMs) for synthetic data
generation is known to induce considerable bias and imprecision into synthetic
data analyses, compromising their inferential utility as opposed to original
data analyses. This bias and uncertainty can be substantial enough to impede
statistical convergence rates, even in seemingly straightforward analyses like
mean calculation. The standard errors of such estimators then exhibit slower
shrinkage with sample size than the typical 1 over root-$n$ rate. This
complicates fundamental calculations like p-values and confidence intervals,
with no straightforward remedy currently available. In response to these
challenges, we propose a new strategy that targets synthetic data created by
DGMs for specific data analyses. Drawing insights from debiased and targeted
machine learning, our approach accounts for biases, enhances convergence rates,
and facilitates the calculation of estimators with easily approximated large
sample variances. We exemplify our proposal through a simulation study on toy
data and two case studies on real-world data, highlighting the importance of
tailoring DGMs for targeted data analysis. This debiasing strategy contributes
to advancing the reliability and applicability of synthetic data in statistical
inference.",stat.ML
Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains,"In this work, we consider the approximation capabilities of shallow neural
networks in weighted Sobolev spaces for functions in the spectral Barron space.
The existing literature already covers several cases, in which the spectral
Barron space can be approximated well, i.e., without curse of dimensionality,
by shallow networks and several different classes of activation function. The
limitations of the existing results are mostly on the error measures that were
considered, in which the results are restricted to Sobolev spaces over a
bounded domain. We will here treat two cases that extend upon the existing
results. Namely, we treat the case with bounded domain and Muckenhoupt weights
and the case, where the domain is allowed to be unbounded and the weights are
required to decay. We first present embedding results for the more general
weighted Fourier-Lebesgue spaces in the weighted Sobolev spaces and then we
establish asymptotic approximation rates for shallow neural networks that come
without curse of dimensionality.",stat.ML
Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits,"Causal knowledge about the relationships among decision variables and a
reward variable in a bandit setting can accelerate the learning of an optimal
decision. Current works often assume the causal graph is known, which may not
always be available a priori. Motivated by this challenge, we focus on the
causal bandit problem in scenarios where the underlying causal graph is unknown
and may include latent confounders. While intervention on the parents of the
reward node is optimal in the absence of latent confounders, this is not
necessarily the case in general. Instead, one must consider a set of possibly
optimal arms/interventions, each being a special subset of the ancestors of the
reward node, making causal discovery beyond the parents of the reward node
essential. For regret minimization, we identify that discovering the full
causal structure is unnecessary; however, no existing work provides the
necessary and sufficient components of the causal graph. We formally
characterize the set of necessary and sufficient latent confounders one needs
to detect or learn to ensure that all possibly optimal arms are identified
correctly. We also propose a randomized algorithm for learning the causal graph
with a limited number of samples, providing a sample complexity guarantee for
any desired confidence level. In the causal bandit setup, we propose a
two-stage approach. In the first stage, we learn the induced subgraph on
ancestors of the reward, along with a necessary and sufficient subset of latent
confounders, to construct the set of possibly optimal arms. The regret incurred
during this phase scales polynomially with respect to the number of nodes in
the causal graph. The second phase involves the application of a standard
bandit algorithm, such as the UCB algorithm. We also establish a regret bound
for our two-phase approach, which is sublinear in the number of rounds.",stat.ML
GUIDE-VAE: Advancing Data Generation with User Information and Pattern Dictionaries,"Generative modelling of multi-user datasets has become prominent in science
and engineering. Generating a data point for a given user requires employing
user information, and conventional generative models, including variational
autoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, a
novel conditional generative model that leverages user embeddings to generate
user-guided data. By allowing the model to benefit from shared patterns across
users, GUIDE-VAE enhances performance in multi-user settings, even under
significant data imbalance. In addition to integrating user information,
GUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)
to improve the realism of generated samples by capturing complex feature
dependencies. While user embeddings drive performance gains, PDCC addresses
common issues such as noise and over-smoothing typically seen in VAEs.
  The proposed GUIDE-VAE was evaluated on a multi-user smart meter dataset
characterized by substantial data imbalance across users. Quantitative results
show that GUIDE-VAE performs effectively in both synthetic data generation and
missing record imputation tasks, while qualitative evaluations reveal that
GUIDE-VAE produces more plausible and less noisy data. These results establish
GUIDE-VAE as a promising tool for controlled, realistic data generation in
multi-user datasets, with potential applications across various domains
requiring user-informed modelling.",stat.ML
Improved Regret of Linear Ensemble Sampling,"In this work, we close the fundamental gap of theory and practice by
providing an improved regret bound for linear ensemble sampling. We prove that
with an ensemble size logarithmic in $T$, linear ensemble sampling can achieve
a frequentist regret bound of $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$, matching
state-of-the-art results for randomized linear bandit algorithms, where $d$ and
$T$ are the dimension of the parameter and the time horizon respectively. Our
approach introduces a general regret analysis framework for linear bandit
algorithms. Additionally, we reveal a significant relationship between linear
ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing
that LinPHE is a special case of linear ensemble sampling when the ensemble
size equals $T$. This insight allows us to derive a new regret bound of
$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ for LinPHE, independent of the number of
arms. Our contributions advance the theoretical foundation of ensemble
sampling, bringing its regret bounds in line with the best known bounds for
other randomized exploration algorithms.",stat.ML
Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data,"Online Reinforcement learning (RL) typically requires high-stakes online
interaction data to learn a policy for a target task. This prompts interest in
leveraging historical data to improve sample efficiency. The historical data
may come from outdated or related source environments with different dynamics.
It remains unclear how to effectively use such data in the target task to
provably enhance learning and sample efficiency. To address this, we propose a
hybrid transfer RL (HTRL) setting, where an agent learns in a target
environment while accessing offline data from a source environment with shifted
dynamics. We show that -- without information on the dynamics shift -- general
shifted-dynamics data, even with subtle shifts, does not reduce sample
complexity in the target environment. However, with prior information on the
degree of the dynamics shift, we design HySRL, a transfer algorithm that
achieves problem-dependent sample complexity and outperforms pure online RL.
Finally, our experimental results demonstrate that HySRL surpasses
state-of-the-art online RL baseline.",stat.ML
Multilingual hierarchical classification of job advertisements for job vacancy statistics,"The goal of this paper is to develop a multilingual classifier and
conditional probability estimator of occupation codes for online job
advertisements according in accordance with the International Standard
Classification of Occupations (ISCO) extended with the Polish Classification of
Occupations and Specializations (KZiS), which is analogous to the European
Classification of Occupations. In this paper, we utilise a range of data
sources, including a novel one, namely the Central Job Offers Database, which
is a register of all vacancies submitted to Public Employment Offices. Their
staff members code the vacancies according to the ISCO and KZiS. A hierarchical
multi-class classifier has been developed based on the transformer
architecture. The classifier begins by encoding the jobs found in
advertisements to the widest 1-digit occupational group, and then narrows the
assignment to a 6-digit occupation code. We show that incorporation of the
hierarchical structure of occupations improves prediction accuracy by 1-2
percentage points, particularly for the hand-coded online job advertisements.
Finally, a bilingual (Polish and English) and multilingual (24 languages) model
is developed based on data translated using closed and open-source software.
The open-source software is provided for the benefit of the official statistics
community, with a particular focus on international comparability.",stat.ML
Variational Inference on the Boolean Hypercube with the Quantum Entropy,"In this paper, we derive variational inference upper-bounds on the
log-partition function of pairwise Markov random fields on the Boolean
hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We
then propose an efficient algorithm to compute these bounds based on
primal-dual optimization. An improvement of these bounds through the use of
''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and
we present a greedy algorithm to select among these relaxations. We carry
extensive numerical experiments and compare with state-of-the-art methods for
this inference problem.",stat.ML
Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.",stat.ML
"Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training Pipelines via Memoization-Awareness","The training or fine-tuning of machine learning, vision, and language models
is often implemented as a pipeline: a sequence of stages encompassing data
preparation, model training and evaluation. In this paper, we exploit pipeline
structures to reduce the cost of hyperparameter tuning for model
training/fine-tuning, which is particularly valuable for language models given
their high costs in GPU-days. We propose a ""memoization-aware"" Bayesian
Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline
caching system, allowing it to evaluate significantly more hyperparameter
candidates per GPU-day than other tuning algorithms. The result is
better-quality hyperparameters in the same amount of search time, or
equivalently, reduced search time to reach the same hyperparameter quality. In
our benchmarks on machine learning (model ensembles), vision (convolutional
architecture) and language (T5 architecture) pipelines, we compare EEIPU
against recent BO algorithms: EEIPU produces an average of $103\%$ more
hyperparameter candidates (within the same budget), and increases the
validation metric by an average of $108\%$ more than other algorithms (where
the increase is measured starting from the end of warm-up iterations).",stat.ML
A Subsampling Based Neural Network for Spatial Data,"The application of deep neural networks in geospatial data has become a
trending research problem in the present day. A significant amount of
statistical research has already been introduced, such as generalized least
square optimization by incorporating spatial variance-covariance matrix,
considering basis functions in the input nodes of the neural networks, and so
on. However, for lattice data, there is no available literature about the
utilization of asymptotic analysis of neural networks in regression for spatial
data. This article proposes a consistent localized two-layer deep neural
network-based regression for spatial data. We have proved the consistency of
this deep neural network for bounded and unbounded spatial domains under a
fixed sampling design of mixed-increasing spatial regions. We have proved that
its asymptotic convergence rate is faster than that of \cite{zhan2024neural}'s
neural network and an improved generalization of \cite{shen2023asymptotic}'s
neural network structure. We empirically observe the rate of convergence of
discrepancy measures between the empirical probability distribution of observed
and predicted data, which will become faster for a less smooth spatial surface.
We have applied our asymptotic analysis of deep neural networks to the
estimation of the monthly average temperature of major cities in the USA from
its satellite image. This application is an effective showcase of non-linear
spatial regression. We demonstrate our methodology with simulated lattice data
in various scenarios.",stat.ML
Designing a Linearized Potential Function in Neural Network Optimization Using Csiszr Type of Tsallis Entropy,"In recent years, learning for neural networks can be viewed as optimization
in the space of probability measures. To obtain the exponential convergence to
the optimizer, the regularizing term based on Shannon entropy plays an
important role. Even though an entropy function heavily affects convergence
results, there is almost no result on its generalization, because of the
following two technical difficulties: one is the lack of sufficient condition
for generalized logarithmic Sobolev inequality, and the other is the
distributional dependence of the potential function within the gradient flow
equation. In this paper, we establish a framework that utilizes a linearized
potential function via Csisz\'{a}r type of Tsallis entropy, which is one of the
generalized entropies. We also show that our new framework enable us to derive
an exponential convergence result.",stat.ML
Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner,"Estimating causal quantities from observational data is crucial for
understanding the safety and effectiveness of medical treatments. However, to
make reliable inferences, medical practitioners require not only estimating
averaged causal quantities, such as the conditional average treatment effect,
but also understanding the randomness of the treatment effect as a random
variable. This randomness is referred to as aleatoric uncertainty and is
necessary for understanding the probability of benefit from treatment or
quantiles of the treatment effect. Yet, the aleatoric uncertainty of the
treatment effect has received surprisingly little attention in the causal
machine learning community. To fill this gap, we aim to quantify the aleatoric
uncertainty of the treatment effect at the covariate-conditional level, namely,
the conditional distribution of the treatment effect (CDTE). Unlike average
causal quantities, the CDTE is not point identifiable without strong additional
assumptions. As a remedy, we employ partial identification to obtain sharp
bounds on the CDTE and thereby quantify the aleatoric uncertainty of the
treatment effect. We then develop a novel, orthogonal learner for the bounds on
the CDTE, which we call AU-learner. We further show that our AU-learner has
several strengths in that it satisfies Neyman-orthogonality and is doubly
robust. Finally, we propose a fully-parametric deep learning instantiation of
our AU-learner.",stat.ML
Solving stochastic partial differential equations using neural networks in the Wiener chaos expansion,"In this paper, we solve stochastic partial differential equations (SPDEs)
numerically by using (possibly random) neural networks in the truncated Wiener
chaos expansion of their corresponding solution. Moreover, we provide some
approximation rates for learning the solution of SPDEs with additive and/or
multiplicative noise. Finally, we apply our results in numerical examples to
approximate the solution of three SPDEs: the stochastic heat equation, the
Heath-Jarrow-Morton equation, and the Zakai equation.",stat.ML
Near-Optimal and Tractable Estimation under Shift-Invariance,"How hard is it to estimate a discrete-time signal $(x_{1}, ..., x_{n}) \in
\mathbb{C}^n$ satisfying an unknown linear recurrence relation of order $s$ and
observed in i.i.d. complex Gaussian noise? The class of all such signals is
parametric but extremely rich: it contains all exponential polynomials over
$\mathbb{C}$ with total degree $s$, including harmonic oscillations with $s$
arbitrary frequencies. Geometrically, this class corresponds to the projection
onto $\mathbb{C}^{n}$ of the union of all shift-invariant subspaces of
$\mathbb{C}^\mathbb{Z}$ of dimension $s$. We show that the statistical
complexity of this class, as measured by the squared minimax radius of the
$(1-\delta)$-confidence $\ell_2$-ball, is nearly the same as for the class of
$s$-sparse signals, namely $O\left(s\log(en) + \log(\delta^{-1})\right) \cdot
\log^2(es) \cdot \log(en/s).$ Moreover, the corresponding near-minimax
estimator is tractable, and it can be used to build a test statistic with a
near-minimax detection threshold in the associated detection problem. These
statistical results rest upon an approximation-theoretic one: we show that
finite-dimensional shift-invariant subspaces admit compactly supported
reproducing kernels whose Fourier spectra have nearly the smallest possible
$\ell_p$-norms, for all $p \in [1,+\infty]$ at once.",stat.ML
Proxy-informed Bayesian transfer learning with unknown sources,"Generalization outside the scope of one's training data requires leveraging
prior knowledge about the effects that transfer, and the effects that don't,
between different data sources. Bayesian transfer learning is a principled
paradigm for specifying this knowledge, and refining it on the basis of data
from the source (training) and target (prediction) tasks. We address the
challenging transfer learning setting where the learner (i) cannot fine-tune in
the target task, and (ii) does not know which source data points correspond to
the same task (i.e., the data sources are unknown). We propose a proxy-informed
robust method for probabilistic transfer learning (PROMPT), which provides a
posterior predictive estimate tailored to the structure of the target task,
without requiring the learner have access to any outcome information from the
target task. Instead, PROMPT relies on the availability of proxy information.
PROMPT uses the same proxy information for two purposes: (i) estimation of
effects specific to the target task, and (ii) construction of a robust
reweighting of the source data for estimation of effects that transfer between
tasks. We provide theoretical results on the effect of this reweighting on the
risk of negative transfer, and demonstrate application of PROMPT in two
synthetic settings.",stat.ML
Online Data Collection for Efficient Semiparametric Inference,"While many works have studied statistical data fusion, they typically assume
that the various datasets are given in advance. However, in practice,
estimation requires difficult data collection decisions like determining the
available data sources, their costs, and how many samples to collect from each
source. Moreover, this process is often sequential because the data collected
at a given time can improve collection decisions in the future. In our setup,
given access to multiple data sources and budget constraints, the agent must
sequentially decide which data source to query to efficiently estimate a target
parameter. We formalize this task using Online Moment Selection, a
semiparametric framework that applies to any parameter identified by a set of
moment conditions. Interestingly, the optimal budget allocation depends on the
(unknown) true parameters. We present two online data collection policies,
Explore-then-Commit and Explore-then-Greedy, that use the parameter estimates
at a given time to optimally allocate the remaining budget in the future steps.
We prove that both policies achieve zero regret (assessed by asymptotic MSE)
relative to an oracle policy. We empirically validate our methods on both
synthetic and real-world causal effect estimation tasks, demonstrating that the
online data collection policies outperform their fixed counterparts.",stat.ML
Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs,"We study episodic linear mixture MDPs with the unknown transition and
adversarial rewards under full-information feedback, employing dynamic regret
as the performance measure. We start with in-depth analyses of the strengths
and limitations of the two most popular methods: occupancy-measure-based and
policy-based methods. We observe that while the occupancy-measure-based method
is effective in addressing non-stationary environments, it encounters
difficulties with the unknown transition. In contrast, the policy-based method
can deal with the unknown transition effectively but faces challenges in
handling non-stationary environments. Building on this, we propose a novel
algorithm that combines the benefits of both methods. Specifically, it employs
(i) an occupancy-measure-based global optimization with a two-layer structure
to handle non-stationary environments; and (ii) a policy-based variance-aware
value-targeted regression to tackle the unknown transition. We bridge these two
parts by a novel conversion. Our algorithm enjoys an $\widetilde{\mathcal{O}}(d
\sqrt{H^3 K} + \sqrt{HK(H + \bar{P}_K)})$ dynamic regret, where $d$ is the
feature dimension, $H$ is the episode length, $K$ is the number of episodes,
$\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up
to logarithmic factors by establishing a matching lower bound. To the best of
our knowledge, this is the first work that achieves near-optimal dynamic regret
for adversarial linear mixture MDPs with the unknown transition without prior
knowledge of the non-stationarity measure.",stat.ML
Benign landscape for Burer-Monteiro factorizations of MaxCut-type semidefinite programs,"We consider MaxCut-type semidefinite programs (SDP) which admit a low rank
solution. To numerically leverage the low rank hypothesis, a standard
algorithmic approach is the Burer-Monteiro factorization, which allows to
significantly reduce the dimensionality of the problem at the cost of its
convexity. We give a sharp condition on the conditioning of the Laplacian
matrix associated with the SDP under which any second-order critical point of
the non-convex problem is a global minimizer. By applying our theorem, we
improve on recent results about the correctness of the Burer-Monteiro approach
on $\mathbb{Z}_2$-synchronization problems.",stat.ML
Correlating Variational Autoencoders Natively For Multi-View Imputation,"Multi-view data from the same source often exhibit correlation. This is
mirrored in correlation between the latent spaces of separate variational
autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is
proposed that incorporates a joint prior with a non-zero correlation structure
between the latent spaces of the VAEs. By enforcing such correlation structure,
more strongly correlated latent spaces are uncovered. Using conditional
distributions to move between these latent spaces, missing views can be imputed
and used for downstream analysis. Learning this correlation structure involves
maintaining validity of the prior distribution, as well as a successful
parameterization that allows end-to-end learning.",stat.ML
Graph Agnostic Causal Bayesian Optimisation,"We study the problem of globally optimising a target variable of an unknown
causal graph on which a sequence of soft or hard interventions can be
performed. The problem of optimising the target variable associated with a
causal graph is formalised as Causal Bayesian Optimisation (CBO). We study the
CBO problem under the cumulative regret objective with unknown causal graphs
for two settings, namely structural causal models with hard interventions and
function networks with soft interventions. We propose Graph Agnostic Causal
Bayesian Optimisation (GACBO), an algorithm that actively discovers the causal
structure that contributes to achieving optimal rewards. GACBO seeks to balance
exploiting the actions that give the best rewards against exploring the causal
structures and functions. To the best of our knowledge, our work is the first
to study causal Bayesian optimization with cumulative regret objectives in
scenarios where the graph is unknown or partially known. We show our proposed
algorithm outperforms baselines in simulated experiments and real-world
applications.",stat.ML
Testing Generalizability in Causal Inference,"Ensuring robust model performance across diverse real-world scenarios
requires addressing both transportability across domains with covariate shifts
and extrapolation beyond observed data ranges. However, there is no formal
procedure for statistically evaluating generalizability in machine learning
algorithms, particularly in causal inference. Existing methods often rely on
arbitrary metrics like AUC or MSE and focus predominantly on toy datasets,
providing limited insights into real-world applicability. To address this gap,
we propose a systematic and quantitative framework for evaluating model
generalizability under covariate distribution shifts, specifically within
causal inference settings. Our approach leverages the frugal parameterization,
allowing for flexible simulations from fully and semi-synthetic benchmarks,
offering comprehensive evaluations for both mean and distributional regression
methods. By basing simulations on real data, our method ensures more realistic
evaluations, which is often missing in current work relying on simplified
datasets. Furthermore, using simulations and statistical testing, our framework
is robust and avoids over-reliance on conventional metrics. Grounded in
real-world data, it provides realistic insights into model performance,
bridging the gap between synthetic evaluations and practical applications.",stat.ML
Your copula is a classifier in disguise: classification-based copula density estimation,"We propose reinterpreting copula density estimation as a discriminative task.
Under this novel estimation scheme, we train a classifier to distinguish
samples from the joint density from those of the product of independent
marginals, recovering the copula density in the process. We derive equivalences
between well-known copula classes and classification problems naturally arising
in our interpretation. Furthermore, we show our estimator achieves theoretical
guarantees akin to maximum likelihood estimation. By identifying a connection
with density ratio estimation, we benefit from the rich literature and models
available for such problems. Empirically, we demonstrate the applicability of
our approach by estimating copulas of real and high-dimensional datasets,
outperforming competing copula estimators in density evaluation as well as
sampling.",stat.ML
Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression: A Distribution-Free Analysis,"We study nonparametric regression by an over-parameterized two-layer neural
network trained by gradient descent (GD) in this paper. We show that, if the
neural network is trained by GD with early stopping, then the trained network
renders a sharp rate of the nonparametric regression risk of $\cO(\eps_n^2)$,
which is the same rate as that for the classical kernel regression trained by
GD with early stopping, where $\eps_n$ is the critical population rate of the
Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of
the training data. It is remarked that our result does not require
distributional assumptions on the training data, in a strong contrast with many
existing results which rely on specific distributions such as the spherical
uniform data distribution or distributions satisfying certain restrictive
conditions. The rate $\cO(\eps_n^2)$ is known to be minimax optimal for
specific cases, such as the case that the NTK has a polynomial eigenvalue decay
rate which happens under certain distributional assumptions. Our result
formally fills the gap between training a classical kernel regression model and
training an over-parameterized but finite-width neural network by GD for
nonparametric regression without distributional assumptions. We also provide
confirmative answers to certain open questions or address particular concerns
in the literature of training over-parameterized neural networks by GD with
early stopping for nonparametric regression, including the characterization of
the stopping time, the lower bound for the network width, and the constant
learning rate used in GD.",stat.ML
ADOPT: Modified Adam Can Converge with Any $_2$ with the Optimal Rate,"Adam is one of the most popular optimization algorithms in deep learning.
However, it is known that Adam does not converge in theory unless choosing a
hyperparameter, i.e., $\beta_2$, in a problem-dependent manner. There have been
many attempts to fix the non-convergence (e.g., AMSGrad), but they require an
impractical assumption that the gradient noise is uniformly bounded. In this
paper, we propose a new adaptive gradient method named ADOPT, which achieves
the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice
of $\beta_2$ without depending on the bounded noise assumption. ADOPT addresses
the non-convergence issue of Adam by removing the current gradient from the
second moment estimate and changing the order of the momentum update and the
normalization by the second moment estimate. We also conduct intensive
numerical experiments, and verify that our ADOPT achieves superior results
compared to Adam and its variants across a wide range of tasks, including image
classification, generative modeling, natural language processing, and deep
reinforcement learning. The implementation is available at
https://github.com/iShohei220/adopt.",stat.ML
Community detection with the Bethe-Hessian,"The Bethe-Hessian matrix, introduced by Saade, Krzakala, and Zdeborov\'a
(2014), is a Hermitian matrix designed for applying spectral clustering
algorithms to sparse networks. Rather than employing a non-symmetric and
high-dimensional non-backtracking operator, a spectral method based on the
Bethe-Hessian matrix is conjectured to also reach the Kesten-Stigum detection
threshold in the sparse stochastic block model (SBM). We provide the first
rigorous analysis of the Bethe-Hessian spectral method in the SBM under both
the bounded expected degree and the growing degree regimes. Specifically, we
demonstrate that: (i) When the expected degree $d\geq 2$, the number of
negative outliers of the Bethe-Hessian matrix can consistently estimate the
number of blocks above the Kesten-Stigum threshold, thus confirming a
conjecture from Saade, Krzakala, and Zdeborov\'a (2014) for $d\geq 2$. (ii) For
sufficiently large $d$, its eigenvectors can be used to achieve weak recovery.
(iii) As $d\to\infty$, we establish the concentration of the locations of its
negative outlier eigenvalues, and weak consistency can be achieved via a
spectral method based on the Bethe-Hessian matrix.",stat.ML
Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.",stat.ML
Generalization and Risk Bounds for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) have achieved great success in the
prediction of sequential data. However, their theoretical studies are still
lagging behind because of their complex interconnected structures. In this
paper, we establish a new generalization error bound for vanilla RNNs, and
provide a unified framework to calculate the Rademacher complexity that can be
applied to a variety of loss functions. When the ramp loss is used, we show
that our bound is tighter than the existing bounds based on the same
assumptions on the Frobenius and spectral norms of the weight matrices and a
few mild conditions. Our numerical results show that our new generalization
bound is the tightest among all existing bounds in three public datasets. Our
bound improves the second tightest one by an average percentage of 13.80% and
3.01% when the $\tanh$ and ReLU activation functions are used, respectively.
Moreover, we derive a sharp estimation error bound for RNN-based estimators
obtained through empirical risk minimization (ERM) in multi-class
classification problems when the loss function satisfies a Bernstein condition.",stat.ML
Automatic doubly robust inference for linear functionals via calibrated debiased machine learning,"In causal inference, many estimands of interest can be expressed as a linear
functional of the outcome regression function; this includes, for example,
average causal effects of static, dynamic and stochastic interventions. For
learning such estimands, in this work, we propose novel debiased machine
learning estimators that are doubly robust asymptotically linear, thus
providing not only doubly robust consistency but also facilitating doubly
robust inference (e.g., confidence intervals and hypothesis tests). To do so,
we first establish a key link between calibration, a machine learning technique
typically used in prediction and classification tasks, and the conditions
needed to achieve doubly robust asymptotic linearity. We then introduce
calibrated debiased machine learning (C-DML), a unified framework for doubly
robust inference, and propose a specific C-DML estimator that integrates
cross-fitting, isotonic calibration, and debiased machine learning estimation.
A C-DML estimator maintains asymptotic linearity when either the outcome
regression or the Riesz representer of the linear functional is estimated
sufficiently well, allowing the other to be estimated at arbitrarily slow rates
or even inconsistently. We propose a simple bootstrap-assisted approach for
constructing doubly robust confidence intervals. Our theoretical and empirical
results support the use of C-DML to mitigate bias arising from the inconsistent
or slow estimation of nuisance functions.",stat.ML
New random projections for isotropic kernels using stable spectral distributions,"Rahimi and Recht [31] introduced the idea of decomposing shift-invariant
kernels by randomly sampling from their spectral distribution. This famous
technique, known as Random Fourier Features (RFF), is in principle applicable
to any shift-invariant kernel whose spectral distribution can be identified and
simulated. In practice, however, it is usually applied to the Gaussian kernel
because of its simplicity, since its spectral distribution is also Gaussian.
Clearly, simple spectral sampling formulas would be desirable for broader
classes of kernel functions. In this paper, we propose to decompose spectral
kernel distributions as a scale mixture of $\alpha$-stable random vectors. This
provides a simple and ready-to-use spectral sampling formula for a very large
class of multivariate shift-invariant kernels, including exponential power
kernels, generalized Mat\'ern kernels, generalized Cauchy kernels, as well as
newly introduced kernels such as the Beta, Kummer, and Tricomi kernels. In
particular, we show that the spectral densities of all these kernels are scale
mixtures of the multivariate Gaussian distribution. This provides a very simple
way to modify existing Random Fourier Features software based on Gaussian
kernels to cover a much richer class of multivariate kernels. This result has
broad applications for support vector machines, kernel ridge regression,
Gaussian processes, and other kernel-based machine learning techniques for
which the random Fourier features technique is applicable.",stat.ML
A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks,"We propose a general framework for deriving generalization bounds for
parallel positively homogeneous neural networks--a class of neural networks
whose input-output map decomposes as the sum of positively homogeneous maps.
Examples of such networks include matrix factorization and sensing,
single-layer multi-head attention mechanisms, tensor factorization, deep linear
and ReLU networks, and more. Our general framework is based on linking the
non-convex empirical risk minimization (ERM) problem to a closely related
convex optimization problem over prediction functions, which provides a global,
achievable lower-bound to the ERM problem. We exploit this convex lower-bound
to perform generalization analysis in the convex space while controlling the
discrepancy between the convex model and its non-convex counterpart. We apply
our general framework to a wide variety of models ranging from low-rank matrix
sensing, to structured matrix sensing, two-layer linear networks, two-layer
ReLU networks, and single-layer multi-head attention mechanisms, achieving
generalization bounds with a sample complexity that scales almost linearly with
the network width.",stat.ML
"Fast, robust approximate message passing","We give a fast, spectral procedure for implementing approximate-message
passing (AMP) algorithms robustly. For any quadratic optimization problem over
symmetric matrices $X$ with independent subgaussian entries, and any separable
AMP algorithm $\mathcal A$, our algorithm performs a spectral pre-processing
step and then mildly modifies the iterates of $\mathcal A$. If given the
perturbed input $X + E \in \mathbb R^{n \times n}$ for any $E$ supported on a
$\varepsilon n \times \varepsilon n$ principal minor, our algorithm outputs a
solution $\hat v$ which is guaranteed to be close to the output of $\mathcal A$
on the uncorrupted $X$, with $\|\mathcal A(X) - \hat v\|_2 \le f(\varepsilon)
\|\mathcal A(X)\|_2$ where $f(\varepsilon) \to 0$ as $\varepsilon \to 0$
depending only on $\varepsilon$.",stat.ML
"Elliptical Wishart distributions: information geometry, maximum likelihood estimator, performance analysis and statistical learning","This paper deals with Elliptical Wishart distributions - which generalize the
Wishart distribution - in the context of signal processing and machine
learning. Two algorithms to compute the maximum likelihood estimator (MLE) are
proposed: a fixed point algorithm and a Riemannian optimization method based on
the derived information geometry of Elliptical Wishart distributions. The
existence and uniqueness of the MLE are characterized as well as the
convergence of both estimation algorithms. Statistical properties of the MLE
are also investigated such as consistency, asymptotic normality and an
intrinsic version of Fisher efficiency. On the statistical learning side, novel
classification and clustering methods are designed. For the $t$-Wishart
distribution, the performance of the MLE and statistical learning algorithms
are evaluated on both simulated and real EEG and hyperspectral data, showcasing
the interest of our proposed methods.",stat.ML
Differentiability and Approximation of Probability Functions under Gaussian Mixture Models: A Bayesian Approach,"In this work, we study probability functions associated with Gaussian mixture
models. Our primary focus is on extending the use of spherical radial
decomposition for multivariate Gaussian random vectors to the context of
Gaussian mixture models, which are not inherently spherical but only
conditionally so. Specifically, the conditional probability distribution, given
a random parameter of the random vector, follows a Gaussian distribution,
allowing us to apply Bayesian analysis tools to the probability function. This
assumption, together with spherical radial decomposition for Gaussian random
vectors, enables us to represent the probability function as an integral over
the Euclidean sphere. Using this representation, we establish sufficient
conditions to ensure the differentiability of the probability function and
provide and integral representation of its gradient. Furthermore, leveraging
the Bayesian decomposition, we approximate the probability function using
random sampling over the parameter space and the Euclidean sphere. Finally, we
present numerical examples that illustrate the advantages of this approach over
classical approximations based on random vector sampling.",stat.ML
Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach,"In this study, the novel hybrid machine learning approach is proposed in
carbon price fluctuation prediction. Specifically, a research framework
integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term
Memory (LSTM) neural network algorithm is proposed. The advantage of the
combined framework is that it can make feature extraction more efficient. Then,
based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty
as regularization method is adopted to predict. Referring to the
characteristics of high correlation between energy indicator price and
blockchain information in previous literature, and we primarily includes
indicators related to blockchain information through regularization process.
Based on the above methods, this paper uses a dataset containing an amount of
data to carry out the carbon price prediction. The experimental results show
that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM
architecture. Blockchain information can effectively predict the price. Since
parameter norm penalty as regularization, Ridge Regression (RR) as L2
regularization is better than Smoothly Clipped Absolute Deviation Penalty
(SCAD) as L1 regularization in price forecasting. Thus, the proposed RR-DILATED
CNN-LSTM approach can effectively and accurately predict the fluctuation trend
of the carbon price. Therefore, the new forecasting methods and theoretical
ecology proposed in this study provide a new basis for trend prediction and
evaluating digital assets policy represented by the carbon price for both the
academia and practitioners.",stat.ML
Point processes with event time uncertainty,"Point processes are widely used statistical models for uncovering the
temporal patterns in dependent event data. In many applications, the event time
cannot be observed exactly, calling for the incorporation of time uncertainty
into the modeling of point process data. In this work, we introduce a framework
to model time-uncertain point processes possibly on a network. We start by
deriving the formulation in the continuous-time setting under a few assumptions
motivated by application scenarios. After imposing a time grid, we obtain a
discrete-time model that facilitates inference and can be computed by
first-order optimization methods such as Gradient Descent or Variation
inequality (VI) using batch-based Stochastic Gradient Descent (SGD). The
parameter recovery guarantee is proved for VI inference at an $O(1/k)$
convergence rate using $k$ SGD steps. Our framework handles non-stationary
processes by modeling the inference kernel as a matrix (or tensor on a network)
and it covers the stationary process, such as the classical Hawkes process, as
a special case. We experimentally show that the proposed approach outperforms
previous General Linear model (GLM) baselines on simulated and real data and
reveals meaningful causal relations on a Sepsis-associated Derangements
dataset.",stat.ML
Explanations that reveal all through the definition of encoding,"Feature attributions attempt to highlight what inputs drive predictive power.
Good attributions or explanations are thus those that produce inputs that
retain this predictive power; accordingly, evaluations of explanations score
their quality of prediction. However, evaluations produce scores better than
what appears possible from the values in the explanation for a class of
explanations, called encoding explanations. Probing for encoding remains a
challenge because there is no general characterization of what gives the extra
predictive power. We develop a definition of encoding that identifies this
extra predictive power via conditional dependence and show that the definition
fits existing examples of encoding. This definition implies, in contrast to
encoding explanations, that non-encoding explanations contain all the
informative inputs used to produce the explanation, giving them a ""what you see
is what you get"" property, which makes them transparent and simple to use.
Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank
non-encoding explanations above encoding ones, and develop STRIPE-X which ranks
them correctly. After empirically demonstrating the theoretical insights, we
use STRIPE-X to uncover encoding in LLM-generated explanations for predicting
the sentiment in movie reviews.",stat.ML
Classifier Chain Networks for Multi-Label Classification,"The classifier chain is a widely used method for analyzing multi-labeled data
sets. In this study, we introduce a generalization of the classifier chain: the
classifier chain network. The classifier chain network enables joint estimation
of model parameters, and allows to account for the influence of earlier label
predictions on subsequent classifiers in the chain. Through simulations, we
evaluate the classifier chain network's performance against multiple benchmark
methods, demonstrating competitive results even in scenarios that deviate from
its modeling assumptions. Furthermore, we propose a new measure for detecting
conditional dependencies between labels and illustrate the classifier chain
network's effectiveness using an empirical data set.",stat.ML
FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.",stat.ML
A Directional Rockafellar-Uryasev Regression,"Most ost Big Data datasets suffer from selection bias. For example, X
(Twitter) training observations differ largely from the testing offline
observations as individuals on Twitter are generally more educated, democratic
or left-leaning. Therefore, one major obstacle to reliable estimation is the
differences between training and testing data. How can researchers make use of
such data even in the presence of non-ignorable selection mechanisms? A number
of methods have been developed for this issue, such as distributionally robust
optimization (DRO) or learning fairness. A possible avenue to reducing the
effect of bias is meta-information. Researchers, being field exerts, might have
prior information on the form and extent of selection bias affecting their
dataset, and in which direction the selection might cause the estimate to
change, e.g. over or under estimation. At the same time, there is no direct way
to leverage these types of information in learning. I propose a loss function
which takes into account two types of meta data information given by the
researcher: quantity and direction (under or over sampling) of bias in the
training set. Estimation with the proposed loss function is then implemented
through a neural network, the directional Rockafellar-Uryasev (dRU) regression
model. I test the dRU model on a biased training dataset, a Big Data online
drawn electoral poll. I apply the proposed model using meta data information
coherent with the political and sampling information obtained from previous
studies. The results show that including meta information improves the
electoral results predictions compared to a model that does not include them.",stat.ML
Distributionally Robust Optimization,"Distributionally robust optimization (DRO) studies decision problems under
uncertainty where the probability distribution governing the uncertain problem
parameters is itself uncertain. A key component of any DRO model is its
ambiguity set, that is, a family of probability distributions consistent with
any available structural or statistical information. DRO seeks decisions that
perform best under the worst distribution in the ambiguity set. This worst case
criterion is supported by findings in psychology and neuroscience, which
indicate that many decision-makers have a low tolerance for distributional
ambiguity. DRO is rooted in statistics, operations research and control theory,
and recent research has uncovered its deep connections to regularization
techniques and adversarial training in machine learning. This survey presents
the key findings of the field in a unified and self-contained manner.",stat.ML
Pretrained transformer efficiently learns low-dimensional target functions in-context,"Transformers can efficiently learn in-context from example demonstrations.
Most existing theoretical analyses studied the in-context learning (ICL)
ability of transformers for linear function classes, where it is typically
shown that the minimizer of the pretraining loss implements one gradient
descent step on the least squares objective. However, this simplified linear
setting arguably does not demonstrate the statistical efficiency of ICL, since
the pretrained transformer does not outperform directly solving linear
regression on the test prompt. In this paper, we study ICL of a nonlinear
function class via transformer with nonlinear MLP layer: given a class of
\textit{single-index} target functions $f_*(\boldsymbol{x}) =
\sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index
features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional
subspace, we show that a nonlinear transformer optimized by gradient descent
(with a pretraining sample complexity that depends on the \textit{information
exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a
prompt length that only depends on the dimension of the distribution of target
functions $r$; in contrast, any algorithm that directly learns $f_*$ on test
prompt yields a statistical complexity that scales with the ambient dimension
$d$. Our result highlights the adaptivity of the pretrained transformer to
low-dimensional structures of the function class, which enables
sample-efficient ICL that outperforms estimators that only have access to the
in-context data.",stat.ML
Linear Causal Bandits: Unknown Graph and Soft Interventions,"Designing causal bandit algorithms depends on two central categories of
assumptions: (i) the extent of information about the underlying causal graphs
and (ii) the extent of information about interventional statistical models.
There have been extensive recent advances in dispensing with assumptions on
either category. These include assuming known graphs but unknown interventional
distributions, and the converse setting of assuming unknown graphs but access
to restrictive hard/$\operatorname{do}$ interventions, which removes the
stochasticity and ancestral dependencies. Nevertheless, the problem in its
general form, i.e., unknown graph and unknown stochastic intervention models,
remains open. This paper addresses this problem and establishes that in a graph
with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after
$T$ interaction rounds the regret upper bound scales as
$\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c>1$ is a
constant and $R$ is a measure of intervention power. A universal minimax lower
bound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.
Importantly, the graph size $N$ has a diminishing effect on the regret as $T$
grows. These bounds have matching behavior in $T$, exponential dependence on
$L$, and polynomial dependence on $d$ (with the gap $d\ $). On the algorithmic
aspect, the paper presents a novel way of designing a computationally efficient
CB algorithm, addressing a challenge that the existing CB algorithms using soft
interventions face.",stat.ML
Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,"Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.",stat.ML
Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate
differential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$
samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians
up to low total variation distance, with differential privacy. Our work
improves over the previous best result [AAL24b] (which required roughly $k^2
d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.
Moreover, we give the first optimal bound for privately learning mixtures of
$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the
sample complexity for privately learning mixtures of univariate Gaussians is
linear in the number of components $k$, whereas the previous best sample
complexity [AAL21] was quadratic in $k$. Our algorithms utilize various
techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],
sample compression for distributions [ABDH+20], and methods for bounding
volumes of sumsets.",stat.ML
On the Utilization of Unique Node Identifiers in Graph Neural Networks,"Graph neural networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.",stat.ML
Towards safe Bayesian optimization with Wiener kernel regression,"Bayesian Optimization (BO) is a data-driven strategy for
minimizing/maximizing black-box functions based on probabilistic surrogate
models. In the presence of safety constraints, the performance of BO crucially
relies on tight probabilistic error bounds related to the uncertainty
surrounding the surrogate model. For the case of Gaussian Process surrogates
and Gaussian measurement noise, we present a novel error bound based on the
recently proposed Wiener kernel regression. We prove that under rather mild
assumptions, the proposed error bound is tighter than bounds previously
documented in the literature which leads to enlarged safety regions. We draw
upon a numerical example to demonstrate the efficacy of the proposed error
bound in safe BO.",stat.ML
Powerful batch conformal prediction for classification,"In a supervised classification split conformal/inductive framework with $K$
classes, a calibration sample of $n$ labeled examples is observed for inference
on the label of a new unlabeled example. In this work, we explore the case
where a ""batch"" of $m$ independent such unlabeled examples is given, and a
multivariate prediction set with $1-\alpha$ coverage should be provided for
this batch. Hence, the batch prediction set takes the form of a collection of
label vectors of size $m$, while the calibration sample only contains
univariate labels. Using the Bonferroni correction consists in concatenating
the individual prediction sets at level $1-\alpha/m$ (Vovk 2013). We propose a
uniformly more powerful solution, based on specific combinations of conformal
$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the
pooled evidence of fairly ""easy"" examples of the batch can help provide
narrower batch prediction sets. We also introduced adaptive versions of the
novel procedure that are particularly effective when the batch prediction set
is expected to be large. The theoretical guarantees are provided when all
examples are iid, as well as more generally when iid is assumed only
conditionally within each class. In particular, our results are also valid
under a label distribution shift since the distribution of the labels need not
be the same in the calibration sample and in the new `batch'. The usefulness of
the method is illustrated on synthetic and real data examples.",stat.ML
Variable Selection in Convex Piecewise Linear Regression,"This paper presents Sparse Gradient Descent as a solution for variable
selection in convex piecewise linear regression where the model is given as
$\mathrm{max}\langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$
where $x \in \mathbb R^d$ is the covariate vector. Here,
$\{a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth
weight vectors and intercepts. A non-asymptotic local convergence analysis is
provided for Sp-GD under sub-Gaussian noise when the covariate distribution
satisfies sub-Gaussianity and anti-concentration property. When the model order
and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given
$\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where
$\sigma_z^2$ denotes the noise variance. This also implies the exact parameter
recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. Since
optimizing the squared loss for sparse max-affine is non-convex, an
initialization scheme is proposed to provide a suitable initial estimate within
the basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke the
convergence guarantees. The initialization scheme uses sparse principal
component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$
then applies an $r$-covering search to estimate the model parameters. A
non-asymptotic analysis is presented for this initialization scheme when the
covariates and noise samples follow Gaussian distributions. When the model
order and parameters are fixed, this initialization scheme provides an
$\epsilon$-accurate estimate given
$\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$
observations. Numerical Monte Carlo results corroborate theoretical findings
for Sp-GD and the initialization scheme.",stat.ML
Targeted Learning for Variable Importance,"Variable importance is one of the most widely used measures for interpreting
machine learning with significant interest from both statistics and machine
learning communities. Recently, increasing attention has been directed toward
uncertainty quantification in these metrics. Current approaches largely rely on
one-step procedures, which, while asymptotically efficient, can present higher
sensitivity and instability in finite sample settings. To address these
limitations, we propose a novel method by employing the targeted learning (TL)
framework, designed to enhance robustness in inference for variable importance
metrics. Our approach is particularly suited for conditional permutation
variable importance. We show that it (i) retains the asymptotic efficiency of
traditional methods, (ii) maintains comparable computational complexity, and
(iii) delivers improved accuracy, especially in finite sample contexts. We
further support these findings with numerical experiments that illustrate the
practical advantages of our method and validate the theoretical results.",stat.ML
Recursive Learning of Asymptotic Variational Objectives,"General state-space models (SSMs) are widely used in statistical machine
learning and are among the most classical generative models for sequential
time-series data. SSMs, comprising latent Markovian states, can be subjected to
variational inference (VI), but standard VI methods like the
importance-weighted autoencoder (IWAE) lack functionality for streaming data.
To enable online VI in SSMs when the observations are received in real time, we
propose maximising an IWAE-type variational lower bound on the asymptotic
contrast function, rather than the standard IWAE ELBO, using stochastic
approximation. Unlike the recursive maximum likelihood method, which directly
maximises the asymptotic contrast, our approach, called online sequential IWAE
(OSIWAE), allows for online learning of both model parameters and a Markovian
recognition model for inferring latent states. By approximating filter state
posteriors and their derivatives using sequential Monte Carlo (SMC) methods, we
create a particle-based framework for online VI in SSMs. This approach is more
theoretically well-founded than recently proposed online variational SMC
methods. We provide rigorous theoretical results on the learning objective and
a numerical study demonstrating the method's efficiency in learning model
parameters and particle proposal kernels.",stat.ML
Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning,"Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.",stat.ML
Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity,"While overparameterization is known to benefit generalization, its impact on
Out-Of-Distribution (OOD) detection is less understood. This paper investigates
the influence of model complexity in OOD detection. We propose an expected OOD
risk metric to evaluate classifiers confidence on both training and OOD
samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD
risk of binary least-squares classifiers applied to Gaussian data. We show that
the OOD risk depicts an infinite peak, when the number of parameters is equal
to the number of samples, which we associate with the double descent
phenomenon. Our experimental study on different OOD detection methods across
multiple neural architectures extends our theoretical insights and highlights a
double descent curve. Our observations suggest that overparameterization does
not necessarily lead to better OOD detection. Using the Neural Collapse
framework, we provide insights to better understand this behavior. To
facilitate reproducibility, our code will be made publicly available upon
publication.",stat.ML
Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks,"The Gauss-Newton (GN) matrix plays an important role in machine learning,
most evident in its use as a preconditioning matrix for a wide family of
popular adaptive methods to speed up optimization. Besides, it can also provide
key insights into the optimization landscape of neural networks. In the context
of deep neural networks, understanding the GN matrix involves studying the
interaction between different weight matrices as well as the dependencies
introduced by the data, thus rendering its analysis challenging. In this work,
we take a first step towards theoretically characterizing the conditioning of
the GN matrix in neural networks. We establish tight bounds on the condition
number of the GN in deep linear networks of arbitrary depth and width, which we
also extend to two-layer ReLU networks. We expand the analysis to further
architectural components, such as residual connections and convolutional
layers. Finally, we empirically validate the bounds and uncover valuable
insights into the influence of the analyzed architectural components.",stat.ML
SpecRaGE: Robust and Generalizable Multi-view Spectral Representation Learning,"Multi-view representation learning (MvRL) has garnered substantial attention
in recent years, driven by the increasing demand for applications that can
effectively process and analyze data from multiple sources. In this context,
graph Laplacian-based MvRL methods have demonstrated remarkable success in
representing multi-view data. However, these methods often struggle with
generalization to new data and face challenges with scalability. Moreover, in
many practical scenarios, multi-view data is contaminated by noise or outliers.
In such cases, modern deep-learning-based MvRL approaches that rely on
alignment or contrastive objectives can lead to misleading results, as they may
impose incorrect consistency between clear and corrupted data sources. We
introduce $\textit{SpecRaGE}$, a novel fusion-based framework that integrates
the strengths of graph Laplacian methods with the power of deep learning to
overcome these challenges. SpecRage uses neural networks to learn parametric
mapping that approximates a joint diagonalization of graph Laplacians. This
solution bypasses the need for alignment while enabling generalizable and
scalable learning of informative and meaningful representations. Moreover, it
incorporates a meta-learning fusion module that dynamically adapts to data
quality, ensuring robustness against outliers and noisy views. Our extensive
experiments demonstrate that SpecRaGE outperforms state-of-the-art methods,
particularly in scenarios with data contamination, paving the way for more
reliable and efficient multi-view learning. Our code will be made publicly
available upon acceptance.",stat.ML
Finite-sample performance of the maximum likelihood estimator in logistic regression,"Logistic regression is a classical model for describing the probabilistic
dependence of binary responses to multivariate covariates. We consider the
predictive performance of the maximum likelihood estimator (MLE) for logistic
regression, assessed in terms of logistic risk. We consider two questions:
first, that of the existence of the MLE (which occurs when the dataset is not
linearly separated), and second that of its accuracy when it exists. These
properties depend on both the dimension of covariates and on the signal
strength. In the case of Gaussian covariates and a well-specified logistic
model, we obtain sharp non-asymptotic guarantees for the existence and excess
logistic risk of the MLE. We then generalize these results in two ways: first,
to non-Gaussian covariates satisfying a certain two-dimensional margin
condition, and second to the general case of statistical learning with a
possibly misspecified logistic model. Finally, we consider the case of a
Bernoulli design, where the behavior of the MLE is highly sensitive to the
parameter direction.",stat.ML
Encoding Multi-level Dynamics in Effect Heterogeneity Estimation,"Earth Observation (EO) data are increasingly used in policy analysis by
enabling granular estimation of treatment effects. However, a challenge in
EO-based causal inference lies in balancing the trade-off between capturing
fine-grained individual heterogeneity and broader contextual information. This
paper introduces Multi-scale Concatenation, a family of composable procedures
that transform arbitrary single-scale CATE estimation algorithms into
multi-scale algorithms. We benchmark the performance of Multi-scale
Concatenation on a CATE estimation pipeline combining Vision Transformer (ViT)
models fine-tuned on satellite images to encode images of different scales with
Causal Forests to obtain the final CATE estimate. We first perform simulation
studies, showing how a multi-scale approach captures multi-level dynamics that
single-scale ViT models fail to capture. We then apply the multi-scale method
to two randomized controlled trials (RCTs) conducted in Peru and Uganda using
Landsat satellite imagery. In the RCT analysis, the Rank Average Treatment
Effect Ratio (RATE Ratio) measure is employed to assess performance without
ground truth individual treatment effects. Results indicate that Multi-scale
Concatenation improves the performance of deep learning models in EO-based CATE
estimation without the complexity of designing new multi-scale architectures
for a specific use case.",stat.ML
Semiparametric conformal prediction,"Many risk-sensitive applications require well-calibrated prediction sets over
multiple, potentially correlated target variables, for which the prediction
algorithm may report correlated non-conformity scores. In this work, we treat
the scores as random vectors and aim to construct the prediction set accounting
for their joint correlation structure. Drawing from the rich literature on
multivariate quantiles and semiparametric statistics, we propose an algorithm
to estimate the $1-\alpha$ quantile of the scores, where $\alpha$ is the
user-specified miscoverage rate. In particular, we flexibly estimate the joint
cumulative distribution function (CDF) of the scores using nonparametric vine
copulas and improve the asymptotic efficiency of the quantile estimate using
its influence function. The vine decomposition allows our method to scale well
to a large number of targets. We report desired coverage and competitive
efficiency on a range of real-world regression problems, including those with
missing-at-random labels in the calibration set.",stat.ML
Low-Rank Tensors for Multi-Dimensional Markov Models,"This work presents a low-rank tensor model for multi-dimensional Markov
chains. A common approach to simplify the dynamical behavior of a Markov chain
is to impose low-rankness on the transition probability matrix. Inspired by the
success of these matrix techniques, we present low-rank tensors for
representing transition probabilities on multi-dimensional state spaces.
Through tensor decomposition, we provide a connection between our method and
classical probabilistic models. Moreover, our proposed model yields a
parsimonious representation with fewer parameters than matrix-based approaches.
Unlike these methods, which impose low-rankness uniformly across all states,
our tensor method accounts for the multi-dimensionality of the state space. We
also propose an optimization-based approach to estimate a Markov model as a
low-rank tensor. Our optimization problem can be solved by the alternating
direction method of multipliers (ADMM), which enjoys convergence to a
stationary solution. We empirically demonstrate that our tensor model estimates
Markov chains more efficiently than conventional techniques, requiring both
fewer samples and parameters. We perform numerical simulations for both a
synthetic low-rank Markov chain and a real-world example with New York City
taxi data, showcasing the advantages of multi-dimensionality for modeling state
spaces.",stat.ML
Amortized Bayesian Experimental Design for Decision-Making,"Many critical decisions, such as personalized medical diagnoses and product
pricing, are made based on insights gained from designing, observing, and
analyzing a series of experiments. This highlights the crucial role of
experimental design, which goes beyond merely collecting information on system
parameters as in traditional Bayesian experimental design (BED), but also plays
a key part in facilitating downstream decision-making. Most recent BED methods
use an amortized policy network to rapidly design experiments. However, the
information gathered through these methods is suboptimal for down-the-line
decision-making, as the experiments are not inherently designed with downstream
objectives in mind. In this paper, we present an amortized decision-aware BED
framework that prioritizes maximizing downstream decision utility. We introduce
a novel architecture, the Transformer Neural Decision Process (TNDP), capable
of instantly proposing the next experimental design, whilst inferring the
downstream decision, thus effectively amortizing both tasks within a unified
workflow. We demonstrate the performance of our method across several tasks,
showing that it can deliver informative designs and facilitate accurate
decision-making.",stat.ML
Theory-inspired Label Shift Adaptation via Aligned Distribution Mixture,"As a prominent challenge in addressing real-world issues within a dynamic
environment, label shift, which refers to the learning setting where the source
(training) and target (testing) label distributions do not match, has recently
received increasing attention. Existing label shift methods solely use
unlabeled target samples to estimate the target label distribution, and do not
involve them during the classifier training, resulting in suboptimal
utilization of available information. One common solution is to directly blend
the source and target distributions during the training of the target
classifier. However, we illustrate the theoretical deviation and limitations of
the direct distribution mixture in the label shift setting. To tackle this
crucial yet unexplored issue, we introduce the concept of aligned distribution
mixture, showcasing its theoretical optimality and generalization error bounds.
By incorporating insights from generalization theory, we propose an innovative
label shift framework named as Aligned Distribution Mixture (ADM). Within this
framework, we enhance four typical label shift methods by introducing
modifications to the classifier training process. Furthermore, we also propose
a one-step approach that incorporates a pioneering coupling weight estimation
strategy. Considering the distinctiveness of the proposed one-step approach, we
develop an efficient bi-level optimization strategy. Experimental results
demonstrate the effectiveness of our approaches, together with their
effectiveness in COVID-19 diagnosis applications.",stat.ML
Towards Harmless Rawlsian Fairness Regardless of Demographic Prior,"Due to privacy and security concerns, recent advancements in group fairness
advocate for model training regardless of demographic information. However,
most methods still require prior knowledge of demographics. In this study, we
explore the potential for achieving fairness without compromising its utility
when no prior demographics are provided to the training set, namely
\emph{harmless Rawlsian fairness}. We ascertain that such a fairness
requirement with no prior demographic information essential promotes training
losses to exhibit a Dirac delta distribution. To this end, we propose a simple
but effective method named VFair to minimize the variance of training losses
inside the optimal set of empirical losses. This problem is then optimized by a
tailored dynamic update approach that operates in both loss and gradient
dimensions, directing the model towards relatively fairer solutions while
preserving its intact utility. Our experimental findings indicate that
regression tasks, which are relatively unexplored from literature, can achieve
significant fairness improvement through VFair regardless of any prior, whereas
classification tasks usually do not because of their quantized utility
measurements. The implementation of our method is publicly available at
\url{https://github.com/wxqpxw/VFair}.",stat.ML
Optimal Classification under Performative Distribution Shift,"Performative learning addresses the increasingly pervasive situations in
which algorithmic decisions may induce changes in the data distribution as a
consequence of their public deployment. We propose a novel view in which these
performative effects are modelled as push-forward measures. This general
framework encompasses existing models and enables novel performative gradient
estimation methods, leading to more efficient and scalable learning strategies.
For distribution shifts, unlike previous models which require full
specification of the data distribution, we only assume knowledge of the shift
operator that represents the performative changes. This approach can also be
integrated into various change-of-variablebased models, such as VAEs or
normalizing flows. Focusing on classification with a linear-in-parameters
performative effect, we prove the convexity of the performative risk under a
new set of assumptions. Notably, we do not limit the strength of performative
effects but rather their direction, requiring only that classification becomes
harder when deploying more accurate models. In this case, we also establish a
connection with adversarially robust classification by reformulating the
minimization of the performative risk as a min-max variational problem.
Finally, we illustrate our approach on synthetic and real datasets.",stat.ML
Learning Controlled Stochastic Differential Equations,"Identification of nonlinear dynamical systems is crucial across various
fields, facilitating tasks such as control, prediction, optimization, and fault
detection. Many applications require methods capable of handling complex
systems while providing strong learning guarantees for safe and reliable
performance. However, existing approaches often focus on simplified scenarios,
such as deterministic models, known diffusion, discrete systems,
one-dimensional dynamics, or systems constrained by strong structural
assumptions such as linearity. This work proposes a novel method for estimating
both drift and diffusion coefficients of continuous, multidimensional,
nonlinear controlled stochastic differential equations with non-uniform
diffusion. We assume regularity of the coefficients within a Sobolev space,
allowing for broad applicability to various dynamical systems in robotics,
finance, climate modeling, and biology. Leveraging the Fokker-Planck equation,
we split the estimation into two tasks: (a) estimating system dynamics for a
finite set of controls, and (b) estimating coefficients that govern those
dynamics. We provide strong theoretical guarantees, including finite-sample
bounds for \(L^2\), \(L^\infty\), and risk metrics, with learning rates
adaptive to coefficients' regularity, similar to those in nonparametric
least-squares regression literature. The practical effectiveness of our
approach is demonstrated through extensive numerical experiments. Our method is
available as an open-source Python library.",stat.ML
Understanding Variational Autoencoders with Intrinsic Dimension and Information Imbalance,"This work presents an analysis of the hidden representations of Variational
Autoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information
Imbalance (II). We show that VAEs undergo a transition in behaviour once the
bottleneck size is larger than the ID of the data, manifesting in a double
hunchback ID profile and a qualitative shift in information processing as
captured by the II. Our results also highlight two distinct training phases for
architectures with sufficiently large bottleneck sizes, consisting of a rapid
fit and a slower generalisation, as assessed by a differentiated behaviour of
ID, II, and KL loss. These insights demonstrate that II and ID could be
valuable tools for aiding architecture search, for diagnosing underfitting in
VAEs, and, more broadly, they contribute to advancing a unified understanding
of deep generative models through geometric analysis.",stat.ML
"See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers","Time series anomaly detection (TSAD) is becoming increasingly vital due to
the rapid growth of time series data across various sectors. Anomalies in web
service data, for example, can signal critical incidents such as system
failures or server malfunctions, necessitating timely detection and response.
However, most existing TSAD methodologies rely heavily on manual feature
engineering or require extensive labeled training data, while also offering
limited interpretability. To address these challenges, we introduce a
pioneering framework called the Time Series Anomaly Multimodal Analyzer (TAMA),
which leverages the power of Large Multimodal Models (LMMs) to enhance both the
detection and interpretation of anomalies in time series data. By converting
time series into visual formats that LMMs can efficiently process, TAMA
leverages few-shot in-context learning capabilities to reduce dependence on
extensive labeled datasets. Our methodology is validated through rigorous
experimentation on multiple real-world datasets, where TAMA consistently
outperforms state-of-the-art methods in TSAD tasks. Additionally, TAMA provides
rich, natural language-based semantic analysis, offering deeper insights into
the nature of detected anomalies. Furthermore, we contribute one of the first
open-source datasets that includes anomaly detection labels, anomaly type
labels, and contextual description, facilitating broader exploration and
advancement within this critical field. Ultimately, TAMA not only excels in
anomaly detection but also provides a comprehensive approach for understanding
the underlying causes of anomalies, pushing TSAD forward through innovative
methodologies and insights.",stat.ML
EXAGREE: Towards Explanation Agreement in Explainable Machine Learning,"Explanations in machine learning are critical for trust, transparency, and
fairness. Yet, complex disagreements among these explanations limit the
reliability and applicability of machine learning models, especially in
high-stakes environments. We formalize four fundamental ranking-based
explanation disagreement problems and introduce a novel framework, EXplanation
AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine
learning, particularly from stakeholder-centered perspectives. Our approach
leverages a Rashomon set for attribution predictions and then optimizes within
this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that
minimize disagreement with diverse stakeholder needs while maintaining
predictive performance. Rigorous empirical analysis on synthetic and real-world
datasets demonstrates that EXAGREE reduces explanation disagreement and
improves fairness across subgroups in various domains. EXAGREE not only
provides researchers with a new direction for studying explanation disagreement
problems but also offers data scientists a tool for making better-informed
decisions in practical applications.",stat.ML
RobPy: a Python Package for Robust Statistical Methods,"Robust estimation provides essential tools for analyzing data that contain
outliers, ensuring that statistical models remain reliable even in the presence
of some anomalous data. While robust methods have long been available in R,
users of Python have lacked a comprehensive package that offers these methods
in a cohesive framework. RobPy addresses this gap by offering a wide range of
robust methods in Python, built upon established libraries including NumPy,
SciPy, and scikit-learn. This package includes tools for robust preprocessing,
univariate estimation, covariance matrices, regression, and principal component
analysis, which are able to detect outliers and to mitigate their effect. In
addition, RobPy provides specialized diagnostic plots for visualizing casewise
and cellwise outliers. This paper presents the structure of the RobPy package,
demonstrates its functionality through examples, and compares its features to
existing implementations in other statistical software. By bringing robust
methods to Python, RobPy enables more users to perform robust data analysis in
a modern and versatile programming language.",stat.ML
You are out of context!,"This research proposes a novel drift detection methodology for machine
learning (ML) models based on the concept of ''deformation'' in the vector
space representation of data. Recognizing that new data can act as forces
stretching, compressing, or twisting the geometric relationships learned by a
model, we explore various mathematical frameworks to quantify this deformation.
We investigate measures such as eigenvalue analysis of covariance matrices to
capture global shape changes, local density estimation using kernel density
estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in
data concentration. Additionally, we draw inspiration from continuum mechanics
by proposing a ''strain tensor'' analogy to capture multi-faceted deformations
across different data types. This requires careful estimation of the
displacement field, and we delve into strategies ranging from density-based
approaches to manifold learning and neural network methods. By continuously
monitoring these deformation metrics and correlating them with model
performance, we aim to provide a sensitive, interpretable, and adaptable drift
detection system capable of distinguishing benign data evolution from true
drift, enabling timely interventions and ensuring the reliability of machine
learning systems in dynamic environments. Addressing the computational
challenges of this methodology, we discuss mitigation strategies like
dimensionality reduction, approximate algorithms, and parallelization for
real-time and large-scale applications. The method's effectiveness is
demonstrated through experiments on real-world text data, focusing on detecting
context shifts in Generative AI. Our results, supported by publicly available
code, highlight the benefits of this deformation-based approach in capturing
subtle drifts that traditional statistical methods often miss. Furthermore, we
present a detailed application example within the healthcare domain, showcasing
the methodology's potential in diverse fields. Future work will focus on
further improving computational efficiency and exploring additional
applications across different ML domains.",stat.ML
Differentially private and decentralized randomized power method,"The randomized power method has gained significant interest due to its
simplicity and efficient handling of large-scale spectral analysis and
recommendation tasks. As modern datasets contain sensitive private information,
we need to give formal guarantees on the possible privacy leaks caused by this
method. This paper focuses on enhancing privacy preserving variants of the
method. We propose a strategy to reduce the variance of the noise introduced to
achieve Differential Privacy (DP). We also adapt the method to a decentralized
framework with a low computational and communication overhead, while preserving
the accuracy. We leverage Secure Aggregation (a form of Multi-Party
Computation) to allow the algorithm to perform computations using data
distributed among multiple users or devices, without revealing individual data.
We show that it is possible to use a noise scale in the decentralized setting
that is similar to the one in the centralized setting. We improve upon existing
convergence bounds for both the centralized and decentralized versions. The
proposed method is especially relevant for decentralized applications such as
distributed recommender systems, where privacy concerns are paramount.",stat.ML
Stein Variational Newton Neural Network Ensembles,"Deep neural network ensembles are powerful tools for uncertainty
quantification, which have recently been re-interpreted from a Bayesian
perspective. However, current methods inadequately leverage second-order
information of the loss landscape, despite the recent availability of efficient
Hessian approximations. We propose a novel approximate Bayesian inference
method that modifies deep ensembles to incorporate Stein Variational Newton
updates. Our approach uniquely integrates scalable modern Hessian
approximations, achieving faster convergence and more accurate posterior
distribution approximations. We validate the effectiveness of our method on
diverse regression and classification tasks, demonstrating superior performance
with a significantly reduced number of training epochs compared to existing
ensemble-based methods, while enhancing uncertainty quantification and
robustness against overfitting.",stat.ML
ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer,"Numerous industrial sectors necessitate models capable of providing robust
forecasts across various horizons. Despite the recent strides in crafting
specific architectures for time-series forecasting and developing pre-trained
universal models, a comprehensive examination of their capability in
accommodating varied-horizon forecasting during inference is still lacking.
This paper bridges this gap through the design and evaluation of the Elastic
Time-Series Transformer (ElasTST). The ElasTST model incorporates a
non-autoregressive design with placeholders and structured self-attention
masks, warranting future outputs that are invariant to adjustments in inference
horizons. A tunable version of rotary position embedding is also integrated
into ElasTST to capture time-series-specific periods and enhance adaptability
to different horizons. Additionally, ElasTST employs a multi-scale patch
design, effectively integrating both fine-grained and coarse-grained
information. During the training phase, ElasTST uses a horizon reweighting
strategy that approximates the effect of random sampling across multiple
horizons with a single fixed horizon setting. Through comprehensive experiments
and comparisons with state-of-the-art time-series architectures and
contemporary foundation models, we demonstrate the efficacy of ElasTST's unique
design elements. Our findings position ElasTST as a robust solution for the
practical necessity of varied-horizon forecasting.",stat.ML
OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning,"Semi-supervised learning (SSL) offers a robust framework for harnessing the
potential of unannotated data. Traditionally, SSL mandates that all classes
possess labeled instances. However, the emergence of open-world SSL (OwSSL)
introduces a more practical challenge, wherein unlabeled data may encompass
samples from unseen classes. This scenario leads to misclassification of unseen
classes as known ones, consequently undermining classification accuracy. To
overcome this challenge, this study revisits two methodologies from
self-supervised and semi-supervised learning, self-labeling and consistency,
tailoring them to address the OwSSL problem. Specifically, we propose an
effective framework called OwMatch, combining conditional self-labeling and
open-world hierarchical thresholding. Theoretically, we analyze the estimation
of class distribution on unlabeled data through rigorous statistical analysis,
thus demonstrating that OwMatch can ensure the unbiasedness of the self-label
assignment estimator with reliability. Comprehensive empirical analyses
demonstrate that our method yields substantial performance enhancements across
both known and unknown classes in comparison to previous studies. Code is
available at https://github.com/niusj03/OwMatch.",stat.ML
Risk-sensitive control as inference with Rnyi divergence,"This paper introduces the risk-sensitive control as inference (RCaI) that
extends CaI by using R\'{e}nyi divergence variational inference. RCaI is shown
to be equivalent to log-probability regularized risk-sensitive control, which
is an extension of the maximum entropy (MaxEnt) control. We also prove that the
risk-sensitive optimal policy can be obtained by solving a soft Bellman
equation, which reveals several equivalences between RCaI, MaxEnt control, the
optimal posterior for CaI, and linearly-solvable control. Moreover, based on
RCaI, we derive the risk-sensitive reinforcement learning (RL) methods: the
policy gradient and the soft actor-critic. As the risk-sensitivity parameter
vanishes, we recover the risk-neutral CaI and RL, which means that RCaI is a
unifying framework. Furthermore, we give another risk-sensitive generalization
of the MaxEnt control using R\'{e}nyi entropy regularization. We show that in
both of our extensions, the optimal policies have the same structure even
though the derivations are very different.",stat.ML
Fixing the Loose Brake: Exponential-Tailed Stopping Time in Best Arm Identification,"The best arm identification problem requires identifying the best alternative
(i.e., arm) in active experimentation using the smallest number of experiments
(i.e., arm pulls), which is crucial for cost-efficient and timely
decision-making processes. In the fixed confidence setting, an algorithm must
stop data-dependently and return the estimated best arm with a correctness
guarantee. Since this stopping time is random, we desire its distribution to
have light tails. Unfortunately, many existing studies focus on high
probability or in expectation bounds on the stopping time, which allow heavy
tails and, for high probability bounds, even not stopping at all. We first
prove that this never-stopping event can indeed happen for some popular
algorithms. Motivated by this, we propose algorithms that provably enjoy an
exponential-tailed stopping time, which improves upon the polynomial tail bound
reported by Kalyanakrishnan et al. (2012). The first algorithm is based on a
fixed budget algorithm called Sequential Halving along with a doubling trick.
The second algorithm is a meta algorithm that takes in any fixed confidence
algorithm with a high probability stopping guarantee and turns it into one that
enjoys an exponential-tailed stopping time. Our results imply that there is
much more to be desired for contemporary fixed confidence algorithms.",stat.ML
Clustering Based on Density Propagation and Subcluster Merging,"We propose the DPSM method, a density-based node clustering approach that
automatically determines the number of clusters and can be applied in both data
space and graph space. Unlike traditional density-based clustering methods,
which necessitate calculating the distance between any two nodes, our proposed
technique determines density through a propagation process, thereby making it
suitable for a graph space. In DPSM, nodes are partitioned into small clusters
based on propagated density. The partitioning technique has been proved to be
sound and complete. We then extend the concept of spectral clustering from
individual nodes to these small clusters, while introducing the CluCut measure
to guide cluster merging. This measure is modified in various ways to account
for cluster properties, thus provides guidance on when to terminate the merging
process. Various experiments have validated the effectiveness of DOSM and the
accuracy of these conclusions.",stat.ML
Mitigating Spurious Correlations via Disagreement Probability,"Models trained with empirical risk minimization (ERM) are prone to be biased
towards spurious correlations between target labels and bias attributes, which
leads to poor performance on data groups lacking spurious correlations. It is
particularly challenging to address this problem when access to bias labels is
not permitted. To mitigate the effect of spurious correlations without bias
labels, we first introduce a novel training objective designed to robustly
enhance model performance across all data samples, irrespective of the presence
of spurious correlations. From this objective, we then derive a debiasing
method, Disagreement Probability based Resampling for debiasing (DPR), which
does not require bias labels. DPR leverages the disagreement between the target
label and the prediction of a biased model to identify bias-conflicting
samples-those without spurious correlations-and upsamples them according to the
disagreement probability. Empirical evaluations on multiple benchmarks
demonstrate that DPR achieves state-of-the-art performance over existing
baselines that do not use bias labels. Furthermore, we provide a theoretical
analysis that details how DPR reduces dependency on spurious correlations.",stat.ML
A General Recipe for Contractive Graph Neural Networks -- Technical Report,"Graph Neural Networks (GNNs) have gained significant popularity for learning
representations of graph-structured data due to their expressive power and
scalability. However, despite their success in domains such as social network
analysis, recommendation systems, and bioinformatics, GNNs often face
challenges related to stability, generalization, and robustness to noise and
adversarial attacks. Regularization techniques have shown promise in addressing
these challenges by controlling model complexity and improving robustness.
Building on recent advancements in contractive GNN architectures, this paper
presents a novel method for inducing contractive behavior in any GNN through
SVD regularization. By deriving a sufficient condition for contractiveness in
the update step and applying constraints on network parameters, we demonstrate
the impact of SVD regularization on the Lipschitz constant of GNNs. Our
findings highlight the role of SVD regularization in enhancing the stability
and generalization of GNNs, contributing to the development of more robust
graph-based learning algorithms dynamics.",stat.ML
Conformal Risk Minimization with Variance Reduction,"Conformal prediction (CP) is a distribution-free framework for achieving
probabilistic guarantees on black-box models. CP is generally applied to a
model post-training. Recent research efforts, on the other hand, have focused
on optimizing CP efficiency during training. We formalize this concept as the
problem of conformal risk minimization (CRM). In this direction, conformal
training (ConfTr) by Stutz et al.(2022) is a technique that seeks to minimize
the expected prediction set size of a model by simulating CP in-between
training updates. Despite its potential, we identify a strong source of sample
inefficiency in ConfTr that leads to overly noisy estimated gradients,
introducing training instability and limiting practical use. To address this
challenge, we propose variance-reduced conformal training (VR-ConfTr), a CRM
method that incorporates a variance reduction technique in the gradient
estimation of the ConfTr objective function. Through extensive experiments on
various benchmark datasets, we demonstrate that VR-ConfTr consistently achieves
faster convergence and smaller prediction sets compared to baselines.",stat.ML
Multiclass Transductive Online Learning,"We consider the problem of multiclass transductive online learning when the
number of labels can be unbounded. Previous works by Ben-David et al. [1997]
and Hanneke et al. [2023b] only consider the case of binary and finite label
spaces, respectively. The latter work determined that their techniques fail to
extend to the case of unbounded label spaces, and they pose the question of
characterizing the optimal mistake bound for unbounded label spaces. We answer
this question by showing that a new dimension, termed the Level-constrained
Littlestone dimension, characterizes online learnability in this setting. Along
the way, we show that the trichotomy of possible minimax rates of the expected
number of mistakes established by Hanneke et al. [2023b] for finite label
spaces in the realizable setting continues to hold even when the label space is
unbounded. In particular, if the learner plays for $T \in \mathbb{N}$ rounds,
its minimax expected number of mistakes can only grow like $\Theta(T)$,
$\Theta(\log T)$, or $\Theta(1)$. To prove this result, we give another
combinatorial dimension, termed the Level-constrained Branching dimension, and
show that its finiteness characterizes constant minimax expected
mistake-bounds. The trichotomy is then determined by a combination of the
Level-constrained Littlestone and Branching dimensions. Quantitatively, our
upper bounds improve upon existing multiclass upper bounds in Hanneke et al.
[2023b] by removing the dependence on the label set size. In doing so, we
explicitly construct learning algorithms that can handle extremely large or
unbounded label spaces. A key and novel component of our algorithm is a new
notion of shattering that exploits the sequential nature of transductive online
learning. Finally, we complete our results by proving expected regret bounds in
the agnostic setting, extending the result of Hanneke et al. [2023b].",stat.ML
"Denoising Diffusions with Optimal Transport: Localization, Curvature, and Multi-Scale Complexity","Adding noise is easy; what about denoising? Diffusion is easy; what about
reverting a diffusion? Diffusion-based generative models aim to denoise a
Langevin diffusion chain, moving from a log-concave equilibrium measure $\nu$,
say isotropic Gaussian, back to a complex, possibly non-log-concave initial
measure $\mu$. The score function performs denoising, going backward in time,
predicting the conditional mean of the past location given the current. We show
that score denoising is the optimal backward map in transportation cost. What
is its localization uncertainty? We show that the curvature function determines
this localization uncertainty, measured as the conditional variance of the past
location given the current. We study in this paper the effectiveness of the
diffuse-then-denoise process: the contraction of the forward diffusion chain,
offset by the possible expansion of the backward denoising chain, governs the
denoising difficulty. For any initial measure $\mu$, we prove that this offset
net contraction at time $t$ is characterized by the curvature complexity of a
smoothed $\mu$ at a specific signal-to-noise ratio (SNR) scale $r(t)$. We
discover that the multi-scale curvature complexity collectively determines the
difficulty of the denoising chain. Our multi-scale complexity quantifies a
fine-grained notion of average-case curvature instead of the worst-case.
Curiously, it depends on an integrated tail function, measuring the relative
mass of locations with positive curvature versus those with negative curvature;
denoising at a specific SNR scale is easy if such an integrated tail is light.
We conclude with several non-log-concave examples to demonstrate how the
multi-scale complexity probes the bottleneck SNR for the diffuse-then-denoise
process.",stat.ML
Counterfactual explainability of black-box prediction models,"It is crucial to be able to explain black-box prediction models to use them
effectively and safely in practice. Most existing tools for model explanations
are associational rather than causal, and we use two paradoxical examples to
show that such explanations are generally inadequate. Motivated by the concept
of genetic heritability in twin studies, we propose a new notion called
counterfactual explainability for black-box prediction models. Counterfactual
explainability has three key advantages: (1) it leverages counterfactual
outcomes and extends methods for global sensitivity analysis (such as
functional analysis of variance and Sobol's indices) to a causal setting; (2)
it is defined not only for the totality of a set of input factors but also for
their interactions (indeed, it is a probability measure on a whole
``explanation algebra''); (3) it also applies to dependent input factors whose
causal relationship can be modeled by a directed acyclic graph, thus
incorporating causal mechanisms into the explanation.",stat.ML
Strategic Conformal Prediction,"When a machine learning model is deployed, its predictions can alter its
environment, as better informed agents strategize to suit their own interests.
With such alterations in mind, existing approaches to uncertainty
quantification break. In this work we propose a new framework, Strategic
Conformal Prediction, which is capable of robust uncertainty quantification in
such a setting. Strategic Conformal Prediction is backed by a series of
theoretical guarantees spanning marginal coverage, training-conditional
coverage, tightness and robustness to misspecification that hold in a
distribution-free manner. Experimental analysis further validates our method,
showing its remarkable effectiveness in face of arbitrary strategic
alterations, whereas other methods break.",stat.ML
Online Graph Learning via Time-Vertex Adaptive Filters: From Theory to Cardiac Fibrillation,"Graph Signal Processing (GSP) provides a powerful framework for analysing
complex, interconnected systems by modelling data as signals on graphs. Recent
advances in GSP have enabled the learning of graph structures from observed
signals, but these methods often struggle with time-varying systems and
real-time applications. Adaptive filtering techniques, while effective for
online learning, have seen limited application in graph topology estimation
from a GSP perspective. To this end, we introduce AdaCGP, an online algorithm
for adaptive estimation of the Graph Shift Operator (GSO) from multivariate
time series. The GSO is estimated from an adaptive time-vertex autoregressive
model through recursive update formulae designed to address sparsity,
shift-invariance and bias. Through simulations, we show that AdaCGP performs
consistently well across various graph topologies, and achieves improvements in
excess of 82% for GSO estimation compared to baseline adaptive vector
autoregressive models. In addition, our online variable splitting approach for
enforcing sparsity enables near-perfect precision in identifying causal
connections while maintaining low false positive rates upon optimisation of the
forecast error. Finally, AdaCGP's ability to track changes in graph structure
is demonstrated on recordings of ventricular fibrillation dynamics in response
to an anti-arrhythmic drug. AdaCGP is shown to be able to identify the
stability of critical conduction patterns that may be maintaining the
arrhythmia in an intuitive way, together with its potential to support
diagnosis and treatment strategies.",stat.ML
Statistical guarantees for denoising reflected diffusion models,"In recent years, denoising diffusion models have become a crucial area of
research due to their abundance in the rapidly expanding field of generative
AI. While recent statistical advances have delivered explanations for the
generation ability of idealised denoising diffusion models for high-dimensional
target data, implementations introduce thresholding procedures for the
generating process to overcome issues arising from the unbounded state space of
such models. This mismatch between theoretical design and implementation of
diffusion models has been addressed empirically by using a \emph{reflected}
diffusion process as the driver of noise instead. In this paper, we study
statistical guarantees of these denoising reflected diffusion models. In
particular, we establish minimax optimal rates of convergence in total
variation, up to a polylogarithmic factor, under Sobolev smoothness
assumptions. Our main contributions include the statistical analysis of this
novel class of denoising reflected diffusion models and a refined score
approximation method in both time and space, leveraging spectral decomposition
and rigorous neural network analysis.",stat.ML
Adaptive Conformal Inference by Particle Filtering under Hidden Markov Models,"Conformal inference is a statistical method used to construct prediction sets
for point predictors, providing reliable uncertainty quantification with
probability guarantees. This method utilizes historical labeled data to
estimate the conformity or nonconformity between predictions and true labels.
However, conducting conformal inference for hidden states under hidden Markov
models (HMMs) presents a significant challenge, as the hidden state data is
unavailable, resulting in the absence of a true label set to serve as a
conformal calibration set. This paper proposes an adaptive conformal inference
framework that leverages a particle filtering approach to address this issue.
Rather than directly focusing on the unobservable hidden state, we innovatively
use weighted particles as an approximation of the actual posterior distribution
of the hidden state. Our goal is to produce prediction sets that encompass
these particles to achieve a specific aggregate weight sum, referred to as the
aggregated coverage level. The proposed framework can adapt online to the
time-varying distribution of data and achieve the defined marginal aggregated
coverage level in both one-step and multi-step inference over the long term. We
verify the effectiveness of this approach through a real-time target
localization simulation study.",stat.ML
G-SPARC: SPectral ARchitectures tackling the Cold-start problem in Graph learning,"Graphs play a central role in modeling complex relationships across various
domains. Most graph learning methods rely heavily on neighborhood information,
raising the question of how to handle cold-start nodes - nodes with no known
connections within the graph. These models often overlook the cold-start nodes,
making them ineffective for real-world scenarios. To tackle this, we propose
G-SPARC, a novel framework addressing cold-start nodes, that leverages
generalizable spectral embedding. This framework enables extension to
state-of-the-art methods making them suitable for practical applications. By
utilizing a key idea of transitioning from graph representation to spectral
representation, our approach is generalizable to cold-start nodes, capturing
the global structure of the graph without relying on adjacency data.
Experimental results demonstrate that our method outperforms existing models on
cold-start nodes across various tasks like node classification, node
clustering, and link prediction. G-SPARC provides a breakthrough built-in
solution to the cold-start problem in graph learning. Our code will be publicly
available upon acceptance.",stat.ML
Educational Effects in Mathematics: Conditional Average Treatment Effect depending on the Number of Treatments,"This study examines the educational effect of the Academic Support Center at
Kogakuin University. Following the initial assessment, it was suggested that
group bias had led to an underestimation of the Center's true impact. To
address this issue, the authors applied the theory of causal inference. By
using T-learner, the conditional average treatment effect (CATE) of the
Center's face-to-face (F2F) personal assistance program was evaluated.
Extending T-learner, the authors produced a new CATE function that depends on
the number of treatments (F2F sessions) and used the estimated function to
predict the CATE performance of F2F assistance.",stat.ML
DSDE: Using Proportion Estimation to Improve Model Selection for Out-of-Distribution Detection,"Model library is an effective tool for improving the performance of
single-model Out-of-Distribution (OoD) detector, mainly through model selection
and detector fusion. However, existing methods in the literature do not provide
uncertainty quantification for model selection results. Additionally, the model
ensemble process primarily focuses on controlling the True Positive Rate (TPR)
while neglecting the False Positive Rate (FPR). In this paper, we emphasize the
significance of the proportion of models in the library that identify the test
sample as an OoD sample. This proportion holds crucial information and directly
influences the error rate of OoD detection.To address this, we propose
inverting the commonly-used sequential p-value strategies. We define the
rejection region initially and then estimate the error rate. Furthermore, we
introduce a novel perspective from change-point detection and propose an
approach for proportion estimation with automatic hyperparameter selection. We
name the proposed approach as DOS-Storey-based Detector Ensemble (DSDE).
Experimental results on CIFAR10 and CIFAR100 demonstrate the effectiveness of
our approach in tackling OoD detection challenges. Specifically, the CIFAR10
experiments show that DSDE reduces the FPR from 11.07% to 3.31% compared to the
top-performing single-model detector.",stat.ML
Scaling Laws with Hidden Structure,"Statistical learning in high-dimensional spaces is challenging without a
strong underlying data structure. Recent advances with foundational models
suggest that text and image data contain such hidden structures, which help
mitigate the curse of dimensionality. Inspired by results from nonparametric
statistics, we hypothesize that this phenomenon can be partially explained in
terms of decomposition of complex tasks into simpler subtasks. In this paper,
we present a controlled experimental framework to test whether neural networks
can indeed exploit such ``hidden factorial structures.'' We find that they do
leverage these latent patterns to learn discrete distributions more
efficiently, and derive scaling laws linking model sizes, hidden
factorizations, and accuracy. We also study the interplay between our
structural assumptions and the models' capacity for generalization.",stat.ML
Network Causal Effect Estimation In Graphical Models Of Contagion And Latent Confounding,"A key question in many network studies is whether the observed correlations
between units are primarily due to contagion or latent confounding. Here, we
study this question using a segregated graph (Shpitser, 2015) representation of
these mechanisms, and examine how uncertainty about the true underlying
mechanism impacts downstream computation of network causal effects,
particularly under full interference -- settings where we only have a single
realization of a network and each unit may depend on any other unit in the
network. Under certain assumptions about asymptotic growth of the network, we
derive likelihood ratio tests that can be used to identify whether different
sets of variables -- confounders, treatments, and outcomes -- across units
exhibit dependence due to contagion or latent confounding. We then propose
network causal effect estimation strategies that provide unbiased and
consistent estimates if the dependence mechanisms are either known or correctly
inferred using our proposed tests. Together, the proposed methods allow network
effect estimation in a wider range of full interference scenarios that have not
been considered in prior work. We evaluate the effectiveness of our methods
with synthetic data and the validity of our assumptions using real-world
networks.",stat.ML
The Implicit Bias of Gradient Descent on Separable Multiclass Data,"Implicit bias describes the phenomenon where optimization-based training
algorithms, without explicit regularization, show a preference for simple
estimators even when more complex estimators have equal objective values.
Multiple works have developed the theory of implicit bias for binary
classification under the assumption that the loss satisfies an exponential tail
property. However, there is a noticeable gap in analysis for multiclass
classification, with only a handful of results which themselves are restricted
to the cross-entropy loss. In this work, we employ the framework of Permutation
Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to
introduce a multiclass extension of the exponential tail property. This class
of losses includes not only cross-entropy but also other losses. Using this
framework, we extend the implicit bias result of Soudry et al. [2018] to
multiclass classification. Furthermore, our proof techniques closely mirror
those of the binary case, thus illustrating the power of the PERM framework for
bridging the binary-multiclass gap.",stat.ML
Generalized Eigenvalue Problems with Generative Priors,"Generalized eigenvalue problems (GEPs) find applications in various fields of
science and engineering. For example, principal component analysis, Fisher's
discriminant analysis, and canonical correlation analysis are specific
instances of GEPs and are widely used in statistical data processing. In this
work, we study GEPs under generative priors, assuming that the underlying
leading generalized eigenvector lies within the range of a Lipschitz continuous
generative model. Under appropriate conditions, we show that any optimal
solution to the corresponding optimization problems attains the optimal
statistical rate. Moreover, from a computational perspective, we propose an
iterative algorithm called the Projected Rayleigh Flow Method (PRFM) to
approximate the optimal solution. We theoretically demonstrate that under
suitable assumptions, PRFM converges linearly to an estimated vector that
achieves the optimal statistical rate. Numerical results are provided to
demonstrate the effectiveness of the proposed method.",stat.ML
FEET: A Framework for Evaluating Embedding Techniques,"In this study, we introduce FEET, a standardized protocol designed to guide
the development and benchmarking of foundation models. While numerous benchmark
datasets exist for evaluating these models, we propose a structured evaluation
protocol across three distinct scenarios to gain a comprehensive understanding
of their practical performance. We define three primary use cases: frozen
embeddings, few-shot embeddings, and fully fine-tuned embeddings. Each scenario
is detailed and illustrated through two case studies: one in sentiment analysis
and another in the medical domain, demonstrating how these evaluations provide
a thorough assessment of foundation models' effectiveness in research
applications. We recommend this protocol as a standard for future research
aimed at advancing representation learning models.",stat.ML
MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning,"Real-world machine learning applications often face simultaneous covariate
and semantic shifts, challenging traditional domain generalization and
out-of-distribution (OOD) detection methods. We introduce Meta-learned Across
Domain Out-of-distribution Detection (MADOD), a novel framework designed to
address both shifts concurrently. MADOD leverages meta-learning and
G-invariance to enhance model generalizability and OOD detection in unseen
domains. Our key innovation lies in task construction: we randomly designate
in-distribution classes as pseudo-OODs within each meta-learning task,
simulating OOD scenarios using existing data. This approach, combined with
energy-based regularization, enables the learning of robust, domain-invariant
features while calibrating decision boundaries for effective OOD detection.
Operating in a test domain-agnostic setting, MADOD eliminates the need for
adaptation during inference, making it suitable for scenarios where test data
is unavailable. Extensive experiments on real-world and synthetic datasets
demonstrate MADOD's superior performance in semantic OOD detection across
unseen domains, achieving an AUPR improvement of 8.48% to 20.81%, while
maintaining competitive in-distribution classification accuracy, representing a
significant advancement in handling both covariate and semantic shifts.",stat.ML
Marginal Causal Flows for Validation and Inference,"Investigating the marginal causal effect of an intervention on an outcome
from complex data remains challenging due to the inflexibility of employed
models and the lack of complexity in causal benchmark datasets, which often
fail to reproduce intricate real-world data patterns. In this paper we
introduce Frugal Flows, a novel likelihood-based machine learning model that
uses normalising flows to flexibly learn the data-generating process, while
also directly inferring the marginal causal quantities from observational data.
We propose that these models are exceptionally well suited for generating
synthetic data to validate causal methods. They can create synthetic datasets
that closely resemble the empirical dataset, while automatically and exactly
satisfying a user-defined average treatment effect. To our knowledge, Frugal
Flows are the first generative model to both learn flexible data
representations and also exactly parameterise quantities such as the average
treatment effect and the degree of unobserved confounding. We demonstrate the
above with experiments on both simulated and real-world datasets.",stat.ML
The impact of MRI image quality on statistical and predictive analysis on voxel based morphology,"Image Quality of MRI brain scans is strongly influenced by within scanner
head movements and the resulting image artifacts alter derived measures like
brain volume and cortical thickness. Automated image quality assessment is key
to controlling for confounding effects of poor image quality. In this study, we
systematically test for the influence of image quality on univariate statistics
and machine learning classification. We analyzed group effects of sex/gender on
local brain volume and made predictions of sex/gender using logistic
regression, while correcting for brain size. From three large publicly
available datasets, two age and sex-balanced samples were derived to test the
generalizability of the effect for pooled sample sizes of n=760 and n=1094.
Results of the Bonferroni corrected t-tests over 3747 gray matter features
showed a strong influence of low-quality data on the ability to find
significant sex/gender differences for the smaller sample. Increasing sample
size and more so image quality showed a stark increase in detecting significant
effects in univariate group comparisons. For the classification of sex/gender
using logistic regression, both increasing sample size and image quality had a
marginal effect on the Area under the Receiver Operating Characteristic Curve
for most datasets and subsamples. Our results suggest a more stringent quality
control for univariate approaches than for multivariate classification with a
leaning towards higher quality for classical group statistics and bigger sample
sizes for machine learning applications in neuroimaging.",stat.ML
ProGen: Revisiting Probabilistic Spatial-Temporal Time Series Forecasting from a Continuous Generative Perspective Using Stochastic Differential Equations,"Accurate forecasting of spatiotemporal data remains challenging due to
complex spatial dependencies and temporal dynamics. The inherent uncertainty
and variability in such data often render deterministic models insufficient,
prompting a shift towards probabilistic approaches, where diffusion-based
generative models have emerged as effective solutions. In this paper, we
present ProGen, a novel framework for probabilistic spatiotemporal time series
forecasting that leverages Stochastic Differential Equations (SDEs) and
diffusion-based generative modeling techniques in the continuous domain. By
integrating a novel denoising score model, graph neural networks, and a
tailored SDE, ProGen provides a robust solution that effectively captures
spatiotemporal dependencies while managing uncertainty. Our extensive
experiments on four benchmark traffic datasets demonstrate that ProGen
outperforms state-of-the-art deterministic and probabilistic models. This work
contributes a continuous, diffusion-based generative approach to spatiotemporal
forecasting, paving the way for future research in probabilistic modeling and
stochastic processes.",stat.ML
Conformalized High-Density Quantile Regression via Dynamic Prototypes-based Probability Density Estimation,"Recent methods in quantile regression have adopted a classification
perspective to handle challenges posed by heteroscedastic, multimodal, or
skewed data by quantizing outputs into fixed bins. Although these
regression-as-classification frameworks can capture high-density prediction
regions and bypass convex quantile constraints, they are restricted by
quantization errors and the curse of dimensionality due to a constant number of
bins per dimension. To address these limitations, we introduce a conformalized
high-density quantile regression approach with a dynamically adaptive set of
prototypes. Our method optimizes the set of prototypes by adaptively adding,
deleting, and relocating quantization bins throughout the training process.
Moreover, our conformal scheme provides valid coverage guarantees, focusing on
regions with the highest probability density. Experiments across diverse
datasets and dimensionalities confirm that our method consistently achieves
high-quality prediction regions with enhanced coverage and robustness, all
while utilizing fewer prototypes and memory, ensuring scalability to higher
dimensions. The code is available at https://github.com/batuceng/max_quantile .",stat.ML
Hierarchical and Density-based Causal Clustering,"Understanding treatment effect heterogeneity is vital for scientific and
policy research. However, identifying and evaluating heterogeneous treatment
effects pose significant challenges due to the typically unknown subgroup
structure. Recently, a novel approach, causal k-means clustering, has emerged
to assess heterogeneity of treatment effect by applying the k-means algorithm
to unknown counterfactual regression functions. In this paper, we expand upon
this framework by integrating hierarchical and density-based clustering
algorithms. We propose plug-in estimators that are simple and readily
implementable using off-the-shelf algorithms. Unlike k-means clustering, which
requires the margin condition, our proposed estimators do not rely on strong
structural assumptions on the outcome process. We go on to study their rate of
convergence, and show that under the minimal regularity conditions, the
additional cost of causal clustering is essentially the estimation error of the
outcome regression functions. Our findings significantly extend the
capabilities of the causal clustering framework, thereby contributing to the
progression of methodologies for identifying homogeneous subgroups in treatment
response, consequently facilitating more nuanced and targeted interventions.
The proposed methods also open up new avenues for clustering with generic
pseudo-outcomes. We explore finite sample properties via simulation, and
illustrate the proposed methods in voting and employment projection datasets.",stat.ML
XNB: Explainable Class-Specific NaIve-Bayes Classifier,"In today's data-intensive landscape, where high-dimensional datasets are
increasingly common, reducing the number of input features is essential to
prevent overfitting and improve model accuracy. Despite numerous efforts to
tackle dimensionality reduction, most approaches apply a universal set of
features across all classes, potentially missing the unique characteristics of
individual classes. This paper presents the Explainable Class-Specific Naive
Bayes (XNB) classifier, which introduces two critical innovations: 1) the use
of Kernel Density Estimation to calculate posterior probabilities, allowing for
a more accurate and flexible estimation process, and 2) the selection of
class-specific feature subsets, ensuring that only the most relevant variables
for each class are utilized. Extensive empirical analysis on high-dimensional
genomic datasets shows that XNB matches the classification performance of
traditional Naive Bayes while drastically improving model interpretability. By
isolating the most relevant features for each class, XNB not only reduces the
feature set to a minimal, distinct subset for each class but also provides
deeper insights into how the model makes predictions. This approach offers
significant advantages in fields where both precision and explainability are
critical.",stat.ML
Federated Learning with Relative Fairness,"This paper proposes a federated learning framework designed to achieve
\textit{relative fairness} for clients. Traditional federated learning
frameworks typically ensure absolute fairness by guaranteeing minimum
performance across all client subgroups. However, this approach overlooks
disparities in model performance between subgroups. The proposed framework uses
a minimax problem approach to minimize relative unfairness, extending previous
methods in distributionally robust optimization (DRO). A novel fairness index,
based on the ratio between large and small losses among clients, is introduced,
allowing the framework to assess and improve the relative fairness of trained
models. Theoretical guarantees demonstrate that the framework consistently
reduces unfairness. We also develop an algorithm, named \textsc{Scaff-PD-IA},
which balances communication and computational efficiency while maintaining
minimax-optimal convergence rates. Empirical evaluations on real-world datasets
confirm its effectiveness in maintaining model performance while reducing
disparity.",stat.ML
Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing,"Federated Learning (FL) is essential for efficient data exchange in Internet
of Things (IoT) environments, as it trains Machine Learning (ML) models locally
and shares only model updates. However, FL is vulnerable to privacy threats
like model inversion and membership inference attacks, which can expose
sensitive training data. To address these privacy concerns, Differential
Privacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML
models degrades performance, especially in dynamic IoT systems where
continuous, lifelong FL learning accumulates excessive noise over time. To
mitigate this issue, we introduce Federated HyperDimensional computing with
Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)
framework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy
carefully manages the balance between privacy and performance by theoretically
tracking cumulative noise from previous rounds and adding only the necessary
incremental noise to meet privacy requirements. In a real-world case study
involving in-process monitoring of manufacturing machining operations,
FedHDPrivacy demonstrates robust performance, outperforming standard FL
frameworks-including Federated Averaging (FedAvg), Federated Stochastic
Gradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized
Averaging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy
also shows potential for future enhancements, such as multimodal data fusion.",stat.ML
Axiomatic Explainer Globalness via Optimal Transport,"Explainability methods are often challenging to evaluate and compare. With a
multitude of explainers available, practitioners must often compare and select
explainers based on quantitative evaluation metrics. One particular
differentiator between explainers is the diversity of explanations for a given
dataset; i.e. whether all explanations are identical, unique and uniformly
distributed, or somewhere between these two extremes. In this work, we define a
complexity measure for explainers, globalness, which enables deeper
understanding of the distribution of explanations produced by feature
attribution and feature selection methods for a given dataset. We establish the
axiomatic properties that any such measure should possess and prove that our
proposed measure, Wasserstein Globalness, meets these criteria. We validate the
utility of Wasserstein Globalness using image, tabular, and synthetic datasets,
empirically showing that it both facilitates meaningful comparison between
explainers and improves the selection process for explainability methods.",stat.ML
Relax and Merge: A Simple Yet Effective Framework for Solving Fair $k$-Means and $k$-sparse Wasserstein Barycenter Problems,"The fairness of clustering algorithms has gained widespread attention across
various areas, including machine learning, In this paper, we study fair
$k$-means clustering in Euclidean space. Given a dataset comprising several
groups, the fairness constraint requires that each cluster should contain a
proportion of points from each group within specified lower and upper bounds.
Due to these fairness constraints, determining the optimal locations of $k$
centers is a quite challenging task. We propose a novel ``Relax and Merge''
framework that returns a $(1+4\rho + O(\epsilon))$-approximate solution, where
$\rho$ is the approximate ratio of an off-the-shelf vanilla $k$-means algorithm
and $O(\epsilon)$ can be an arbitrarily small positive number. If equipped with
a PTAS of $k$-means, our solution can achieve an approximation ratio of
$(5+O(\epsilon))$ with only a slight violation of the fairness constraints,
which improves the current state-of-the-art approximation guarantee.
Furthermore, using our framework, we can also obtain a $(1+4\rho
+O(\epsilon))$-approximate solution for the $k$-sparse Wasserstein Barycenter
problem, which is a fundamental optimization problem in the field of optimal
transport, and a $(2+6\rho)$-approximate solution for the strictly fair
$k$-means clustering with no violation, both of which are better than the
current state-of-the-art methods. In addition, the empirical results
demonstrate that our proposed algorithm can significantly outperform baseline
approaches in terms of clustering cost.",stat.ML
Artificial Intelligence for Microbiology and Microbiome Research,"Advancements in artificial intelligence (AI) have transformed many scientific
fields, with microbiology and microbiome research now experiencing significant
breakthroughs through machine learning and deep learning applications. This
review provides a comprehensive overview of AI-driven approaches tailored for
microbiology and microbiome studies, emphasizing both technical advancements
and biological insights. We begin with an introduction to foundational AI
techniques, including primary machine learning paradigms and various deep
learning architectures, and offer guidance on choosing between machine learning
and deep learning methods based on specific research goals. The primary section
on application scenarios spans diverse research areas, from taxonomic
profiling, functional annotation & prediction, microbe-X interactions,
microbial ecology, metabolic modeling, precision nutrition, clinical
microbiology, to prevention & therapeutics. Finally, we discuss challenges
unique to this field, including the balance between interpretability and
complexity, the ""small n, large p"" problem, and the critical need for
standardized benchmarking datasets to validate and compare models. Together,
this review underscores AI's transformative role in microbiology and microbiome
research, paving the way for innovative methodologies and applications that
enhance our understanding of microbial life and its impact on our planet and
our health.",stat.ML
An unified approach to link prediction in collaboration networks,"This article investigates and compares three approaches to link prediction in
colaboration networks, namely, an ERGM (Exponential Random Graph Model; Robins
et al. 2007), a GCN (Graph Convolutional Network; Kipf and Welling 2017), and a
Word2Vec+MLP model (Word2Vec model combined with a multilayer neural network;
Mikolov et al. 2013a and Goodfellow et al. 2016). The ERGM, grounded in
statistical methods, is employed to capture general structural patterns within
the network, while the GCN and Word2Vec+MLP models leverage deep learning
techniques to learn adaptive structural representations of nodes and their
relationships. The predictive performance of the models is assessed through
extensive simulation exercises using cross-validation, with metrics based on
the receiver operating characteristic curve. The results clearly show the
superiority of machine learning approaches in link prediction, particularly in
large networks, where traditional models such as ERGM exhibit limitations in
scalability and the ability to capture inherent complexities. These findings
highlight the potential benefits of integrating statistical modeling techniques
with deep learning methods to analyze complex networks, providing a more robust
and effective framework for future research in this field.",stat.ML
Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired
data-for example, images and their corresponding text captions-to learn general
representations that transfer efficiently to downstream tasks. While such
approaches are generally applied to two modalities, domains such as robotics,
healthcare, and video need to support many types of data at once. We show that
the pairwise application of CLIP fails to capture joint information between
modalities, thereby limiting the quality of the learned representations. To
address this issue, we present Symile, a simple contrastive learning approach
that captures higher-order information between any number of modalities. Symile
provides a flexible, architecture-agnostic objective for learning
modality-specific representations. To develop Symile's objective, we derive a
lower bound on total correlation, and show that Symile representations for any
set of modalities form a sufficient statistic for predicting the remaining
modalities. Symile outperforms pairwise CLIP, even with modalities missing in
the data, on cross-modal classification and retrieval across several
experiments including on an original multilingual dataset of 33M image, text
and audio samples and a clinical dataset of chest X-rays, electrocardiograms,
and laboratory measurements. All datasets and code used in this work are
publicly available at https://github.com/rajesh-lab/symile.",stat.ML
Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference,"Model selection in Gaussian processes scales prohibitively with the size of
the training dataset, both in time and memory. While many approximations exist,
all incur inevitable approximation error. Recent work accounts for this error
in the form of computational uncertainty, which enables -- at the cost of
quadratic complexity -- an explicit tradeoff between computation and precision.
Here we extend this development to model selection, which requires significant
enhancements to the existing approach, including linear-time scaling in the
size of the dataset. We propose a novel training loss for hyperparameter
optimization and demonstrate empirically that the resulting method can
outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model
selection, on medium to large-scale datasets. Our experiments show that model
selection for computation-aware GPs trained on 1.8 million data points can be
done within a few hours on a single GPU. As a result of this work, Gaussian
processes can be trained on large-scale datasets without significantly
compromising their ability to quantify uncertainty -- a fundamental
prerequisite for optimal decision-making.",stat.ML
Abstracted Shapes as Tokens -- A Generalizable and Interpretable Model for Time-series Classification,"In time-series analysis, many recent works seek to provide a unified view and
representation for time-series across multiple domains, leading to the
development of foundation models for time-series data. Despite diverse modeling
techniques, existing models are black boxes and fail to provide insights and
explanations about their representations. In this paper, we present VQShape, a
pre-trained, generalizable, and interpretable model for time-series
representation learning and classification. By introducing a novel
representation for time-series data, we forge a connection between the latent
space of VQShape and shape-level features. Using vector quantization, we show
that time-series from different domains can be described using a unified set of
low-dimensional codes, where each code can be represented as an abstracted
shape in the time domain. On classification tasks, we show that the
representations of VQShape can be utilized to build interpretable classifiers,
achieving comparable performance to specialist models. Additionally, in
zero-shot learning, VQShape and its codebook can generalize to previously
unseen datasets and domains that are not included in the pre-training process.
The code and pre-trained weights are available at
https://github.com/YunshiWen/VQShape.",stat.ML
Automated Assessment of Residual Plots with Computer Vision Models,"Plotting the residuals is a recommended procedure to diagnose deviations from
linear model assumptions, such as non-linearity, heteroscedasticity, and
non-normality. The presence of structure in residual plots can be tested using
the lineup protocol to do visual inference. There are a variety of conventional
residual tests, but the lineup protocol, used as a statistical test, performs
better for diagnostic purposes because it is less sensitive and applies more
broadly to different types of departures. However, the lineup protocol relies
on human judgment which limits its scalability. This work presents a solution
by providing a computer vision model to automate the assessment of residual
plots. It is trained to predict a distance measure that quantifies the
disparity between the residual distribution of a fitted classical normal linear
regression model and the reference distribution, based on Kullback-Leibler
divergence. From extensive simulation studies, the computer vision model
exhibits lower sensitivity than conventional tests but higher sensitivity than
human visual tests. It is slightly less effective on non-linearity patterns.
Several examples from classical papers and contemporary data illustrate the new
procedures, highlighting its usefulness in automating the diagnostic process
and supplementing existing methods.",stat.ML
Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers,"Per-example gradient norms are a vital ingredient for estimating gradient
noise scale (GNS) with minimal variance. Observing the tensor contractions
required to compute them, we propose a method with minimal FLOPs in 3D or
greater tensor regimes by simultaneously computing the norms while computing
the parameter gradients. Using this method we are able to observe the GNS of
different layers at higher accuracy than previously possible. We find that the
total GNS of contemporary transformer models is predicted well by the GNS of
only the normalization layers. As a result, focusing only on the normalization
layer, we develop a custom kernel to compute the per-example gradient norms
while performing the LayerNorm backward pass with zero throughput overhead.
Tracking GNS on only those layers, we are able to guide a practical batch size
schedule that reduces training time by 18% on a Chinchilla-optimal language
model.",stat.ML
Magnitude Pruning of Large Pretrained Transformer Models with a Mixture Gaussian Prior,"Large pretrained transformer models have revolutionized modern AI
applications with their state-of-the-art performance in natural language
processing (NLP). However, their substantial parameter count poses challenges
for real-world deployment. To address this, researchers often reduce model size
by pruning parameters based on their magnitude or sensitivity. Previous
research has demonstrated the limitations of magnitude pruning, especially in
the context of transfer learning for modern NLP tasks. In this paper, we
introduce a new magnitude-based pruning algorithm called mixture Gaussian prior
pruning (MGPP), which employs a mixture Gaussian prior for regularization. MGPP
prunes non-expressive weights under the guidance of the mixture Gaussian prior,
aiming to retain the model's expressive capability. Extensive evaluations
across various NLP tasks, including natural language understanding, question
answering, and natural language generation, demonstrate the superiority of MGPP
over existing pruning methods, particularly in high sparsity settings.
Additionally, we provide a theoretical justification for the consistency of the
sparse transformer, shedding light on the effectiveness of the proposed pruning
method.",stat.ML
A Semiparametric Approach to Causal Inference,"In causal inference, an important problem is to quantify the effects of
interventions or treatments. Many studies focus on estimating the mean causal
effects; however, these estimands may offer limited insight since two
distributions can share the same mean yet exhibit significant differences.
Examining the causal effects from a distributional perspective provides a more
thorough understanding. In this paper, we employ a semiparametric density ratio
model (DRM) to characterize the counterfactual distributions, introducing a
framework that assumes a latent structure shared by these distributions. Our
model offers flexibility by avoiding strict parametric assumptions on the
counterfactual distributions. Specifically, the DRM incorporates a
nonparametric component that can be estimated through the method of empirical
likelihood (EL), using the data from all the groups stemming from multiple
interventions. Consequently, the EL-DRM framework enables inference of the
counterfactual distribution functions and their functionals, facilitating
direct and transparent causal inference from a distributional perspective.
Numerical studies on both synthetic and real-world data validate the
effectiveness of our approach.",stat.ML
Higher-Order Causal Message Passing for Experimentation with Complex Interference,"Accurate estimation of treatment effects is essential for decision-making
across various scientific fields. This task, however, becomes challenging in
areas like social sciences and online marketplaces, where treating one
experimental unit can influence outcomes for others through direct or indirect
interactions. Such interference can lead to biased treatment effect estimates,
particularly when the structure of these interactions is unknown. We address
this challenge by introducing a new class of estimators based on causal
message-passing, specifically designed for settings with pervasive, unknown
interference. Our estimator draws on information from the sample mean and
variance of unit outcomes and treatments over time, enabling efficient use of
observed data to estimate the evolution of the system state. Concretely, we
construct non-linear features from the moments of unit outcomes and treatments
and then learn a function that maps these features to future mean and variance
of unit outcomes. This allows for the estimation of the treatment effect over
time. Extensive simulations across multiple domains, using synthetic and real
network data, demonstrate the efficacy of our approach in estimating total
treatment effect dynamics, even in cases where interference exhibits
non-monotonic behavior in the probability of treatment.",stat.ML
Dimension-free Private Mean Estimation for Anisotropic Distributions,"We present differentially private algorithms for high-dimensional mean
estimation. Previous private estimators on distributions over $\mathbb{R}^d$
suffer from a curse of dimensionality, as they require $\Omega(d^{1/2})$
samples to achieve non-trivial error, even in cases where $O(1)$ samples
suffice without privacy. This rate is unavoidable when the distribution is
isotropic, namely, when the covariance is a multiple of the identity matrix, or
when accuracy is measured with respect to the affine-invariant Mahalanobis
distance. Yet, real-world data is often highly anisotropic, with signals
concentrated on a small number of principal components. We develop estimators
that are appropriate for such signals$\unicode{x2013}$our estimators are
$(\varepsilon,\delta)$-differentially private and have sample complexity that
is dimension-independent for anisotropic subgaussian distributions. Given $n$
samples from a distribution with known covariance-proxy $\Sigma$ and unknown
mean $\mu$, we present an estimator $\hat{\mu}$ that achieves error
$\|\hat{\mu}-\mu\|_2\leq \alpha$, as long as
$n\gtrsim\mathrm{tr}(\Sigma)/\alpha^2+
\mathrm{tr}(\Sigma^{1/2})/(\alpha\varepsilon)$. In particular, when
$\pmb{\sigma}^2=(\sigma_1^2, \ldots, \sigma_d^2)$ are the singular values of
$\Sigma$, we have $\mathrm{tr}(\Sigma)=\|\pmb{\sigma}\|_2^2$ and
$\mathrm{tr}(\Sigma^{1/2})=\|\pmb{\sigma}\|_1$, and hence our bound avoids
dimension-dependence when the signal is concentrated in a few principal
components. We show that this is the optimal sample complexity for this task up
to logarithmic factors. Moreover, for the case of unknown covariance, we
present an algorithm whose sample complexity has improved dependence on the
dimension, from $d^{1/2}$ to $d^{1/4}$.",stat.ML
Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete Flow Matching,"Outperforming autoregressive models on categorical data distributions, such
as textual data, remains challenging for continuous diffusion and flow models.
Discrete flow matching, a recent framework for modeling categorical data, has
shown competitive performance with autoregressive models. Despite its
similarities with continuous flow matching, the rectification strategy applied
in the continuous version does not directly extend to the discrete one due to
the inherent stochasticity of discrete paths. This limitation necessitates
exploring alternative methods to minimize state transitions during generation.
To address this, we propose a dynamic-optimal-transport-like minimization
objective for discrete flows with convex interpolants and derive its equivalent
Kantorovich formulation. The latter defines transport cost solely in terms of
inter-state similarity and is optimized using a minibatch strategy. Another
limitation we address in the discrete flow framework is model evaluation.
Unlike continuous flows, wherein the instantaneous change of variables enables
density estimation, discrete models lack a similar mechanism due to the
inherent non-determinism and discontinuity of their paths. To alleviate this
issue, we propose an upper bound on the perplexity of discrete flow models,
enabling performance evaluation and comparison with other methods.",stat.ML
SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities, but
their outputs can sometimes be unreliable or factually incorrect. To address
this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding
framework that enhances the truthfulness of LLMs without relying on external
knowledge bases or requiring further fine-tuning. From an optimization
perspective, our SLED framework leverages the latent knowledge embedded within
the LLM by contrasting the output logits from the final layer with those from
early layers. It then utilizes an approximate gradient approach to enable
latent knowledge to guide the self-refinement of outputs, thereby effectively
improving factual accuracy. Extensive experiments have been conducted on
established benchmarks across a diverse range of model families (LLaMA 2, LLaMA
3, Gemma) and scales (from 2B to 70B), including more advanced architectural
configurations such as the mixture of experts (MoE). Our evaluation spans a
wide variety of tasks, including multi-choice, open-generation, and adaptations
to chain-of-thought reasoning tasks. The results demonstrate that SLED
consistently improves factual accuracy by up to 20\% compared to existing
decoding methods while maintaining natural language fluency and negligible
latency overhead. Furthermore, it can be flexibly combined with other decoding
methods to further enhance their performance.",stat.ML
"Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms","We study learning in a dynamically evolving environment modeled as a Markov
game between a learner and a strategic opponent that can adapt to the learner's
strategies. While most existing works in Markov games focus on external regret
as the learning objective, external regret becomes inadequate when the
adversaries are adaptive. In this work, we focus on \emph{policy regret} -- a
counterfactual notion that aims to compete with the return that would have been
attained if the learner had followed the best fixed sequence of policy, in
hindsight. We show that if the opponent has unbounded memory or if it is
non-stationary, then sample-efficient learning is not possible. For
memory-bounded and stationary, we show that learning is still statistically
hard if the set of feasible strategies for the learner is exponentially large.
To guarantee learnability, we introduce a new notion of \emph{consistent}
adaptive adversaries, wherein, the adversary responds similarly to similar
strategies of the learner. We provide algorithms that achieve $\sqrt{T}$ policy
regret against memory-bounded, stationary, and consistent adversaries.",stat.ML
Zipfian Whitening,"The word embedding space in neural models is skewed, and correcting this can
improve task performance. We point out that most approaches for modeling,
correcting, and measuring the symmetry of an embedding space implicitly assume
that the word frequencies are uniform; in reality, word frequencies follow a
highly non-uniform distribution, known as Zipf's law. Surprisingly, simply
performing PCA whitening weighted by the empirical word frequency that follows
Zipf's law significantly improves task performance, surpassing established
baselines. From a theoretical perspective, both our approach and existing
methods can be clearly categorized: word representations are distributed
according to an exponential family with either uniform or Zipfian base
measures. By adopting the latter approach, we can naturally emphasize
informative low-frequency words in terms of their vector norm, which becomes
evident from the information-geometric perspective, and in terms of the loss
functions for imbalanced classification. Additionally, our theory corroborates
that popular natural language processing methods, such as skip-gram negative
sampling, WhiteningBERT, and headless language models, work well just because
their word embeddings encode the empirical word frequency into the underlying
probabilistic model.",stat.ML
Fast Spectrum Estimation of Some Kernel Matrices,"In data science, individual observations are often assumed to come
independently from an underlying probability space. Kernel matrices formed from
large sets of such observations arise frequently, for example during
classification tasks. It is desirable to know the eigenvalue decay properties
of these matrices without explicitly forming them, such as when determining if
a low-rank approximation is feasible. In this work, we introduce a new
eigenvalue quantile estimation framework for some kernel matrices. This
framework gives meaningful bounds for all the eigenvalues of a kernel matrix
while avoiding the cost of constructing the full matrix. The kernel matrices
under consideration come from a kernel with quick decay away from the diagonal
applied to uniformly-distributed sets of points in Euclidean space of any
dimension. We prove the efficacy of this framework given certain bounds on the
kernel function, and we provide empirical evidence for its accuracy. In the
process, we also prove a very general interlacing-type theorem for finite sets
of numbers. Additionally, we indicate an application of this framework to the
study of the intrinsic dimension of data, as well as several other directions
in which to generalize this work.",stat.ML
Variational Neural Stochastic Differential Equations with Change Points,"In this work, we explore modeling change points in time-series data using
neural stochastic differential equations (neural SDEs). We propose a novel
model formulation and training procedure based on the variational autoencoder
(VAE) framework for modeling time-series as a neural SDE. Unlike existing
algorithms training neural SDEs as VAEs, our proposed algorithm only
necessitates a Gaussian prior of the initial state of the latent stochastic
process, rather than a Wiener process prior on the entire latent stochastic
process. We develop two methodologies for modeling and estimating change points
in time-series data with distribution shifts. Our iterative algorithm
alternates between updating neural SDE parameters and updating the change
points based on either a maximum likelihood-based approach or a change point
detection algorithm using the sequential likelihood ratio test. We provide a
theoretical analysis of this proposed change point detection scheme. Finally,
we present an empirical evaluation that demonstrates the expressive power of
our proposed model, showing that it can effectively model both classical
parametric SDEs and some real datasets with distribution shifts.",stat.ML
Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum annotations,"In drug discovery, accurate lung tumor segmentation is an important step for
assessing tumor size and its progression using \textit{in-vivo} imaging such as
MRI. While deep learning models have been developed to automate this process,
the focus has predominantly been on human subjects, neglecting the pivotal role
of animal models in pre-clinical drug development. In this work, we focus on
optimizing lung tumor segmentation in mice. First, we demonstrate that the
nnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most
importantly, we achieve better results with nnU-Net 3D models than 2D models,
indicating the importance of spatial context for segmentation tasks in MRI mice
scans. This study demonstrates the importance of 3D input over 2D input images
for lung tumor segmentation in MRI scans. Finally, we outperform the prior
state-of-the-art approach that involves the combined segmentation of lungs and
tumors within the lungs. Our work achieves comparable results using only lung
tumor annotations requiring fewer annotations, saving time and annotation
efforts. This work
(https://anonymous.4open.science/r/lung-tumour-mice-mri-64BB) is an important
step in automating pre-clinical animal studies to quantify the efficacy of
experimental drugs, particularly in assessing tumor changes.",stat.ML
Nonparametric estimation of Hawkes processes with RKHSs,"This paper addresses nonparametric estimation of nonlinear multivariate
Hawkes processes, where the interaction functions are assumed to lie in a
reproducing kernel Hilbert space (RKHS). Motivated by applications in
neuroscience, the model allows complex interaction functions, in order to
express exciting and inhibiting effects, but also a combination of both (which
is particularly interesting to model the refractory period of neurons), and
considers in return that conditional intensities are rectified by the ReLU
function. The latter feature incurs several methodological challenges, for
which workarounds are proposed in this paper. In particular, it is shown that a
representer theorem can be obtained for approximated versions of the
log-likelihood and the least-squares criteria. Based on it, we propose an
estimation method, that relies on two simple approximations (of the ReLU
function and of the integral operator). We provide an approximation bound,
justifying the negligible statistical effect of these approximations. Numerical
results on synthetic data confirm this fact as well as the good asymptotic
behavior of the proposed estimator. It also shows that our method achieves a
better performance compared to related nonparametric estimation techniques and
suits neuronal applications.",stat.ML
"Small coresets via negative dependence: DPPs, linear statistics, and concentration","Determinantal point processes (DPPs) are random configurations of points with
tunable negative dependence. Because sampling is tractable, DPPs are natural
candidates for subsampling tasks, such as minibatch selection or coreset
construction. A \emph{coreset} is a subset of a (large) training set, such that
minimizing an empirical loss averaged over the coreset is a controlled
replacement for the intractable minimization of the original empirical loss.
Typically, the control takes the form of a guarantee that the average loss over
the coreset approximates the total loss uniformly across the parameter space.
Recent work has provided significant empirical support in favor of using DPPs
to build randomized coresets, coupled with interesting theoretical results that
are suggestive but leave some key questions unanswered. In particular, the
central question of whether the cardinality of a DPP-based coreset is
fundamentally smaller than one based on independent sampling remained open. In
this paper, we answer this question in the affirmative, demonstrating that
\emph{DPPs can provably outperform independently drawn coresets}. In this vein,
we contribute a conceptual understanding of coreset loss as a \emph{linear
statistic} of the (random) coreset. We leverage this structural observation to
connect the coresets problem to a more general problem of concentration
phenomena for linear statistics of DPPs, wherein we obtain \emph{effective
concentration inequalities that extend well-beyond the state-of-the-art},
encompassing general non-projection, even non-symmetric kernels. The latter
have been recently shown to be of interest in machine learning beyond coresets,
but come with a limited theoretical toolbox, to the extension of which our
result contributes. Finally, we are also able to address the coresets problem
for vector-valued objective functions, a novelty in the coresets literature.",stat.ML
Constrained Sampling with Primal-Dual Langevin Monte Carlo,"This work considers the problem of sampling from a probability distribution
known up to a normalization constant while satisfying a set of statistical
constraints specified by the expected values of general nonlinear functions.
This problem finds applications in, e.g., Bayesian inference, where it can
constrain moments to evaluate counterfactual scenarios or enforce desiderata
such as prediction fairness. Methods developed to handle support constraints,
such as those based on mirror maps, barriers, and penalties, are not suited for
this task. This work therefore relies on gradient descent-ascent dynamics in
Wasserstein space to put forward a discrete-time primal-dual Langevin Monte
Carlo algorithm (PD-LMC) that simultaneously constrains the target distribution
and samples from it. We analyze the convergence of PD-LMC under standard
assumptions on the target distribution and constraints, namely (strong)
convexity and log-Sobolev inequalities. To do so, we bring classical
optimization arguments for saddle-point algorithms to the geometry of
Wasserstein space. We illustrate the relevance and effectiveness of PD-LMC in
several applications.",stat.ML
Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning,"Subspace learning is a critical endeavor in contemporary machine learning,
particularly given the vast dimensions of modern datasets. In this study, we
delve into the training dynamics of a single-layer GAN model from the
perspective of subspace learning, framing these GANs as a novel approach to
this fundamental task. Through a rigorous scaling limit analysis, we offer
insights into the behavior of this model. Extending beyond prior research that
primarily focused on sequential feature learning, we investigate the
non-sequential scenario, emphasizing the pivotal role of inter-feature
interactions in expediting training and enhancing performance, particularly
with an uninformed initialization strategy. Our investigation encompasses both
synthetic and real-world datasets, such as MNIST and Olivetti Faces,
demonstrating the robustness and applicability of our findings to practical
scenarios. By bridging our analysis to the realm of subspace learning, we
systematically compare the efficacy of GAN-based methods against conventional
approaches, both theoretically and empirically. Notably, our results unveil
that while all methodologies successfully capture the underlying subspace, GANs
exhibit a remarkable capability to acquire a more informative basis, owing to
their intrinsic ability to generate new data samples. This elucidates the
unique advantage of GAN-based approaches in subspace learning tasks.",stat.ML
HAVER: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning,"We study the problem of estimating the \emph{value} of the largest mean among
$K$ distributions via samples from them (rather than estimating \emph{which}
distribution has the largest mean), which arises from various machine learning
tasks including Q-learning and Monte Carlo tree search. While there have been a
few proposed algorithms, their performance analyses have been limited to their
biases rather than a precise error metric. In this paper, we propose a novel
algorithm called HAVER (Head AVERaging) and analyze its mean squared error. Our
analysis reveals that HAVER has a compelling performance in two respects.
First, HAVER estimates the maximum mean as well as the oracle who knows the
identity of the best distribution and reports its sample mean. Second, perhaps
surprisingly, HAVER exhibits even better rates than this oracle when there are
many distributions near the best one. Both of these improvements are the first
of their kind in the literature, and we also prove that the naive algorithm
that reports the largest empirical mean does not achieve these bounds. Finally,
we confirm our theoretical findings via numerical experiments including bandits
and Q-learning scenarios where HAVER outperforms baseline methods.",stat.ML
Classification problem in liability insurance using machine learning models: a comparative study,"Underwriting is one of the important stages in an insurance company. The
insurance company uses different factors to classify the policyholders. In this
study, we apply several machine learning models such as nearest neighbour and
logistic regression to the Actuarial Challenge dataset used by Qazvini (2019)
to classify liability insurance policies into two groups: 1 - policies with
claims and 2 - policies without claims.",stat.ML
"Unified theory of upper confidence bound policies for bandit problems targeting total reward, maximal reward, and more","The upper confidence bound (UCB) policy is recognized as an order-optimal
solution for the classical total-reward bandit problem. While similar UCB-based
approaches have been applied to the max bandit problem, which aims to maximize
the cumulative maximal reward, their order optimality remains unclear. In this
study, we clarify the unified conditions under which the UCB policy achieves
the order optimality in both total-reward and max bandit problems. A key
concept of our theory is the oracle quantity, which identifies the best arm by
its highest value. This allows a unified definition of the UCB policy as
pulling the arm with the highest UCB of the oracle quantity. Additionally,
under this setting, optimality analysis can be conducted by replacing
traditional regret with the number of failures as a core measure. One
consequence of our analysis is that the confidence interval of the oracle
quantity must narrow appropriately as trials increase to ensure the order
optimality of UCB policies. From this consequence, we prove that the previously
proposed MaxSearch algorithm satisfies this condition and is an order-optimal
policy for the max bandit problem. We also demonstrate that new bandit problems
and their order-optimal UCB algorithms can be systematically derived by
providing the appropriate oracle quantity and its confidence interval. Building
on this, we propose PIUCB algorithms, which aim to pull the arm with the
highest probability of improvement (PI). These algorithms can be applied to the
max bandit problem in practice and perform comparably or better than the
MaxSearch algorithm in toy examples. This suggests that our theory has the
potential to generate new policies tailored to specific oracle quantities.",stat.ML
How many classifiers do we need?,"As performance gains through scaling data and/or model size experience
diminishing returns, it is becoming increasingly popular to turn to ensembling,
where the predictions of multiple models are combined to improve accuracy. In
this paper, we provide a detailed analysis of how the disagreement and the
polarization (a notion we introduce and define in this paper) among classifiers
relate to the performance gain achieved by aggregating individual classifiers,
for majority vote strategies in classification tasks. We address these
questions in the following ways. (1) An upper bound for polarization is
derived, and we propose what we call a neural polarization law: most
interpolating neural network models are 4/3-polarized. Our empirical results
not only support this conjecture but also show that polarization is nearly
constant for a dataset, regardless of hyperparameters or architectures of
classifiers. (2) The error of the majority vote classifier is considered under
restricted entropy conditions, and we present a tight upper bound that
indicates that the disagreement is linearly correlated with the target, and
that the slope is linear in the polarization. (3) We prove results for the
asymptotic behavior of the disagreement in terms of the number of classifiers,
which we show can help in predicting the performance for a larger number of
classifiers from that of a smaller number. Our theories and claims are
supported by empirical results on several image classification tasks with
various types of neural networks.",stat.ML
Forecasting Mortality in the Middle-Aged and Older Population of England: A 1D-CNN Approach,"Convolutional Neural Networks (CNNs) are proven to be effective when data are
homogeneous such as images, or when there is a relationship between consecutive
data such as time series data. Although CNNs are not famous for tabular data,
we show that we can use them in longitudinal data, where individuals'
information is recorded over a period and therefore there is a relationship
between them. This study considers the English Longitudinal Study of Ageing
(ELSA) survey, conducted every two years. We use one-dimensional convolutional
neural networks (1D-CNNs) to forecast mortality using socio-demographics,
diseases, mobility impairment, Activities of Daily Living (ADLs), Instrumental
Activities of Daily Living (IADLs), and lifestyle factors. As our dataset is
highly imbalanced, we try different over and undersampling methods and find
that over-representing the small class improves the results. We also try our
model with different activation functions. Our results show that swish
nonlinearity outperforms other functions.",stat.ML
Analysis of ELSA COVID-19 Substudy response rate using machine learning algorithms,"National Statistical Organisations every year spend time and money to collect
information through surveys. Some of these surveys include follow-up studies,
and usually, some participants due to factors such as death, immigration,
change of employment, health, etc, do not participate in future surveys. In
this study, we focus on the English Longitudinal Study of Ageing (ELSA)
COVID-19 Substudy, which was carried out during the COVID-19 pandemic in two
waves. In this substudy, some participants from wave 1 did not participate in
wave 2. Our purpose is to predict non-responses using Machine Learning (ML)
algorithms such as K-nearest neighbours (KNN), random forest (RF), AdaBoost,
logistic regression, neural networks (NN), and support vector classifier (SVC).
We find that RF outperforms other models in terms of balanced accuracy, KNN in
terms of precision and test accuracy, and logistics regressions in terms of the
area under the receiver operating characteristic curve (ROC), i.e. AUC.",stat.ML
MBExplainer: Multilevel bandit-based explanations for downstream models with augmented graph embeddings,"In many industrial applications, it is common that the graph embeddings
generated from training GNNs are used in an ensemble model where the embeddings
are combined with other tabular features (e.g., original node or edge features)
in a downstream ML task. The tabular features may even arise naturally if,
e.g., one tries to build a graph such that some of the node or edge features
are stored in a tabular format. Here we address the problem of explaining the
output of such ensemble models for which the input features consist of learned
neural graph embeddings combined with additional tabular features. We propose
MBExplainer, a model-agnostic explanation approach for downstream models with
augmented graph embeddings. MBExplainer returns a human-legible triple as an
explanation for an instance prediction of the whole pipeline consisting of
three components: a subgraph with the highest importance, the topmost important
nodal features, and the topmost important augmented downstream features. A
game-theoretic formulation is used to take the contributions of each component
and their interactions into account by assigning three Shapley values
corresponding to their own specific games. Finding the explanation requires an
efficient search through the corresponding local search spaces corresponding to
each component. MBExplainer applies a novel multilevel search algorithm that
enables simultaneous pruning of local search spaces in a computationally
tractable way. In particular, three interweaved Monte Carlo Tree Search are
utilized to iteratively prune the local search spaces. MBExplainer also
includes a global search algorithm that uses contextual bandits to efficiently
allocate pruning budget among the local search spaces. We show the
effectiveness of MBExplainer by presenting a set of comprehensive numerical
examples on multiple public graph datasets for both node and graph
classification tasks.",stat.ML
Efficient Model Compression for Bayesian Neural Networks,"Model Compression has drawn much attention within the deep learning community
recently. Compressing a dense neural network offers many advantages including
lower computation cost, deployability to devices of limited storage and
memories, and resistance to adversarial attacks. This may be achieved via
weight pruning or fully discarding certain input features. Here we demonstrate
a novel strategy to emulate principles of Bayesian model selection in a deep
learning setup. Given a fully connected Bayesian neural network with
spike-and-slab priors trained via a variational algorithm, we obtain the
posterior inclusion probability for every node that typically gets lost. We
employ these probabilities for pruning and feature selection on a host of
simulated and real-world benchmark data and find evidence of better
generalizability of the pruned model in all our experiments.",stat.ML
"Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond","Deep learning sometimes appears to work in unexpected ways. In pursuit of a
deeper understanding of its surprising behaviors, we investigate the utility of
a simple yet accurate model of a trained neural network consisting of a
sequence of first-order approximations telescoping out into a single
empirically operational tool for practical analysis. Across three case studies,
we illustrate how it can be applied to derive new empirical insights on a
diverse range of prominent phenomena in the literature -- including double
descent, grokking, linear mode connectivity, and the challenges of applying
deep learning on tabular data -- highlighting that this model allows us to
construct and extract metrics that help predict and understand the a priori
unexpected performance of neural networks. We also demonstrate that this model
presents a pedagogical formalism allowing us to isolate components of the
training process even in complex contemporary settings, providing a lens to
reason about the effects of design choices such as architecture & optimization
strategy, and reveals surprising parallels between neural network learning and
gradient boosting.",stat.ML
Minimum Empirical Divergence for Sub-Gaussian Linear Bandits,"We propose a novel linear bandit algorithm called LinMED (Linear Minimum
Empirical Divergence), which is a linear extension of the MED algorithm that
was originally designed for multi-armed bandits. LinMED is a randomized
algorithm that admits a closed-form computation of the arm sampling
probabilities, unlike the popular randomized algorithm called linear Thompson
sampling. Such a feature proves useful for off-policy evaluation where the
unbiased evaluation requires accurately computing the sampling probability. We
prove that LinMED enjoys a near-optimal regret bound of $d\sqrt{n}$ up to
logarithmic factors where $d$ is the dimension and $n$ is the time horizon. We
further show that LinMED enjoys a
$\frac{d^2}{\Delta}\left(\log^2(n)\right)\log\left(\log(n)\right)$
problem-dependent regret where $\Delta$ is the smallest sub-optimality gap,
which is lower than $\frac{d^2}{\Delta}\log^3(n)$ of the standard algorithm
OFUL (Abbasi-yadkori et al., 2011). Our empirical study shows that LinMED has a
competitive performance with the state-of-the-art algorithms.",stat.ML
Inclusive KL Minimization: A Wasserstein-Fisher-Rao Gradient Flow Perspective,"Otto's (2001) Wasserstein gradient flow of the exclusive KL divergence
functional provides a powerful and mathematically principled perspective for
analyzing learning and inference algorithms. In contrast, algorithms for the
inclusive KL inference, i.e., minimizing $ \mathrm{KL}(\pi \| \mu) $ with
respect to $ \mu $ for some target $ \pi $, are rarely analyzed using tools
from mathematical analysis. This paper shows that a general-purpose approximate
inclusive KL inference paradigm can be constructed using the theory of gradient
flows derived from PDE analysis. We uncover that several existing learning
algorithms can be viewed as particular realizations of the inclusive KL
inference paradigm. For example, existing sampling algorithms such as Arbel et
al. (2019) and Korba et al. (2021) can be viewed in a unified manner as
inclusive-KL inference with approximate gradient estimators. Finally, we
provide the theoretical foundation for the Wasserstein-Fisher-Rao gradient
flows for minimizing the inclusive KL divergence.",stat.ML
Learning Mixtures of Unknown Causal Interventions,"The ability to conduct interventions plays a pivotal role in learning causal
relationships among variables, thus facilitating applications across diverse
scientific disciplines such as genomics, economics, and machine learning.
However, in many instances within these applications, the process of generating
interventional data is subject to noise: rather than data being sampled
directly from the intended interventional distribution, interventions often
yield data sampled from a blend of both intended and unintended interventional
distributions.
  We consider the fundamental challenge of disentangling mixed interventional
and observational data within linear Structural Equation Models (SEMs) with
Gaussian additive noise without the knowledge of the true causal graph. We
demonstrate that conducting interventions, whether do or soft, yields
distributions with sufficient diversity and properties conducive to efficiently
recovering each component within the mixture. Furthermore, we establish that
the sample complexity required to disentangle mixed data inversely correlates
with the extent of change induced by an intervention in the equations governing
the affected variable values. As a result, the causal graph can be identified
up to its interventional Markov Equivalence Class, similar to scenarios where
no noise influences the generation of interventional data. We further support
our theoretical findings by conducting simulations wherein we perform causal
discovery from such mixed data.",stat.ML
Kernel Operator-Theoretic Bayesian Filter for Nonlinear Dynamical Systems,"Motivated by the surge of interest in Koopman operator theory, we propose a
machine-learning alternative based on a functional Bayesian perspective for
operator-theoretic modeling of unknown, data-driven, nonlinear dynamical
systems. This formulation is directly done in an infinite-dimensional space of
linear operators or Hilbert space with universal approximation property. The
theory of reproducing kernel Hilbert space (RKHS) allows the lifting of
nonlinear dynamics to a potentially infinite-dimensional space via linear
embeddings, where a general nonlinear function is represented as a set of
linear functions or operators in the functional space. This allows us to apply
classical linear Bayesian methods such as the Kalman filter directly in the
Hilbert space, yielding nonlinear solutions in the original input space. This
kernel perspective on the Koopman operator offers two compelling advantages.
First, the Hilbert space can be constructed deterministically, agnostic to the
nonlinear dynamics. The Gaussian kernel is universal, approximating uniformly
an arbitrary continuous target function over any compact domain. Second,
Bayesian filter is an adaptive, linear minimum-variance algorithm, allowing the
system to update the Koopman operator and continuously track the changes across
an extended period of time, ideally suited for modern data-driven applications
such as real-time machine learning using streaming data. In this paper, we
present several practical implementations to obtain a finite-dimensional
approximation of the functional Bayesian filter (FBF). Due to the rapid decay
of the Gaussian kernel, excellent approximation is obtained with a small
dimension. We demonstrate that this practical approach can obtain accurate
results and outperform finite-dimensional Koopman decomposition.",stat.ML
Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research,"Large Language Models (LLMs) are distinguished by their architecture, which
dictates their parameter size and performance capabilities. Social scientists
have increasingly adopted LLMs for text classification tasks, which are
difficult to scale with human coders. While very large, closed-source models
often deliver superior performance, their use presents significant risks. These
include lack of transparency, potential exposure of sensitive data, challenges
to replicability, and dependence on proprietary systems. Additionally, their
high costs make them impractical for large-scale research projects.
  In contrast, open-source models, although available in various sizes, may
underperform compared to commercial alternatives if used without further
fine-tuning. However, open-source models offer distinct advantages: they can be
run locally (ensuring data privacy), fine-tuned for specific tasks, shared
within the research community, and integrated into reproducible workflows.
  This study demonstrates that small, fine-tuned open-source LLMs can achieve
equal or superior performance to models such as ChatGPT-4. We further explore
the relationship between training set size and fine-tuning efficacy in
open-source models. Finally, we propose a hybrid workflow that leverages the
strengths of both open and closed models, offering a balanced approach to
performance, transparency, and reproducibility.",stat.ML
Residual Deep Gaussian Processes on Manifolds,"We propose practical deep Gaussian process models on Riemannian manifolds,
similar in spirit to residual neural networks. With manifold-to-manifold hidden
layers and an arbitrary last layer, they can model manifold- and scalar-valued
functions, as well as vector fields. We target data inherently supported on
manifolds, which is too complex for shallow Gaussian processes thereon. For
example, while the latter perform well on high-altitude wind data, they
struggle with the more intricate, nonstationary patterns at low altitudes. Our
models significantly improve performance in these settings, enhancing
prediction quality and uncertainty calibration, and remain robust to
overfitting, reverting to shallow models when additional complexity is
unneeded. We further showcase our models on Bayesian optimisation problems on
manifolds, using stylised examples motivated by robotics, and obtain
substantial improvements in later stages of the optimisation process. Finally,
we show our models to have potential for speeding up inference for non-manifold
data, when, and if, it can be mapped to a proxy manifold well enough.",stat.ML
A Geometric Framework for Understanding Memorization in Generative Models,"As deep generative models have progressed, recent work has shown them to be
capable of memorizing and reproducing training datapoints when deployed. These
findings call into question the usability of generative models, especially in
light of the legal and privacy risks brought about by memorization. To better
understand this phenomenon, we propose the manifold memorization hypothesis
(MMH), a geometric framework which leverages the manifold hypothesis into a
clear language in which to reason about memorization. We propose to analyze
memorization in terms of the relationship between the dimensionalities of $(i)$
the ground truth data manifold and $(ii)$ the manifold learned by the model.
This framework provides a formal standard for ""how memorized"" a datapoint is
and systematically categorizes memorized data into two types: memorization
driven by overfitting and memorization driven by the underlying data
distribution. By analyzing prior work in the context of the MMH, we explain and
unify assorted observations in the literature. We empirically validate the MMH
using synthetic data and image datasets up to the scale of Stable Diffusion,
developing new tools for detecting and preventing generation of memorized
samples in the process.",stat.ML
Prospective Learning: Learning for a Dynamic Future,"In real-world applications, the distribution of the data, and our goals,
evolve over time. The prevailing theoretical framework for studying machine
learning, namely probably approximately correct (PAC) learning, largely ignores
time. As a consequence, existing strategies to address the dynamic nature of
data and goals exhibit poor real-world performance. This paper develops a
theoretical framework called ""Prospective Learning"" that is tailored for
situations when the optimal hypothesis changes over time. In PAC learning,
empirical risk minimization (ERM) is known to be consistent. We develop a
learner called Prospective ERM, which returns a sequence of predictors that
make predictions on future data. We prove that the risk of prospective ERM
converges to the Bayes risk under certain assumptions on the stochastic process
generating the data. Prospective ERM, roughly speaking, incorporates time as an
input in addition to the data. We show that standard ERM as done in PAC
learning, without incorporating time, can result in failure to learn when
distributions are dynamic. Numerical experiments illustrate that prospective
ERM can learn synthetic and visual recognition problems constructed from MNIST
and CIFAR-10.",stat.ML
Robust Gaussian Processes via Relevance Pursuit,"Gaussian processes (GPs) are non-parametric probabilistic regression models
that are popular due to their flexibility, data efficiency, and well-calibrated
uncertainty estimates. However, standard GP models assume homoskedastic
Gaussian noise, while many real-world applications are subject to non-Gaussian
corruptions. Variants of GPs that are more robust to alternative noise models
have been proposed, and entail significant trade-offs between accuracy and
robustness, and between computational requirements and theoretical guarantees.
In this work, we propose and study a GP model that achieves robustness against
sparse outliers by inferring data-point-specific noise levels with a sequential
selection procedure maximizing the log marginal likelihood that we refer to as
relevance pursuit. We show, surprisingly, that the model can be parameterized
such that the associated log marginal likelihood is strongly concave in the
data-point-specific noise variances, a property rarely found in either robust
regression objectives or GP marginal likelihoods. This in turn implies the weak
submodularity of the corresponding subset selection problem, and thereby proves
approximation guarantees for the proposed algorithm. We compare the model's
performance relative to other approaches on diverse regression and Bayesian
optimization tasks, including the challenging but common setting of sparse
corruptions of the labels within or close to the function range.",stat.ML
Bridging Geometric States via Geometric Diffusion Bridge,"The accurate prediction of geometric state evolution in complex systems is
critical for advancing scientific domains such as quantum chemistry and
material modeling. Traditional experimental and computational methods face
challenges in terms of environmental constraints and computational demands,
while current deep learning approaches still fall short in terms of precision
and generality. In this work, we introduce the Geometric Diffusion Bridge
(GDB), a novel generative modeling framework that accurately bridges initial
and target geometric states. GDB leverages a probabilistic approach to evolve
geometric state distributions, employing an equivariant diffusion bridge
derived by a modified version of Doob's $h$-transform for connecting geometric
states. This tailored diffusion process is anchored by initial and target
geometric states as fixed endpoints and governed by equivariant transition
kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB
framework by using a chain of equivariant diffusion bridges, providing a more
detailed and accurate characterization of evolution dynamics. Theoretically, we
conduct a thorough examination to confirm our framework's ability to preserve
joint distributions of geometric states and capability to completely model the
underlying dynamics inducing trajectory distributions with negligible error.
Experimental evaluations across various real-world scenarios show that GDB
surpasses existing state-of-the-art approaches, opening up a new pathway for
accurately bridging geometric states and tackling crucial scientific challenges
with improved accuracy and applicability.",stat.ML
Understanding Optimization in Deep Learning with Central Flows,"Optimization in deep learning remains poorly understood, even in the simple
setting of deterministic (i.e. full-batch) training. A key difficulty is that
much of an optimizer's behavior is implicitly determined by complex oscillatory
dynamics, referred to as the ""edge of stability."" The main contribution of this
paper is to show that an optimizer's implicit behavior can be explicitly
captured by a ""central flow:"" a differential equation which models the
time-averaged optimization trajectory. We show that these flows can empirically
predict long-term optimization trajectories of generic neural networks with a
high degree of numerical accuracy. By interpreting these flows, we reveal for
the first time 1) the precise sense in which RMSProp adapts to the local loss
landscape, and 2) an ""acceleration via regularization"" mechanism, wherein
adaptive optimizers implicitly navigate towards low-curvature regions in which
they can take larger steps. This mechanism is key to the efficacy of these
adaptive optimizers. Overall, we believe that central flows constitute a
promising tool for reasoning about optimization in deep learning.",stat.ML
Conformal prediction of circular data,"Split conformal prediction techniques are applied to regression problems with
circular responses by introducing a suitable conformity score, leading to
prediction sets with adaptive arc length and finite-sample coverage guarantees
for any circular predictive model under exchangeable data. Leveraging the high
performance of existing predictive models designed for linear responses, we
analyze a general projection procedure that converts any linear response
regression model into one suitable for circular responses. When random forests
serve as basis models in this projection procedure, we harness the out-of-bag
dynamics to eliminate the necessity for a separate calibration sample in the
construction of prediction sets. For synthetic and real datasets the resulting
projected random forests model produces more efficient out-of-bag conformal
prediction sets, with shorter median arc length, when compared to the split
conformal prediction sets generated by two existing alternative models.",stat.ML
Label Noise: Ignorance Is Bliss,"We establish a new theoretical framework for learning under multi-class,
instance-dependent label noise. This framework casts learning with label noise
as a form of domain adaptation, in particular, domain adaptation under
posterior drift. We introduce the concept of \emph{relative signal strength}
(RSS), a pointwise measure that quantifies the transferability from noisy to
clean posterior. Using RSS, we establish nearly matching upper and lower bounds
on the excess risk. Our theoretical findings support the simple \emph{Noise
Ignorant Empirical Risk Minimization (NI-ERM)} principle, which minimizes
empirical risk while ignoring label noise. Finally, we translate this
theoretical insight into practice: by using NI-ERM to fit a linear classifier
on top of a self-supervised feature extractor, we achieve state-of-the-art
performance on the CIFAR-N data challenge.",stat.ML
$\boldsymbol\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling,"Sharpness Aware Minimization (SAM) enhances performance across various neural
architectures and datasets. As models are continually scaled up to improve
performance, a rigorous understanding of SAM's scaling behaviour is paramount.
To this end, we study the infinite-width limit of neural networks trained with
SAM, using the Tensor Programs framework. Our findings reveal that the dynamics
of standard SAM effectively reduce to applying SAM solely in the last layer in
wide neural networks, even with optimal hyperparameters. In contrast, we
identify a stable parameterization with layerwise perturbation scaling, which
we call $\textit{Maximal Update and Perturbation Parameterization}$
($\mu$P$^2$), that ensures all layers are both feature learning and effectively
perturbed in the limit. Through experiments with MLPs, ResNets and Vision
Transformers, we empirically demonstrate that $\mu$P$^2$ is the first
parameterization to achieve hyperparameter transfer of the joint optimum of
learning rate and perturbation radius across model scales. Moreover, we provide
an intuitive condition to derive $\mu$P$^2$ for other perturbation rules like
Adaptive SAM and SAM-ON, also ensuring balanced perturbation effects across all
layers.",stat.ML
Demystifying Linear MDPs and Novel Dynamics Aggregation Framework,"In this work, we prove that, in linear MDPs, the feature dimension $d$ is
lower bounded by $S/U$ in order to aptly represent transition probabilities,
where $S$ is the size of the state space and $U$ is the maximum size of
directly reachable states. Hence, $d$ can still scale with $S$ depending on the
direct reachability of the environment. To address this limitation of linear
MDPs, we propose a novel structural aggregation framework based on dynamics,
named as the ""dynamics aggregation"". For this newly proposed framework, we
design a provably efficient hierarchical reinforcement learning algorithm in
linear function approximation that leverages aggregated sub-structures. Our
proposed algorithm exhibits statistical efficiency, achieving a regret of $
\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ represents
the feature dimension of aggregated subMDPs and $N$ signifies the number of
aggregated subMDPs. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ is
readily met in most real-world environments with hierarchical structures,
enabling a substantial improvement in the regret bound compared to LSVI-UCB,
which enjoys a regret of $ \tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the best
of our knowledge, this work presents the first HRL algorithm with linear
function approximation that offers provable guarantees.",stat.ML
Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models,"Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and
often requires advanced sampling techniques like Markov chain Monte Carlo
(MCMC). A common approach is to write the model in a probabilistic programming
language and then sample via Hamiltonian Monte Carlo (HMC). However, there are
many ways a user can transform a model that make inference more or less
efficient. In particular, marginalizing some variables can greatly improve
inference but is difficult for users to do manually. We develop an algorithm to
easily marginalize random effects in LMMs. A naive approach introduces cubic
time operations within an inference algorithm like HMC, but we reduce the
running time to linear using fast linear algebra techniques. We show that
marginalization is always beneficial when applicable and highlight improvements
in various models, especially ones from cognitive sciences.",stat.ML
Identifying General Mechanism Shifts in Linear Causal Representations,"We consider the linear causal representation learning setting where we
observe a linear mixing of $d$ unknown latent factors, which follow a linear
structural causal model. Recent work has shown that it is possible to recover
the latent factors as well as the underlying structural causal model over them,
up to permutation and scaling, provided that we have at least $d$ environments,
each of which corresponds to perfect interventions on a single latent node
(factor). After this powerful result, a key open problem faced by the community
has been to relax these conditions: allow for coarser than perfect single-node
interventions, and allow for fewer than $d$ of them, since the number of latent
factors $d$ could be very large. In this work, we consider precisely such a
setting, where we allow a smaller than $d$ number of environments, and also
allow for very coarse interventions that can very coarsely \textit{change the
entire causal graph over the latent factors}. On the flip side, we relax what
we wish to extract to simply the \textit{list of nodes that have shifted
between one or more environments}. We provide a surprising identifiability
result that it is indeed possible, under some very mild standard assumptions,
to identify the set of shifted nodes. Our identifiability proof moreover is a
constructive one: we explicitly provide necessary and sufficient conditions for
a node to be a shifted node, and show that we can check these conditions given
observed data. Our algorithm lends itself very naturally to the sample setting
where instead of just interventional distributions, we are provided datasets of
samples from each of these distributions. We corroborate our results on both
synthetic experiments as well as an interesting psychometric dataset. The code
can be found at https://github.com/TianyuCodings/iLCS.",stat.ML
EigenVI: score-based variational inference with orthogonal function expansions,"We develop EigenVI, an eigenvalue-based approach for black-box variational
inference (BBVI). EigenVI constructs its variational approximations from
orthogonal function expansions. For distributions over $\mathbb{R}^D$, the
lowest order term in these expansions provides a Gaussian variational
approximation, while higher-order terms provide a systematic way to model
non-Gaussianity. These approximations are flexible enough to model complex
distributions (multimodal, asymmetric), but they are simple enough that one can
calculate their low-order moments and draw samples from them. EigenVI can also
model other types of random variables (e.g., nonnegative, bounded) by
constructing variational approximations from different families of orthogonal
functions. Within these families, EigenVI computes the variational
approximation that best matches the score function of the target distribution
by minimizing a stochastic estimate of the Fisher divergence. Notably, this
optimization reduces to solving a minimum eigenvalue problem, so that EigenVI
effectively sidesteps the iterative gradient-based optimizations that are
required for many other BBVI algorithms. (Gradient-based methods can be
sensitive to learning rates, termination criteria, and other tunable
hyperparameters.) We use EigenVI to approximate a variety of target
distributions, including a benchmark suite of Bayesian models from posteriordb.
On these distributions, we find that EigenVI is more accurate than existing
methods for Gaussian BBVI.",stat.ML
A Visual Case Study of the Training Dynamics in Neural Networks,"This paper introduces a visual sandbox designed to explore the training
dynamics of a small-scale transformer model, with the embedding dimension
constrained to $d=2$. This restriction allows for a comprehensive
two-dimensional visualization of each layer's dynamics. Through this approach,
we gain insights into training dynamics, circuit transferability, and the
causes of loss spikes, including those induced by the high curvature of
normalization layers. We propose strategies to mitigate these spikes,
demonstrating how good visualization facilitates the design of innovative ideas
of practical interest. Additionally, we believe our sandbox could assist
theoreticians in assessing essential training dynamics mechanisms and
integrating them into future theories. The code is available at
https://github.com/facebookresearch/pal.",stat.ML
Robust Sparse Regression with Non-Isotropic Designs,"We develop a technique to design efficiently computable estimators for sparse
linear regression in the simultaneous presence of two adversaries: oblivious
and adaptive. We design several robust algorithms that outperform the state of
the art even in the special case when oblivious adversary simply adds Gaussian
noise. In particular, we provide a polynomial-time algorithm that with high
probability recovers the signal up to error $O(\sqrt{\varepsilon})$ as long as
the number of samples $n \ge \tilde{O}(k^2/\varepsilon)$, only assuming some
bounds on the third and the fourth moments of the distribution ${D}$ of the
design.
  In addition, prior to this work, even in the special case of Gaussian design
and noise, no polynomial time algorithm was known to achieve error
$o(\sqrt{\varepsilon})$ in the sparse setting $n < d^2$. We show that under
some assumptions on the fourth and the eighth moments of ${D}$, there is a
polynomial-time algorithm that achieves error $o(\sqrt{\varepsilon})$ as long
as $n \ge \tilde{O}(k^4 / \varepsilon^3)$. For Gaussian distribution, this
algorithm achieves error $O(\varepsilon^{3/4})$. Moreover, our algorithm
achieves error $o(\sqrt{\varepsilon})$ for all log-concave distributions if
$\varepsilon \le 1/\text{polylog(d)}$.
  Our algorithms are based on the filtering of the covariates that uses
sum-of-squares relaxations, and weighted Huber loss minimization with $\ell_1$
regularizer. We provide a novel analysis of weighted penalized Huber loss that
is suitable for heavy-tailed designs in the presence of two adversaries.
Furthermore, we complement our algorithmic results with Statistical Query lower
bounds, providing evidence that our estimators are likely to have nearly
optimal sample complexity.",stat.ML
Disentangling Interactions and Dependencies in Feature Attribution,"In explainable machine learning, global feature importance methods try to
determine how much each individual feature contributes to predicting the target
variable, resulting in one importance score for each feature. But often,
predicting the target variable requires interactions between several features
(such as in the XOR function), and features might have complex statistical
dependencies that allow to partially replace one feature with another one. In
commonly used feature importance scores these cooperative effects are conflated
with the features' individual contributions, making them prone to
misinterpretations. In this work, we derive DIP, a new mathematical
decomposition of individual feature importance scores that disentangles three
components: the standalone contribution and the contributions stemming from
interactions and dependencies. We prove that the DIP decomposition is unique
and show how it can be estimated in practice. Based on these results, we
propose a new visualization of feature importance scores that clearly
illustrates the different contributions.",stat.ML
Evolving Alignment via Asymmetric Self-Play,"Current RLHF frameworks for aligning large language models (LLMs) typically
assume a fixed prompt distribution, which is sub-optimal and limits the
scalability of alignment and generalizability of models. To address this, we
introduce a general open-ended RLHF framework that casts alignment as an
asymmetric game between two players: (i) a creator that generates increasingly
informative prompt distributions using the reward model, and (ii) a solver that
learns to produce more preferred responses on prompts produced by the creator.
This framework of Evolving Alignment via Asymmetric Self-Play (eva), results in
a simple and efficient approach that can utilize any existing RLHF algorithm
for scalable alignment. eva outperforms state-of-the-art methods on widely-used
benchmarks, without the need of any additional human crafted prompts.
Specifically, eva improves the win rate of Gemma-2-9B-it on Arena-Hard from
51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7%
with SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and
matching claude-3-opus. This improvement is persistent even when new human
crafted prompts are introduced. Finally, we show eva is effective and robust
under various ablation settings.",stat.ML
Wide Two-Layer Networks can Learn from Adversarial Perturbations,"Adversarial examples have raised several open questions, such as why they can
deceive classifiers and transfer between different models. A prevailing
hypothesis to explain these phenomena suggests that adversarial perturbations
appear as random noise but contain class-specific features. This hypothesis is
supported by the success of perturbation learning, where classifiers trained
solely on adversarial examples and the corresponding incorrect labels
generalize well to correctly labeled test data. Although this hypothesis and
perturbation learning are effective in explaining intriguing properties of
adversarial examples, their solid theoretical foundation is limited. In this
study, we theoretically explain the counterintuitive success of perturbation
learning. We assume wide two-layer networks and the results hold for any data
distribution. We prove that adversarial perturbations contain sufficient
class-specific features for networks to generalize from them. Moreover, the
predictions of classifiers trained on mislabeled adversarial examples coincide
with those of classifiers trained on correctly labeled clean samples. The code
is available at https://github.com/s-kumano/perturbation-learning.",stat.ML
Provable Benefit of Cutout and CutMix for Feature Learning,"Patch-level data augmentation techniques such as Cutout and CutMix have
demonstrated significant efficacy in enhancing the performance of vision tasks.
However, a comprehensive theoretical understanding of these methods remains
elusive. In this paper, we study two-layer neural networks trained using three
distinct methods: vanilla training without augmentation, Cutout training, and
CutMix training. Our analysis focuses on a feature-noise data model, which
consists of several label-dependent features of varying rarity and
label-independent noises of differing strengths. Our theorems demonstrate that
Cutout training can learn low-frequency features that vanilla training cannot,
while CutMix training can learn even rarer features that Cutout cannot capture.
From this, we establish that CutMix yields the highest test accuracy among the
three. Our novel analysis reveals that CutMix training makes the network learn
all features and noise vectors ""evenly"" regardless of the rarity and strength,
which provides an interesting insight into understanding patch-level
augmentation.",stat.ML
Projected Neural Differential Equations for Learning Constrained Dynamics,"Neural differential equations offer a powerful approach for learning dynamics
from data. However, they do not impose known constraints that should be obeyed
by the learned model. It is well-known that enforcing constraints in surrogate
models can enhance their generalizability and numerical stability. In this
paper, we introduce projected neural differential equations (PNDEs), a new
method for constraining neural differential equations based on projection of
the learned vector field to the tangent space of the constraint manifold. In
tests on several challenging examples, including chaotic dynamical systems and
state-of-the-art power grid models, PNDEs outperform existing methods while
requiring fewer hyperparameters. The proposed approach demonstrates significant
potential for enhancing the modeling of constrained dynamical systems,
particularly in complex domains where accuracy and reliability are essential.",stat.ML
Online Consistency of the Nearest Neighbor Rule,"In the realizable online setting, a learner is tasked with making predictions
for a stream of instances, where the correct answer is revealed after each
prediction. A learning rule is online consistent if its mistake rate eventually
vanishes. The nearest neighbor rule (Fix and Hodges, 1951) is a fundamental
prediction strategy, but it is only known to be consistent under strong
statistical or geometric assumptions: the instances come i.i.d. or the label
classes are well-separated. We prove online consistency for all measurable
functions in doubling metric spaces under the mild assumption that the
instances are generated by a process that is uniformly absolutely continuous
with respect to a finite, upper doubling measure.",stat.ML
Sample-Efficient Agnostic Boosting,"The theory of boosting provides a computational framework for aggregating
approximate weak learning algorithms, which perform marginally better than a
random predictor, into an accurate strong learner. In the realizable case, the
success of the boosting approach is underscored by a remarkable fact that the
resultant sample complexity matches that of a computationally demanding
alternative, namely Empirical Risk Minimization (ERM). This in particular
implies that the realizable boosting methodology has the potential to offer
computational relief without compromising on sample efficiency.
  Despite recent progress, in agnostic boosting, where assumptions on the
conditional distribution of labels given feature descriptions are absent, ERM
outstrips the agnostic boosting methodology in being quadratically more sample
efficient than all known agnostic boosting algorithms. In this paper, we make
progress on closing this gap, and give a substantially more sample efficient
agnostic boosting algorithm than those known, without compromising on the
computational (or oracle) complexity. A key feature of our algorithm is that it
leverages the ability to reuse samples across multiple rounds of boosting,
while guaranteeing a generalization error strictly better than those obtained
by blackbox applications of uniform convergence arguments. We also apply our
approach to other previously studied learning problems, including boosting for
reinforcement learning, and demonstrate improved results.",stat.ML
Identifiability Guarantees for Causal Disentanglement from Purely Observational Data,"Causal disentanglement aims to learn about latent causal factors behind data,
holding the promise to augment existing representation learning methods in
terms of interpretability and extrapolation. Recent advances establish
identifiability results assuming that interventions on (single) latent factors
are available; however, it remains debatable whether such assumptions are
reasonable due to the inherent nature of intervening on latent variables.
Accordingly, we reconsider the fundamentals and ask what can be learned using
just observational data.
  We provide a precise characterization of latent factors that can be
identified in nonlinear causal models with additive Gaussian noise and linear
mixing, without any interventions or graphical restrictions. In particular, we
show that the causal variables can be identified up to a layer-wise
transformation and that further disentanglement is not possible. We transform
these theoretical results into a practical algorithm consisting of solving a
quadratic program over the score estimation of the observed data. We provide
simulation results to support our theoretical guarantees and demonstrate that
our algorithm can derive meaningful causal representations from purely
observational data.",stat.ML
Global Convergence in Training Large-Scale Transformers,"Despite the widespread success of Transformers across various domains, their
optimization guarantees in large-scale model settings are not well-understood.
This paper rigorously analyzes the convergence properties of gradient flow in
training Transformers with weight decay regularization. First, we construct the
mean-field limit of large-scale Transformers, showing that as the model width
and depth go to infinity, gradient flow converges to the Wasserstein gradient
flow, which is represented by a partial differential equation. Then, we
demonstrate that the gradient flow reaches a global minimum consistent with the
PDE solution when the weight decay regularization parameter is sufficiently
small. Our analysis is based on a series of novel mean-field techniques that
adapt to Transformers. Compared with existing tools for deep networks (Lu et
al., 2020) that demand homogeneity and global Lipschitz smoothness, we utilize
a refined analysis assuming only $\textit{partial homogeneity}$ and
$\textit{local Lipschitz smoothness}$. These new techniques may be of
independent interest.",stat.ML
"Linearized Wasserstein Barycenters: Synthesis, Analysis, Representational Capacity, and Applications","We propose the \textit{linear barycentric coding model (LBCM)} that utilizes
the linear optimal transport (LOT) metric for analysis and synthesis of
probability measures. We provide a closed-form solution to the variational
problem characterizing the probability measures in the LBCM and establish
equivalence of the LBCM to the set of Wasserstein-2 barycenters in the special
case of compatible measures. Computational methods for synthesizing and
analyzing measures in the LBCM are developed with finite sample guarantees. One
of our main theoretical contributions is to identify an LBCM, expressed in
terms of a simple family, which is sufficient to express all probability
measures on the interval $[0,1]$. We show that a natural analogous construction
of an LBCM in $\mathbb{R}^2$ fails, and we leave it as an open problem to
identify the proper extension in more than one dimension. We conclude by
demonstrating the utility of LBCM for covariance estimation and data
imputation.",stat.ML
Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling,"Online learning methods, like the seminal Passive-Aggressive (PA) classifier,
are still highly effective for high-dimensional streaming data, out-of-core
processing, and other throughput-sensitive applications. Many such algorithms
rely on fast adaptation to individual errors as a key to their convergence.
While such algorithms enjoy low theoretical regret, in real-world deployment
they can be sensitive to individual outliers that cause the algorithm to
over-correct. When such outliers occur at the end of the data stream, this can
cause the final solution to have unexpectedly low accuracy. We design a
weighted reservoir sampling (WRS) approach to obtain a stable ensemble model
from the sequence of solutions without requiring additional passes over the
data, hold-out sets, or a growing amount of memory. Our key insight is that
good solutions tend to be error-free for more iterations than bad solutions,
and thus, the number of passive rounds provides an estimate of a solution's
relative quality. Our reservoir thus contains $K$ previous intermediate weight
vectors with high survival times. We demonstrate our WRS approach on the
Passive-Aggressive Classifier (PAC) and First-Order Sparse Online Learning
(FSOL), where our method consistently and significantly outperforms the
unmodified approach. We show that the risk of the ensemble classifier is
bounded with respect to the regret of the underlying online learning method.",stat.ML
Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis,"The success of machine learning models relies heavily on effectively
representing high-dimensional data. However, ensuring data representations
capture human-understandable concepts remains difficult, often requiring the
incorporation of prior knowledge and decomposition of data into multiple
subspaces. Traditional linear methods fall short in modeling more than one
space, while more expressive deep learning approaches lack interpretability.
Here, we introduce Supervised Independent Subspace Principal Component Analysis
($\texttt{sisPCA}$), a PCA extension designed for multi-subspace learning.
Leveraging the Hilbert-Schmidt Independence Criterion (HSIC), $\texttt{sisPCA}$
incorporates supervision and simultaneously ensures subspace disentanglement.
We demonstrate $\texttt{sisPCA}$'s connections with autoencoders and
regularized linear regression and showcase its ability to identify and separate
hidden data structures through extensive applications, including breast cancer
diagnosis from image features, learning aging-associated DNA methylation
changes, and single-cell analysis of malaria infection. Our results reveal
distinct functional pathways associated with malaria colonization, underscoring
the essentiality of explainable representation in high-dimensional data
analysis.",stat.ML
How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?,"Real-world data is often assumed to lie within a low-dimensional structure
embedded in high-dimensional space. In practical settings, we observe only a
finite set of samples, forming what we refer to as the sample data subspace. It
serves an essential approximation supporting tasks such as dimensionality
reduction and generation. A major challenge lies in whether generative models
can reliably synthesize samples that stay within this subspace rather than
drifting away from the underlying structure. In this work, we provide
theoretical insights into this challenge by leveraging Flow Matching models,
which transform a simple prior into a complex target distribution via a learned
velocity field. By treating the real data distribution as discrete, we derive
analytical expressions for the optimal velocity field under a Gaussian prior,
showing that generated samples memorize real data points and represent the
sample data subspace exactly. To generalize to suboptimal scenarios, we
introduce the Orthogonal Subspace Decomposition Network (OSDNet), which
systematically decomposes the velocity field into subspace and off-subspace
components. Our analysis shows that the off-subspace component decays, while
the subspace component generalizes within the sample data subspace, ensuring
generated samples preserve both proximity and diversity.",stat.ML
All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling,"We analyze identifiability as a possible explanation for the ubiquity of
linear properties across language models, such as the vector difference between
the representations of ""easy"" and ""easiest"" being parallel to that between
""lucky"" and ""luckiest"". For this, we ask whether finding a linear property in
one model implies that any model that induces the same distribution has that
property, too. To answer that, we first prove an identifiability result to
characterize distribution-equivalent next-token predictors, lifting a diversity
requirement of previous results. Second, based on a refinement of relational
linearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how
many notions of linearity are amenable to our analysis. Finally, we show that
under suitable conditions, these linear properties either hold in all or none
distribution-equivalent next-token predictors.",stat.ML
Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems,"Causal discovery with time series data remains a challenging yet increasingly
important task across many scientific domains. Convergent cross mapping (CCM)
and related methods have been proposed to study time series that are generated
by dynamical systems, where traditional approaches like Granger causality are
unreliable. However, CCM often yields inaccurate results depending upon the
quality of the data. We propose the Tangent Space Causal Inference (TSCI)
method for detecting causalities in dynamical systems. TSCI works by
considering vector fields as explicit representations of the systems' dynamics
and checks for the degree of synchronization between the learned vector fields.
The TSCI approach is model-agnostic and can be used as a drop-in replacement
for CCM and its generalizations. We first present a basic version of the TSCI
algorithm, which is shown to be more effective than the basic CCM algorithm
with very little additional computation. We additionally present augmented
versions of TSCI that leverage the expressive power of latent variable models
and deep learning. We validate our theory on standard systems, and we
demonstrate improved causal inference performance across a number of benchmark
tasks.",stat.ML
Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm,"Reinforcement learning utilizing kernel ridge regression to predict the
expected value function represents a powerful method with great
representational capacity. This setting is a highly versatile framework
amenable to analytical results. We consider kernel-based function approximation
for RL in the infinite horizon average reward setting, also referred to as the
undiscounted setting. We propose an optimistic algorithm, similar to
acquisition function based algorithms in the special case of bandits. We
establish novel no-regret performance guarantees for our algorithm, under
kernel-based modelling assumptions. Additionally, we derive a novel confidence
interval for the kernel-based prediction of the expected value function,
applicable across various RL problems.",stat.ML
Multi-fidelity Machine Learning for Uncertainty Quantification and Optimization,"In system analysis and design optimization, multiple computational models are
typically available to represent a given physical system. These models can be
broadly classified as high-fidelity models, which provide highly accurate
predictions but require significant computational resources, and low-fidelity
models, which are computationally efficient but less accurate. Multi-fidelity
methods integrate high- and low-fidelity models to balance computational cost
and predictive accuracy. This perspective paper provides an in-depth overview
of the emerging field of machine learning-based multi-fidelity methods, with a
particular emphasis on uncertainty quantification and optimization. For
uncertainty quantification, a particular focus is on multi-fidelity graph
neural networks, compared with multi-fidelity polynomial chaos expansion. For
optimization, our emphasis is on multi-fidelity Bayesian optimization, offering
a unified perspective on multi-fidelity priors and proposing an application
strategy when the objective function is an integral or a weighted sum. We
highlight the current state of the art, identify critical gaps in the
literature, and outline key research opportunities in this evolving field.",stat.ML
Return Augmented Decision Transformer for Off-Dynamics Reinforcement Learning,"We study offline off-dynamics reinforcement learning (RL) to utilize data
from an easily accessible source domain to enhance policy learning in a target
domain with limited data. Our approach centers on return-conditioned supervised
learning (RCSL), particularly focusing on the decision transformer (DT), which
can predict actions conditioned on desired return guidance and complete
trajectory history. Previous works tackle the dynamics shift problem by
augmenting the reward in the trajectory from the source domain to match the
optimal trajectory in the target domain. However, this strategy can not be
directly applicable in RCSL owing to (1) the unique form of the RCSL policy
class, which explicitly depends on the return, and (2) the absence of a
straightforward representation of the optimal trajectory distribution. We
propose the Return Augmented Decision Transformer (RADT) method, where we
augment the return in the source domain by aligning its distribution with that
in the target domain. We provide the theoretical analysis demonstrating that
the RCSL policy learned from RADT achieves the same level of suboptimality as
would be obtained without a dynamics shift. We introduce two practical
implementations RADT-DARA and RADT-MV respectively. Extensive experiments
conducted on D4RL datasets reveal that our methods generally outperform dynamic
programming based methods in off-dynamics RL scenarios.",stat.ML
BAMITA: Bayesian Multiple Imputation for Tensor Arrays,"Data increasingly take the form of a multi-way array, or tensor, in several
biomedical domains. Such tensors are often incompletely observed. For example,
we are motivated by longitudinal microbiome studies in which several timepoints
are missing for several subjects. There is a growing literature on missing data
imputation for tensors. However, existing methods give a point estimate for
missing values without capturing uncertainty. We propose a multiple imputation
approach for tensors in a flexible Bayesian framework, that yields realistic
simulated values for missing entries and can propagate uncertainty through
subsequent analyses. Our model uses efficient and widely applicable conjugate
priors for a CANDECOMP/PARAFAC (CP) factorization, with a separable residual
covariance structure. This approach is shown to perform well with respect to
both imputation accuracy and uncertainty calibration, for scenarios in which
either single entries or entire fibers of the tensor are missing. For two
microbiome applications, it is shown to accurately capture uncertainty in the
full microbiome profile at missing timepoints and used to infer trends in
species diversity for the population. Documented R code to perform our multiple
imputation approach is available at
https://github.com/lockEF/MultiwayImputation .",stat.ML
FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions,"Material discovery is a critical area of research with the potential to
revolutionize various fields, including carbon capture, renewable energy, and
electronics. However, the immense scale of the chemical space makes it
challenging to explore all possible materials experimentally. In this paper, we
introduce FlowLLM, a novel generative model that combines large language models
(LLMs) and Riemannian flow matching (RFM) to design novel crystalline
materials. FlowLLM first fine-tunes an LLM to learn an effective base
distribution of meta-stable crystals in a text representation. After converting
to a graph representation, the RFM model takes samples from the LLM and
iteratively refines the coordinates and lattice parameters. Our approach
significantly outperforms state-of-the-art methods, increasing the generation
rate of stable materials by over three times and increasing the rate for
stable, unique, and novel crystals by $\sim50\%$ - a huge improvement on a
difficult problem. Additionally, the crystals generated by FlowLLM are much
closer to their relaxed state when compared with another leading model,
significantly reducing post-hoc computational cost.",stat.ML
Sequential Order-Robust Mamba for Time Series Forecasting,"Mamba has recently emerged as a promising alternative to Transformers,
offering near-linear complexity in processing sequential data. However, while
channels in time series (TS) data have no specific order in general, recent
studies have adopted Mamba to capture channel dependencies (CD) in TS,
introducing a sequential order bias. To address this issue, we propose
SOR-Mamba, a TS forecasting method that 1) incorporates a regularization
strategy to minimize the discrepancy between two embedding vectors generated
from data with reversed channel orders, thereby enhancing robustness to channel
order, and 2) eliminates the 1D-convolution originally designed to capture
local information in sequential data. Furthermore, we introduce channel
correlation modeling (CCM), a pretraining task aimed at preserving correlations
between channels from the data space to the latent space in order to enhance
the ability to capture CD. Extensive experiments demonstrate the efficacy of
the proposed method across standard and transfer learning scenarios. Code is
available at https://github.com/seunghan96/SOR-Mamba.",stat.ML
Provable Acceleration for Diffusion Models under Minimal Assumptions,"While score-based diffusion models have achieved exceptional sampling
quality, their sampling speeds are often limited by the high computational
burden of score function evaluations. Despite the recent remarkable empirical
advances in speeding up the score-based samplers, theoretical understanding of
acceleration techniques remains largely limited. To bridge this gap, we propose
a novel training-free acceleration scheme for stochastic samplers. Under
minimal assumptions -- namely, $L^2$-accurate score estimates and a finite
second-moment condition on the target distribution -- our accelerated sampler
provably achieves $\varepsilon$-accuracy in total variation within
$\widetilde{O}(d^{5/4}/\sqrt{\varepsilon})$ iterations, thereby significantly
improving upon the $\widetilde{O}(d/\varepsilon)$ iteration complexity of
standard score-based samplers. Notably, our convergence theory does not rely on
restrictive assumptions on the target distribution or higher-order score
estimation guarantees.",stat.ML
Conditional Forecasting of Margin Calls using Dynamic Graph Neural Networks,"We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for
solving conditional $m$-steps ahead forecasting problems in temporal financial
networks. The proposed DGNN is validated on simulated data from a temporal
financial network model capturing stylized features of Interest Rate Swaps
(IRSs) transaction networks, where financial entities trade swap contracts
dynamically and the network topology evolves conditionally on a reference rate.
The proposed model is able to produce accurate conditional forecasts of net
variation margins up to a $21$-day horizon by leveraging conditional
information under pre-determined stress test scenarios. Our work shows that the
network dynamics can be successfully incorporated into stress-testing
practices, thus providing regulators and policymakers with a crucial tool for
systemic risk monitoring.",stat.ML
Progression: an extrapolation principle for regression,"The problem of regression extrapolation, or out-of-distribution
generalization, arises when predictions are required at test points outside the
range of the training data. In such cases, the non-parametric guarantees for
regression methods from both statistics and machine learning typically fail.
Based on the theory of tail dependence, we propose a novel statistical
extrapolation principle. After a suitable, data-adaptive marginal
transformation, it assumes a simple relationship between predictors and the
response at the boundary of the training predictor samples. This assumption
holds for a wide range of models, including non-parametric regression functions
with additive noise. Our semi-parametric method, progression, leverages this
extrapolation principle and offers guarantees on the approximation error beyond
the training data range. We demonstrate how this principle can be effectively
integrated with existing approaches, such as random forests and additive
models, to improve extrapolation performance on out-of-distribution samples.",stat.ML
Very fast Bayesian Additive Regression Trees on GPU,"Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian
regression technique based on an ensemble of decision trees. It is part of the
toolbox of many statisticians. The overall statistical quality of the
regression is typically higher than other generic alternatives, and it requires
less manual tuning, making it a good default choice. However, it is a niche
method compared to its natural competitor XGBoost, due to the longer running
time, making sample sizes above 10,000-100,000 a nuisance. I present a
GPU-enabled implementation of BART, faster by up to 200x relative to a single
CPU core, making BART competitive in running time with XGBoost. This
implementation is available in the Python package bartz.",stat.ML
Partial Channel Dependence with Channel Masks for Time Series Foundation Models,"Recent advancements in foundation models have been successfully extended to
the time series (TS) domain, facilitated by the emergence of large-scale TS
datasets. However, previous efforts have primarily focused on designing model
architectures to address explicit heterogeneity among datasets such as various
numbers of channels, while often overlooking implicit heterogeneity such as
varying dependencies between channels. In this work, we introduce the concept
of partial channel dependence (PCD), which enables a more sophisticated
adjustment of channel dependencies based on dataset-specific information. To
achieve PCD, we propose a channel mask that captures the relationships between
channels within a dataset using two key components: 1) a correlation matrix
that encodes relative dependencies between channels, and 2) domain parameters
that learn the absolute dependencies specific to each dataset, refining the
correlation matrix. We validate the effectiveness of PCD across four tasks in
TS including forecasting, classification, imputation, and anomaly detection,
under diverse settings, including few-shot and zero-shot scenarios with both TS
foundation models and single-task models. Code is available at
https://github.com/seunghan96/CM.",stat.ML
Improved convergence rate of kNN graph Laplacians,"In graph-based data analysis, $k$-nearest neighbor ($k$NN) graphs are widely
used due to their adaptivity to local data densities. Allowing weighted edges
in the graph, the kernelized graph affinity provides a more general type of
$k$NN graph where the $k$NN distance is used to set the kernel bandwidth
adaptively. In this work, we consider a general class of $k$NN graph where the
graph affinity is $W_{ij} = \epsilon^{-d/2} \; k_0 ( \| x_i - x_j \|^2 /
\epsilon \phi( \widehat{\rho}(x_i), \widehat{\rho}(x_j) )^2 ) $, with
$\widehat{\rho}(x)$ being the (rescaled) $k$NN distance at the point $x$,
$\phi$ a symmetric bi-variate function, and $k_0$ a non-negative function on
$[0,\infty)$. Under the manifold data setting, where $N$ i.i.d. samples $x_i$
are drawn from a density $p$ on a $d$-dimensional unknown manifold embedded in
a high dimensional Euclidean space, we prove the point-wise convergence of the
$k$NN graph Laplacian to the limiting manifold operator (depending on $p$) at
the rate of $O(N^{-2/(d+6)}\,)$, up to a log factor, when $k_0$ and $\phi$ have
$C^3$ regularity and satisfy other technical conditions. This fast rate is
obtained when $\epsilon \sim N^{-2/(d+6)}\,$ and $k \sim N^{6/(d+6)}\,$, both
at the optimal order to balance the theoretical bias and variance errors. When
$k_0$ and $\phi$ have lower regularities, including when $k_0$ is a compactly
supported function as in the standard $k$NN graph, the convergence rate
degenerates to $O(N^{-1/(d+4)}\,)$. Our improved convergence rate is based on a
refined analysis of the $k$NN estimator, which can be of independent interest.
We validate our theory by numerical experiments on simulated data.",stat.ML
Functional Gradient Flows for Constrained Sampling,"Recently, through a unified gradient flow perspective of Markov chain Monte
Carlo (MCMC) and variational inference (VI), particle-based variational
inference methods (ParVIs) have been proposed that tend to combine the best of
both worlds. While typical ParVIs such as Stein Variational Gradient Descent
(SVGD) approximate the gradient flow within a reproducing kernel Hilbert space
(RKHS), many attempts have been made recently to replace RKHS with more
expressive function spaces, such as neural networks. While successful, these
methods are mainly designed for sampling from unconstrained domains. In this
paper, we offer a general solution to constrained sampling by introducing a
boundary condition for the gradient flow which would confine the particles
within the specific domain. This allows us to propose a new functional gradient
ParVI method for constrained sampling, called constrained functional gradient
flow (CFG), with provable continuous-time convergence in total variation (TV).
We also present novel numerical strategies to handle the boundary integral term
arising from the domain constraints. Our theory and experiments demonstrate the
effectiveness of the proposed framework.",stat.ML
QWO: Speeding Up Permutation-Based Causal Discovery in LiGAMs,"Causal discovery is essential for understanding relationships among variables
of interest in many scientific domains. In this paper, we focus on
permutation-based methods for learning causal graphs in Linear Gaussian Acyclic
Models (LiGAMs), where the permutation encodes a causal ordering of the
variables. Existing methods in this setting are not scalable due to their high
computational complexity. These methods are comprised of two main components:
(i) constructing a specific DAG, $\mathcal{G}^\pi$, for a given permutation
$\pi$, which represents the best structure that can be learned from the
available data while adhering to $\pi$, and (ii) searching over the space of
permutations (i.e., causal orders) to minimize the number of edges in
$\mathcal{G}^\pi$. We introduce QWO, a novel approach that significantly
enhances the efficiency of computing $\mathcal{G}^\pi$ for a given permutation
$\pi$. QWO has a speed-up of $O(n^2)$ ($n$ is the number of variables) compared
to the state-of-the-art BIC-based method, making it highly scalable. We show
that our method is theoretically sound and can be integrated into existing
search strategies such as GRASP and hill-climbing-based methods to improve
their performance.",stat.ML
FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique Splits and Feature Selection,"Traditional decision trees are limited by axis-orthogonal splits, which can
perform poorly when true decision boundaries are oblique. While oblique
decision tree methods address this limitation, they often face high
computational costs, difficulties with multi-class classification, and a lack
of effective feature selection. In this paper, we introduce LDATree and
FoLDTree, two novel frameworks that integrate Uncorrelated Linear Discriminant
Analysis (ULDA) and Forward ULDA into a decision tree structure. These methods
enable efficient oblique splits, handle missing values, support feature
selection, and provide both class labels and probabilities as model outputs.
Through evaluations on simulated and real-world datasets, LDATree and FoLDTree
consistently outperform axis-orthogonal and other oblique decision tree
methods, achieving accuracy levels comparable to the random forest. The results
highlight the potential of these frameworks as robust alternatives to
traditional single-tree methods.",stat.ML
Why Fine-grained Labels in Pretraining Benefit Generalization?,"Recent studies show that pretraining a deep neural network with fine-grained
labeled data, followed by fine-tuning on coarse-labeled data for downstream
tasks, often yields better generalization than pretraining with coarse-labeled
data. While there is ample empirical evidence supporting this, the theoretical
justification remains an open problem. This paper addresses this gap by
introducing a ""hierarchical multi-view"" structure to confine the input data
distribution. Under this framework, we prove that: 1) coarse-grained
pretraining only allows a neural network to learn the common features well,
while 2) fine-grained pretraining helps the network learn the rare features in
addition to the common ones, leading to improved accuracy on hard downstream
test samples.",stat.ML
Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes,"We study the optimal memorization capacity of modern Hopfield models and
Kernelized Hopfield Models (KHMs), a transformer-compatible class of Dense
Associative Memories. We present a tight analysis by establishing a connection
between the memory configuration of KHMs and spherical codes from information
theory. Specifically, we treat the stored memory set as a specialized spherical
code. This enables us to cast the memorization problem in KHMs into a point
arrangement problem on a hypersphere. We show that the optimal capacity of KHMs
occurs when the feature space allows memories to form an optimal spherical
code. This unique perspective leads to: (i) An analysis of how KHMs achieve
optimal memory capacity, and identify corresponding necessary conditions.
Importantly, we establish an upper capacity bound that matches the well-known
exponential lower bound in the literature. This provides the first tight and
optimal asymptotic memory capacity for modern Hopfield models. (ii) A
sub-linear time algorithm $\mathtt{U}\text{-}\mathtt{Hop}$+ to reach KHMs'
optimal capacity. (iii) An analysis of the scaling behavior of the required
feature dimension relative to the number of stored memories. These efforts
improve both the retrieval capability of KHMs and the representation learning
of corresponding transformers. Experimentally, we provide thorough numerical
results to back up theoretical findings.",stat.ML
Statistical-Computational Trade-offs for Density Estimation,"We study the density estimation problem defined as follows: given $k$
distributions $p_1, \ldots, p_k$ over a discrete domain $[n]$, as well as a
collection of samples chosen from a ``query'' distribution $q$ over $[n]$,
output $p_i$ that is ``close'' to $q$. Recently~\cite{aamand2023data} gave the
first and only known result that achieves sublinear bounds in {\em both} the
sampling complexity and the query time while preserving polynomial data
structure space. However, their improvement over linear samples and time is
only by subpolynomial factors.
  Our main result is a lower bound showing that, for a broad class of data
structures, their bounds cannot be significantly improved. In particular, if an
algorithm uses $O(n/\log^c k)$ samples for some constant $c>0$ and polynomial
space, then the query time of the data structure must be at least
$k^{1-O(1)/\log \log k}$, i.e., close to linear in the number of distributions
$k$. This is a novel \emph{statistical-computational} trade-off for density
estimation, demonstrating that any data structure must use close to a linear
number of samples or take close to linear query time. The lower bound holds
even in the realizable case where $q=p_i$ for some $i$, and when the
distributions are flat (specifically, all distributions are uniform over half
of the domain $[n]$). We also give a simple data structure for our lower bound
instance with asymptotically matching upper bounds. Experiments show that the
data structure is quite efficient in practice.",stat.ML
Graph Integration for Diffusion-Based Manifold Alignment,"Data from individual observations can originate from various sources or
modalities but are often intrinsically linked. Multimodal data integration can
enrich information content compared to single-source data. Manifold alignment
is a form of data integration that seeks a shared, underlying low-dimensional
representation of multiple data sources that emphasizes similarities between
alternative representations of the same entities. Semi-supervised manifold
alignment relies on partially known correspondences between domains, either
through shared features or through other known associations. In this paper, we
introduce two semi-supervised manifold alignment methods. The first method,
Shortest Paths on the Union of Domains (SPUD), forms a unified graph structure
using known correspondences to establish graph edges. By learning inter-domain
geodesic distances, SPUD creates a global, multi-domain structure. The second
method, MASH (Manifold Alignment via Stochastic Hopping), learns local geometry
within each domain and forms a joint diffusion operator using known
correspondences to iteratively learn new inter-domain correspondences through a
random-walk approach. Through the diffusion process, MASH forms a coupling
matrix that links heterogeneous domains into a unified structure. We compare
SPUD and MASH with existing semi-supervised manifold alignment methods and show
that they outperform competing methods in aligning true correspondences and
cross-domain classification. In addition, we show how these methods can be
applied to transfer label information between domains.",stat.ML
ELBOing Stein: Variational Bayes with Stein Mixture Inference,"Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs
approximate Bayesian inference by representing the posterior with a set of
particles. However, SVGD suffers from variance collapse, i.e. poor predictions
due to underestimating uncertainty [Ba et al., 2021], even for
moderately-dimensional models such as small Bayesian neural networks (BNNs). To
address this issue, we generalize SVGD by letting each particle parameterize a
component distribution in a mixture model. Our method, Stein Mixture Inference
(SMI), optimizes a lower bound to the evidence (ELBO) and introduces
user-specified guides parameterized by particles. SMI extends the Nonlinear
SVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI
effectively avoids variance collapse, judging by a previously described test
developed for this purpose, and performs well on standard data sets. In
addition, SMI requires considerably fewer particles than SVGD to accurately
estimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO
optimization and user-specified guides establishes a promising approach towards
variational Bayesian inference in the case of tall and wide data.",stat.ML
Automatic feature selection and weighting using Differentiable Information Imbalance,"Feature selection is a common process in many applications, but it is
accompanied by uncertainties such as: What is the optimal dimensionality of an
interpretable, reduced feature space to retain a maximum amount of information?
How to account for different units of measure in features? How to weight
different features according to their importance? To address these challenges,
we introduce the Differentiable Information Imbalance (DII), an automatic data
analysis method to rank information content between sets of features. Based on
the nearest neighbors according to distances in the ground truth feature space,
the method finds a low-dimensional subset of the input features, within which
the pairwise distance relations are most similar to the ground truth. By
employing the Differentiable Information Imbalance as a loss function, the
relative feature weights of the inputs are optimized, simultaneously performing
unit alignment and relative importance scaling, while preserving
interpretability. Furthermore, this method can generate sparse solutions and
determine the optimal size of the reduced feature space. We illustrate the
usefulness of this approach on two prototypical benchmark problems: (1)
Identifying a small set of collective variables capable of describing the
conformational space of a biomolecule, and (2) selecting a subset of features
for training a machine-learning force field. The results highlight the
potential of the Differentiable Information Imbalance in addressing feature
selection challenges and optimizing dimensionality in various applications. The
method is implemented in the Python library DADApy.",stat.ML
Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents,"In this paper, we present the Federated Upper Confidence Bound Value
Iteration algorithm ($\texttt{Fed-UCBVI}$), a novel extension of the
$\texttt{UCBVI}$ algorithm (Azar et al., 2017) tailored for the federated
learning framework. We prove that the regret of $\texttt{Fed-UCBVI}$ scales as
$\tilde{\mathcal{O}}(\sqrt{H^3 |\mathcal{S}| |\mathcal{A}| T / M})$, with a
small additional term due to heterogeneity, where $|\mathcal{S}|$ is the number
of states, $|\mathcal{A}|$ is the number of actions, $H$ is the episode length,
$M$ is the number of agents, and $T$ is the number of episodes. Notably, in the
single-agent setting, this upper bound matches the minimax lower bound up to
polylogarithmic factors, while in the multi-agent scenario,
$\texttt{Fed-UCBVI}$ has linear speed-up. To conduct our analysis, we introduce
a new measure of heterogeneity, which may hold independent theoretical
interest. Furthermore, we show that, unlike existing federated reinforcement
learning approaches, $\texttt{Fed-UCBVI}$'s communication complexity only
marginally increases with the number of agents.",stat.ML
Generalization Bounds via Conditional $f$-Information,"In this work, we introduce novel information-theoretic generalization bounds
using the conditional $f$-information framework, an extension of the
traditional conditional mutual information (MI) framework. We provide a generic
approach to derive generalization bounds via $f$-information in the supersample
setting, applicable to both bounded and unbounded loss functions. Unlike
previous MI-based bounds, our proof strategy does not rely on upper bounding
the cumulant-generating function (CGF) in the variational formula of MI.
Instead, we set the CGF or its upper bound to zero by carefully selecting the
measurable function invoked in the variational formula. Although some of our
techniques are partially inspired by recent advances in the coin-betting
framework (e.g., Jang et al. (2023)), our results are independent of any
previous findings from regret guarantees of online gambling algorithms.
Additionally, our newly derived MI-based bound recovers many previous results
and improves our understanding of their potential limitations. Finally, we
empirically compare various $f$-information measures for generalization,
demonstrating the improvement of our new bounds over the previous bounds.",stat.ML
Data subsampling for Poisson regression with pth-root-link,"We develop and analyze data subsampling techniques for Poisson regression,
the standard model for count data $y\in\mathbb{N}$. In particular, we consider
the Poisson generalized linear model with ID- and square root-link functions.
We consider the method of coresets, which are small weighted subsets that
approximate the loss function of Poisson regression up to a factor of
$1\pm\varepsilon$. We show $\Omega(n)$ lower bounds against coresets for
Poisson regression that continue to hold against arbitrary data reduction
techniques up to logarithmic factors. By introducing a novel complexity
parameter and a domain shifting approach, we show that sublinear coresets with
$1\pm\varepsilon$ approximation guarantee exist when the complexity parameter
is small. In particular, the dependence on the number of input points can be
reduced to polylogarithmic. We show that the dependence on other input
parameters can also be bounded sublinearly, though not always logarithmically.
In particular, we show that the square root-link admits an $O(\log(y_{\max}))$
dependence, where $y_{\max}$ denotes the largest count presented in the data,
while the ID-link requires a $\Theta(\sqrt{y_{\max}/\log(y_{\max})})$
dependence. As an auxiliary result for proving the tightness of the bound with
respect to $y_{\max}$ in the case of the ID-link, we show an improved bound on
the principal branch of the Lambert $W_0$ function, which may be of independent
interest. We further show the limitations of our analysis when $p$th degree
root-link functions for $p\geq 3$ are considered, which indicate that other
analytical or computational methods would be required if such a generalization
is even possible.",stat.ML
Hyperparameter Optimization in Machine Learning,"Hyperparameters are configuration variables controlling the behavior of
machine learning algorithms. They are ubiquitous in machine learning and
artificial intelligence and the choice of their values determine the
effectiveness of systems based on these technologies. Manual hyperparameter
search is often unsatisfactory and becomes unfeasible when the number of
hyperparameters is large. Automating the search is an important step towards
automating machine learning, freeing researchers and practitioners alike from
the burden of finding a good set of hyperparameters by trial and error. In this
survey, we present a unified treatment of hyperparameter optimization,
providing the reader with examples and insights into the state-of-the-art. We
cover the main families of techniques to automate hyperparameter search, often
referred to as hyperparameter optimization or tuning, including random and
quasi-random search, bandit-, model- and gradient- based approaches. We further
discuss extensions, including online, constrained, and multi-objective
formulations, touch upon connections with other fields such as meta-learning
and neural architecture search, and conclude with open questions and future
research directions.",stat.ML
Universality of the $^2/6$ Pathway in Avoiding Model Collapse,"Researchers in empirical machine learning recently spotlighted their fears of
so-called Model Collapse. They imagined a discard workflow, where an initial
generative model is trained with real data, after which the real data are
discarded, and subsequently, the model generates synthetic data on which a new
model is trained. They came to the conclusion that models degenerate as
model-fitting generations proceed. However, other researchers considered an
augment workflow, where the original real data continue to be used in each
generation of training, augmented by synthetic data from models fit in all
earlier generations. Empirical results on canonical datasets and learning
procedures confirmed the occurrence of model collapse under the discard
workflow and avoidance of model collapse under the augment workflow. Under the
augment workflow, theoretical evidence also confirmed avoidance in particular
instances; specifically, Gerstgrasser et al. (2024) found that for classical
Linear Regression, test risk at any later generation is bounded by a moderate
multiple, viz. pi-squared-over-6 of the test risk of training with the original
real data alone. Some commentators questioned the generality of theoretical
conclusions based on the generative model assumed in Gerstgrasser et al.
(2024): could similar conclusions be reached for other task/model pairings? In
this work, we demonstrate the universality of the pi-squared-over-6 augment
risk bound across a large family of canonical statistical models, offering key
insights into exactly why collapse happens under the discard workflow and is
avoided under the augment workflow. In the process, we provide a framework that
is able to accommodate a large variety of workflows (beyond discard and
augment), thereby enabling an experimenter to judge the comparative merits of
multiple different workflows by simulating a simple Gaussian process.",stat.ML
An Overview of Causal Inference using Kernel Embeddings,"Kernel embeddings have emerged as a powerful tool for representing
probability measures in a variety of statistical inference problems. By mapping
probability measures into a reproducing kernel Hilbert space (RKHS), kernel
embeddings enable flexible representations of complex relationships between
variables. They serve as a mechanism for efficiently transferring the
representation of a distribution downstream to other tasks, such as hypothesis
testing or causal effect estimation. In the context of causal inference, the
main challenges include identifying causal associations and estimating the
average treatment effect from observational data, where confounding variables
may obscure direct cause-and-effect relationships. Kernel embeddings provide a
robust nonparametric framework for addressing these challenges. They allow for
the representations of distributions of observational data and their seamless
transformation into representations of interventional distributions to estimate
relevant causal quantities. We overview recent research that leverages the
expressiveness of kernel embeddings in tandem with causal inference.",stat.ML
Understanding Aggregations of Proper Learners in Multiclass Classification,"Multiclass learnability is known to exhibit a properness barrier: there are
learnable classes which cannot be learned by any proper learner. Binary
classification faces no such barrier for learnability, but a similar one for
optimal learning, which can in general only be achieved by improper learners.
Fortunately, recent advances in binary classification have demonstrated that
this requirement can be satisfied using aggregations of proper learners, some
of which are strikingly simple. This raises a natural question: to what extent
can simple aggregations of proper learners overcome the properness barrier in
multiclass classification?
  We give a positive answer to this question for classes which have finite
Graph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners
of Hanneke, Larsen, and Aden-Ali et al. (appropriately generalized to the
multiclass setting) achieve sample complexity $O\left(\frac{d_G + \ln(1 /
\delta)}{\epsilon}\right)$. This forms a strict improvement upon the sample
complexity of ERM. We complement this with a lower bound demonstrating that for
certain classes of Graph dimension $d_G$, majorities of ERM learners require
$\Omega \left( \frac{d_G + \ln(1 / \delta)}{\epsilon}\right)$ samples.
Furthermore, we show that a single ERM requires $\Omega \left(\frac{d_G \ln(1 /
\epsilon) + \ln(1 / \delta)}{\epsilon}\right)$ samples on such classes,
exceeding the lower bound of Daniely et al. (2015) by a factor of $\ln(1 /
\epsilon)$. For multiclass learning in full generality -- i.e., for classes of
finite DS dimension but possibly infinite Graph dimension -- we give a strong
refutation to these learning strategies, by exhibiting a learnable class which
cannot be learned to constant error by any aggregation of a finite number of
proper learners.",stat.ML
Explainable Artificial Intelligence for Dependent Features: Additive Effects of Collinearity,"Explainable Artificial Intelligence (XAI) emerged to reveal the internal
mechanism of machine learning models and how the features affect the prediction
outcome. Collinearity is one of the big issues that XAI methods face when
identifying the most informative features in the model. Current XAI approaches
assume the features in the models are independent and calculate the effect of
each feature toward model prediction independently from the rest of the
features. However, such assumption is not realistic in real life applications.
We propose an Additive Effects of Collinearity (AEC) as a novel XAI method that
aim to considers the collinearity issue when it models the effect of each
feature in the model on the outcome. AEC is based on the idea of dividing
multivariate models into several univariate models in order to examine their
impact on each other and consequently on the outcome. The proposed method is
implemented using simulated and real data to validate its efficiency comparing
with the a state of arts XAI method. The results indicate that AEC is more
robust and stable against the impact of collinearity when it explains AI models
compared with the state of arts XAI method.",stat.ML
"Identifying Drift, Diffusion, and Causal Structure from Temporal Snapshots","Stochastic differential equations (SDEs) are a fundamental tool for modelling
dynamic processes, including gene regulatory networks (GRNs), contaminant
transport, financial markets, and image generation. However, learning the
underlying SDE from observational data is a challenging task, especially when
individual trajectories are not observable. Motivated by burgeoning research in
single-cell datasets, we present the first comprehensive approach for jointly
estimating the drift and diffusion of an SDE from its temporal marginals.
Assuming linear drift and additive diffusion, we prove that these parameters
are identifiable from marginals if and only if the initial distribution is not
invariant to a class of generalized rotations, a condition that is satisfied by
most distributions. We further prove that the causal graph of any SDE with
additive diffusion can be recovered from the SDE parameters. To complement this
theory, we adapt entropy-regularized optimal transport to handle anisotropic
diffusion, and introduce APPEX (Alternating Projection Parameter Estimation
from $X_0$), an iterative algorithm designed to estimate the drift, diffusion,
and causal graph of an additive noise SDE, solely from temporal marginals. We
show that each of these steps are asymptotically optimal with respect to the
Kullback-Leibler divergence, and demonstrate APPEX's effectiveness on simulated
data from linear additive noise SDEs.",stat.ML
Video prediction using score-based conditional density estimation,"Temporal prediction is inherently uncertain, but representing the ambiguity
in natural image sequences is a challenging high-dimensional probabilistic
inference problem. For natural scenes, the curse of dimensionality renders
explicit density estimation statistically and computationally intractable.
Here, we describe an implicit regression-based framework for learning and
sampling the conditional density of the next frame in a video given previous
observed frames. We show that sequence-to-image deep networks trained on a
simple resilience-to-noise objective function extract adaptive representations
for temporal prediction. Synthetic experiments demonstrate that this
score-based framework can handle occlusion boundaries: unlike classical methods
that average over bifurcating temporal trajectories, it chooses among likely
trajectories, selecting more probable options with higher frequency.
Furthermore, analysis of networks trained on natural image sequences reveals
that the representation automatically weights predictive evidence by its
reliability, which is a hallmark of statistical inference",stat.ML
A Theoretical Perspective for Speculative Decoding Algorithm,"Transformer-based autoregressive sampling has been the major bottleneck for
slowing down large language model inferences. One effective way to accelerate
inference is \emph{Speculative Decoding}, which employs a small model to sample
a sequence of draft tokens and a large model to validate. Given its empirical
effectiveness, the theoretical understanding of Speculative Decoding is falling
behind. This paper tackles this gap by conceptualizing the decoding problem via
markov chain abstraction and studying the key properties, \emph{output quality
and inference acceleration}, from a theoretical perspective. Our analysis
covers the theoretical limits of speculative decoding, batch algorithms, and
output quality-inference acceleration tradeoffs. Our results reveal the
fundamental connections between different components of LLMs via total
variation distances and show how they jointly affect the efficiency of decoding
algorithms.",stat.ML
Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse,"Machine learning models are often used to automate or support decisions in
applications such as lending and hiring. In such settings, consumer protection
rules mandate that we provide a list of ""principal reasons"" to consumers who
receive adverse decisions. In practice, lenders and employers identify
principal reasons by returning the top-scoring features from a feature
attribution method. In this work, we study how such practices align with one of
the underlying goals of consumer protection - recourse - i.e., educating
individuals on how they can attain a desired outcome. We show that standard
attribution methods can mislead individuals by highlighting reasons without
recourse - i.e., by presenting consumers with features that cannot be changed
to achieve recourse. We propose to address these issues by scoring features on
the basis of responsiveness - i.e., the probability that an individual can
attain a desired outcome by changing a specific feature. We develop efficient
methods to compute responsiveness scores for any model and any dataset under
complex actionability constraints. We present an extensive empirical study on
the responsiveness of explanations in lending and demonstrate how
responsiveness scores can be used to construct feature-highlighting
explanations that lead to recourse and mitigate harm by flagging instances with
fixed predictions.",stat.ML
CausAdv: A Causal-based Framework for Detecting Adversarial Examples,"Deep learning has led to tremendous success in many real-world applications
of computer vision, thanks to sophisticated architectures such as Convolutional
neural networks (CNNs). However, CNNs have been shown to be vulnerable to
crafted adversarial perturbations in inputs. These inputs appear almost
indistinguishable from natural images, yet they are incorrectly classified by
CNN architectures. This vulnerability of adversarial examples has led
researchers to focus on enhancing the robustness of deep learning models in
general, and CNNs in particular, by creating defense and detection methods to
distinguish adversarials inputs from natural ones. In this paper, we address
the adversarial robustness of CNNs through causal reasoning.
  We propose CausAdv: a causal framework for detecting adversarial examples
based on counterfactual reasoning. CausAdv learns causal and non-causal
features of every input, and quantifies the counterfactual information (CI) of
every filter of the last convolutional layer. Then we perform statistical
analysis on the filters CI of every sample, whether clan or adversarials, to
demonstrate how adversarial examples indeed exhibit different CI distributions
compared to clean samples. Our results show that causal reasoning enhances the
process of adversarials detection without the need to train a separate
detector. In addition, we illustrate the efficiency of causal explanations as a
helpful detection technique through visualizing the causal features. The
results can be reproduced using the code available in the repository:
https://github.com/HichemDebbi/CausAdv.",stat.ML
Inference in Partially Linear Models under Dependent Data with Deep Neural Networks,"I consider inference in a partially linear regression model under stationary
$\beta$-mixing data after first stage deep neural network (DNN) estimation.
Using the DNN results of Brown (2024), I show that the estimator for the finite
dimensional parameter, constructed using DNN-estimated nuisance components,
achieves $\sqrt{n}$-consistency and asymptotic normality. By avoiding sample
splitting, I address one of the key challenges in applying machine learning
techniques to econometric models with dependent data. In a future version of
this work, I plan to extend these results to obtain general conditions for
semiparametric inference after DNN estimation of nuisance components, which
will allow for considerations such as more efficient estimation procedures, and
instrumental variable settings.",stat.ML
Flow Matching for Posterior Inference with Simulator Feedback,"Flow-based generative modeling is a powerful tool for solving inverse
problems in physical sciences that can be used for sampling and likelihood
evaluation with much lower inference times than traditional methods. We propose
to refine flows with additional control signals based on a simulator. Control
signals can include gradients and a problem-specific cost function if the
simulator is differentiable, or they can be fully learned from the simulator
output. In our proposed method, we pretrain the flow network and include
feedback from the simulator exclusively for finetuning, therefore requiring
only a small amount of additional parameters and compute. We motivate our
design choices on several benchmark problems for simulation-based inference and
evaluate flow matching with simulator feedback against classical MCMC methods
for modeling strong gravitational lens systems, a challenging inverse problem
in astronomy. We demonstrate that including feedback from the simulator
improves the accuracy by $53\%$, making it competitive with traditional
techniques while being up to $67$x faster for inference.",stat.ML
"Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components","Disentanglement, or identifying salient statistically independent factors of
the data, is of interest in many areas of machine learning and statistics, with
relevance to synthetic data generation with controlled properties, robust
classification of features, parsimonious encoding, and a greater understanding
of the generative process underlying the data. Disentanglement arises in
several generative paradigms, including Variational Autoencoders (VAEs),
Generative Adversarial Networks and diffusion models. Particular progress has
recently been made in understanding disentanglement in VAEs, where the choice
of diagonal posterior covariance matrices is shown to promote mutual
orthogonality between columns of the decoder's Jacobian. We continue this
thread to show how this linear independence translates to statistical
independence, completing the chain in understanding how the VAE's objective
identifies independent components of, or disentangles, the data.",stat.ML
Unlocking Point Processes through Point Set Diffusion,"Point processes model the distribution of random point sets in mathematical
spaces, such as spatial and temporal domains, with applications in fields like
seismology, neuroscience, and economics. Existing statistical and machine
learning models for point processes are predominantly constrained by their
reliance on the characteristic intensity function, introducing an inherent
trade-off between efficiency and flexibility. In this paper, we introduce Point
Set Diffusion, a diffusion-based latent variable model that can represent
arbitrary point processes on general metric spaces without relying on the
intensity function. By directly learning to stochastically interpolate between
noise and data point sets, our approach enables efficient, parallel sampling
and flexible generation for complex conditional tasks defined on the metric
space. Experiments on synthetic and real-world datasets demonstrate that Point
Set Diffusion achieves state-of-the-art performance in unconditional and
conditional generation of spatial and spatiotemporal point processes while
providing up to orders of magnitude faster sampling than autoregressive
baselines.",stat.ML
Privacy-Preserving Dynamic Assortment Selection,"With the growing demand for personalized assortment recommendations, concerns
over data privacy have intensified, highlighting the urgent need for effective
privacy-preserving strategies. This paper presents a novel framework for
privacy-preserving dynamic assortment selection using the multinomial logit
(MNL) bandits model. Our approach employs a perturbed upper confidence bound
method, integrating calibrated noise into user utility estimates to balance
between exploration and exploitation while ensuring robust privacy protection.
We rigorously prove that our policy satisfies Joint Differential Privacy (JDP),
which better suits dynamic environments than traditional differential privacy,
effectively mitigating inference attack risks. This analysis is built upon a
novel objective perturbation technique tailored for MNL bandits, which is also
of independent interest. Theoretically, we derive a near-optimal regret bound
of $\tilde{O}(\sqrt{T})$ for our policy and explicitly quantify how privacy
protection impacts regret. Through extensive simulations and an application to
the Expedia hotel dataset, we demonstrate substantial performance enhancements
over the benchmark method.",stat.ML
Optimizing Posterior Samples for Bayesian Optimization via Rootfinding,"Bayesian optimization devolves the global optimization of a costly objective
function to the global optimization of a sequence of acquisition functions.
This inner-loop optimization can be catastrophically difficult if it involves
posterior samples, especially in higher dimensions. We introduce an efficient
global optimization strategy for posterior samples based on global rootfinding.
It provides gradient-based optimizers with judiciously selected starting
points, designed to combine exploitation and exploration. The algorithm scales
practically linearly to high dimensions. For posterior sample-based acquisition
functions such as Gaussian process Thompson sampling (GP-TS) and variants of
entropy search, we demonstrate remarkable improvement in both inner- and
outer-loop optimization, surprisingly outperforming alternatives like EI and
GP-UCB in most cases. We also propose a sample-average formulation of GP-TS,
which has a parameter to explicitly control exploitation and can be computed at
the cost of one posterior sample. Our implementation is available at
https://github.com/UQUH/TSRoots .",stat.ML
Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization,"This paper aims at developing novel shuffling gradient-based methods for
tackling two classes of minimax problems: nonconvex-linear and
nonconvex-strongly concave settings. The first algorithm addresses the
nonconvex-linear minimax model and achieves the state-of-the-art oracle
complexity typically observed in nonconvex optimization. It also employs a new
shuffling estimator for the ""hyper-gradient"", departing from standard shuffling
techniques in optimization. The second method consists of two variants:
semi-shuffling and full-shuffling schemes. These variants tackle the
nonconvex-strongly concave minimax setting. We establish their oracle
complexity bounds under standard assumptions, which, to our best knowledge, are
the best-known for this specific setting. Numerical examples demonstrate the
performance of our algorithms and compare them with two other methods. Our
results show that the new methods achieve comparable performance with SGD,
supporting the potential of incorporating shuffling strategies into minimax
algorithms.",stat.ML
"Batch, match, and patch: low-rank approximations for score-based variational inference","Black-box variational inference (BBVI) scales poorly to high dimensional
problems when it is used to estimate a multivariate Gaussian approximation with
a full covariance matrix. In this paper, we extend the batch-and-match (BaM)
framework for score-based BBVI to problems where it is prohibitively expensive
to store such covariance matrices, let alone to estimate them. Unlike classical
algorithms for BBVI, which use gradient descent to minimize the reverse
Kullback-Leibler divergence, BaM uses more specialized updates to match the
scores of the target density and its Gaussian approximation. We extend the
updates for BaM by integrating them with a more compact parameterization of
full covariance matrices. In particular, borrowing ideas from factor analysis,
we add an extra step to each iteration of BaM -- a patch -- that projects each
newly updated covariance matrix into a more efficiently parameterized family of
diagonal plus low rank matrices. We evaluate this approach on a variety of
synthetic target distributions and real-world problems in high-dimensional
inference.",stat.ML
Fourier Head: Helping Large Language Models Learn Complex Probability Distributions,"As the quality of large language models has improved, there has been
increased interest in using them to model non-linguistic tokens. For example,
the Decision Transformer recasts agentic decision making as a sequence modeling
problem, using a decoder-only LLM to model the distribution over the discrete
action space for an Atari agent. However, when adapting LLMs to non-linguistic
domains, it remains unclear if softmax over discrete bins captures the
continuous structure of the tokens and the potentially complex distributions
needed for high quality token generation. We introduce a neural network layer,
constructed using Fourier series, which we can easily substitute for any linear
layer if we want the outputs to have a more continuous structure. We perform
extensive analysis on synthetic datasets, as well as on large-scale decision
making and time series forecasting tasks. We also provide theoretical evidence
that this layer can better learn signal from data while ignoring high-frequency
noise. All of our results support the effectiveness of our proposed Fourier
head in scenarios where the underlying data distribution has a natural
continuous structure. For example, the Fourier head improves a Decision
Transformer agent's returns by 46% on the Atari Seaquest game, and increases a
state-of-the-art times series foundation model's forecasting performance by
3.5% across 20 benchmarks unseen during training.",stat.ML
LipKernel: Lipschitz-Bounded Convolutional Neural Networks via Dissipative Layers,"We propose a novel layer-wise parameterization for convolutional neural
networks (CNNs) that includes built-in robustness guarantees by enforcing a
prescribed Lipschitz bound. Each layer in our parameterization is designed to
satisfy a linear matrix inequality (LMI), which in turn implies dissipativity
with respect to a specific supply rate. Collectively, these layer-wise LMIs
ensure Lipschitz boundedness for the input-output mapping of the neural
network, yielding a more expressive parameterization than through spectral
bounds or orthogonal layers. Our new method LipKernel directly parameterizes
dissipative convolution kernels using a 2-D Roesser-type state space model.
This means that the convolutional layers are given in standard form after
training and can be evaluated without computational overhead. In numerical
experiments, we show that the run-time using our method is orders of magnitude
faster than state-of-the-art Lipschitz-bounded networks that parameterize
convolutions in the Fourier domain, making our approach particularly attractive
for improving robustness of learning-based real-time perception or control in
robotics, autonomous vehicles, or automation systems. We focus on CNNs, and in
contrast to previous works, our approach accommodates a wide variety of layers
typically used in CNNs, including 1-D and 2-D convolutional layers, maximum and
average pooling layers, as well as strided and dilated convolutions and zero
padding. However, our approach naturally extends beyond CNNs as we can
incorporate any layer that is incrementally dissipative.",stat.ML
Model-free Estimation of Latent Structure via Multiscale Nonparametric Maximum Likelihood,"Multivariate distributions often carry latent structures that are difficult
to identify and estimate, and which better reflect the data generating
mechanism than extrinsic structures exhibited simply by the raw data. In this
paper, we propose a model-free approach for estimating such latent structures
whenever they are present, without assuming they exist a priori. Given an
arbitrary density $p_0$, we construct a multiscale representation of the
density and propose data-driven methods for selecting representative models
that capture meaningful discrete structure. Our approach uses a nonparametric
maximum likelihood estimator to estimate the latent structure at different
scales and we further characterize their asymptotic limits. By carrying out
such a multiscale analysis, we obtain coarseto-fine structures inherent in the
original distribution, which are integrated via a model selection procedure to
yield an interpretable discrete representation of it. As an application, we
design a clustering algorithm based on the proposed procedure and demonstrate
its effectiveness in capturing a wide range of latent structures.",stat.ML
Abrupt Learning in Transformers: A Case Study on Matrix Completion,"Recent analysis on the training dynamics of Transformers has unveiled an
interesting characteristic: the training loss plateaus for a significant number
of training steps, and then suddenly (and sharply) drops to near--optimal
values. To understand this phenomenon in depth, we formulate the low-rank
matrix completion problem as a masked language modeling (MLM) task, and show
that it is possible to train a BERT model to solve this task to low error.
Furthermore, the loss curve shows a plateau early in training followed by a
sudden drop to near-optimal values, despite no changes in the training
procedure or hyper-parameters. To gain interpretability insights into this
sudden drop, we examine the model's predictions, attention heads, and hidden
states before and after this transition. Concretely, we observe that (a) the
model transitions from simply copying the masked input to accurately predicting
the masked entries; (b) the attention heads transition to interpretable
patterns relevant to the task; and (c) the embeddings and hidden states encode
information relevant to the problem. We also analyze the training dynamics of
individual model components to understand the sudden drop in loss.",stat.ML
$r$Age-$k$: Communication-Efficient Federated Learning Using Age Factor,"Federated learning (FL) is a collaborative approach where multiple clients,
coordinated by a parameter server (PS), train a unified machine-learning model.
The approach, however, suffers from two key challenges: data heterogeneity and
communication overhead. Data heterogeneity refers to inconsistencies in model
training arising from heterogeneous data at different clients. Communication
overhead arises from the large volumes of parameter updates exchanged between
the PS and clients. Existing solutions typically address these challenges
separately. This paper introduces a new communication-efficient algorithm that
uses the age of information metric to simultaneously tackle both limitations of
FL. We introduce age vectors at the PS, which keep track of how often the
different model parameters are updated from the clients. The PS uses this to
selectively request updates for specific gradient indices from each client.
Further, the PS employs age vectors to identify clients with statistically
similar data and group them into clusters. The PS combines the age vectors of
the clustered clients to efficiently coordinate gradient index updates among
clients within a cluster. We evaluate our approach using the MNIST and CIFAR10
datasets in highly non-i.i.d. settings. The experimental results show that our
proposed method can expedite training, surpassing other communication-efficient
strategies in efficiency.",stat.ML
Deep Q-Exponential Processes,"Motivated by deep neural networks, the deep Gaussian process (DGP)
generalizes the standard GP by stacking multiple layers of GPs. Despite the
enhanced expressiveness, GP, as an $L_2$ regularization prior, tends to be
over-smooth and sub-optimal for inhomogeneous subjects, such as images with
edges. Recently, Q-exponential process (Q-EP) has been proposed as an $L_q$
relaxation to GP and demonstrated with more desirable regularization properties
through a parameter $q>0$ with $q=2$ corresponding to GP. Sharing the similar
tractability of posterior and predictive distributions with GP, Q-EP can also
be stacked to improve its modeling flexibility. In this paper, we generalize
Q-EP to deep Q-EP to enjoy both proper regularization and improved
expressiveness. The generalization is realized by introducing shallow Q-EP as a
latent variable model and then building a hierarchy of the shallow Q-EP layers.
Sparse approximation by inducing points and scalable variational strategy are
applied to facilitate the inference. We demonstrate the numerical advantages of
the proposed deep Q-EP model by comparing with multiple state-of-the-art deep
probabilistic models.",stat.ML
Where Do Large Learning Rates Lead Us?,"It is generally accepted that starting neural networks training with large
learning rates (LRs) improves generalization. Following a line of research
devoted to understanding this effect, we conduct an empirical study in a
controlled setting focusing on two questions: 1) how large an initial LR is
required for obtaining optimal quality, and 2) what are the key differences
between models trained with different LRs? We discover that only a narrow range
of initial LRs slightly above the convergence threshold lead to optimal results
after fine-tuning with a small LR or weight averaging. By studying the local
geometry of reached minima, we observe that using LRs from this optimal range
allows for the optimization to locate a basin that only contains high-quality
minima. Additionally, we show that these initial LRs result in a sparse set of
learned features, with a clear focus on those most relevant for the task. In
contrast, starting training with too small LRs leads to unstable minima and
attempts to learn all features simultaneously, resulting in poor
generalization. Conversely, using initial LRs that are too large fails to
detect a basin with good solutions and extract meaningful patterns from the
data.",stat.ML
Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks,"We study the implicit bias of the general family of steepest descent
algorithms, which includes gradient descent, sign descent and coordinate
descent, in deep homogeneous neural networks. We prove that an
algorithm-dependent geometric margin starts increasing once the networks reach
perfect training accuracy and characterize the late-stage bias of the
algorithms. In particular, we define a generalized notion of stationarity for
optimization problems and show that the algorithms progressively reduce a
(generalized) Bregman divergence, which quantifies proximity to such stationary
points of a margin-maximization problem. We then experimentally zoom into the
trajectories of neural networks optimized with various steepest descent
algorithms, highlighting connections to the implicit bias of Adam.",stat.ML
Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient,"We analyze the error rates of the Hamiltonian Monte Carlo algorithm with
leapfrog integrator for Bayesian neural network inference. We show that due to
the non-differentiability of activation functions in the ReLU family, leapfrog
HMC for networks with these activation functions has a large local error rate
of $\Omega(\epsilon)$ rather than the classical error rate of $O(\epsilon^3)$.
This leads to a higher rejection rate of the proposals, making the method
inefficient. We then verify our theoretical findings through empirical
simulations as well as experiments on a real-world dataset that highlight the
inefficiency of HMC inference on ReLU-based neural networks compared to
analytical networks.",stat.ML
Node Regression on Latent Position Random Graphs via Local Averaging,"Node regression consists in predicting the value of a graph label at a node,
given observations at the other nodes. To gain some insight into the
performance of various estimators for this task, we perform a theoretical study
in a context where the graph is random. Specifically, we assume that the graph
is generated by a Latent Position Model, where each node of the graph has a
latent position, and the probability that two nodes are connected depend on the
distance between the latent positions of the two nodes. In this context, we
begin by studying the simplest possible estimator for graph regression, which
consists in averaging the value of the label at all neighboring nodes. We show
that in Latent Position Models this estimator tends to a Nadaraya Watson
estimator in the latent space, and that its rate of convergence is in fact the
same. One issue with this standard estimator is that it averages over a region
consisting of all neighbors of a node, and that depending on the graph model
this may be too much or too little. An alternative consists in first estimating
the true distances between the latent positions, then injecting these estimated
distances into a classical Nadaraya Watson estimator. This enables averaging in
regions either smaller or larger than the typical graph neighborhood. We show
that this method can achieve standard nonparametric rates in certain instances
even when the graph neighborhood is too large or too small.",stat.ML
"Individualised recovery trajectories of patients with impeded mobility, using distance between probability distributions of learnt graphs","Patients who are undergoing physical rehabilitation, benefit from feedback
that follows from reliable assessment of their cumulative performance attained
at a given time. In this paper, we provide a method for the learning of the
recovery trajectory of an individual patient, as they undertake exercises as
part of their physical therapy towards recovery of their loss of movement
ability, following a critical illness. The difference between the Movement
Recovery Scores (MRSs) attained by a patient, when undertaking a given exercise
routine on successive instances, is given by a statistical distance/divergence
between the (posterior) probabilities of random graphs that are Bayesianly
learnt using time series data on locations of 20 of the patient's joints,
recorded on an e-platform as the patient exercises. This allows for the
computation of the MRS on every occasion the patient undertakes this exercise,
using which, the recovery trajectory is drawn. We learn each graph as a Random
Geometric Graph drawn in a probabilistic metric space, and identify the
closed-form marginal posterior of any edge of the graph, given the correlation
structure of the multivariate time series data on joint locations. On the basis
of our recovery learning, we offer recommendations on the optimal exercise
routines for patients with given level of mobility impairment.",stat.ML
Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss,"Traditional implicit generative models are capable of learning highly complex
data distributions. However, their training involves distinguishing real data
from synthetically generated data using adversarial discriminators, which can
lead to unstable training dynamics and mode dropping issues. In this work, we
build on the \textit{invariant statistical loss} (ISL) method introduced in
\cite{de2024training}, and extend it to handle heavy-tailed and multivariate
data distributions.
  The data generated by many real-world phenomena can only be properly
characterised using heavy-tailed probability distributions, and traditional
implicit methods struggle to effectively capture their asymptotic behavior. To
address this problem, we introduce a generator trained with ISL, that uses
input noise from a generalised Pareto distribution (GPD). We refer to this
generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate
that Pareto-ISL accurately models the tails of the distributions while still
effectively capturing their central characteristics.
  The original ISL function was conceived for 1D data sets. When the actual
data is $n$-dimensional, a straightforward extension of the method was obtained
by targeting the $n$ marginal distributions of the data. This approach is
computationally infeasible and ineffective in high-dimensional spaces. To
overcome this, we extend the 1D approach using random projections and define a
new loss function suited for multivariate data, keeping problems tractable by
adjusting the number of projections. We assess its performance in
multidimensional generative modeling and explore its potential as a pretraining
technique for generative adversarial networks (GANs) to prevent mode collapse,
reporting promising results and highlighting its robustness across various
hyperparameter settings.",stat.ML
Identifiability Analysis of Linear ODE Systems with Hidden Confounders,"The identifiability analysis of linear Ordinary Differential Equation (ODE)
systems is a necessary prerequisite for making reliable causal inferences about
these systems. While identifiability has been well studied in scenarios where
the system is fully observable, the conditions for identifiability remain
unexplored when latent variables interact with the system. This paper aims to
address this gap by presenting a systematic analysis of identifiability in
linear ODE systems incorporating hidden confounders. Specifically, we
investigate two cases of such systems. In the first case, latent confounders
exhibit no causal relationships, yet their evolution adheres to specific
functional forms, such as polynomial functions of time $t$. Subsequently, we
extend this analysis to encompass scenarios where hidden confounders exhibit
causal dependencies, with the causal structure of latent variables described by
a Directed Acyclic Graph (DAG). The second case represents a more intricate
variation of the first case, prompting a more comprehensive identifiability
analysis. Accordingly, we conduct detailed identifiability analyses of the
second system under various observation conditions, including both continuous
and discrete observations from single or multiple trajectories. To validate our
theoretical results, we perform a series of simulations, which support and
substantiate our findings.",stat.ML
Cross-Entropy Is All You Need To Invert the Data Generating Process,"Supervised learning has become a cornerstone of modern machine learning, yet
a comprehensive theory explaining its effectiveness remains elusive. Empirical
phenomena, such as neural analogy-making and the linear representation
hypothesis, suggest that supervised models can learn interpretable factors of
variation in a linear fashion. Recent advances in self-supervised learning,
particularly nonlinear Independent Component Analysis, have shown that these
methods can recover latent structures by inverting the data generating process.
We extend these identifiability results to parametric instance discrimination,
then show how insights transfer to the ubiquitous setting of supervised
learning with cross-entropy minimization. We prove that even in standard
classification tasks, models learn representations of ground-truth factors of
variation up to a linear transformation. We corroborate our theoretical
contribution with a series of empirical studies. First, using simulated data
matching our theoretical assumptions, we demonstrate successful disentanglement
of latent factors. Second, we show that on DisLib, a widely-used
disentanglement benchmark, simple classification tasks recover latent
structures up to linear transformations. Finally, we reveal that models trained
on ImageNet encode representations that permit linear decoding of proxy factors
of variation. Together, our theoretical findings and experiments offer a
compelling explanation for recent observations of linear representations, such
as superposition in neural networks. This work takes a significant step toward
a cohesive theory that accounts for the unreasonable effectiveness of
supervised deep learning.",stat.ML
Hierarchical mixtures of Unigram models for short text clustering: the role of Beta-Liouville priors,"This paper presents a variant of the Multinomial mixture model tailored for
the unsupervised classification of short text data. Traditionally, the
Multinomial probability vector in this hierarchical model is assigned a
Dirichlet prior distribution. Here, however, we explore an alternative prior -
the Beta-Liouville distribution - which offers a more flexible correlation
structure than the Dirichlet. We examine the theoretical properties of the
Beta-Liouville distribution, focusing on its conjugacy with the Multinomial
likelihood. This property enables the derivation of update equations for a CAVI
(Coordinate Ascent Variational Inference) variational algorithm, facilitating
the approximate posterior estimation of model parameters. Additionally, we
propose a stochastic variant of the CAVI algorithm that enhances scalability.
The paper concludes with data examples that demonstrate effective strategies
for setting the Beta-Liouville hyperparameters.",stat.ML
Joint Estimation of Conditional Mean and Covariance for Unbalanced Panels,"We propose a novel nonparametric kernel-based estimator of cross-sectional
conditional mean and covariance matrices for large unbalanced panels. We show
its consistency and provide finite-sample guarantees. In an empirical
application, we estimate conditional mean and covariance matrices for a large
unbalanced panel of monthly stock excess returns given macroeconomic and
firm-specific covariates from 1962 to 2021.The estimator performs well with
respect to statistical measures. It is informative for empirical asset pricing,
generating conditional mean-variance efficient portfolios with substantial
out-of-sample Sharpe ratios far beyond equal-weighted benchmarks.",stat.ML
Exponentially Consistent Statistical Classification of Continuous Sequences with Distribution Uncertainty,"In multiple classification, one aims to determine whether a testing sequence
is generated from the same distribution as one of the M training sequences or
not. Unlike most of existing studies that focus on discrete-valued sequences
with perfect distribution match, we study multiple classification for
continuous sequences with distribution uncertainty, where the generating
distributions of the testing and training sequences deviate even under the true
hypothesis. In particular, we propose distribution free tests and prove that
the error probabilities of our tests decay exponentially fast for three
different test designs: fixed-length, sequential, and two-phase tests. We first
consider the simple case without the null hypothesis, where the testing
sequence is known to be generated from a distribution close to the generating
distribution of one of the training sequences. Subsequently, we generalize our
results to a more general case with the null hypothesis by allowing the testing
sequence to be generated from a distribution that is vastly different from the
generating distributions of all training sequences.",stat.ML
On the Statistical Complexity of Estimating VENDI Scores from Empirical Data,"Reference-free evaluation metrics for generative models have recently been
studied in the machine learning community. As a reference-free metric, the
VENDI score quantifies the diversity of generative models using matrix-based
entropy from information theory. The VENDI score is usually computed through
the eigendecomposition of an $n \times n$ kernel matrix for $n$ generated
samples. However, due to the high computational cost of eigendecomposition for
large $n$, the score is often computed on sample sizes limited to a few tens of
thousands. In this paper, we explore the statistical convergence of the VENDI
score and demonstrate that for kernel functions with an infinite feature map
dimension, the evaluated score for a limited sample size may not converge to
the matrix-based entropy statistic. We introduce an alternative statistic
called the $t$-truncated VENDI statistic. We show that the existing Nystr\""om
method and the FKEA approximation method for the VENDI score will both converge
to the defined truncated VENDI statistic given a moderate sample size. We
perform several numerical experiments to illustrate the concentration of the
empirical VENDI score around the truncated VENDI statistic and discuss how this
statistic correlates with the visual diversity of image data.",stat.ML
Minimax optimality of deep neural networks on dependent data via PAC-Bayes bounds,"In a groundbreaking work, Schmidt-Hieber (2020) proved the minimax optimality
of deep neural networks with ReLu activation for least-square regression
estimation over a large class of functions defined by composition. In this
paper, we extend these results in many directions. First, we remove the i.i.d.
assumption on the observations, to allow some time dependence. The observations
are assumed to be a Markov chain with a non-null pseudo-spectral gap. Then, we
study a more general class of machine learning problems, which includes
least-square and logistic regression as special cases. Leveraging on PAC-Bayes
oracle inequalities and a version of Bernstein inequality due to Paulin (2015),
we derive upper bounds on the estimation risk for a generalized Bayesian
estimator. In the case of least-square regression, this bound matches (up to a
logarithmic factor) the lower bound of Schmidt-Hieber (2020). We establish a
similar lower bound for classification with the logistic loss, and prove that
the proposed DNN estimator is optimal in the minimax sense.",stat.ML
On the Role of Depth and Looping for In-Context Learning with Task Diversity,"The intriguing in-context learning (ICL) abilities of deep Transformer models
have lately garnered significant attention. By studying in-context linear
regression on unimodal Gaussian data, recent empirical and theoretical works
have argued that ICL emerges from Transformers' abilities to simulate learning
algorithms like gradient descent. However, these works fail to capture the
remarkable ability of Transformers to learn multiple tasks in context. To this
end, we study in-context learning for linear regression with diverse tasks,
characterized by data covariance matrices with condition numbers ranging from
$[1, \kappa]$, and highlight the importance of depth in this setting. More
specifically, (a) we show theoretical lower bounds of $\log(\kappa)$ (or
$\sqrt{\kappa}$) linear attention layers in the unrestricted (or restricted)
attention setting and, (b) we show that multilayer Transformers can indeed
solve such tasks with a number of layers that matches the lower bounds.
However, we show that this expressivity of multilayer Transformer comes at the
price of robustness. In particular, multilayer Transformers are not robust to
even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance,
where $L$ is the depth of the network. We then demonstrate that Looped
Transformers -- a special class of multilayer Transformers with weight-sharing
-- not only exhibit similar expressive power but are also provably robust under
mild assumptions. Besides out-of-distribution generalization, we also show that
Looped Transformers are the only models that exhibit a monotonic behavior of
loss with respect to depth.",stat.ML
The Effects of Multi-Task Learning on ReLU Neural Network Functions,"This paper studies the properties of solutions to multi-task shallow ReLU
neural network learning problems, wherein the network is trained to fit a
dataset with minimal sum of squared weights. Remarkably, the solutions learned
for each individual task resemble those obtained by solving a kernel method,
revealing a novel connection between neural networks and kernel methods. It is
known that single-task neural network training problems are equivalent to
minimum norm interpolation problem in a non-Hilbertian Banach space, and that
the solutions of such problems are generally non-unique. In contrast, we prove
that the solutions to univariate-input, multi-task neural network interpolation
problems are almost always unique, and coincide with the solution to a
minimum-norm interpolation problem in a Sobolev (Reproducing Kernel) Hilbert
Space. We also demonstrate a similar phenomenon in the multivariate-input case;
specifically, we show that neural network learning problems with large numbers
of diverse tasks are approximately equivalent to an $\ell^2$ (Hilbert space)
minimization problem over a fixed kernel determined by the optimal neurons.",stat.ML
How Does Critical Batch Size Scale in Pre-training?,"Training large-scale models under given resources requires careful design of
parallelism strategies. In particular, the efficiency notion of critical batch
size, concerning the compromise between time and compute, marks the threshold
beyond which greater data parallelism leads to diminishing returns. To
operationalize it, we propose a measure of CBS and pre-train a series of
auto-regressive language models, ranging from 85 million to 1.2 billion
parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and
careful control on factors such as batch size, momentum, and learning rate
along with its scheduling, we systematically investigate the impact of scale on
CBS. Then we fit scaling laws with respect to model and data sizes to decouple
their effects. Overall, our results demonstrate that CBS scales primarily with
data size rather than model size, a finding we justify theoretically through
the analysis of infinite-width limits of neural networks and
infinite-dimensional least squares regression. Of independent interest, we
highlight the importance of common hyper-parameter choices and strategies for
studying large-scale pre-training beyond fixed training durations.",stat.ML
Sequential choice in ordered bundles,"Experience goods such as sporting and artistic events, songs, videos, news
stories, podcasts, and television series, are often packaged and consumed in
bundles. Many such bundles are ordered in the sense that the individual items
are consumed sequentially, one at a time. We examine if an individual's
decision to consume the next item in an ordered bundle can be predicted based
on his/her consumption pattern for the preceding items. We evaluate several
predictive models, including two custom Transformers using decoder-only and
encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a
reinforcement learning model, two Markov models, and a zero-order model. Using
data from Spotify, we find that the custom Transformer with a decoder-only
architecture provides the most accurate predictions, both for individual
choices and aggregate demand. This model captures a general form of state
dependence. Analysis of Transformer attention weights suggests that the
consumption of the next item in a bundle is based on approximately equal
weighting of all preceding choices. Our results indicate that the Transformer
can assist in queuing the next item that an individual is likely to consume
from an ordered bundle, predicting the demand for individual items, and
personalizing promotions to increase demand.",stat.ML
Refined Risk Bounds for Unbounded Losses via Transductive Priors,"We revisit the sequential variants of linear regression with the squared
loss, classification problems with hinge loss, and logistic regression, all
characterized by unbounded losses in the setup where no assumptions are made on
the magnitude of design vectors and the norm of the optimal vector of
parameters. The key distinction from existing results lies in our assumption
that the set of design vectors is known in advance (though their order is not),
a setup sometimes referred to as transductive online learning. While this
assumption seems similar to fixed design regression or denoising, we
demonstrate that the sequential nature of our algorithms allows us to convert
our bounds into statistical ones with random design without making any
additional assumptions about the distribution of the design vectors--an
impossibility for standard denoising results. Our key tools are based on the
exponential weights algorithm with carefully chosen transductive
(design-dependent) priors, which exploit the full horizon of the design
vectors.
  Our classification regret bounds have a feature that is only attributed to
bounded losses in the literature: they depend solely on the dimension of the
parameter space and on the number of rounds, independent of the design vectors
or the norm of the optimal solution. For linear regression with squared loss,
we further extend our analysis to the sparse case, providing sparsity regret
bounds that additionally depend on the magnitude of the response variables. We
argue that these improved bounds are specific to the transductive setting and
unattainable in the worst-case sequential setup. Our algorithms, in several
cases, have polynomial time approximations and reduce to sampling with respect
to log-concave measures instead of aggregating over hard-to-construct
$\varepsilon$-covers of classes.",stat.ML
ATLAS: Adapting Trajectory Lengths and Step-Size for Hamiltonian Monte Carlo,"Hamiltonian Monte-Carlo (HMC) and its auto-tuned variant, the No U-Turn
Sampler (NUTS) can struggle to accurately sample distributions with complex
geometries, e.g., varying curvature, due to their constant step size for
leapfrog integration and fixed mass matrix. In this work, we develop a strategy
to locally adapt the step size parameter of HMC at every iteration by
evaluating a low-rank approximation of the local Hessian and estimating its
largest eigenvalue. We combine it with a strategy to similarly adapt the
trajectory length by monitoring the no U-turn condition, resulting in an
adaptive sampler, ATLAS: adapting trajectory length and step-size. We further
use a delayed rejection framework for making multiple proposals that improves
the computational efficiency of ATLAS, and develop an approach for
automatically tuning its hyperparameters during warmup. We compare ATLAS with
state-of-the-art samplers like NUTS on a suite of synthetic and real world
examples, and show that i) unlike NUTS, ATLAS is able to accurately sample
difficult distributions with complex geometries, ii) it is computationally
competitive to NUTS for simpler distributions, and iii) it is more robust to
the tuning of hyperparamters.",stat.ML
L3Ms -- Lagrange Large Language Models,"Supervised fine-tuning (SFT) and alignment of large language models (LLMs)
are key steps in providing a good user experience. However, the concept of an
appropriate alignment is inherently application-dependent, and current methods
often rely on heuristic choices to drive the optimization. In this work, we
formulate SFT and alignment as a constrained optimization problem, where the
LLM is trained on a task while being required to meet application-specific
requirements, without resorting to heuristics. To solve this, we propose
Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to
enforce the constraints. This approach allows for the customization of L3Ms
across diverse applications while avoiding heuristic-driven processes. We
demonstrate experimentally the versatility and efficacy of L3Ms in achieving
tailored alignments for various applications.",stat.ML
Deep Learning Methods for the Noniterative Conditional Expectation G-Formula for Causal Inference from Complex Observational Data,"The g-formula can be used to estimate causal effects of sustained treatment
strategies using observational data under the identifying assumptions of
consistency, positivity, and exchangeability. The non-iterative conditional
expectation (NICE) estimator of the g-formula also requires correct estimation
of the conditional distribution of the time-varying treatment, confounders, and
outcome. Parametric models, which have been traditionally used for this
purpose, are subject to model misspecification, which may result in biased
causal estimates. Here, we propose a unified deep learning framework for the
NICE g-formula estimator that uses multitask recurrent neural networks for
estimation of the joint conditional distributions. Using simulated data, we
evaluated our model's bias and compared it with that of the parametric
g-formula estimator. We found lower bias in the estimates of the causal effect
of sustained treatment strategies on a survival outcome when using the deep
learning estimator compared with the parametric NICE estimator in settings with
simple and complex temporal dependencies between covariates. These findings
suggest that our Deep Learning g-formula estimator may be less sensitive to
model misspecification than the classical parametric NICE estimator when
estimating the causal effect of sustained treatment strategies from complex
observational data.",stat.ML
Sum-of-squares lower bounds for Non-Gaussian Component Analysis,"Non-Gaussian Component Analysis (NGCA) is the statistical task of finding a
non-Gaussian direction in a high-dimensional dataset. Specifically, given
i.i.d.\ samples from a distribution $P^A_{v}$ on $\mathbb{R}^n$ that behaves
like a known distribution $A$ in a hidden direction $v$ and like a standard
Gaussian in the orthogonal complement, the goal is to approximate the hidden
direction. The standard formulation posits that the first $k-1$ moments of $A$
match those of the standard Gaussian and the $k$-th moment differs. Under mild
assumptions, this problem has sample complexity $O(n)$. On the other hand, all
known efficient algorithms require $\Omega(n^{k/2})$ samples. Prior work
developed sharp Statistical Query and low-degree testing lower bounds
suggesting an information-computation tradeoff for this problem.
  Here we study the complexity of NGCA in the Sum-of-Squares (SoS) framework.
Our main contribution is the first super-constant degree SoS lower bound for
NGCA. Specifically, we show that if the non-Gaussian distribution $A$ matches
the first $(k-1)$ moments of $\mathcal{N}(0, 1)$ and satisfies other mild
conditions, then with fewer than $n^{(1 - \varepsilon)k/2}$ many samples from
the normal distribution, with high probability, degree $(\log n)^{{1\over
2}-o_n(1)}$ SoS fails to refute the existence of such a direction $v$. Our
result significantly strengthens prior work by establishing a super-polynomial
information-computation tradeoff against a broader family of algorithms. As
corollaries, we obtain SoS lower bounds for several problems in robust
statistics and the learning of mixture models.
  Our SoS lower bound proof introduces a novel technique, that we believe may
be of broader interest, and a number of refinements over existing methods.",stat.ML
High-Dimensional Gaussian Process Regression with Soft Kernel Interpolation,"We introduce Soft Kernel Interpolation (SoftKI) designed for scalable
Gaussian Process (GP) regression on high-dimensional datasets. Inspired by
Structured Kernel Interpolation (SKI), which approximates a GP kernel via
interpolation from a structured lattice, SoftKI approximates a kernel via
softmax interpolation from a smaller number of learned interpolation (i.e,
inducing) points. By abandoning the lattice structure used in SKI-based
methods, SoftKI separates the cost of forming an approximate GP kernel from the
dimensionality of the data, making it well-suited for high-dimensional
datasets. We demonstrate the effectiveness of SoftKI across various examples,
and demonstrate that its accuracy exceeds that of other scalable GP methods
when the data dimensionality is modest (around $10$).",stat.ML
Inferring the Morphology of the Galactic Center Excess with Gaussian Processes,"Descriptions of the Galactic Center using Fermi gamma-ray data have so far
modeled the Galactic Center Excess (GCE) as a template with fixed spatial
morphology or as a linear combination of such templates. Although these
templates are informed by various physical expectations, the morphology of the
excess is a priori unknown. For the first time, we describe the GCE using a
flexible, non-parametric machine learning model -- the Gaussian process (GP).
We assess our model's performance on synthetic data, demonstrating that the
model can recover the templates used to generate the data. We then fit the
\Fermi data with our model in a single energy bin from 2-20 GeV (leaving a
spectral GP analysis of the GCE for future work) using a variety of template
models of diffuse gamma-ray emission to quantify our fits' systematic
uncertainties associated with diffuse emission modeling. We interpret our
best-fit GP in terms of GCE templates consisting of an NFW squared template and
a bulge component to determine which bulge models can best describe the fitted
GP and to what extent the best-fit GP is described better by an NFW squared
template versus a bulge template. The best-fit GP contains morphological
features that are typically not associated with traditional GCE studies. These
include a localized bright source at around $(\ell,b) = (20^{\circ},
0^{\circ})$ and a diagonal arm extending Northwest from the Galactic Center. In
spite of these novel features, the fitted GP is explained best by a
template-based model consisting of the bulge presented in Coleman et al. (2020)
and a squared NFW component. Our results suggest that the physical
interpretation of the GCE in terms of stellar bulge and NFW-like components is
highly sensitive to the assumed morphologies, background models, and the region
of the sky used for inference.",stat.ML
Modular Duality in Deep Learning,"An old idea in optimization theory says that since the gradient is a dual
vector it may not be subtracted from the weights without first being mapped to
the primal space where the weights reside. We take this idea seriously in this
paper and construct such a duality map for general neural networks. Our map,
which we call modular dualization, forms a unifying theoretical basis for
training algorithms that are a) fast and b) scalable. Modular dualization
involves first assigning operator norms to layers based on the semantics of
each layer, and then using these layerwise norms to recursively induce a
duality map on the weight space of the full neural architecture. We conclude by
deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers
-- the latter two methods are based on a new rectangular Newton-Schulz
iteration that we propose. Our iteration was recently used to set new speed
records for training NanoGPT. Overall, we hope that our theory of modular
duality will yield a next generation of fast and scalable optimizers for
general neural architectures.",stat.ML
Adaptive Transfer Clustering: A Unified Framework,"We propose a general transfer learning framework for clustering given a main
dataset and an auxiliary one about the same subjects. The two datasets may
reflect similar but different latent grouping structures of the subjects. We
propose an adaptive transfer clustering (ATC) algorithm that automatically
leverages the commonality in the presence of unknown discrepancy, by optimizing
an estimated bias-variance decomposition. It applies to a broad class of
statistical models including Gaussian mixture models, stochastic block models,
and latent class models. A theoretical analysis proves the optimality of ATC
under the Gaussian mixture model and explicitly quantifies the benefit of
transfer. Extensive simulations and real data experiments confirm our method's
effectiveness in various scenarios.",stat.ML
BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference,"Large-scale foundation models have demonstrated exceptional performance in
language and vision tasks. However, the numerous dense matrix-vector operations
involved in these large networks pose significant computational challenges
during inference. To address these challenges, we introduce the Block-Level
Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient
structures prevalent in the weight matrices of linear layers within deep
learning models. Compared to existing structured matrices, the BLAST matrix
offers substantial flexibility, as it can represent various types of structures
that are either learned from data or computed from pre-existing weight
matrices. We demonstrate the efficiency of using the BLAST matrix for
compressing both language and vision tasks, showing that (i) for medium-sized
models such as ViT and GPT-2, training with BLAST weights boosts performance
while reducing complexity by 70% and 40%, respectively; and (ii) for large
foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x
compression while exhibiting the lowest performance degradation among all
tested structured matrices. Our code is available at
https://github.com/changwoolee/BLAST.",stat.ML
SoS Certifiability of Subgaussian Distributions and its Algorithmic Applications,"We prove that there is a universal constant $C>0$ so that for every $d \in
\mathbb N$, every centered subgaussian distribution $\mathcal D$ on $\mathbb
R^d$, and every even $p \in \mathbb N$, the $d$-variate polynomial $(Cp)^{p/2}
\cdot \|v\|_{2}^p - \mathbb E_{X \sim \mathcal D} \langle v,X\rangle^p$ is a
sum of square polynomials. This establishes that every subgaussian distribution
is \emph{SoS-certifiably subgaussian} -- a condition that yields efficient
learning algorithms for a wide variety of high-dimensional statistical tasks.
As a direct corollary, we obtain computationally efficient algorithms with
near-optimal guarantees for the following tasks, when given samples from an
arbitrary subgaussian distribution: robust mean estimation, list-decodable mean
estimation, clustering mean-separated mixture models, robust covariance-aware
mean estimation, robust covariance estimation, and robust linear regression.
Our proof makes essential use of Talagrand's generic chaining/majorizing
measures theorem.",stat.ML
Resilience in Knowledge Graph Embeddings,"In recent years, knowledge graphs have gained interest and witnessed
widespread applications in various domains, such as information retrieval,
question-answering, recommendation systems, amongst others. Large-scale
knowledge graphs to this end have demonstrated their utility in effectively
representing structured knowledge. To further facilitate the application of
machine learning techniques, knowledge graph embedding (KGE) models have been
developed. Such models can transform entities and relationships within
knowledge graphs into vectors. However, these embedding models often face
challenges related to noise, missing information, distribution shift,
adversarial attacks, etc. This can lead to sub-optimal embeddings and incorrect
inferences, thereby negatively impacting downstream applications. While the
existing literature has focused so far on adversarial attacks on KGE models,
the challenges related to the other critical aspects remain unexplored. In this
paper, we, first of all, give a unified definition of resilience, encompassing
several factors such as generalisation, performance consistency, distribution
adaption, and robustness. After formalizing these concepts for machine learning
in general, we define them in the context of knowledge graphs. To find the gap
in the existing works on resilience in the context of knowledge graphs, we
perform a systematic survey, taking into account all these aspects mentioned
previously. Our survey results show that most of the existing works focus on a
specific aspect of resilience, namely robustness. After categorizing such works
based on their respective aspects of resilience, we discuss the challenges and
future research directions.",stat.ML
Trajectory Flow Matching with Applications to Clinical Time Series Modeling,"Modeling stochastic and irregularly sampled time series is a challenging
problem found in a wide range of applications, especially in medicine. Neural
stochastic differential equations (Neural SDEs) are an attractive modeling
technique for this problem, which parameterize the drift and diffusion terms of
an SDE with neural networks. However, current algorithms for training Neural
SDEs require backpropagation through the SDE dynamics, greatly limiting their
scalability and stability. To address this, we propose Trajectory Flow Matching
(TFM), which trains a Neural SDE in a simulation-free manner, bypassing
backpropagation through the dynamics. TFM leverages the flow matching technique
from generative modeling to model time series. In this work we first establish
necessary conditions for TFM to learn time series data. Next, we present a
reparameterization trick which improves training stability. Finally, we adapt
TFM to the clinical time series setting, demonstrating improved performance on
three clinical time series datasets both in terms of absolute performance and
uncertainty prediction.",stat.ML
LLM-initialized Differentiable Causal Discovery,"The discovery of causal relationships between random variables is an
important yet challenging problem that has applications across many scientific
domains. Differentiable causal discovery (DCD) methods are effective in
uncovering causal relationships from observational data; however, these
approaches often suffer from limited interpretability and face challenges in
incorporating domain-specific prior knowledge. In contrast, Large Language
Models (LLMs)-based causal discovery approaches have recently been shown
capable of providing useful priors for causal discovery but struggle with
formal causal reasoning. In this paper, we propose LLM-DCD, which uses an LLM
to initialize the optimization of the maximum likelihood objective function of
DCD approaches, thereby incorporating strong priors into the discovery method.
To achieve this initialization, we design our objective function to depend on
an explicitly defined adjacency matrix of the causal graph as its only
variational parameter. Directly optimizing the explicitly defined adjacency
matrix provides a more interpretable approach to causal discovery.
Additionally, we demonstrate higher accuracy on key benchmarking datasets of
our approach compared to state-of-the-art alternatives, and provide empirical
evidence that the quality of the initialization directly impacts the quality of
the final output of our DCD approach. LLM-DCD opens up new opportunities for
traditional causal discovery methods like DCD to benefit from future
improvements in the causal reasoning capabilities of LLMs.",stat.ML
Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy,"Finding meaningful distances between high-dimensional data samples is an
important scientific task. To this end, we propose a new tree-Wasserstein
distance (TWD) for high-dimensional data with two key aspects. First, our TWD
is specifically designed for data with a latent feature hierarchy, i.e., the
features lie in a hierarchical space, in contrast to the usual focus on
embedding samples in hyperbolic space. Second, while the conventional use of
TWD is to speed up the computation of the Wasserstein distance, we use its
inherent tree as a means to learn the latent feature hierarchy. The key idea of
our method is to embed the features into a multi-scale hyperbolic space using
diffusion geometry and then present a new tree decoding method by establishing
analogies between the hyperbolic embedding and trees. We show that our TWD
computed based on data observations provably recovers the TWD defined with the
latent feature hierarchy and that its computation is efficient and scalable. We
showcase the usefulness of the proposed TWD in applications to word-document
and single-cell RNA-sequencing datasets, demonstrating its advantages over
existing TWDs and methods based on pre-trained models.",stat.ML
Difference-in-Differences with Time-varying Continuous Treatments using Double/Debiased Machine Learning,"We propose a difference-in-differences (DiD) method for a time-varying
continuous treatment and multiple time periods. Our framework assesses the
average treatment effect on the treated (ATET) when comparing two non-zero
treatment doses. The identification is based on a conditional parallel trend
assumption imposed on the mean potential outcome under the lower dose, given
observed covariates and past treatment histories. We employ kernel-based ATET
estimators for repeated cross-sections and panel data adopting the
double/debiased machine learning framework to control for covariates and past
treatment histories in a data-adaptive manner. We also demonstrate the
asymptotic normality of our estimation approach under specific regularity
conditions. In a simulation study, we find a compelling finite sample
performance of undersmoothed versions of our estimators in setups with several
thousand observations.",stat.ML
Stronger Regret Bounds for Safe Online Reinforcement Learning in the Linear Quadratic Regulator,"Many practical applications of online reinforcement learning require the
satisfaction of safety constraints while learning about the unknown
environment. In this work, we study Linear Quadratic Regulator (LQR) learning
with unknown dynamics, but with the additional constraint that the position
must stay within a safe region for the entire trajectory with high probability.
Unlike in previous works, we allow for both bounded and unbounded noise
distributions and study stronger baselines of nonlinear controllers that are
better suited for constrained problems than linear controllers. Due to these
complications, we focus on 1-dimensional state- and action- spaces, however we
also discuss how we expect the high-level takeaways can generalize to higher
dimensions. Our primary contribution is the first
$\tilde{O}_T(\sqrt{T})$-regret bound for constrained LQR learning, which we
show relative to a specific baseline of non-linear controllers. We then prove
that, for any non-linear baseline satisfying natural assumptions,
$\tilde{O}_T(\sqrt{T})$-regret is possible when the noise distribution has
sufficiently large support and $\tilde{O}_T(T^{2/3})$-regret is possible for
any subgaussian noise distribution. An overarching theme of our results is that
enforcing safety provides ""free exploration"" that compensates for the added
cost of uncertainty in safety constrained control, resulting in the same regret
rate as in the unconstrained problem.",stat.ML
Computable Lipschitz Bounds for Deep Neural Networks,"Deriving sharp and computable upper bounds of the Lipschitz constant of deep
neural networks is crucial to formally guarantee the robustness of
neural-network based models. We analyse three existing upper bounds written for
the $l^2$ norm. We highlight the importance of working with the $l^1$ and
$l^\infty$ norms and we propose two novel bounds for both feed-forward
fully-connected neural networks and convolutional neural networks. We treat the
technical difficulties related to convolutional neural networks with two
different methods, called explicit and implicit. Several numerical tests
empirically confirm the theoretical results, help to quantify the relationship
between the presented bounds and establish the better accuracy of the new
bounds. Four numerical tests are studied: two where the output is derived from
an analytical closed form are proposed; another one with random matrices; and
the last one for convolutional neural networks trained on the MNIST dataset. We
observe that one of our bound is optimal in the sense that it is exact for the
first test with the simplest analytical form and it is better than other bounds
for the other tests.",stat.ML
Disentangled and Self-Explainable Node Representation Learning,"Node representations, or embeddings, are low-dimensional vectors that capture
node properties, typically learned through unsupervised structural similarity
objectives or supervised tasks. While recent efforts have focused on explaining
graph model decisions, the interpretability of unsupervised node embeddings
remains underexplored. To bridge this gap, we introduce DiSeNE (Disentangled
and Self-Explainable Node Embedding), a framework that generates
self-explainable embeddings in an unsupervised manner. Our method employs
disentangled representation learning to produce dimension-wise interpretable
embeddings, where each dimension is aligned with distinct topological structure
of the graph. We formalize novel desiderata for disentangled and interpretable
embeddings, which drive our new objective functions, optimizing simultaneously
for both interpretability and disentanglement. Additionally, we propose several
new metrics to evaluate representation quality and human interpretability.
Extensive experiments across multiple benchmark datasets demonstrate the
effectiveness of our approach.",stat.ML
BanditCAT and AutoIRT: Machine Learning Approaches to Computerized Adaptive Testing and Item Calibration,"In this paper, we present a complete framework for quickly calibrating and
administering a robust large-scale computerized adaptive test (CAT) with a
small number of responses. Calibration - learning item parameters in a test -
is done using AutoIRT, a new method that uses automated machine learning
(AutoML) in combination with item response theory (IRT), originally proposed in
[Sharpnack et al., 2024]. AutoIRT trains a non-parametric AutoML grading model
using item features, followed by an item-specific parametric model, which
results in an explanatory IRT model. In our work, we use tabular AutoML tools
(AutoGluon.tabular, [Erickson et al., 2020]) along with BERT embeddings and
linguistically motivated NLP features. In this framework, we use Bayesian
updating to obtain test taker ability posterior distributions for
administration and scoring.
  For administration of our adaptive test, we propose the BanditCAT framework,
a methodology motivated by casting the problem in the contextual bandit
framework and utilizing item response theory (IRT). The key insight lies in
defining the bandit reward as the Fisher information for the selected item,
given the latent test taker ability from IRT assumptions. We use Thompson
sampling to balance between exploring items with different psychometric
characteristics and selecting highly discriminative items that give more
precise information about ability. To control item exposure, we inject noise
through an additional randomization step before computing the Fisher
information. This framework was used to initially launch two new item types on
the DET practice test using limited training data. We outline some reliability
and exposure metrics for the 5 practice test experiments that utilized this
framework.",stat.ML
A Stein Gradient Descent Approach for Doubly Intractable Distributions,"Bayesian inference for doubly intractable distributions is challenging
because they include intractable terms, which are functions of parameters of
interest. Although several alternatives have been developed for such models,
they are computationally intensive due to repeated auxiliary variable
simulations. We propose a novel Monte Carlo Stein variational gradient descent
(MC-SVGD) approach for inference for doubly intractable distributions. Through
an efficient gradient approximation, our MC-SVGD approach rapidly transforms an
arbitrary reference distribution to approximate the posterior distribution of
interest, without necessitating any predefined variational distribution class
for the posterior. Such a transport map is obtained by minimizing
Kullback-Leibler divergence between the transformed and posterior distributions
in a reproducing kernel Hilbert space (RKHS). We also investigate the
convergence rate of the proposed method. We illustrate the application of the
method to challenging examples, including a Potts model, an exponential random
graph model, and a Conway--Maxwell--Poisson regression model. The proposed
method achieves substantial computational gains over existing algorithms, while
providing comparable inferential performance for the posterior distributions.",stat.ML
Deep Recurrent Stochastic Configuration Networks for Modelling Nonlinear Dynamic Systems,"Deep learning techniques have shown promise in many domain applications. This
paper proposes a novel deep reservoir computing framework, termed deep
recurrent stochastic configuration network (DeepRSCN) for modelling nonlinear
dynamic systems. DeepRSCNs are incrementally constructed, with all reservoir
nodes directly linked to the final output. The random parameters are assigned
in the light of a supervisory mechanism, ensuring the universal approximation
property of the built model. The output weights are updated online using the
projection algorithm to handle the unknown dynamics. Given a set of training
samples, DeepRSCNs can quickly generate learning representations, which consist
of random basis functions with cascaded input and readout weights. Experimental
results over a time series prediction, a nonlinear system identification
problem, and two industrial data predictive analyses demonstrate that the
proposed DeepRSCN outperforms the single-layer network in terms of modelling
efficiency, learning capability, and generalization performance.",stat.ML
BSD: a Bayesian framework for parametric models of neural spectra,"The analysis of neural power spectra plays a crucial role in understanding
brain function and dysfunction. While recent efforts have led to the
development of methods for decomposing spectral data, challenges remain in
performing statistical analysis and group-level comparisons. Here, we introduce
Bayesian Spectral Decomposition (BSD), a Bayesian framework for analysing
neural spectral power. BSD allows for the specification, inversion, comparison,
and analysis of parametric models of neural spectra, addressing limitations of
existing methods. We first establish the face validity of BSD on simulated data
and show how it outperforms an established method (\fooof{}) for peak detection
on artificial spectral data. We then demonstrate the efficacy of BSD on a
group-level study of EEG spectra in 204 healthy subjects from the LEMON
dataset. Our results not only highlight the effectiveness of BSD in model
selection and parameter estimation, but also illustrate how BSD enables
straightforward group-level regression of the effect of continuous covariates
such as age. By using Bayesian inference techniques, BSD provides a robust
framework for studying neural spectral data and their relationship to brain
function and dysfunction.",stat.ML
Valid Bootstraps for Networks with Applications to Network Visualisation,"Quantifying uncertainty in networks is an important step in modelling
relationships and interactions between entities. We consider the challenge of
bootstrapping an inhomogeneous random graph when only a single observation of
the network is made and the underlying data generating function is unknown. We
utilise an exchangeable network test that can empirically validate bootstrap
samples generated by any method, by testing if the observed and bootstrapped
networks are statistically distinguishable. We find that existing methods fail
this test. To address this, we propose a principled, novel, distribution-free
network bootstrap using k-nearest neighbour smoothing, that can regularly pass
this exchangeable network test in both synthetic and real-data scenarios. We
demonstrate the utility of this work in combination with the popular data
visualisation method t-SNE, where uncertainty estimates from bootstrapping are
used to explain whether visible structures represent real statistically sound
structures.",stat.ML
Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability,"Recently, several methods have leveraged deep generative modeling to produce
example-based explanations of decision algorithms for high-dimensional input
data. Despite promising results, a disconnect exists between these methods and
the classical explainability literature, which focuses on lower-dimensional
data with semantically meaningful features. This conceptual and communication
gap leads to misunderstandings and misalignments in goals and expectations. In
this paper, we bridge this gap by proposing a novel probabilistic framework for
local example-based explanations. Our framework integrates the critical
characteristics of classical local explanation desiderata while being amenable
to high-dimensional data and their modeling through deep generative models. Our
aim is to facilitate communication, foster rigor and transparency, and improve
the quality of peer discussion and research progress.",stat.ML
On Probabilistic Pullback Metrics on Latent Hyperbolic Manifolds,"Gaussian Process Latent Variable Models (GPLVMs) have proven effective in
capturing complex, high-dimensional data through lower-dimensional
representations. Recent advances show that using Riemannian manifolds as latent
spaces provides more flexibility to learn higher quality embeddings. This paper
focuses on the hyperbolic manifold, a particularly suitable choice for modeling
hierarchical relationships. While previous approaches relied on hyperbolic
geodesics for interpolating the latent space, this often results in paths
crossing low-data regions, leading to highly uncertain predictions. Instead, we
propose augmenting the hyperbolic metric with a pullback metric to account for
distortions introduced by the GPLVM's nonlinear mapping. Through various
experiments, we demonstrate that geodesics on the pullback metric not only
respect the geometry of the hyperbolic latent space but also align with the
underlying data distribution, significantly reducing uncertainty in
predictions.",stat.ML
Scaling-based Data Augmentation for Generative Models and its Theoretical Extension,"This paper studies stable learning methods for generative models that enable
high-quality data generation. Noise injection is commonly used to stabilize
learning. However, selecting a suitable noise distribution is challenging.
Diffusion-GAN, a recently developed method, addresses this by using the
diffusion process with a timestep-dependent discriminator. We investigate
Diffusion-GAN and reveal that data scaling is a key component for stable
learning and high-quality data generation. Building on our findings, we propose
a learning algorithm, Scale-GAN, that uses data scaling and variance-based
regularization. Furthermore, we theoretically prove that data scaling controls
the bias-variance trade-off of the estimation error bound. As a theoretical
extension, we consider GAN with invertible data augmentations. Comparative
evaluations on benchmark datasets demonstrate the effectiveness of our method
in improving stability and accuracy.",stat.ML
Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting,"Sequence modeling faces challenges in capturing long-range dependencies
across diverse tasks. Recent linear and transformer-based forecasters have
shown superior performance in time series forecasting. However, they are
constrained by their inherent inability to effectively address long-range
dependencies in time series data, primarily due to using fixed-size inputs for
prediction. Furthermore, they typically sacrifice essential temporal
correlation among consecutive training samples by shuffling them into
mini-batches. To overcome these limitations, we introduce a fast and effective
Spectral Attention mechanism, which preserves temporal correlations among
samples and facilitates the handling of long-range information while
maintaining the base model structure. Spectral Attention preserves long-period
trends through a low-pass filter and facilitates gradient to flow between
samples. Spectral Attention can be seamlessly integrated into most sequence
models, allowing models with fixed-sized look-back windows to capture
long-range dependencies over thousands of steps. Through extensive experiments
on 11 real-world time series datasets using 7 recent forecasting models, we
consistently demonstrate the efficacy of our Spectral Attention mechanism,
achieving state-of-the-art results.",stat.ML
Robust Estimation for Kernel Exponential Families with Smoothed Total Variation Distances,"In statistical inference, we commonly assume that samples are independent and
identically distributed from a probability distribution included in a
pre-specified statistical model. However, such an assumption is often violated
in practice. Even an unexpected extreme sample called an {\it outlier} can
significantly impact classical estimators. Robust statistics studies how to
construct reliable statistical methods that efficiently work even when the
ideal assumption is violated. Recently, some works revealed that robust
estimators such as Tukey's median are well approximated by the generative
adversarial net (GAN), a popular learning method for complex generative models
using neural networks. GAN is regarded as a learning method using integral
probability metrics (IPM), which is a discrepancy measure for probability
distributions. In most theoretical analyses of Tukey's median and its GAN-based
approximation, however, the Gaussian or elliptical distribution is assumed as
the statistical model. In this paper, we explore the application of GAN-like
estimators to a general class of statistical models. As the statistical model,
we consider the kernel exponential family that includes both finite and
infinite-dimensional models. To construct a robust estimator, we propose the
smoothed total variation (STV) distance as a class of IPMs. Then, we
theoretically investigate the robustness properties of the STV-based
estimators. Our analysis reveals that the STV-based estimator is robust against
the distribution contamination for the kernel exponential family. Furthermore,
we analyze the prediction accuracy of a Monte Carlo approximation method, which
circumvents the computational difficulty of the normalization constant.",stat.ML
Likelihood approximations via Gaussian approximate inference,"Non-Gaussian likelihoods are essential for modelling complex real-world
observations but pose significant computational challenges in learning and
inference. Even with Gaussian priors, non-Gaussian likelihoods often lead to
analytically intractable posteriors, necessitating approximation methods. To
this end, we propose efficient schemes to approximate the effects of
non-Gaussian likelihoods by Gaussian densities based on variational inference
and moment matching in transformed bases. These enable efficient inference
strategies originally designed for models with a Gaussian likelihood to be
deployed. Our empirical results demonstrate that the proposed matching
strategies attain good approximation quality for binary and multiclass
classification in large-scale point-estimate and distributional inferential
settings. In challenging streaming problems, the proposed methods outperform
all existing likelihood approximations and approximate inference methods in the
exact models. As a by-product, we show that the proposed approximate
log-likelihoods are a superior alternative to least-squares on raw labels for
neural network classification.",stat.ML
Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM Alignment,"Recent advances in aligning large language models with human preferences have
corroborated the growing importance of best-of-N distillation (BOND). However,
the iterative BOND algorithm is prohibitively expensive in practice due to the
sample and computation inefficiency. This paper addresses the problem by
revealing a unified game-theoretic connection between iterative BOND and
self-play alignment, which unifies seemingly disparate algorithmic paradigms.
Based on the connection, we establish a novel framework, WIN rate Dominance
(WIND), with a series of efficient algorithms for regularized win rate
dominance optimization that approximates iterative BOND in the parameter space.
We provides provable sample efficiency guarantee for one of the WIND variant
with the square loss objective. The experimental results confirm that our
algorithm not only accelerates the computation, but also achieves superior
sample efficiency compared to existing methods.",stat.ML
Segmenting Watermarked Texts From Language Models,"Watermarking is a technique that involves embedding nearly unnoticeable
statistical signals within generated content to help trace its source. This
work focuses on a scenario where an untrusted third-party user sends prompts to
a trusted language model (LLM) provider, who then generates a text from their
LLM with a watermark. This setup makes it possible for a detector to later
identify the source of the text if the user publishes it. The user can modify
the generated text by substitutions, insertions, or deletions. Our objective is
to develop a statistical method to detect if a published text is LLM-generated
from the perspective of a detector. We further propose a methodology to segment
the published text into watermarked and non-watermarked sub-strings. The
proposed approach is built upon randomization tests and change point detection
techniques. We demonstrate that our method ensures Type I and Type II error
control and can accurately identify watermarked sub-strings by finding the
corresponding change point locations. To validate our technique, we apply it to
texts generated by several language models with prompts extracted from Google's
C4 dataset and obtain encouraging numerical results. We release all code
publicly at https://github.com/doccstat/llm-watermark-cpd.",stat.ML
A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data,"Federated Learning (FL) has emerged as a groundbreaking paradigm in
collaborative machine learning, emphasizing decentralized model training to
address data privacy concerns. While significant progress has been made in
optimizing federated learning, the exploration of generalization error,
particularly in heterogeneous settings, has been limited, focusing mainly on
parametric cases. This paper investigates the generalization properties of deep
federated regression within a two-stage sampling model. Our findings highlight
that the intrinsic dimension, defined by the entropic dimension, is crucial for
determining convergence rates when appropriate network sizes are used.
Specifically, if the true relationship between response and explanatory
variables is charecterized by a $\beta$-H\""older function and there are $n$
independent and identically distributed (i.i.d.) samples from $m$ participating
clients, the error rate for participating clients scales at most as
$\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))}\right)$,
and for non-participating clients, it scales as $\tilde{O}\left(\Delta \cdot
m^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta +
\bar{d}_{2\beta}(\lambda))}\right)$. Here, $\bar{d}_{2\beta}(\lambda)$
represents the $2\beta$-entropic dimension of $\lambda$, the marginal
distribution of the explanatory variables, and $\Delta$ characterizes the
dependence between the sampling stages. Our results explicitly account for the
""closeness"" of clients, demonstrating that the convergence rates of deep
federated learners depend on intrinsic rather than nominal high-dimensionality.",stat.ML
NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks,"The performance of neural networks improves when more parameters are used.
However, the model sizes are constrained by the available on-device memory
during training and inference. Although applying techniques like quantization
can alleviate the constraint, they suffer from performance degradation. In this
work, we introduce NeuZip, a new weight compression scheme based on the entropy
of floating-point numbers in neural networks. With NeuZip, we are able to
achieve memory-efficient training and inference without sacrificing
performance. Notably, we significantly reduce the memory footprint of training
a Llama-3 8B model from 31GB to less than 16GB, while keeping the training
dynamics fully unchanged. In inference, our method can reduce memory usage by
more than half while maintaining near-lossless performance. Our code is
publicly available.",stat.ML
Learning Variational Inequalities from Data: Fast Generalization Rates under Strong Monotonicity,"Variational inequalities (VIs) are a broad class of optimization problems
encompassing machine learning problems ranging from standard convex
minimization to more complex scenarios like min-max optimization and computing
the equilibria of multi-player games. In convex optimization, strong convexity
allows for fast statistical learning rates requiring only $\Theta(1/\epsilon)$
stochastic first-order oracle calls to find an $\epsilon$-optimal solution,
rather than the standard $\Theta(1/\epsilon^2)$ calls. In this paper, we
explain how one can similarly obtain fast $\Theta(1/\epsilon)$ rates for
learning VIs that satisfy strong monotonicity, a generalization of strong
convexity. Specifically, we demonstrate that standard stability-based
generalization arguments for convex minimization extend directly to VIs when
the domain admits a small covering, or when the operator is integrable and
suboptimality is measured by potential functions; such as when finding
equilibria in multi-player games.",stat.ML
General Causal Imputation via Synthetic Interventions,"Given two sets of elements (such as cell types and drug compounds),
researchers typically only have access to a limited subset of their
interactions. The task of causal imputation involves using this subset to
predict unobserved interactions. Squires et al. (2022) have proposed two
estimators for this task based on the synthetic interventions (SI) estimator:
SI-A (for actions) and SI-C (for contexts). We extend their work and introduce
a novel causal imputation estimator, generalized synthetic interventions (GSI).
We prove the identifiability of this estimator for data generated from a more
complex latent factor model. On synthetic and real data we show empirically
that it recovers or outperforms their estimators.",stat.ML
Injectivity capacity of ReLU gates,"We consider the injectivity property of the ReLU networks layers. Determining
the ReLU injectivity capacity (ratio of the number of layer's inputs and
outputs) is established as isomorphic to determining the capacity of the
so-called $\ell_0$ spherical perceptron. Employing \emph{fully lifted random
duality theory} (fl RDT) a powerful program is developed and utilized to handle
the $\ell_0$ spherical perceptron and implicitly the ReLU layers injectivity.
To put the entire fl RDT machinery in practical use, a sizeable set of
numerical evaluations is conducted as well. The lifting mechanism is observed
to converge remarkably fast with relative corrections in the estimated
quantities not exceeding $\sim 0.1\%$ already on the third level of lifting.
Closed form explicit analytical relations among key lifting parameters are
uncovered as well. In addition to being of incredible importance in handling
all the required numerical work, these relations also shed a new light on
beautiful parametric interconnections within the lifting structure. Finally,
the obtained results are also shown to fairly closely match the replica
predictions from [40].",stat.ML
Near Optimal Pure Exploration in Logistic Bandits,"Bandit algorithms have garnered significant attention due to their practical
applications in real-world scenarios. However, beyond simple settings such as
multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no
optimal solution exists for pure exploration problems in the context of
generalized linear model (GLM) bandits. In this paper, we narrow this gap and
develop the first track-and-stop algorithm for general pure exploration
problems under the logistic bandit called logistic track-and-stop (Log-TS).
Log-TS is an efficient algorithm that asymptotically matches an approximation
for the instance-specific lower bound of the expected sample complexity up to a
logarithmic factor.",stat.ML
Kernel Approximation of Fisher-Rao Gradient Flows,"The purpose of this paper is to answer a few open questions in the interface
of kernel methods and PDE gradient flows. Motivated by recent advances in
machine learning, particularly in generative modeling and sampling, we present
a rigorous investigation of Fisher-Rao and Wasserstein type gradient flows
concerning their gradient structures, flow equations, and their kernel
approximations. Specifically, we focus on the Fisher-Rao (also known as
Hellinger) geometry and its various kernel-based approximations, developing a
principled theoretical framework using tools from PDE gradient flows and
optimal transport theory. We also provide a complete characterization of
gradient flows in the maximum-mean discrepancy (MMD) space, with connections to
existing learning and inference algorithms. Our analysis reveals precise
theoretical insights linking Fisher-Rao flows, Stein flows, kernel
discrepancies, and nonparametric regression. We then rigorously prove
evolutionary $\Gamma$-convergence for kernel-approximated Fisher-Rao flows,
providing theoretical guarantees beyond pointwise convergence. Finally, we
analyze energy dissipation using the Helmholtz-Rayleigh principle, establishing
important connections between classical theory in mechanics and modern machine
learning practice. Our results provide a unified theoretical foundation for
understanding and analyzing approximations of gradient flows in machine
learning applications through a rigorous gradient flow and variational method
perspective.",stat.ML
A successive approximation method in functional spaces for hierarchical optimal control problems and its application to learning,"We consider a class of learning problem of point estimation for modeling
high-dimensional nonlinear functions, whose learning dynamics is guided by
model training dataset, while the estimated parameter in due course provides an
acceptable prediction accuracy on a different model validation dataset. Here,
we establish an evidential connection between such a learning problem and a
hierarchical optimal control problem that provides a framework how to account
appropriately for both generalization and regularization at the optimization
stage. In particular, we consider the following two objectives: (i) The first
one is a controllability-type problem, i.e., generalization, which consists of
guaranteeing the estimated parameter to reach a certain target set at some
fixed final time, where such a target set is associated with model validation
dataset. (ii) The second one is a regularization-type problem ensuring the
estimated parameter trajectory to satisfy some regularization property over a
certain finite time interval. First, we partition the control into two control
strategies that are compatible with two abstract agents, namely, a leader,
which is responsible for the controllability-type problem and that of a
follower, which is associated with the regularization-type problem. Using the
notion of Stackelberg's optimization, we provide conditions on the existence of
admissible optimal controls for such a hierarchical optimal control problem
under which the follower is required to respond optimally to the strategy of
the leader, so as to achieve the overall objectives that ultimately leading to
an optimal parameter estimate. Moreover, we provide a nested algorithm,
arranged in a hierarchical structure-based on successive approximation methods,
for solving the corresponding optimal control problem. Finally, we present some
numerical results for a typical nonlinear regression problem.",stat.ML
Practical Bayesian Algorithm Execution via Posterior Sampling,"We consider Bayesian algorithm execution (BAX), a framework for efficiently
selecting evaluation points of an expensive function to infer a property of
interest encoded as the output of a base algorithm. Since the base algorithm
typically requires more evaluations than are feasible, it cannot be directly
applied. Instead, BAX methods sequentially select evaluation points using a
probabilistic numerical approach. Current BAX methods use expected information
gain to guide this selection. However, this approach is computationally
intensive. Observing that, in many tasks, the property of interest corresponds
to a target set of points defined by the function, we introduce PS-BAX, a
simple, effective, and scalable BAX method based on posterior sampling. PS-BAX
is applicable to a wide range of problems, including many optimization variants
and level set estimation. Experiments across diverse tasks demonstrate that
PS-BAX performs competitively with existing baselines while being significantly
faster, simpler to implement, and easily parallelizable, setting a strong
baseline for future research. Additionally, we establish conditions under which
PS-BAX is asymptotically convergent, offering new insights into posterior
sampling as an algorithm design paradigm.",stat.ML
Toward Conditional Distribution Calibration in Survival Prediction,"Survival prediction often involves estimating the time-to-event distribution
from censored datasets. Previous approaches have focused on enhancing
discrimination and marginal calibration. In this paper, we highlight the
significance of conditional calibration for real-world applications --
especially its role in individual decision-making. We propose a method based on
conformal prediction that uses the model's predicted individual survival
probability at that instance's observed time. This method effectively improves
the model's marginal and conditional calibration, without compromising
discrimination. We provide asymptotic theoretical guarantees for both marginal
and conditional calibration and test it extensively across 15 diverse
real-world datasets, demonstrating the method's practical effectiveness and
versatility in various settings.",stat.ML
Info-CELS: Informative Saliency Map Guided Counterfactual Explanation,"As the demand for interpretable machine learning approaches continues to
grow, there is an increasing necessity for human involvement in providing
informative explanations for model decisions. This is necessary for building
trust and transparency in AI-based systems, leading to the emergence of the
Explainable Artificial Intelligence (XAI) field. Recently, a novel
counterfactual explanation model, CELS, has been introduced. CELS learns a
saliency map for the interest of an instance and generates a counterfactual
explanation guided by the learned saliency map. While CELS represents the first
attempt to exploit learned saliency maps not only to provide intuitive
explanations for the reason behind the decision made by the time series
classifier but also to explore post hoc counterfactual explanations, it
exhibits limitations in terms of high validity for the sake of ensuring high
proximity and sparsity. In this paper, we present an enhanced approach that
builds upon CELS. While the original model achieved promising results in terms
of sparsity and proximity, it faced limitations in validity. Our proposed
method addresses this limitation by removing mask normalization to provide more
informative and valid counterfactual explanations. Through extensive
experimentation on datasets from various domains, we demonstrate that our
approach outperforms the CELS model, achieving higher validity and producing
more informative explanations.",stat.ML
Hamiltonian Score Matching and Generative Flows,"Classical Hamiltonian mechanics has been widely used in machine learning in
the form of Hamiltonian Monte Carlo for applications with predetermined force
fields. In this work, we explore the potential of deliberately designing force
fields for Hamiltonian ODEs, introducing Hamiltonian velocity predictors (HVPs)
as a tool for score matching and generative models. We present two innovations
constructed with HVPs: Hamiltonian Score Matching (HSM), which estimates score
functions by augmenting data via Hamiltonian trajectories, and Hamiltonian
Generative Flows (HGFs), a novel generative model that encompasses diffusion
models and flow matching as HGFs with zero force fields. We showcase the
extended design space of force fields by introducing Oscillation HGFs, a
generative model inspired by harmonic oscillators. Our experiments validate our
theoretical insights about HSM as a novel score matching metric and demonstrate
that HGFs rival leading generative modeling techniques.",stat.ML
TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time Series Forecasting,"Multi-dimensional time series data, such as matrix and tensor-variate time
series, are increasingly prevalent in fields such as economics, finance, and
climate science. Traditional Transformer models, though adept with sequential
data, do not effectively preserve these multi-dimensional structures, as their
internal operations in effect flatten multi-dimensional observations into
vectors, thereby losing critical multi-dimensional relationships and patterns.
To address this, we introduce the Tensor-Augmented Transformer (TEAFormer), a
novel method that incorporates tensor expansion and compression within the
Transformer framework to maintain and leverage the inherent multi-dimensional
structures, thus reducing computational costs and improving prediction
accuracy. The core feature of the TEAFormer, the Tensor-Augmentation (TEA)
module, utilizes tensor expansion to enhance multi-view feature learning and
tensor compression for efficient information aggregation and reduced
computational load. The TEA module is not just a specific model architecture
but a versatile component that is highly compatible with the attention
mechanism and the encoder-decoder structure of Transformers, making it
adaptable to existing Transformer architectures. Our comprehensive experiments,
which integrate the TEA module into three popular time series Transformer
models across three real-world benchmarks, show significant performance
enhancements, highlighting the potential of TEAFormers for cutting-edge time
series forecasting.",stat.ML
Integrating uncertainty quantification into randomized smoothing based robustness guarantees,"Deep neural networks have proven to be extremely powerful, however, they are
also vulnerable to adversarial attacks which can cause hazardous incorrect
predictions in safety-critical applications. Certified robustness via
randomized smoothing gives a probabilistic guarantee that the smoothed
classifier's predictions will not change within an $\ell_2$-ball around a given
input. On the other hand (uncertainty) score-based rejection is a technique
often applied in practice to defend models against adversarial attacks. In this
work, we fuse these two approaches by integrating a classifier that abstains
from predicting when uncertainty is high into the certified robustness
framework. This allows us to derive two novel robustness guarantees for
uncertainty aware classifiers, namely (i) the radius of an $\ell_2$-ball around
the input in which the same label is predicted and uncertainty remains low and
(ii) the $\ell_2$-radius of a ball in which the predictions will either not
change or be uncertain. While the former provides robustness guarantees with
respect to attacks aiming at increased uncertainty, the latter informs about
the amount of input perturbation necessary to lead the uncertainty aware model
into a wrong prediction. Notably, this is on CIFAR10 up to 20.93% larger than
for models not allowing for uncertainty based rejection. We demonstrate, that
the novel framework allows for a systematic robustness evaluation of different
network architectures and uncertainty measures and to identify desired
properties of uncertainty quantification techniques. Moreover, we show that
leveraging uncertainty in a smoothed classifier helps out-of-distribution
detection.",stat.ML
Low-rank Bayesian matrix completion via geodesic Hamiltonian Monte Carlo on Stiefel manifolds,"We present a new sampling-based approach for enabling efficient computation
of low-rank Bayesian matrix completion and quantifying the associated
uncertainty. Firstly, we design a new prior model based on the
singular-value-decomposition (SVD) parametrization of low-rank matrices. Our
prior is analogous to the seminal nuclear-norm regularization used in
non-Bayesian setting and enforces orthogonality in the factor matrices by
constraining them to Stiefel manifolds. Then, we design a geodesic Hamiltonian
Monte Carlo (-within-Gibbs) algorithm for generating posterior samples of the
SVD factor matrices. We demonstrate that our approach resolves the sampling
difficulties encountered by standard Gibbs samplers for the common two-matrix
factorization used in matrix completion. More importantly, the geodesic
Hamiltonian sampler allows for sampling in cases with more general likelihoods
than the typical Gaussian likelihood and Gaussian prior assumptions adopted in
most of the existing Bayesian matrix completion literature. We demonstrate an
applications of our approach to fit the categorical data of a mice protein
dataset and the MovieLens recommendation problem. Numerical examples
demonstrate superior sampling performance, including better mixing and faster
convergence to a stationary distribution. Moreover, they demonstrate improved
accuracy on the two real-world benchmark problems we considered.",stat.ML
Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model,"``Distribution shift'' is the main obstacle to the success of offline
reinforcement learning. A learning policy may take actions beyond the behavior
policy's knowledge, referred to as Out-of-Distribution (OOD) actions. The
Q-values for these OOD actions can be easily overestimated. As a result, the
learning policy is biased by using incorrect Q-value estimates. One common
approach to avoid Q-value overestimation is to make a pessimistic adjustment.
Our key idea is to penalize the Q-values of OOD actions associated with high
uncertainty. In this work, we propose Q-Distribution Guided Q-Learning (QDQ),
which applies a pessimistic adjustment to Q-values in OOD regions based on
uncertainty estimation. This uncertainty measure relies on the conditional
Q-value distribution, learned through a high-fidelity and efficient consistency
model. Additionally, to prevent overly conservative estimates, we introduce an
uncertainty-aware optimization objective for updating the Q-value function. The
proposed QDQ demonstrates solid theoretical guarantees for the accuracy of
Q-value distribution learning and uncertainty measurement, as well as the
performance of the learning policy. QDQ consistently shows strong performance
on the D4RL benchmark and achieves significant improvements across many tasks.",stat.ML
"A Systematic Review of Machine Learning Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases","Social media platforms like Twitter, Facebook, and Instagram have facilitated
the spread of misinformation, necessitating automated detection systems. This
systematic review evaluates 36 studies that apply machine learning (ML) and
deep learning (DL) models to detect fake news, spam, and fake accounts on
social media. Using the Prediction model Risk Of Bias ASsessment Tool
(PROBAST), the review identified key biases across the ML lifecycle: selection
bias due to non-representative sampling, inadequate handling of class
imbalance, insufficient linguistic preprocessing (e.g., negations), and
inconsistent hyperparameter tuning. Although models such as Support Vector
Machines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks
showed strong potential, over-reliance on accuracy as an evaluation metric in
imbalanced data settings was a common flaw. The review highlights the need for
improved data preprocessing (e.g., resampling techniques), consistent
hyperparameter tuning, and the use of appropriate metrics like precision,
recall, F1 score, and AUROC. Addressing these limitations can lead to more
reliable and generalizable ML/DL models for detecting deceptive content,
ultimately contributing to the reduction of misinformation on social media.",stat.ML
On the Gaussian process limit of Bayesian Additive Regression Trees,"Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian
regression technique of rising fame. It is a sum-of-decision-trees model, and
is in some sense the Bayesian version of boosting. In the limit of infinite
trees, it becomes equivalent to Gaussian process (GP) regression. This limit is
known but has not yet led to any useful analysis or application. For the first
time, I derive and compute the exact BART prior covariance function. With it I
implement the infinite trees limit of BART as GP regression. Through empirical
tests, I show that this limit is worse than standard BART in a fixed
configuration, but also that tuning the hyperparameters in the natural GP way
yields a competitive method, although a properly tuned BART is still superior.
The advantage of using a GP surrogate of BART is the analytical likelihood,
which simplifies model building and sidesteps the complex BART MCMC. More
generally, this study opens new ways to understand and develop BART and GP
regression. The implementation of BART as GP is available in the Python package
https://github.com/Gattocrucco/lsqfitgp .",stat.ML
Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL,"In order to mitigate the sample complexity of real-world reinforcement
learning, common practice is to first train a policy in a simulator where
samples are cheap, and then deploy this policy in the real world, with the hope
that it generalizes effectively. Such \emph{direct sim2real} transfer is not
guaranteed to succeed, however, and in cases where it fails, it is unclear how
to best utilize the simulator. In this work, we show that in many regimes,
while direct sim2real transfer may fail, we can utilize the simulator to learn
a set of \emph{exploratory} policies which enable efficient exploration in the
real world. In particular, in the setting of low-rank MDPs, we show that
coupling these exploratory policies with simple, practical approaches --
least-squares regression oracles and naive randomized exploration -- yields a
polynomial sample complexity in the real world, an exponential improvement over
direct sim2real transfer, or learning without access to a simulator. To the
best of our knowledge, this is the first evidence that simulation transfer
yields a provable gain in reinforcement learning in settings where direct
sim2real transfer fails. We validate our theoretical results on several
realistic robotic simulators and a real-world robotic sim2real task,
demonstrating that transferring exploratory policies can yield substantial
gains in practice as well.",stat.ML
Robust Model Evaluation over Large-scale Federated Networks,"In this paper, we address the challenge of certifying the performance of a
machine learning model on an unseen target network, using measurements from an
available source network. We focus on a scenario where heterogeneous datasets
are distributed across a source network of clients, all connected to a central
server. Specifically, consider a source network ""A"" composed of $K$ clients,
each holding private data from unique and heterogeneous distributions, which
are assumed to be independent samples from a broader meta-distribution $\mu$.
Our goal is to provide certified guarantees for the model's performance on a
different, unseen target network ""B,"" governed by another meta-distribution
$\mu'$, assuming the deviation between $\mu$ and $\mu'$ is bounded by either
the Wasserstein distance or an $f$-divergence. We derive theoretical guarantees
for the model's empirical average loss and provide uniform bounds on the risk
CDF, where the latter correspond to novel and adversarially robust versions of
the Glivenko-Cantelli theorem and the Dvoretzky-Kiefer-Wolfowitz (DKW)
inequality. Our bounds are computable in polynomial time with a polynomial
number of queries to the $K$ clients, preserving client privacy by querying
only the model's (potentially adversarial) loss on private data. We also
establish non-asymptotic generalization bounds that consistently converge to
zero as both $K$ and the minimum client sample size grow. Extensive empirical
evaluations validate the robustness and practicality of our bounds across
real-world tasks.",stat.ML
Convergence Guarantees for the DeepWalk Embedding on Block Models,"Graph embeddings have emerged as a powerful tool for understanding the
structure of graphs. Unlike classical spectral methods, recent methods such as
DeepWalk, Node2Vec, etc. are based on solving nonlinear optimization problems
on the graph, using local information obtained by performing random walks.
These techniques have empirically been shown to produce ''better'' embeddings
than their classical counterparts. However, due to their reliance on solving a
nonconvex optimization problem, obtaining theoretical guarantees on the
properties of the solution has remained a challenge, even for simple classes of
graphs. In this work, we show convergence properties for the DeepWalk algorithm
on graphs obtained from the Stochastic Block Model (SBM). Despite being
simplistic, the SBM has proved to be a classic model for analyzing the behavior
of algorithms on large graphs. Our results mirror the existing ones for
spectral embeddings on SBMs, showing that even in the case of one-dimensional
embeddings, the output of the DeepWalk algorithm provably recovers the cluster
structure with high probability.",stat.ML
Uncertainty-Penalized Direct Preference Optimization,"Aligning Large Language Models (LLMs) to human preferences in content, style,
and presentation is challenging, in part because preferences are varied,
context-dependent, and sometimes inherently ambiguous. While successful,
Reinforcement Learning from Human Feedback (RLHF) and Direct Preference
Optimization (DPO) are prone to the issue of proxy reward overoptimization.
Analysis of the DPO loss reveals a critical need for regularization for
mislabeled or ambiguous preference pairs to avoid reward hacking. In this work,
we develop a pessimistic framework for DPO by introducing preference
uncertainty penalization schemes, inspired by offline reinforcement learning.
The penalization serves as a correction to the loss which attenuates the loss
gradient for uncertain samples. Evaluation of the methods is performed with
GPT2 Medium on the Anthropic-HH dataset using a model ensemble to obtain
uncertainty estimates, and shows improved overall performance compared to
vanilla DPO, as well as better completions on prompts from high-uncertainty
chosen/rejected responses.",stat.ML
DeepMIDE: A Multivariate Spatio-Temporal Method for Ultra-Scale Offshore Wind Energy Forecasting,"To unlock access to stronger winds, the offshore wind industry is advancing
with significantly larger and taller wind turbines. This massive upscaling
motivates a departure from univariate wind forecasting methods that
traditionally focused on a single representative height. To fill this gap, we
propose DeepMIDE--a statistical deep learning method which jointly models the
offshore wind speeds across space, time, and height. DeepMIDE is formulated as
a multi-output integro-difference equation model with a multivariate,
nonstationary, and state-dependent kernel characterized by a set of advection
vectors that encode the physics of wind field formation and propagation.
Embedded within DeepMIDE, an advanced deep learning architecture learns these
advection vectors from high dimensional streams of exogenous weather
information, which, along with other parameters, are plugged back into the
statistical model for probabilistic multi-height space-time forecasting. Tested
on real-world data from future offshore wind energy sites in the Northeastern
United States, the wind speed and power forecasts from DeepMIDE are shown to
outperform those from prevalent time series, spatio-temporal, and deep learning
methods.",stat.ML
Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD,"We consider the problem of high-dimensional heavy-tailed statistical
estimation in the streaming setting, which is much harder than the traditional
batch setting due to memory constraints. We cast this problem as stochastic
convex optimization with heavy tailed stochastic gradients, and prove that the
widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical
rates whenever the second moment of the stochastic gradient noise is finite.
More precisely, with $T$ samples, we show that Clipped-SGD, for smooth and
strongly convex objectives, achieves an error of
$\sqrt{\frac{\mathsf{Tr}(\Sigma)+\sqrt{\mathsf{Tr}(\Sigma)\|\Sigma\|_2}\log(\frac{\log(T)}{\delta})}{T}}$
with probability $1-\delta$, where $\Sigma$ is the covariance of the clipped
gradient. Note that the fluctuations (depending on $\frac{1}{\delta}$) are of
lower order than the term $\mathsf{Tr}(\Sigma)$. This improves upon the current
best rate of $\sqrt{\frac{\mathsf{Tr}(\Sigma)\log(\frac{1}{\delta})}{T}}$ for
Clipped-SGD, known only for smooth and strongly convex objectives. Our results
also extend to smooth convex and lipschitz convex objectives. Key to our result
is a novel iterative refinement strategy for martingale concentration,
improving upon the PAC-Bayes approach of Catoni and Giulini.",stat.ML
Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations,"Understanding how neural networks transform input data across layers is
fundamental to unraveling their learning and generalization capabilities.
Although prior work has used insights from kernel methods to study neural
networks, a global analysis of how the similarity between hidden
representations evolves across layers remains underexplored. In this paper, we
introduce a theoretical framework for the evolution of the kernel sequence,
which measures the similarity between the hidden representation for two
different inputs. Operating under the mean-field regime, we show that the
kernel sequence evolves deterministically via a kernel map, which only depends
on the activation function. By expanding activation using Hermite polynomials
and using their algebraic properties, we derive an explicit form for kernel map
and fully characterize its fixed points. Our analysis reveals that for
nonlinear activations, the kernel sequence converges globally to a unique fixed
point, which can correspond to orthogonal or similar representations depending
on the activation and network architecture. We further extend our results to
networks with residual connections and normalization layers, demonstrating
similar convergence behaviors. This work provides new insights into the
implicit biases of deep neural networks and how architectural choices influence
the evolution of representations across layers.",stat.ML
Understanding the Effect of GCN Convolutions in Regression Tasks,"Graph Convolutional Networks (GCNs) have become a pivotal method in machine
learning for modeling functions over graphs. Despite their widespread success
across various applications, their statistical properties (e.g. consistency,
convergence rates) remain ill-characterized. To begin addressing this knowledge
gap, in this paper, we provide a formal analysis of the impact of convolution
operators on regression tasks over homophilic networks. Focusing on estimators
based solely on neighborhood aggregation, we examine how two common
convolutions - the original GCN and GraphSage convolutions - affect the
learning error as a function of the neighborhood topology and the number of
convolutional layers. We explicitly characterize the bias-variance trade-off
incurred by GCNs as a function of the neighborhood size and identify specific
graph topologies where convolution operators are less effective. Our
theoretical findings are corroborated by synthetic experiments, and provide a
start to a deeper quantitative understanding of convolutional effects in GCNs
for offering rigorous guidelines for practitioners.",stat.ML
Mechanism learning: Reverse causal inference in the presence of multiple unknown confounding through front-door causal bootstrapping,"A major limitation of machine learning (ML) prediction models is that they
recover associational, rather than causal, predictive relationships between
variables. In high-stakes automation applications of ML this is problematic, as
the model often learns spurious, non-causal associations. This paper proposes
mechanism learning, a simple method which uses front-door causal bootstrapping
to deconfound observational data such that any appropriate ML model is forced
to learn predictive relationships between effects and their causes (reverse
causal inference), despite the potential presence of multiple unknown and
unmeasured confounding. Effect variables can be very high dimensional, and the
predictive relationship nonlinear, as is common in ML applications. This novel
method is widely applicable, the only requirement is the existence of a
mechanism variable mediating the cause (prediction target) and effect (feature
data), which is independent of the (unmeasured) confounding variables. We test
our method on fully synthetic, semi-synthetic and real-world datasets,
demonstrating that it can discover reliable, unbiased, causal ML predictors
where by contrast, the same ML predictor trained naively using classical
supervised learning on the original observational data, is heavily biased by
spurious associations. We provide code to implement the results in the paper,
online.",stat.ML
Unsupervised Machine Learning for Detecting and Locating Human-Made Objects in 3D Point Cloud,"A 3D point cloud is an unstructured, sparse, and irregular dataset, typically
collected by airborne LiDAR systems over a geological region. Laser pulses
emitted from these systems reflect off objects both on and above the ground,
resulting in a dataset containing the longitude, latitude, and elevation of
each point, as well as information about the corresponding laser pulse
strengths. A widely studied research problem, addressed in many previous works,
is ground filtering, which involves partitioning the points into ground and
non-ground subsets. This research introduces a novel task: detecting and
identifying human-made objects amidst natural tree structures. This task is
performed on the subset of non-ground points derived from the ground filtering
stage. Marked Point Fields (MPFs) are used as models well-suited to these
tasks. The proposed methodology consists of three stages: ground filtering,
local information extraction (LIE), and clustering. In the ground filtering
stage, a statistical method called One-Sided Regression (OSR) is introduced,
addressing the limitations of prior ground filtering methods on uneven
terrains. In the LIE stage, unsupervised learning methods are lacking. To
mitigate this, a kernel-based method for the Hessian matrix of the MPF is
developed. In the clustering stage, the Gaussian Mixture Model (GMM) is applied
to the results of the LIE stage to partition the non-ground points into trees
and human-made objects. The underlying assumption is that LiDAR points from
trees exhibit a three-dimensional distribution, while those from human-made
objects follow a two-dimensional distribution. The Hessian matrix of the MPF
effectively captures this distinction. Experimental results demonstrate that
the proposed ground filtering method outperforms previous techniques, and the
LIE method successfully distinguishes between points representing trees and
human-made objects.",stat.ML
Dimension reduction via score ratio matching,"Gradient-based dimension reduction decreases the cost of Bayesian inference
and probabilistic modeling by identifying maximally informative (and informed)
low-dimensional projections of the data and parameters, allowing
high-dimensional problems to be reformulated as cheaper low-dimensional
problems. A broad family of such techniques identify these projections and
provide error bounds on the resulting posterior approximations, via
eigendecompositions of certain diagnostic matrices. Yet these matrices require
gradients or even Hessians of the log-likelihood, excluding the purely
data-driven setting and many problems of simulation-based inference. We propose
a framework, derived from score-matching, to extend gradient-based dimension
reduction to problems where gradients are unavailable. Specifically, we
formulate an objective function to directly learn the score ratio function
needed to compute the diagnostic matrices, propose a tailored parameterization
for the score ratio network, and introduce regularization methods that
capitalize on the hypothesized low-dimensional structure. We also introduce a
novel algorithm to iteratively identify the low-dimensional reduced basis
vectors more accurately with limited data based on eigenvalue deflation
methods. We show that our approach outperforms standard score-matching for
problems with low-dimensional structure, and demonstrate its effectiveness for
PDE-constrained Bayesian inverse problems and conditional generative modeling.",stat.ML
Statistical Inference in Classification of High-dimensional Gaussian Mixture,"We consider the classification problem of a high-dimensional mixture of two
Gaussians with general covariance matrices. Using the replica method from
statistical physics, we investigate the asymptotic behavior of a general class
of regularized convex classifiers in the high-dimensional limit, where both the
sample size $n$ and the dimension $p$ approach infinity while their ratio
$\alpha=n/p$ remains fixed. Our focus is on the generalization error and
variable selection properties of the estimators. Specifically, based on the
distributional limit of the classifier, we construct a de-biased estimator to
perform variable selection through an appropriate hypothesis testing procedure.
Using $L_1$-regularized logistic regression as an example, we conducted
extensive computational experiments to confirm that our analytical findings are
consistent with numerical simulations in finite-sized systems. We also explore
the influence of the covariance structure on the performance of the de-biased
estimator.",stat.ML
Provable optimal transport with transformers: The essence of depth and prompt engineering,"Can we establish provable performance guarantees for transformers?
Establishing such theoretical guarantees is a milestone in developing
trustworthy generative AI. In this paper, we take a step toward addressing this
question by focusing on optimal transport, a fundamental problem at the
intersection of combinatorial and continuous optimization. Leveraging the
computational power of attention layers, we prove that a transformer with fixed
parameters can effectively solve the optimal transport problem in Wasserstein-2
with entropic regularization for an arbitrary number of points. Consequently,
the transformer can sort lists of arbitrary sizes up to an approximation
factor. Our results rely on an engineered prompt that enables the transformer
to implement gradient descent with adaptive stepsizes on the dual optimal
transport. Combining the convergence analysis of gradient descent with Sinkhorn
dynamics, we establish an explicit approximation bound for optimal transport
with transformers, which improves as depth increases. Our findings provide
novel insights into the essence of prompt engineering and depth for solving
optimal transport. In particular, prompt engineering boosts the algorithmic
expressivity of transformers, allowing them implement an optimization method.
With increasing depth, transformers can simulate several iterations of gradient
descent.",stat.ML
On the Benefits of Active Data Collection in Operator Learning,"We investigate active data collection strategies for operator learning when
the target operator is linear and the input functions are drawn from a
mean-zero stochastic process with continuous covariance kernels. With an active
data collection strategy, we establish an error convergence rate in terms of
the decay rate of the eigenvalues of the covariance kernel. Thus, with
sufficiently rapid eigenvalue decay of the covariance kernels, arbitrarily fast
error convergence rates can be achieved. This contrasts with the passive
(i.i.d.) data collection strategies, where the convergence rate is never faster
than $\sim n^{-1}$. In fact, for our setting, we establish a
\emph{non-vanishing} lower bound for any passive data collection strategy,
regardless of the eigenvalues decay rate of the covariance kernel. Overall, our
results show the benefit of active over passive data collection strategies in
operator learning.",stat.ML
Learning the Regularization Strength for Deep Fine-Tuning via a Data-Emphasized Variational Objective,"A number of popular transfer learning methods rely on grid search to select
regularization hyperparameters that control over-fitting. This grid search
requirement has several key disadvantages: the search is computationally
expensive, requires carving out a validation set that reduces the size of
available data for model training, and requires practitioners to specify
candidate values. In this paper, we propose an alternative to grid search:
directly learning regularization hyperparameters on the full training set via
model selection techniques based on the evidence lower bound (""ELBo"") objective
from variational methods. For deep neural networks with millions of parameters,
we specifically recommend a modified ELBo that upweights the influence of the
data likelihood relative to the prior while remaining a valid bound on the
evidence for Bayesian model selection. Our proposed technique overcomes all
three disadvantages of grid search. We demonstrate effectiveness on image
classification tasks on several datasets, yielding heldout accuracy comparable
to existing approaches with far less compute time.",stat.ML
Spatial Shortcuts in Graph Neural Controlled Differential Equations,"We incorporate prior graph topology information into a Neural Controlled
Differential Equation (NCDE) to predict the future states of a dynamical system
defined on a graph. The informed NCDE infers the future dynamics at the
vertices of simulated advection data on graph edges with a known causal graph,
observed only at vertices during training. We investigate different positions
in the model architecture to inform the NCDE with graph information and
identify an outer position between hidden state and control as theoretically
and empirically favorable. Our such informed NCDE requires fewer parameters to
reach a lower Mean Absolute Error (MAE) compared to previous methods that do
not incorporate additional graph topology information.",stat.ML
Considerations for Distribution Shift Robustness of Diagnostic Models in Healthcare,"We consider robustness to distribution shifts in the context of diagnostic
models in healthcare, where the prediction target $Y$, e.g., the presence of a
disease, is causally upstream of the observations $X$, e.g., a biomarker.
Distribution shifts may occur, for instance, when the training data is
collected in a domain with patients having particular demographic
characteristics while the model is deployed on patients from a different
demographic group. In the domain of applied ML for health, it is common to
predict $Y$ from $X$ without considering further information about the patient.
However, beyond the direct influence of the disease $Y$ on biomarker $X$, a
predictive model may learn to exploit confounding dependencies (or shortcuts)
between $X$ and $Y$ that are unstable under certain distribution shifts. In
this work, we highlight a data generating mechanism common to healthcare
settings and discuss how recent theoretical results from the causality
literature can be applied to build robust predictive models. We theoretically
show why ignoring covariates as well as common invariant learning approaches
will in general not yield robust predictors in the studied setting, while
including certain covariates into the prediction model will. In an extensive
simulation study, we showcase the robustness (or lack thereof) of different
predictors under various data generating processes. Lastly, we analyze the
performance of the different approaches using the PTB-XL dataset, a public
dataset of annotated ECG recordings.",stat.ML
LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data,"Discovering the underlying Directed Acyclic Graph (DAG) from time series
observational data is highly challenging due to the dynamic nature and complex
nonlinear interactions between variables. Existing methods often struggle with
inefficiency and the handling of high-dimensional data. To address these
research gap, we propose LOCAL, a highly efficient, easy-to-implement, and
constraint-free method for recovering dynamic causal structures. LOCAL is the
first attempt to formulate a quasi-maximum likelihood-based score function for
learning the dynamic DAG equivalent to the ground truth. On this basis, we
propose two adaptive modules for enhancing the algebraic characterization of
acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and
Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using
learnable priority vectors and the Gumbel-Sigmoid function, ensuring the
creation of DAGs while optimizing computational efficiency. DGPL transforms
causal learning into decomposed matrix products, capturing the dynamic causal
structure of high-dimensional data and enhancing interpretability. Extensive
experiments on synthetic and real-world datasets demonstrate that LOCAL
significantly outperforms existing methods, and highlight LOCAL's potential as
a robust and efficient method for dynamic causal discovery. Our code will be
available soon.",stat.ML
Learned Reference-based Diffusion Sampling for multi-modal distributions,"Over the past few years, several approaches utilizing score-based diffusion
have been proposed to sample from probability distributions, that is without
having access to exact samples and relying solely on evaluations of
unnormalized densities. The resulting samplers approximate the time-reversal of
a noising diffusion process, bridging the target distribution to an
easy-to-sample base distribution. In practice, the performance of these methods
heavily depends on key hyperparameters that require ground truth samples to be
accurately tuned. Our work aims to highlight and address this fundamental
issue, focusing in particular on multi-modal distributions, which pose
significant challenges for existing sampling methods. Building on existing
approaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a
methodology specifically designed to leverage prior knowledge on the location
of the target modes in order to bypass the obstacle of hyperparameter tuning.
LRDS proceeds in two steps by (i) learning a reference diffusion model on
samples located in high-density space regions and tailored for multimodality,
and (ii) using this reference model to foster the training of a diffusion-based
sampler. We experimentally demonstrate that LRDS best exploits prior knowledge
on the target distribution compared to competing algorithms on a variety of
challenging distributions.",stat.ML
Analyzing Generative Models by Manifold Entropic Metrics,"Good generative models should not only synthesize high quality data, but also
utilize interpretable representations that aid human understanding of their
behavior. However, it is difficult to measure objectively if and to what degree
desirable properties of disentangled representations have been achieved.
Inspired by the principle of independent mechanisms, we address this difficulty
by introducing a novel set of tractable information-theoretic evaluation
metrics. We demonstrate the usefulness of our metrics on illustrative toy
examples and conduct an in-depth comparison of various normalizing flow
architectures and $\beta$-VAEs on the EMNIST dataset. Our method allows to sort
latent features by importance and assess the amount of residual correlations of
the resulting concepts. The most interesting finding of our experiments is a
ranking of model architectures and training procedures in terms of their
inductive bias to converge to aligned and disentangled representations during
training.",stat.ML
Noise-Aware Differentially Private Variational Inference,"Differential privacy (DP) provides robust privacy guarantees for statistical
inference, but this can lead to unreliable results and biases in downstream
applications. While several noise-aware approaches have been proposed which
integrate DP perturbation into the inference, they are limited to specific
types of simple probabilistic models. In this work, we propose a novel method
for noise-aware approximate Bayesian inference based on stochastic gradient
variational inference which can also be applied to high-dimensional and
non-conjugate models. We also propose a more accurate evaluation method for
noise-aware posteriors. Empirically, our inference method has similar
performance to existing methods in the domain where they are applicable.
Outside this domain, we obtain accurate coverages on high-dimensional Bayesian
linear regression and well-calibrated predictive probabilities on Bayesian
logistic regression with the UCI Adult dataset.",stat.ML
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network
linear layers with the Mahalanobis distance, offering a new perspective on
neural network interpretability. While previous studies have explored
activation functions primarily for performance optimization, our work
interprets these functions through statistical distance measures, a less
explored area in neural network research. By establishing this connection, we
provide a foundation for developing more interpretable neural network models,
which is crucial for applications requiring transparency. Although this work is
theoretical and does not include empirical data, the proposed distance-based
interpretation has the potential to enhance model robustness, improve
generalization, and provide more intuitive explanations of neural network
decisions.",stat.ML
Simpler Diffusion (SiD2): 1.5 FID on ImageNet512 with pixel-space diffusion,"Latent diffusion models have become the popular choice for scaling up
diffusion models for high resolution image synthesis. Compared to pixel-space
models that are trained end-to-end, latent models are perceived to be more
efficient and to produce higher image quality at high resolution. Here we
challenge these notions, and show that pixel-space models can in fact be very
competitive to latent approaches both in quality and efficiency, achieving 1.5
FID on ImageNet512 and new SOTA results on ImageNet128 and ImageNet256.
  We present a simple recipe for scaling end-to-end pixel-space diffusion
models to high resolutions. 1: Use the sigmoid loss (Kingma & Gao, 2023) with
our prescribed hyper-parameters. 2: Use our simplified memory-efficient
architecture with fewer skip-connections. 3: Scale the model to favor
processing the image at high resolution with fewer parameters, rather than
using more parameters but at a lower resolution. When combining these three
steps with recently proposed tricks like guidance intervals, we obtain a family
of pixel-space diffusion models we call Simple Diffusion v2 (SiD2).",stat.ML
No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models,"Generative models have shown impressive capabilities in synthesizing
high-quality outputs across various domains. However, a persistent challenge is
the occurrence of ""hallucinations"", where the model produces outputs that are
plausible but invalid. While empirical strategies have been explored to
mitigate this issue, a rigorous theoretical understanding remains elusive. In
this paper, we develop a theoretical framework to analyze the learnability of
non-hallucinating generative models from a learning-theoretic perspective. Our
results reveal that non-hallucinating learning is statistically impossible when
relying solely on the training dataset, even for a hypothesis class of size two
and when the entire training set is truthful. To overcome these limitations, we
show that incorporating inductive biases aligned with the actual facts into the
learning process is essential. We provide a systematic approach to achieve this
by restricting the facts set to a concept class of finite VC-dimension and
demonstrate its effectiveness under various learning paradigms. Although our
findings are primarily conceptual, they represent a first step towards a
principled approach to addressing hallucinations in learning generative models.",stat.ML
Binary Classification: Is Boosting stronger than Bagging?,"Random Forests have been one of the most popular bagging methods in the past
few decades, especially due to their success at handling tabular datasets. They
have been extensively studied and compared to boosting models, like XGBoost,
which are generally considered more performant. Random Forests adopt several
simplistic assumptions, such that all samples and all trees that form the
forest are equally important for building the final model. We introduce
Enhanced Random Forests, an extension of vanilla Random Forests with extra
functionalities and adaptive sample and model weighting. We develop an
iterative algorithm for adapting the training sample weights, by favoring the
hardest examples, and an approach for finding personalized tree weighting
schemes for each new sample. Our method significantly improves upon regular
Random Forests across 15 different binary classification datasets and
considerably outperforms other tree methods, including XGBoost, when run with
default hyperparameters, which indicates the robustness of our approach across
datasets, without the need for extensive hyperparameter tuning. Our
tree-weighting methodology results in enhanced or comparable performance to the
uniformly weighted ensemble, and is, more importantly, leveraged to define
importance scores for trees based on their contributions to classifying each
new sample. This enables us to only focus on a small number of trees as the
main models that define the outcome of a new sample and, thus, to partially
recover interpretability, which is critically missing from both bagging and
boosting methods. In binary classification problems, the proposed extensions
and the corresponding results suggest the equivalence of bagging and boosting
methods in performance, and the edge of bagging in interpretability by
leveraging a few learners of the ensemble, which is not an option in the less
explainable boosting methods.",stat.ML
Enriching GNNs with Text Contextual Representations for Detecting Disinformation Campaigns on Social Media,"Disinformation on social media poses both societal and technical challenges.
While previous studies have integrated textual information into propagation
networks, they have yet to fully leverage the advancements in Transformer-based
language models for high-quality contextual text representations. This work
investigates the impact of incorporating textual features into Graph Neural
Networks (GNNs) for fake news detection. Our experiments demonstrate that
contextual representations improve performance by 9.3% in Macro F1 over static
ones and 33.8% over GNNs without textual features. However, noisy data
augmentation degrades performance and increases instability. We expect our
methodology to open avenues for further research, and all code is made publicly
available.",stat.ML
Cross Spline Net and a Unified World,"In today's machine learning world for tabular data, XGBoost and fully
connected neural network (FCNN) are two most popular methods due to their good
model performance and convenience to use. However, they are highly complicated,
hard to interpret, and can be overfitted. In this paper, we propose a new
modeling framework called cross spline net (CSN) that is based on a combination
of spline transformation and cross-network (Wang et al. 2017, 2021). We will
show CSN is as performant and convenient to use, and is less complicated, more
interpretable and robust. Moreover, the CSN framework is flexible, as the
spline layer can be configured differently to yield different models. With
different choices of the spline layer, we can reproduce or approximate a set of
non-neural network models, including linear and spline-based statistical
models, tree, rule-fit, tree-ensembles (gradient boosting trees, random
forest), oblique tree/forests, multi-variate adaptive regression spline (MARS),
SVM with polynomial kernel, etc. Therefore, CSN provides a unified modeling
framework that puts the above set of non-neural network models under the same
neural network framework. By using scalable and powerful gradient descent
algorithms available in neural network libraries, CSN avoids some pitfalls
(such as being ad-hoc, greedy or non-scalable) in the case-specific
optimization methods used in the above non-neural network models. We will use a
special type of CSN, TreeNet, to illustrate our point. We will compare TreeNet
with XGBoost and FCNN to show the benefits of TreeNet. We believe CSN will
provide a flexible and convenient framework for practitioners to build
performant, robust and more interpretable models.",stat.ML
Structured Diffusion Models with Mixture of Gaussians as Prior Distribution,"We propose a class of structured diffusion models, in which the prior
distribution is chosen as a mixture of Gaussians, rather than a standard
Gaussian distribution. The specific mixed Gaussian distribution, as prior, can
be chosen to incorporate certain structured information of the data. We develop
a simple-to-implement training procedure that smoothly accommodates the use of
mixed Gaussian as prior. Theory is provided to quantify the benefits of our
proposed models, compared to the classical diffusion models. Numerical
experiments with synthetic, image and operational data are conducted to show
comparative advantages of our model. Our method is shown to be robust to
mis-specifications and in particular suits situations where training resources
are limited or faster training in real time is desired.",stat.ML
Initialization Matters: On the Benign Overfitting of Two-Layer ReLU CNN with Fully Trainable Layers,"Benign overfitting refers to how over-parameterized neural networks can fit
training data perfectly and generalize well to unseen data. While this has been
widely investigated theoretically, existing works are limited to two-layer
networks with fixed output layers, where only the hidden weights are trained.
We extend the analysis to two-layer ReLU convolutional neural networks (CNNs)
with fully trainable layers, which is closer to the practice. Our results show
that the initialization scaling of the output layer is crucial to the training
dynamics: large scales make the model training behave similarly to that with
the fixed output, the hidden layer grows rapidly while the output layer remains
largely unchanged; in contrast, small scales result in more complex layer
interactions, the hidden layer initially grows to a specific ratio relative to
the output layer, after which both layers jointly grow and maintain that ratio
throughout training. Furthermore, in both settings, we provide nearly matching
upper and lower bounds on the test errors, identifying the sharp conditions on
the initialization scaling and signal-to-noise ratio (SNR) in which the benign
overfitting can be achieved or not. Numerical experiments back up the
theoretical results.",stat.ML
Maximum a Posteriori Inference for Factor Graphs via Benders' Decomposition,"Many Bayesian statistical inference problems come down to computing a maximum
a-posteriori (MAP) assignment of latent variables. Yet, standard methods for
estimating the MAP assignment do not have a finite time guarantee that the
algorithm has converged to a fixed point. Previous research has found that MAP
inference can be represented in dual form as a linear programming problem with
a non-polynomial number of constraints. A Lagrangian relaxation of the dual
yields a statistical inference algorithm as a linear programming problem.
However, the decision as to which constraints to remove in the relaxation is
often heuristic. We present a method for maximum a-posteriori inference in
general Bayesian factor models that sequentially adds constraints to the fully
relaxed dual problem using Benders' decomposition. Our method enables the
incorporation of expressive integer and logical constraints in clustering
problems such as must-link, cannot-link, and a minimum number of whole samples
allocated to each cluster. Using this approach, we derive MAP estimation
algorithms for the Bayesian Gaussian mixture model and latent Dirichlet
allocation. Empirical results show that our method produces a higher optimal
posterior value compared to Gibbs sampling and variational Bayes methods for
standard data sets and provides certificate of convergence.",stat.ML
A spectral method for multi-view subspace learning using the product of projections,"Multi-view data provides complementary information on the same set of
observations, with multi-omics and multimodal sensor data being common
examples. Analyzing such data typically requires distinguishing between shared
(joint) and unique (individual) signal subspaces from noisy, high-dimensional
measurements. Despite many proposed methods, the conditions for reliably
identifying joint and individual subspaces remain unclear. We rigorously
quantify these conditions, which depend on the ratio of the signal rank to the
ambient dimension, principal angles between true subspaces, and noise levels.
Our approach characterizes how spectrum perturbations of the product of
projection matrices, derived from each view's estimated subspaces, affect
subspace separation. Using these insights, we provide an easy-to-use and
scalable estimation algorithm. In particular, we employ rotational bootstrap
and random matrix theory to partition the observed spectrum into joint,
individual, and noise subspaces. Diagnostic plots visualize this partitioning,
providing practical and interpretable insights into the estimation performance.
In simulations, our method estimates joint and individual subspaces more
accurately than existing approaches. Applications to multi-omics data from
colorectal cancer patients and nutrigenomic study of mice demonstrate improved
performance in downstream predictive tasks.",stat.ML
Conditional diffusions for neural posterior estimation,"Neural posterior estimation (NPE), a simulation-based computational approach
for Bayesian inference, has shown great success in situations where posteriors
are intractable or likelihood functions are treated as ""black boxes."" Existing
NPE methods typically rely on normalizing flows, which transform a base
distributions into a complex posterior by composing many simple, invertible
transformations. But flow-based models, while state of the art for NPE, are
known to suffer from several limitations, including training instability and
sharp trade-offs between representational power and computational cost. In this
work, we demonstrate the effectiveness of conditional diffusions as an
alternative to normalizing flows for NPE. Conditional diffusions address many
of the challenges faced by flow-based methods. Our results show that, across a
highly varied suite of benchmarking problems for NPE architectures, diffusions
offer improved stability, superior accuracy, and faster training times, even
with simpler, shallower models. These gains persist across a variety of
different encoder or ""summary network"" architectures, as well as in situations
where no summary network is required. The code will be publicly available at
\url{https://github.com/TianyuCodings/cDiff}.",stat.ML
Inherently Interpretable Tree Ensemble Learning,"Tree ensemble models like random forests and gradient boosting machines are
widely used in machine learning due to their excellent predictive performance.
However, a high-performance ensemble consisting of a large number of decision
trees lacks sufficient transparency and explainability. In this paper, we
demonstrate that when shallow decision trees are used as base learners, the
ensemble learning algorithms can not only become inherently interpretable
subject to an equivalent representation as the generalized additive models but
also sometimes lead to better generalization performance. First, an
interpretation algorithm is developed that converts the tree ensemble into the
functional ANOVA representation with inherent interpretability. Second, two
strategies are proposed to further enhance the model interpretability, i.e., by
adding constraints in the model training stage and post-hoc effect pruning.
Experiments on simulations and real-world datasets show that our proposed
methods offer a better trade-off between model interpretation and predictive
performance, compared with its counterpart benchmarks.",stat.ML
Provable Tempered Overfitting of Minimal Nets and Typical Nets,"We study the overfitting behavior of fully connected deep Neural Networks
(NNs) with binary weights fitted to perfectly classify a noisy training set. We
consider interpolation using both the smallest NN (having the minimal number of
weights) and a random interpolating NN. For both learning rules, we prove
overfitting is tempered. Our analysis rests on a new bound on the size of a
threshold circuit consistent with a partial function. To the best of our
knowledge, ours are the first theoretical results on benign or tempered
overfitting that: (1) apply to deep NNs, and (2) do not require a very high or
very low input dimension.",stat.ML
FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models,"Survival analysis is an important research topic with applications in
healthcare, business, and manufacturing. One essential tool in this area is the
Cox proportional hazards (CPH) model, which is widely used for its
interpretability, flexibility, and predictive performance. However, for modern
data science challenges such as high dimensionality (both $n$ and $p$) and high
feature correlations, current algorithms to train the CPH model have drawbacks,
preventing us from using the CPH model at its full potential. The root cause is
that the current algorithms, based on the Newton method, have trouble
converging due to vanishing second order derivatives when outside the local
region of the minimizer. To circumvent this problem, we propose new
optimization methods by constructing and minimizing surrogate functions that
exploit hidden mathematical structures of the CPH model. Our new methods are
easy to implement and ensure monotonic loss decrease and global convergence.
Empirically, we verify the computational efficiency of our methods. As a direct
application, we show how our optimization methods can be used to solve the
cardinality-constrained CPH problem, producing very sparse high-quality models
that were not previously practical to construct. We list several extensions
that our breakthrough enables, including optimization opportunities,
theoretical questions on CPH's mathematical structure, as well as other
CPH-related applications.",stat.ML
A Generalized Framework for Multiscale State-Space Modeling with Nested Nonlinear Dynamics: An Application to Bayesian Learning under Switching Regimes,"In this work, we introduce a generalized framework for multiscale state-space
modeling that incorporates nested nonlinear dynamics, with a specific focus on
Bayesian learning under switching regimes. Our framework captures the complex
interactions between fast and slow processes within systems, allowing for the
analysis of how these dynamics influence each other across various temporal
scales. We model these interactions through a hierarchical structure in which
finer time-scale dynamics are nested within coarser ones, while facilitating
feedback between the scales. To promote the practical application of our
framework, we address the problem of identifying switching regimes and
transient dynamics. In particular, we develop a Bayesian learning approach to
estimate latent states and indicators corresponding to switching dynamics,
enabling the model to adapt effectively to regime changes. We employ Sequential
Monte Carlo, or particle filtering, for inference. We illustrate the utility of
our framework through simulations. The results demonstrate that our Bayesian
learning approach effectively tracks state transitions and achieves accurate
identification of switching dynamics in multiscale systems.",stat.ML
Less Discriminatory Alternative and Interpretable XGBoost Framework for Binary Classification,"Fair lending practices and model interpretability are crucial concerns in the
financial industry, especially given the increasing use of complex machine
learning models. In response to the Consumer Financial Protection Bureau's
(CFPB) requirement to protect consumers against unlawful discrimination, we
introduce LDA-XGB1, a novel less discriminatory alternative (LDA) machine
learning model for fair and interpretable binary classification. LDA-XGB1 is
developed through biobjective optimization that balances accuracy and fairness,
with both objectives formulated using binning and information value. It
leverages the predictive power and computational efficiency of XGBoost while
ensuring inherent model interpretability, including the enforcement of
monotonic constraints. We evaluate LDA-XGB1 on two datasets: SimuCredit, a
simulated credit approval dataset, and COMPAS, a real-world recidivism
prediction dataset. Our results demonstrate that LDA-XGB1 achieves an effective
balance between predictive accuracy, fairness, and interpretability, often
outperforming traditional fair lending models. This approach equips financial
institutions with a powerful tool to meet regulatory requirements for fair
lending while maintaining the advantages of advanced machine learning
techniques.",stat.ML
Context is Key: A Benchmark for Forecasting with Essential Textual Information,"Forecasting is a critical task in decision making across various domains.
While numerical data provides a foundation, it often lacks crucial context
necessary for accurate predictions. Human forecasters frequently rely on
additional information, such as background knowledge or constraints, which can
be efficiently communicated through natural language. However, the ability of
existing forecasting models to effectively integrate this textual information
remains an open question. To address this, we introduce ""Context is Key"" (CiK),
a time series forecasting benchmark that pairs numerical data with diverse
types of carefully crafted textual context, requiring models to integrate both
modalities. We evaluate a range of approaches, including statistical models,
time series foundation models, and LLM-based forecasters, and propose a simple
yet effective LLM prompting method that outperforms all other tested methods on
our benchmark. Our experiments highlight the importance of incorporating
contextual information, demonstrate surprising performance when using LLM-based
forecasting models, and also reveal some of their critical shortcomings. By
presenting this benchmark, we aim to advance multimodal forecasting, promoting
models that are both accurate and accessible to decision-makers with varied
technical expertise. The benchmark can be visualized at
https://servicenow.github.io/context-is-key-forecasting/v0/ .",stat.ML
A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities,"A key property of neural networks is their capacity of adapting to data
during training. Yet, our current mathematical understanding of feature
learning and its relationship to generalization remain limited. In this work,
we provide a random matrix analysis of how fully-connected two-layer neural
networks adapt to the target function after a single, but aggressive, gradient
descent step. We rigorously establish the equivalence between the updated
features and an isotropic spiked random feature model, in the limit of large
batch size. For the latter model, we derive a deterministic equivalent
description of the feature empirical covariance matrix in terms of certain
low-dimensional operators. This allows us to sharply characterize the impact of
training in the asymptotic feature spectrum, and in particular, provides a
theoretical grounding for how the tails of the feature spectrum modify with
training. The deterministic equivalent further yields the exact asymptotic
generalization error, shedding light on the mechanisms behind its improvement
in the presence of feature learning. Our result goes beyond standard random
matrix ensembles, and therefore we believe it is of independent technical
interest. Different from previous work, our result holds in the challenging
maximal learning rate regime, is fully rigorous and allows for finitely
supported second layer initialization, which turns out to be crucial for
studying the functional expressivity of the learned features. This provides a
sharp description of the impact of feature learning in the generalization of
two-layer neural networks, beyond the random features and lazy training
regimes.",stat.ML
AutoStep: Locally adaptive involutive MCMC,"Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a
deterministic involutive proposal with a step size parameter. Selecting an
appropriate step size is often a challenging task in practice; and for complex
multiscale targets, there may not be one choice of step size that works well
globally. In this work, we address this problem with a novel class of
involutive MCMC methods -- AutoStep MCMC -- that selects an appropriate step
size at each iteration adapted to the local geometry of the target
distribution. We prove that AutoStep MCMC is $\pi$-invariant and has other
desirable properties under mild assumptions on the target distribution $\pi$
and involutive proposal. Empirical results examine the effect of various step
size selection design choices, and show that AutoStep MCMC is competitive with
state-of-the-art methods in terms of effective sample size per unit cost on a
range of challenging target distributions.",stat.ML
MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data,"Causal discovery in real-world systems, such as biological networks, is often
complicated by feedback loops and incomplete data. Standard algorithms, which
assume acyclic structures or fully observed data, struggle with these
challenges. To address this gap, we propose MissNODAG, a differentiable
framework for learning both the underlying cyclic causal graph and the
missingness mechanism from partially observed data, including data missing not
at random. Our framework integrates an additive noise model with an
expectation-maximization procedure, alternating between imputing missing values
and optimizing the observed data likelihood, to uncover both the cyclic
structures and the missingness mechanism. We demonstrate the effectiveness of
MissNODAG through synthetic experiments and an application to real-world gene
perturbation data.",stat.ML
Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints,"Pure exploration in bandits models multiple real-world problems, such as
tuning hyper-parameters or conducting user studies, where different safety,
resource, and fairness constraints on the decision space naturally appear. We
study these problems as pure exploration in multi-armed bandits with unknown
linear constraints, where the aim is to identify an $r$$\textit{-good feasible
policy}$. First, we propose a Lagrangian relaxation of the sample complexity
lower bound for pure exploration under constraints. We show how this lower
bound evolves with the sequential estimation of constraints. Second, we
leverage the Lagrangian lower bound and the properties of convex optimisation
to propose two computationally efficient extensions of Track-and-Stop and
Gamified Explorer, namely LATS and LAGEX. To this end, we propose a
constraint-adaptive stopping rule, and while tracking the lower bound, use
pessimistic estimate of the feasible set at each step. We show that these
algorithms achieve asymptotically optimal sample complexity upper bounds up to
constraint-dependent constants. Finally, we conduct numerical experiments with
different reward distributions and constraints that validate efficient
performance of LAGEX and LATS with respect to baselines.",stat.ML
High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws,"A growing number of machine learning scenarios rely on knowledge distillation
where one uses the output of a surrogate model as labels to supervise the
training of a target model. In this work, we provide a sharp characterization
of this process for ridgeless, high-dimensional regression, under two settings:
(i) model shift, where the surrogate model is arbitrary, and (ii) distribution
shift, where the surrogate model is the solution of empirical risk minimization
with out-of-distribution data. In both cases, we characterize the precise risk
of the target model through non-asymptotic bounds in terms of sample size and
data distribution under mild conditions. As a consequence, we identify the form
of the optimal surrogate model, which reveals the benefits and limitations of
discarding weak features in a data-dependent fashion. In the context of
weak-to-strong (W2S) generalization, this has the interpretation that (i) W2S
training, with the surrogate as the weak model, can provably outperform
training with strong labels under the same data budget, but (ii) it is unable
to improve the data scaling law. We validate our results on numerical
experiments both on ridgeless regression and on neural network architectures.",stat.ML
Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality,"The denoising diffusion probabilistic model (DDPM) has emerged as a
mainstream generative model in generative AI. While sharp convergence
guarantees have been established for the DDPM, the iteration complexity is, in
general, proportional to the ambient data dimension, resulting in overly
conservative theory that fails to explain its practical efficiency. This has
motivated the recent work Li and Yan (2024a) to investigate how the DDPM can
achieve sampling speed-ups through automatic exploitation of intrinsic low
dimensionality of data. We strengthen this line of work by demonstrating, in
some sense, optimal adaptivity to unknown low dimensionality. For a broad class
of data distributions with intrinsic dimension $k$, we prove that the iteration
complexity of the DDPM scales nearly linearly with $k$, which is optimal when
using KL divergence to measure distributional discrepancy. Notably, our work is
closely aligned with the independent concurrent work Potaptchik et al. (2024)
-- posted two weeks prior to ours -- in establishing nearly linear-$k$
convergence guarantees for the DDPM.",stat.ML
Rethinking Softmax: Self-Attention with Polynomial Activations,"This paper challenges the conventional belief that softmax attention in
transformers is effective primarily because it generates a probability
distribution for attention allocation. Instead, we theoretically show that its
success lies in its ability to implicitly regularize the Frobenius norm of the
attention matrix during training. We then explore alternative activations that
regularize the Frobenius norm of the attention matrix, demonstrating that
certain polynomial activations can achieve this effect, making them suitable
for attention-based architectures. Empirical results indicate these activations
perform comparably or better than softmax across various computer vision and
language tasks, suggesting new possibilities for attention mechanisms beyond
softmax.",stat.ML
Heterogeneous Random Forest,"Random forest (RF) stands out as a highly favored machine learning approach
for classification problems. The effectiveness of RF hinges on two key factors:
the accuracy of individual trees and the diversity among them. In this study,
we introduce a novel approach called heterogeneous RF (HRF), designed to
enhance tree diversity in a meaningful way. This diversification is achieved by
deliberately introducing heterogeneity during the tree construction.
Specifically, features used for splitting near the root node of previous trees
are assigned lower weights when constructing the feature sub-space of the
subsequent trees. As a result, dominant features in the prior trees are less
likely to be employed in the next iteration, leading to a more diverse set of
splitting features at the nodes. Through simulation studies, it was confirmed
that the HRF method effectively mitigates the selection bias of trees within
the ensemble, increases the diversity of the ensemble, and demonstrates
superior performance on datasets with fewer noise features. To assess the
comparative performance of HRF against other widely adopted ensemble methods,
we conducted tests on 52 datasets, comprising both real-world and synthetic
data. HRF consistently outperformed other ensemble methods in terms of accuracy
across the majority of datasets.",stat.ML
Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy,"Local Differential Privacy (LDP) offers strong privacy guarantees without
requiring users to trust external parties. However, LDP applies uniform
protection to all data features, including less sensitive ones, which degrades
performance of downstream tasks. To overcome this limitation, we propose a
Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that
enables feature-specific privacy quantification. This more nuanced approach
complements LDP by adjusting privacy protection according to the sensitivity of
each feature, enabling improved performance of downstream tasks without
compromising privacy. We characterize the properties of BCDP and articulate its
connections with standard non-Bayesian privacy frameworks. We further apply our
BCDP framework to the problems of private mean estimation and ordinary
least-squares regression. The BCDP-based approach obtains improved accuracy
compared to a purely LDP-based approach, without compromising on privacy.",stat.ML
Revisiting Differentiable Structure Learning: Inconsistency of $\ell_1$ Penalty and Beyond,"Recent advances in differentiable structure learning have framed the
combinatorial problem of learning directed acyclic graphs as a continuous
optimization problem. Various aspects, including data standardization, have
been studied to identify factors that influence the empirical performance of
these methods. In this work, we investigate critical limitations in
differentiable structure learning methods, focusing on settings where the true
structure can be identified up to Markov equivalence classes, particularly in
the linear Gaussian case. While Ng et al. (2024) highlighted potential
non-convexity issues in this setting, we demonstrate and explain why the use of
$\ell_1$-penalized likelihood in such cases is fundamentally inconsistent, even
if the global optimum of the optimization problem can be found. To resolve this
limitation, we develop a hybrid differentiable structure learning method based
on $\ell_0$-penalized likelihood with hard acyclicity constraint, where the
$\ell_0$ penalty can be approximated by different techniques including
Gumbel-Softmax. Specifically, we first estimate the underlying moral graph, and
use it to restrict the search space of the optimization problem, which helps
alleviate the non-convexity issue. Experimental results show that the proposed
method enhances empirical performance both before and after data
standardization, providing a more reliable path for future advancements in
differentiable structure learning, especially for learning Markov equivalence
classes.",stat.ML
Causal Order Discovery based on Monotonic SCMs,"In this paper, we consider the problem of causal order discovery within the
framework of monotonic Structural Causal Models (SCMs), which have gained
attention for their potential to enable causal inference and causal discovery
from observational data. While existing approaches either assume prior
knowledge about the causal order or use complex optimization techniques to
impose sparsity in the Jacobian of Triangular Monotonic Increasing maps, our
work introduces a novel sequential procedure that directly identifies the
causal order by iteratively detecting the root variable. This method eliminates
the need for sparsity assumptions and the associated optimization challenges,
enabling the identification of a unique SCM without the need for multiple
independence tests to break the Markov equivalence class. We demonstrate the
effectiveness of our approach in sequentially finding the root variable,
comparing it to methods that maximize Jacobian sparsity.",stat.ML
Calibrating Deep Neural Network using Euclidean Distance,"Uncertainty is a fundamental aspect of real-world scenarios, where perfect
information is rarely available. Humans naturally develop complex internal
models to navigate incomplete data and effectively respond to unforeseen or
partially observed events. In machine learning, Focal Loss is commonly used to
reduce misclassification rates by emphasizing hard-to-classify samples.
However, it does not guarantee well-calibrated predicted probabilities and may
result in models that are overconfident or underconfident. High calibration
error indicates a misalignment between predicted probabilities and actual
outcomes, affecting model reliability. This research introduces a novel loss
function called Focal Calibration Loss (FCL), designed to improve probability
calibration while retaining the advantages of Focal Loss in handling difficult
samples. By minimizing the Euclidean norm through a strictly proper loss, FCL
penalizes the instance-wise calibration error and constrains bounds. We provide
theoretical validation for proposed method and apply it to calibrate CheXNet
for potential deployment in web-based health-care systems. Extensive
evaluations on various models and datasets demonstrate that our method achieves
SOTA performance in both calibration and accuracy metrics.",stat.ML
Stabilizing black-box model selection with the inflated argmax,"Model selection is the process of choosing from a class of candidate models
given data. For instance, methods such as the LASSO and sparse identification
of nonlinear dynamics (SINDy) formulate model selection as finding a sparse
solution to a linear system of equations determined by training data. However,
absent strong assumptions, such methods are highly unstable: if a single data
point is removed from the training set, a different model may be selected. This
paper presents a new approach to stabilizing model selection that leverages a
combination of bagging and an ""inflated"" argmax operation. Our method selects a
small collection of models that all fit the data, and it is stable in that,
with high probability, the removal of any training point will result in a
collection of selected models that overlaps with the original collection. In
addition to developing theoretical guarantees, we illustrate this method in (a)
a simulation in which strongly correlated covariates make standard LASSO model
selection highly unstable and (b) a Lotka-Volterra model selection problem
focused on identifying how competition in an ecosystem influences species'
abundances. In both settings, the proposed method yields stable and compact
collections of selected models, outperforming a variety of benchmarks.",stat.ML
TabDPT: Scaling Tabular Foundation Models,"The challenges faced by neural networks on tabular data are well-documented
and have hampered the progress of tabular foundation models. Techniques
leveraging in-context learning (ICL) have shown promise here, allowing for
dynamic adaptation to unseen data. ICL can provide predictions for entirely new
datasets without further training or hyperparameter tuning, therefore providing
very fast inference when encountering a novel task. However, scaling ICL for
tabular data remains an issue: approaches based on large language models cannot
efficiently process numeric tables, and tabular-specific techniques have not
been able to effectively harness the power of real data to improve performance
and generalization. We are able to overcome these challenges by training
tabular-specific ICL-based architectures on real data with self-supervised
learning and retrieval, combining the best of both worlds. Our resulting model
-- the Tabular Discriminative Pre-trained Transformer (TabDPT) -- achieves
state-of-the-art performance on the CC18 (classification) and CTR23
(regression) benchmarks with no task-specific fine-tuning, demonstrating the
adapatability and speed of ICL once the model is pre-trained. TabDPT also
demonstrates strong scaling as both model size and amount of available data
increase, pointing towards future improvements simply through the curation of
larger tabular pre-training datasets and training larger models.",stat.ML
Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration,"Unsupervised pretraining has been transformative in many supervised domains.
However, applying such ideas to reinforcement learning (RL) presents a unique
challenge in that fine-tuning does not involve mimicking task-specific data,
but rather exploring and locating the solution through iterative
self-improvement. In this work, we study how unlabeled prior trajectory data
can be leveraged to learn efficient exploration strategies. While prior data
can be used to pretrain a set of low-level skills, or as additional off-policy
data for online RL, it has been unclear how to combine these ideas effectively
for online exploration. Our method SUPE (Skills from Unlabeled Prior data for
Exploration) demonstrates that a careful combination of these ideas compounds
their benefits. Our method first extracts low-level skills using a variational
autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an
optimistic reward model, transforming prior data into high-level, task-relevant
examples. Finally, SUPE uses these transformed examples as additional
off-policy data for online RL to learn a high-level policy that composes
pretrained low-level skills to explore efficiently. We empirically show that
SUPE reliably outperforms prior strategies, successfully solving a suite of
long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",stat.ML
Stochastic gradient descent in high dimensions for multi-spiked tensor PCA,"We study the dynamics in high dimensions of online stochastic gradient
descent for the multi-spiked tensor model. This multi-index model arises from
the tensor principal component analysis (PCA) problem with multiple spikes,
where the goal is to estimate $r$ unknown signal vectors within the
$N$-dimensional unit sphere through maximum likelihood estimation from noisy
observations of a $p$-tensor. We determine the number of samples and the
conditions on the signal-to-noise ratios (SNRs) required to efficiently recover
the unknown spikes from natural random initializations. We show that full
recovery of all spikes is possible provided a number of sample scaling as
$N^{p-2}$, matching the algorithmic threshold identified in the rank-one case
[Ben Arous, Gheissari, Jagannath 2020, 2021]. Our results are obtained through
a detailed analysis of a low-dimensional system that describes the evolution of
the correlations between the estimators and the spikes, while controlling the
noise in the dynamics. We find that the spikes are recovered sequentially in a
process we term ""sequential elimination"": once a correlation exceeds a critical
threshold, all correlations sharing a row or column index become sufficiently
small, allowing the next correlation to grow and become macroscopic. The order
in which correlations become macroscopic depends on their initial values and
the corresponding SNRs, leading to either exact recovery or recovery of a
permutation of the spikes. In the matrix case, when $p=2$, if the SNRs are
sufficiently separated, we achieve exact recovery of the spikes, whereas equal
SNRs lead to recovery of the subspace spanned by the spikes.",stat.ML
Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices,"Analyzing the structure of sampled features from an input data distribution
is challenging when constrained by limited measurements in both the number of
inputs and features. Traditional approaches often rely on the eigenvalue
spectrum of the sample covariance matrix derived from finite measurement
matrices; however, these spectra are sensitive to the size of the measurement
matrix, leading to biased insights. In this paper, we introduce a novel
algorithm that provides unbiased estimates of the spectral moments of the
kernel integral operator in the limit of infinite inputs and features from
finitely sampled measurement matrices. Our method, based on dynamic
programming, is efficient and capable of estimating the moments of the operator
spectrum. We demonstrate the accuracy of our estimator on radial basis function
(RBF) kernels, highlighting its consistency with the theoretical spectra.
Furthermore, we showcase the practical utility and robustness of our method in
understanding the geometry of learned representations in neural networks.",stat.ML
Semi-Implicit Functional Gradient Flow,"Particle-based variational inference methods (ParVIs) use non-parametric
variational families represented by particles to approximate the target
distribution according to the kernelized Wasserstein gradient flow for the
Kullback-Leibler (KL) divergence. Recent works introduce functional gradient
flows to substitute the kernel for better flexibility. However, the
deterministic updating mechanism may suffer from limited exploration and
require expensive repetitive runs for new samples. In this paper, we propose
Semi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI
method that uses perturbed particles as the approximation family. The
corresponding functional gradient flow, which can be estimated via denoising
score matching, exhibits strong theoretical convergence guarantee. We also
present an adaptive version of our method to automatically choose the suitable
noise magnitude. Extensive experiments demonstrate the effectiveness and
efficiency of the proposed framework on both simulated and real data problems.",stat.ML
Deep learning for model correction of dynamical systems with data scarcity,"We present a deep learning framework for correcting existing dynamical system
models utilizing only a scarce high-fidelity data set. In many practical
situations, one has a low-fidelity model that can capture the dynamics
reasonably well but lacks high resolution, due to the inherent limitation of
the model and the complexity of the underlying physics. When high resolution
data become available, it is natural to seek model correction to improve the
resolution of the model predictions. We focus on the case when the amount of
high-fidelity data is so small that most of the existing data driven modeling
methods cannot be applied. In this paper, we address these challenges with a
model-correction method which only requires a scarce high-fidelity data set.
Our method first seeks a deep neural network (DNN) model to approximate the
existing low-fidelity model. By using the scarce high-fidelity data, the method
then corrects the DNN model via transfer learning (TL). After TL, an improved
DNN model with high prediction accuracy to the underlying dynamics is obtained.
One distinct feature of the propose method is that it does not assume a
specific form of the model correction terms. Instead, it offers an inherent
correction to the low-fidelity model via TL. A set of numerical examples are
presented to demonstrate the effectiveness of the proposed method.",stat.ML
Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity,"Real-world applications of reinforcement learning often involve environments
where agents operate on complex, high-dimensional observations, but the
underlying (''latent'') dynamics are comparatively simple. However, outside of
restrictive settings such as small latent spaces, the fundamental statistical
requirements and algorithmic principles for reinforcement learning under latent
dynamics are poorly understood.
  This paper addresses the question of reinforcement learning under
$\textit{general}$ latent dynamics from a statistical and algorithmic
perspective. On the statistical side, our main negative result shows that most
well-studied settings for reinforcement learning with function approximation
become intractable when composed with rich observations; we complement this
with a positive result, identifying latent pushforward coverability as a
general condition that enables statistical tractability. Algorithmically, we
develop provably efficient observable-to-latent reductions -- that is,
reductions that transform an arbitrary algorithm for the latent MDP into an
algorithm that can operate on rich observations -- in two settings: one where
the agent has access to hindsight observations of the latent dynamics [LADZ23],
and one where the agent can estimate self-predictive latent models [SAGHCB20].
Together, our results serve as a first step toward a unified statistical and
algorithmic theory for reinforcement learning under latent dynamics.",stat.ML
Identifiable Representation and Model Learning for Latent Dynamic Systems,"Learning identifiable representations and models from low-level observations
is useful for an intelligent spacecraft to reliability finish downstream tasks.
For temporal observations, to ensure that the data generating process is
provably inverted, most existing works either assume the noise variables in the
dynamic mechanisms are (conditionally) independent, or require interventions
which can directly affect each latent variable. However, in practice, the
relationship between the exogenous inputs/interventions and the latent
variables may follow some complex deterministic mechanisms. In this work, we
study the problem of identifiable representation and model learning for latent
dynamic systems. The key idea is that we use an inductive bias inspired by
controllable canonical forms, which is invariant, sparse, and input dependent
by definition. We prove that, for linear or affine nonlinear latent dynamic
systems, it is possible to identify the representations up to scaling and
determine the models up to some simple transformations. The results have
potential to provide some theoretical guarantees for developing more
trustworthy decision-making and control methods for intelligent spacecrafts.",stat.ML
Fast and interpretable electricity consumption scenario generation for individual consumers,"To enable the transition from fossil fuels towards renewable energy, the
low-voltage grid needs to be reinforced at a faster pace and on a larger scale
than was historically the case. To efficiently plan reinforcements, one needs
to estimate the currents and voltages throughout the grid, which are unknown
but can be calculated from the grid layout and the electricity consumption time
series of each consumer. However, for many consumers, these time series are
unknown and have to be estimated from the available consumer information. We
refer to this task as scenario generation. The state-of-the-art approach that
generates electricity consumption scenarios is complex, resulting in a
computationally expensive procedure with only limited interpretability. To
alleviate these drawbacks, we propose a fast and interpretable scenario
generation technique based on predictive clustering trees (PCTs) that does not
compromise accuracy. In our experiments on three datasets from different
locations, we found that our proposed approach generates time series that are
at least as accurate as the state-of-the-art while being at least 7 times
faster in training and prediction. Moreover, the interpretability of the PCT
allows domain experts to gain insight into their data while simultaneously
building trust in the predictions of the model.",stat.ML
Ranking of Multi-Response Experiment Treatments,"We present a probabilistic ranking model to identify the optimal treatment in
multiple-response experiments. In contemporary practice, treatments are applied
over individuals with the goal of achieving multiple ideal properties on them
simultaneously. However, often there are competing properties, and the
optimality of one cannot be achieved without compromising the optimality of
another. Typically, we still want to know which treatment is the overall best.
In our framework, we first formulate overall optimality in terms of treatment
ranks. Then we infer the latent ranking that allow us to report treatments from
optimal to least optimal, provided ideal desirable properties. We demonstrate
through simulations and real data analysis how we can achieve reliability of
inferred ranks in practice. We adopt a Bayesian approach and derive an
associated Markov Chain Monte Carlo algorithm to fit our model to data.
Finally, we discuss the prospects of adoption of our method as a standard tool
for experiment evaluation in trials-based research.",stat.ML
Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees,"We propose the first learning scheme for functional differential equations
(FDEs). FDEs play a fundamental role in physics, mathematics, and optimal
control. However, the numerical analysis of FDEs has faced challenges due to
its unrealistic computational costs and has been a long standing problem over
decades. Thus, numerical approximations of FDEs have been developed, but they
often oversimplify the solutions. To tackle these two issues, we propose a
hybrid approach combining physics-informed neural networks (PINNs) with the
\textit{cylindrical approximation}. The cylindrical approximation expands
functions and functional derivatives with an orthonormal basis and transforms
FDEs into high-dimensional PDEs. To validate the reliability of the cylindrical
approximation for FDE applications, we prove the convergence theorems of
approximated functional derivatives and solutions. Then, the derived
high-dimensional PDEs are numerically solved with PINNs. Through the
capabilities of PINNs, our approach can handle a broader class of functional
derivatives more efficiently than conventional discretization-based methods,
improving the scalability of the cylindrical approximation. As a proof of
concept, we conduct experiments on two FDEs and demonstrate that our model can
successfully achieve typical $L^1$ relative error orders of PINNs $\sim
10^{-3}$. Overall, our work provides a strong backbone for physicists,
mathematicians, and machine learning experts to analyze previously challenging
FDEs, thereby democratizing their numerical analysis, which has received
limited attention. Code is available at
\url{https://github.com/TaikiMiyagawa/FunctionalPINN}.",stat.ML
Deep Autoencoder with SVD-Like Convergence and Flat Minima,"Representation learning for high-dimensional, complex physical systems aims
to identify a low-dimensional intrinsic latent space, which is crucial for
reduced-order modeling and modal analysis. To overcome the well-known
Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent
years, but they often suffer from poor convergence behavior as the rank of the
latent space increases. To address this issue, we propose the learnable
weighted hybrid autoencoder, a hybrid approach that combines the strengths of
singular value decomposition (SVD) with deep autoencoders through a learnable
weighted framework. We find that the introduction of learnable weighting
parameters is essential - without them, the resulting model would either
collapse into a standard POD or fail to exhibit the desired convergence
behavior. Additionally, we empirically find that our trained model has a
sharpness thousands of times smaller compared to other models. Our experiments
on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and
forced isotropic turbulence datasets, demonstrate that our approach
significantly improves generalization performance compared to several competing
methods, paving the way for robust representation learning of high-dimensional,
complex physical systems.",stat.ML
MEC-IP: Efficient Discovery of Markov Equivalent Classes via Integer Programming,"This paper presents a novel Integer Programming (IP) approach for discovering
the Markov Equivalent Class (MEC) of Bayesian Networks (BNs) through
observational data. The MEC-IP algorithm utilizes a unique clique-focusing
strategy and Extended Maximal Spanning Graphs (EMSG) to streamline the search
for MEC, thus overcoming the computational limitations inherent in other
existing algorithms. Our numerical results show that not only a remarkable
reduction in computational time is achieved by our algorithm but also an
improvement in causal discovery accuracy is seen across diverse datasets. These
findings underscore this new algorithm's potential as a powerful tool for
researchers and practitioners in causal discovery and BNSL, offering a
significant leap forward toward the efficient and accurate analysis of complex
data structures.",stat.ML
Scalable Implicit Graphon Learning,"Graphons are continuous models that represent the structure of graphs and
allow the generation of graphs of varying sizes. We propose Scalable Implicit
Graphon Learning (SIGL), a scalable method that combines implicit neural
representations (INRs) and graph neural networks (GNNs) to estimate a graphon
from observed graphs. Unlike existing methods, which face important limitations
like fixed resolution and scalability issues, SIGL learns a continuous graphon
at arbitrary resolutions. GNNs are used to determine the correct node ordering,
improving graph alignment. Furthermore, we characterize the asymptotic
consistency of our estimator, showing that more expressive INRs and GNNs lead
to consistent estimators. We evaluate SIGL in synthetic and real-world graphs,
showing that it outperforms existing methods and scales effectively to larger
graphs, making it ideal for tasks like graph data augmentation.",stat.ML
Learning Graph Filters for Structure-Function Coupling based Hub Node Identification,"Over the past two decades, tools from network science have been leveraged to
characterize the organization of both structural and functional networks of the
brain. One such measure of network organization is hub node identification.
Hubs are specialized nodes within a network that link distinct brain units
corresponding to specialized functional processes. Conventional methods for
identifying hub nodes utilize different types of centrality measures and
participation coefficient to profile various aspects of nodal importance. These
methods solely rely on the functional connectivity networks constructed from
functional magnetic resonance imaging (fMRI), ignoring the structure-function
coupling in the brain. In this paper, we introduce a graph signal processing
(GSP) based hub detection framework that utilizes both the structural
connectivity and the functional activation to identify hub nodes. The proposed
framework models functional activity as graph signals on the structural
connectivity. Hub nodes are then detected based on the premise that hub nodes
are sparse, have higher level of activity compared to their neighbors, and the
non-hub nodes' activity can be modeled as the output of a graph-based filter.
Based on these assumptions, an optimization framework, GraFHub, is formulated
to learn the coefficients of the optimal polynomial graph filter and detect the
hub nodes. The proposed framework is evaluated on both simulated data and
resting state fMRI (rs-fMRI) data from Human Connectome Project (HCP).",stat.ML
Cooperative Multi-Agent Constrained Stochastic Linear Bandits,"In this study, we explore a collaborative multi-agent stochastic linear
bandit setting involving a network of $N$ agents that communicate locally to
minimize their collective regret while keeping their expected cost under a
specified threshold $\tau$. Each agent encounters a distinct linear bandit
problem characterized by its own reward and cost parameters, i.e., local
parameters. The goal of the agents is to determine the best overall action
corresponding to the average of these parameters, or so-called global
parameters. In each round, an agent is randomly chosen to select an action
based on its current knowledge of the system. This chosen action is then
executed by all agents, then they observe their individual rewards and costs.
We propose a safe distributed upper confidence bound algorithm, so called
\textit{MA-OPLB}, and establish a high probability bound on its $T$-round
regret. MA-OPLB utilizes an accelerated consensus method, where agents can
compute an estimate of the average rewards and costs across the network by
communicating the proper information with their neighbors. We show that our
regret bound is of order $
\mathcal{O}\left(\frac{d}{\tau-c_0}\frac{\log(NT)^2}{\sqrt{N}}\sqrt{\frac{T}{\log(1/|\lambda_2|)}}\right)$,
where $\lambda_2$ is the second largest (in absolute value) eigenvalue of the
communication matrix, and $\tau-c_0$ is the known cost gap of a feasible
action. We also experimentally show the performance of our proposed algorithm
in different network structures.",stat.ML
Computing Optimal Regularizers for Online Linear Optimization,"Follow-the-Regularized-Leader (FTRL) algorithms are a popular class of
learning algorithms for online linear optimization (OLO) that guarantee
sub-linear regret, but the choice of regularizer can significantly impact
dimension-dependent factors in the regret bound. We present an algorithm that
takes as input convex and symmetric action sets and loss sets for a specific
OLO instance, and outputs a regularizer such that running FTRL with this
regularizer guarantees regret within a universal constant factor of the best
possible regret bound. In particular, for any choice of (convex, symmetric)
action set and loss set we prove that there exists an instantiation of FTRL
which achieves regret within a constant factor of the best possible learning
algorithm, strengthening the universality result of Srebro et al., 2011.
  Our algorithm requires preprocessing time and space exponential in the
dimension $d$ of the OLO instance, but can be run efficiently online assuming a
membership and linear optimization oracle for the action and loss sets,
respectively (and is fully polynomial time for the case of constant dimension
$d$). We complement this with a lower bound showing that even deciding whether
a given regularizer is $\alpha$-strongly-convex with respect to a given norm is
NP-hard.",stat.ML
Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error,"Algorithmic robust statistics has traditionally focused on the contamination
model where a small fraction of the samples are arbitrarily corrupted. We
consider a recent contamination model that combines two kinds of corruptions:
(i) small fraction of arbitrary outliers, as in classical robust statistics,
and (ii) local perturbations, where samples may undergo bounded shifts on
average. While each noise model is well understood individually, the combined
contamination model poses new algorithmic challenges, with only partial results
known. Existing efficient algorithms are limited in two ways: (i) they work
only for a weak notion of local perturbations, and (ii) they obtain suboptimal
error for isotropic subgaussian distributions (among others). The latter
limitation led [NGS24, COLT'24] to hypothesize that improving the error might,
in fact, be computationally hard. Perhaps surprisingly, we show that
information theoretically optimal error can indeed be achieved in polynomial
time, under an even \emph{stronger} local perturbation model (the
sliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our
analysis reveals that the entire family of stability-based robust mean
estimators continues to work optimally in a black-box manner for the combined
contamination model. This generalization is particularly useful in real-world
scenarios where the specific form of data corruption is not known in advance.
We also present efficient algorithms for distribution learning and principal
component analysis in the combined contamination model.",stat.ML
Covariance estimation using Markov chain Monte Carlo,"We investigate the complexity of covariance matrix estimation for Gibbs
distributions based on dependent samples from a Markov chain. We show that when
$\pi$ satisfies a Poincar\'e inequality and the chain possesses a spectral gap,
we can achieve similar sample complexity using MCMC as compared to an estimator
constructed using i.i.d. samples, with potentially much better query
complexity. As an application of our methods, we show improvements for the
query complexity in both constrained and unconstrained settings for concrete
instances of MCMC. In particular, we provide guarantees regarding isotropic
rounding procedures for sampling uniformly on convex bodies.",stat.ML
Understanding Transfer Learning via Mean-field Analysis,"We propose a novel framework for exploring generalization errors of transfer
learning through the lens of differential calculus on the space of probability
measures. In particular, we consider two main transfer learning scenarios,
$\alpha$-ERM and fine-tuning with the KL-regularized empirical risk
minimization and establish generic conditions under which the generalization
error and the population risk convergence rates for these scenarios are
studied. Based on our theoretical results, we show the benefits of transfer
learning with a one-hidden-layer neural network in the mean-field regime under
some suitable integrability and regularity assumptions on the loss and
activation functions.",stat.ML
Optimal Design for Reward Modeling in RLHF,"Reinforcement Learning from Human Feedback (RLHF) has become a popular
approach to align language models (LMs) with human preferences. This method
involves collecting a large dataset of human pairwise preferences across
various text generations and using it to infer (implicitly or explicitly) a
reward model. Numerous methods have been proposed to learn the reward model and
align a LM with it. However, the costly process of collecting human preferences
has received little attention and could benefit from theoretical insights. This
paper addresses this issue and aims to formalize the reward training model in
RLHF. We frame the selection of an effective dataset as a simple regret
minimization task, using a linear contextual dueling bandit method. Given the
potentially large number of arms, this approach is more coherent than the
best-arm identification setting. We then propose an offline framework for
solving this problem. Under appropriate assumptions - linearity of the reward
model in the embedding space, and boundedness of the reward parameter - we
derive bounds on the simple regret. Finally, we provide a lower bound that
matches our upper bound up to constant and logarithmic terms. To our knowledge,
this is the first theoretical contribution in this area to provide an offline
approach as well as worst-case guarantees.",stat.ML
Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections,"Bayesian deep learning all too often underfits so that the Bayesian
prediction is less accurate than a simple point estimate. Uncertainty
quantification then comes at the cost of accuracy. For linearized models, the
null space of the generalized Gauss-Newton matrix corresponds to parameters
that preserve the training predictions of the point estimate. We propose to
build Bayesian approximations in this null space, thereby guaranteeing that the
Bayesian predictive does not underfit. We suggest a matrix-free algorithm for
projecting onto this null space, which scales linearly with the number of
parameters and quadratically with the number of output dimensions. We further
propose an approximation that only scales linearly with parameters to make the
method applicable to generative models. An extensive empirical evaluation shows
that the approach scales to large models, including vision transformers with 28
million parameters.",stat.ML
Federated Causal Inference: Multi-Centric ATE Estimation beyond Meta-Analysis,"We study Federated Causal Inference, an approach to estimate treatment
effects from decentralized data across centers. We compare three classes of
Average Treatment Effect (ATE) estimators derived from the Plug-in G-Formula,
ranging from simple meta-analysis to one-shot and multi-shot federated
learning, the latter leveraging the full data to learn the outcome model
(albeit requiring more communication). Focusing on Randomized Controlled Trials
(RCTs), we derive the asymptotic variance of these estimators for linear
models. Our results provide practical guidance on selecting the appropriate
estimator for various scenarios, including heterogeneity in sample sizes,
covariate distributions, treatment assignment schemes, and center effects. We
validate these findings with a simulation study.",stat.ML
Klein Model for Hyperbolic Neural Networks,"Hyperbolic neural networks (HNNs) have been proved effective in modeling
complex data structures. However, previous works mainly focused on the
Poincar\'e ball model and the hyperboloid model as coordinate representations
of the hyperbolic space, often neglecting the Klein model. Despite this, the
Klein model offers its distinct advantages thanks to its straight-line
geodesics, which facilitates the well-known Einstein midpoint construction,
previously leveraged to accompany HNNs in other models. In this work, we
introduce a framework for hyperbolic neural networks based on the Klein model.
We provide detailed formulation for representing useful operations using the
Klein model. We further study the Klein linear layer and prove that the
""tangent space construction"" of the scalar multiplication and parallel
transport are exactly the Einstein scalar multiplication and the Einstein
addition, analogous to the M\""obius operations used in the Poincar\'e ball
model. We show numerically that the Klein HNN performs on par with the
Poincar\'e ball model, providing a third option for HNN that works as a
building block for more complicated architectures.",stat.ML
Error estimates between SGD with momentum and underdamped Langevin diffusion,"Stochastic gradient descent with momentum is a popular variant of stochastic
gradient descent, which has recently been reported to have a close relationship
with the underdamped Langevin diffusion. In this paper, we establish a
quantitative error estimate between them in the 1-Wasserstein and total
variation distances.",stat.ML
Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks,"When dealing with right-censored data, where some outcomes are missing due to
a limited observation period, survival analysis -- known as time-to-event
analysis -- focuses on predicting the time until an event of interest occurs.
Multiple classes of outcomes lead to a classification variant: predicting the
most likely event, a less explored area known as competing risks. Classic
competing risks models couple architecture and loss, limiting scalability.To
address these issues, we design a strictly proper censoring-adjusted separable
scoring rule, allowing optimization on a subset of the data as each observation
is evaluated independently. The loss estimates outcome probabilities and
enables stochastic optimization for competing risks, which we use for efficient
gradient boosting trees. SurvivalBoost not only outperforms 12 state-of-the-art
models across several metrics on 4 real-life datasets, both in competing risks
and survival settings, but also provides great calibration, the ability to
predict across any time horizon, and computation times faster than existing
methods.",stat.ML
Theoretical Convergence Guarantees for Variational Autoencoders,"Variational Autoencoders (VAE) are popular generative models used to sample
from complex data distributions. Despite their empirical success in various
machine learning tasks, significant gaps remain in understanding their
theoretical properties, particularly regarding convergence guarantees. This
paper aims to bridge that gap by providing non-asymptotic convergence
guarantees for VAE trained using both Stochastic Gradient Descent and Adam
algorithms.We derive a convergence rate of $\mathcal{O}(\log n / \sqrt{n})$,
where $n$ is the number of iterations of the optimization algorithm, with
explicit dependencies on the batch size, the number of variational samples, and
other key hyperparameters. Our theoretical analysis applies to both Linear VAE
and Deep Gaussian VAE, as well as several VAE variants, including $\beta$-VAE
and IWAE. Additionally, we empirically illustrate the impact of hyperparameters
on convergence, offering new insights into the theoretical understanding of VAE
training.",stat.ML
Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors,"In our paper,we focus on robust variable selection for missing data and
measurement error.Missing data and measurement errors can lead to confusing
data distribution.We propose an exponential loss function with tuning parameter
to apply to Missing and measurement errors data.By adjusting the parameter,the
loss functioncan be better and more robust under various different data
distributions.We use inverse probability weighting and additivityerrormodels to
address missing data and measurement errors.Also,we find that the Atan
punishment method works better.We used Monte Carlo simulations to assess the
validity of robust variable selection and validated our findings with the
breast cancer dataset",stat.ML
A class of modular and flexible covariate-based covariance functions for nonstationary spatial modeling,"The assumptions of stationarity and isotropy often stated over spatial
processes have not aged well during the last two decades, partly explained by
the combination of computational developments and the increasing availability
of high-resolution spatial data. While a plethora of approaches have been
developed to relax these assumptions, it is often a costly tradeoff between
flexibility and a diversity of computational challenges. In this paper, we
present a class of covariance functions that relies on fixed, observable
spatial information that provides a convenient tradeoff while offering an extra
layer of numerical and visual representation of the flexible spatial
dependencies. This model allows for separate parametric structures for
different sources of nonstationarity, such as marginal standard deviation,
geometric anisotropy, and smoothness. It simplifies to a Mat\'ern covariance
function in its basic form and is adaptable for large datasets, enhancing
flexibility and computational efficiency. We analyze the capabilities of the
presented model through simulation studies and an application to Swiss
precipitation data.",stat.ML
Universal approximation property of ODENet and ResNet with a single activation function,"We study a universal approximation property of ODENet and ResNet. The ODENet
is a map from an initial value to the final value of an ODE system in a finite
interval. It is considered a mathematical model of a ResNet-type deep learning
system. We consider dynamical systems with vector fields given by a single
composition of the activation function and an affine mapping, which is the most
common choice of the ODENet or ResNet vector field in actual machine learning
systems. We show that such an ODENet and ResNet with a restricted vector field
can uniformly approximate ODENet with a general vector field.",stat.ML
Lower Bounds for Time-Varying Kernelized Bandits,"The optimization of black-box functions with noisy observations is a
fundamental problem with widespread applications, and has been widely studied
under the assumption that the function lies in a reproducing kernel Hilbert
space (RKHS). This problem has been studied extensively in the stationary
setting, and near-optimal regret bounds are known via developments in both
upper and lower bounds. In this paper, we consider non-stationary scenarios,
which are crucial for certain applications but are currently less
well-understood. Specifically, we provide the first algorithm-independent lower
bounds, where the time variations are subject satisfying a total variation
budget according to some function norm. Under $\ell_{\infty}$-norm variations,
our bounds are found to be close to the state-of-the-art upper bound (Hong
\emph{et al.}, 2023). Under RKHS norm variations, the upper and lower bounds
are still reasonably close but with more of a gap, raising the interesting open
question of whether non-minor improvements in the upper bound are possible.",stat.ML
Parsimonious Dynamic Mode Decomposition: A Robust and Automated Approach for Optimally Sparse Mode Selection in Complex Systems,"This paper introduces the Parsimonious Dynamic Mode Decomposition (parsDMD),
a novel algorithm designed to automatically select an optimally sparse subset
of dynamic modes for both spatiotemporal and purely temporal data. By
incorporating time-delay embedding and leveraging Orthogonal Matching Pursuit
(OMP), parsDMD ensures robustness against noise and effectively handles
complex, nonlinear dynamics. The algorithm is validated on a diverse range of
datasets, including standing wave signals, identifying hidden dynamics, fluid
dynamics simulations (flow past a cylinder and transonic buffet), and
atmospheric sea-surface temperature (SST) data. ParsDMD addresses a significant
limitation of the traditional sparsity-promoting DMD (spDMD), which requires
manual tuning of sparsity parameters through a rigorous trial-and-error process
to balance between single-mode and all-mode solutions. In contrast, parsDMD
autonomously determines the optimally sparse subset of modes without user
intervention, while maintaining minimal computational complexity. Comparative
analyses demonstrate that parsDMD consistently outperforms spDMD by providing
more accurate mode identification and effective reconstruction in noisy
environments. These advantages render parsDMD an effective tool for real-time
diagnostics, forecasting, and reduced-order model construction across various
disciplines.",stat.ML
