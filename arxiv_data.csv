title,summary,category
Scaling Properties of Diffusion Models for Perceptual Tasks,"In this paper, we argue that iterative computation with diffusion models
offers a powerful paradigm for not only generation but also visual perception
tasks. We unify tasks such as depth estimation, optical flow, and segmentation
under image-to-image translation, and show how diffusion models benefit from
scaling training and test-time compute for these perception tasks. Through a
careful analysis of these scaling behaviors, we present various techniques to
efficiently train diffusion models for visual perception tasks. Our models
achieve improved or comparable performance to state-of-the-art methods using
significantly less data and compute. To use our code and models, see
https://scaling-diffusion-perception.github.io .",cs.AI
GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation,"While 3D content generation has advanced significantly, existing methods
still face challenges with input formats, latent space design, and output
representations. This paper introduces a novel 3D generation framework that
addresses these challenges, offering scalable, high-quality 3D generation with
an interactive Point Cloud-structured Latent space. Our framework employs a
Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)
renderings as input, using a unique latent space design that preserves 3D shape
information, and incorporates a cascaded latent diffusion model for improved
shape-texture disentanglement. The proposed method, GaussianAnything, supports
multi-modal conditional 3D generation, allowing for point cloud, caption, and
single/multi-view image inputs. Notably, the newly proposed latent space
naturally enables geometry-texture disentanglement, thus allowing 3D-aware
editing. Experimental results demonstrate the effectiveness of our approach on
multiple datasets, outperforming existing methods in both text- and
image-conditioned 3D generation.",cs.AI
Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data,"In real-world NLP applications, Large Language Models (LLMs) offer promising
solutions due to their extensive training on vast datasets. However, the large
size and high computation demands of LLMs limit their practicality in many
applications, especially when further fine-tuning is required. To address these
limitations, smaller models are typically preferred for deployment. However,
their training is hindered by the scarcity of labeled data. In contrast,
unlabeled data is often readily which can be leveraged by using LLMs to
generate pseudo-labels for training smaller models. This enables the smaller
models (student) to acquire knowledge from LLMs(teacher) while reducing
computational costs. This process introduces challenges, such as potential
noisy pseudo-labels. Selecting high-quality and informative data is therefore
critical to enhance model performance while improving the efficiency of data
utilization. To address this, we propose LLKD that enables Learning with Less
computational resources and less data for Knowledge Distillation from LLMs.
LLKD is an adaptive sample selection method that incorporates signals from both
the teacher and student. Specifically, it prioritizes samples where the teacher
demonstrates high confidence in its labeling, indicating reliable labels, and
where the student exhibits a high information need, identifying challenging
samples that require further learning. Our comprehensive experiments show that
LLKD achieves superior performance across various datasets with higher data
efficiency.",cs.AI
LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models,"Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.",cs.AI
Leonardo vindicated: Pythagorean trees for minimal reconstruction of the natural branching structures,"Trees continue to fascinate with their natural beauty and as engineering
masterpieces optimal with respect to several independent criteria. Pythagorean
tree is a well-known fractal design that realistically mimics the natural tree
branching structures. We study various types of Pythagorean-like fractal trees
with different shapes of the base, branching angles and relaxed scales in an
attempt to identify and explain which variants are the closest match to the
branching structures commonly observed in the natural world. Pursuing
simultaneously the realism and minimalism of the fractal tree model, we have
developed a flexibly parameterised and fast algorithm to grow and visually
examine deep Pythagorean-inspired fractal trees with the capability to orderly
over- or underestimate the Leonardo da Vinci's tree branching rule as well as
control various imbalances and branching angles. We tested the realism of the
generated fractal tree images by means of the classification accuracy of
detecting natural tree with the transfer-trained deep Convolutional Neural
Networks (CNNs). Having empirically established the parameters of the fractal
trees that maximize the CNN's natural tree class classification accuracy we
have translated them back to the scales and angles of branches and came to the
interesting conclusions that support the da Vinci branching rule and golden
ratio based scaling for both the shape of the branch and imbalance between the
child branches, and claim the flexibly parameterized fractal trees can be used
to generate artificial examples to train robust detectors of different species
of trees.",cs.AI
Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.",cs.AI
Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings,"Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.",cs.AI
Investigating the Effectiveness of Explainability Methods in Parkinson's Detection from Speech,"Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.",cs.AI
ExpressivityArena: Can LLMs Express Information Implicitly?,"While Large Language Models (LLMs) have demonstrated remarkable performance
in certain dimensions, their ability to express implicit language cues that
human use for effective communication remains unclear. This paper presents
ExpressivityArena, a Python library for measuring the implicit communication
abilities of LLMs. We provide a comprehensive framework to evaluate
expressivity of arbitrary LLMs and explore its practical implications. To this
end, we refine the definition and measurements of ``expressivity,'' and use our
framework in a set of small experiments. These experiments test LLMs in
creative and logical tasks such as poetry, coding, and emotion-based responses.
They are then evaluated by an automated grader, through ExpressivityArena,
which we verify to be the most pragmatic for testing expressivity. Building on
these experiments, we deepen our understanding of the expressivity of LLMs by
assessing their ability to remain expressive in conversations. Our findings
indicate that LLMs are capable of generating and understanding expressive
content, however, with some limitations. These insights will inform the future
development and deployment of expressive LLMs. We provide the code for
ExpressivityArena alongside our paper.",cs.AI
Can adversarial attacks by large language models be attributed?,"Attributing outputs from Large Language Models (LLMs) in adversarial
settings-such as cyberattacks and disinformation-presents significant
challenges that are likely to grow in importance. We investigate this
attribution problem using formal language theory, specifically language
identification in the limit as introduced by Gold and extended by Angluin. By
modeling LLM outputs as formal languages, we analyze whether finite text
samples can uniquely pinpoint the originating model. Our results show that due
to the non-identifiability of certain language classes, under some mild
assumptions about overlapping outputs from fine-tuned models it is
theoretically impossible to attribute outputs to specific LLMs with certainty.
This holds also when accounting for expressivity limitations of Transformer
architectures. Even with direct model access or comprehensive monitoring,
significant computational hurdles impede attribution efforts. These findings
highlight an urgent need for proactive measures to mitigate risks posed by
adversarial LLM use as their influence continues to expand.",cs.AI
Derivational Morphology Reveals Analogical Generalization in Large Language Models,"What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.",cs.AI
Gini Coefficient as a Unified Metric for Evaluating Many-versus-Many Similarity in Vector Spaces,"We demonstrate that Gini coefficients can be used as unified metrics to
evaluate many-versus-many (all-to-all) similarity in vector spaces. Our
analysis of various image datasets shows that images with the highest Gini
coefficients tend to be the most similar to one another, while images with the
lowest Gini coefficients are the least similar. We also show that this
relationship holds true for vectorized text embeddings from various corpuses,
highlighting the consistency of our method and its broad applicability across
different types of data. Additionally, we demonstrate that selecting machine
learning training samples that closely match the distribution of the testing
dataset is far more important than ensuring data diversity. Selection of
exemplary and iconic training samples with higher Gini coefficients leads to
significantly better model performance compared to simply having a diverse
training set with lower Gini coefficients. Thus, Gini coefficients can serve as
effective criteria for selecting machine learning training samples, with our
selection method outperforming random sampling methods in very sparse
information settings.",cs.AI
"Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization","Second-order optimization has been shown to accelerate the training of deep
neural networks in many applications, often yielding faster progress per
iteration on the training loss compared to first-order optimizers. However, the
generalization properties of second-order methods are still being debated.
Theoretical investigations have proved difficult to carry out outside the
tractable settings of heavily simplified model classes -- thus, the relevance
of existing theories to practical deep learning applications remains unclear.
Similarly, empirical studies in large-scale models and real datasets are
significantly confounded by the necessity to approximate second-order updates
in practice. It is often unclear whether the observed generalization behaviour
arises specifically from the second-order nature of the parameter updates, or
instead reflects the specific structured (e.g.\ Kronecker) approximations used
or any damping-based interpolation towards first-order updates. Here, we show
for the first time that exact Gauss-Newton (GN) updates take on a tractable
form in a class of deep reversible architectures that are sufficiently
expressive to be meaningfully applied to common benchmark datasets. We exploit
this novel setting to study the training and generalization properties of the
GN optimizer. We find that exact GN generalizes poorly. In the mini-batch
training setting, this manifests as rapidly saturating progress even on the
\emph{training} loss, with parameter updates found to overfit each
mini-batchatch without producing the features that would support generalization
to other mini-batches. We show that our experiments run in the ``lazy'' regime,
in which the neural tangent kernel (NTK) changes very little during the course
of training. This behaviour is associated with having no significant changes in
neural representations, explaining the lack of generalization.",cs.AI
DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring,"Coronary artery disease (CAD), one of the most common cause of mortality in
the world. Coronary artery calcium (CAC) scoring using computed tomography (CT)
is key for risk assessment to prevent coronary disease. Previous studies on
risk assessment and calcification detection in CT scans primarily use
approaches based on UNET architecture, frequently implemented on pre-built
models. However, these models are limited by the availability of annotated CT
scans containing CAC and suffering from imbalanced dataset, decreasing
performance of CAC segmentation and scoring. In this study, we extend this
approach by incorporating the self-supervised learning (SSL) technique of DINO
(self-distillation with no labels) to eliminate limitations of scarce annotated
data in CT scans. The DINO model's ability to train without requiring CAC area
annotations enhances its robustness in generating distinct features. The DINO
model is trained on to focus specifically on calcified areas by using labels,
aiming to generate features that effectively capture and highlight key
characteristics. The label-guided DINO (DINO-LG) enhances classification by
distinguishing CT slices that contain calcification from those that do not,
performing 57% better than the standard DINO model in this task. CAC scoring
and segmentation tasks are performed by a basic U-NET architecture, fed
specifically with CT slices containing calcified areas as identified by the
DINO-LG model. This targeted identification performed by DINO-LG model improves
CAC segmentation performance by approximately 10% and significant increase in
CAC scoring accuracy.",cs.AI
JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation,"We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.",cs.AI
"How To Discover Short, Shorter, and the Shortest Proofs of Unsatisfiability: A Branch-and-Bound Approach for Resolution Proof Length Minimization","Modern software for propositional satisfiability problems gives a powerful
automated reasoning toolkit, capable of outputting not only a
satisfiable/unsatisfiable signal but also a justification of unsatisfiability
in the form of resolution proof (or a more expressive proof), which is commonly
used for verification purposes. Empirically, modern SAT solvers produce
relatively short proofs, however, there are no inherent guarantees that these
proofs cannot be significantly reduced. This paper proposes a novel
branch-and-bound algorithm for finding the shortest resolution proofs; to this
end, we introduce a layer list representation of proofs that groups clauses by
their level of indirection. As we show, this representation breaks all
permutational symmetries, thereby improving upon the state-of-the-art
symmetry-breaking and informing the design of a novel workflow for proof
minimization. In addition to that, we design pruning procedures that reason on
proof length lower bound, clause subsumption, and dominance. Our experiments
suggest that the proofs from state-of-the-art solvers could be shortened by
30-60% on the instances from SAT Competition 2002 and by 25-50% on small
synthetic formulas. When treated as an algorithm for finding the shortest
proof, our approach solves twice as many instances as the previous work based
on SAT solving and reduces the time to optimality by orders of magnitude for
the instances solved by both approaches.",cs.AI
Towards Low-bit Communication for Tensor Parallel LLM Inference,"Tensor parallelism provides an effective way to increase server large
language model (LLM) inference efficiency despite adding an additional
communication cost. However, as server LLMs continue to scale in size, they
will need to be distributed across more devices, magnifying the communication
cost. One way to approach this problem is with quantization, but current
methods for LLMs tend to avoid quantizing the features that tensor parallelism
needs to communicate. Taking advantage of consistent outliers in communicated
features, we introduce a quantization method that reduces communicated values
on average from 16 bits to 4.2 bits while preserving nearly all of the original
performance. For instance, our method maintains around 98.0% and 99.5% of Gemma
2 27B's and Llama 2 13B's original performance, respectively, averaged across
all tasks we evaluated on.",cs.AI
DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with Generative Adversarial Networks,"Computed tomography (CT) provides highly detailed three-dimensional (3D)
medical images but is costly, time-consuming, and often inaccessible in
intraoperative settings (Organization et al. 2011). Recent advancements have
explored reconstructing 3D chest volumes from sparse 2D X-rays, such as
single-view or orthogonal double-view images. However, current models tend to
process 2D images in a planar manner, prioritizing visual realism over
structural accuracy. In this work, we introduce DuoLift Generative Adversarial
Networks (DuoLift-GAN), a novel architecture with dual branches that
independently elevate 2D images and their features into 3D representations.
These 3D outputs are merged into a unified 3D feature map and decoded into a
complete 3D chest volume, enabling richer 3D information capture. We also
present a masked loss function that directs reconstruction towards critical
anatomical regions, improving structural accuracy and visual quality. This
paper demonstrates that DuoLift-GAN significantly enhances reconstruction
accuracy while achieving superior visual realism compared to existing methods.",cs.AI
Automatic dataset shift identification to support root cause analysis of AI performance drift,"Shifts in data distribution can substantially harm the performance of
clinical AI models. Hence, various methods have been developed to detect the
presence of such shifts at deployment time. However, root causes of dataset
shifts are varied, and the choice of shift mitigation strategies is highly
dependent on the precise type of shift encountered at test time. As such,
detecting test-time dataset shift is not sufficient: precisely identifying
which type of shift has occurred is critical. In this work, we propose the
first unsupervised dataset shift identification framework, effectively
distinguishing between prevalence shift (caused by a change in the label
distribution), covariate shift (caused by a change in input characteristics)
and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the
importance of self-supervised encoders for detecting subtle covariate shifts
and propose a novel shift detector leveraging both self-supervised encoders and
task model outputs for improved shift detection. We report promising results
for the proposed shift identification framework across three different imaging
modalities (chest radiography, digital mammography, and retinal fundus images)
on five types of real-world dataset shifts, using four large publicly available
datasets.",cs.AI
Doubly Mild Generalization for Offline Reinforcement Learning,"Offline Reinforcement Learning (RL) suffers from the extrapolation error and
value overestimation. From a generalization perspective, this issue can be
attributed to the over-generalization of value functions or policies towards
out-of-distribution (OOD) actions. Significant efforts have been devoted to
mitigating such generalization, and recent in-sample learning approaches have
further succeeded in entirely eschewing it. Nevertheless, we show that mild
generalization beyond the dataset can be trusted and leveraged to improve
performance under certain conditions. To appropriately exploit generalization
in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild
action generalization and (ii) mild generalization propagation. The former
refers to selecting actions in a close neighborhood of the dataset to maximize
the Q values. Even so, the potential erroneous generalization can still be
propagated, accumulated, and exacerbated by bootstrapping. In light of this,
the latter concept is introduced to mitigate the generalization propagation
without impeding the propagation of RL learning signals. Theoretically, DMG
guarantees better performance than the in-sample optimal policy in the oracle
generalization scenario. Even under worst-case generalization, DMG can still
control value overestimation at a certain level and lower bound the
performance. Empirically, DMG achieves state-of-the-art performance across
Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting
from its flexibility in both generalization aspects, DMG enjoys a seamless
transition from offline to online learning and attains strong online
fine-tuning performance.",cs.AI
INTRABENCH: Interactive Radiological Benchmark,"Current interactive segmentation approaches, inspired by the success of
META's Segment Anything model, have achieved notable advancements, however,
they come with substantial limitations that hinder their practical application
in real clinical scenarios. These include unrealistic human interaction
requirements, such as slice-by-slice operations for 2D models on 3D data, a
lack of iterative refinement, and insufficient evaluation experiments. These
shortcomings prevent accurate assessment of model performance and lead to
inconsistent outcomes across studies. IntRaBench overcomes these challenges by
offering a comprehensive and reproducible framework for evaluating interactive
segmentation methods in realistic, clinically relevant scenarios. It includes
diverse datasets, target structures, and segmentation models, and provides a
flexible codebase that allows seamless integration of new models and prompting
strategies. Additionally, we introduce advanced techniques to minimize
clinician interaction, ensuring fair comparisons between 2D and 3D models. By
open-sourcing IntRaBench, we invite the research community to integrate their
models and prompting techniques, ensuring continuous and transparent evaluation
of interactive segmentation models in 3D medical imaging.",cs.AI
Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules,"Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.",cs.AI
Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer's Disease,"The rapid advancements in Large Language Models (LLMs) and Vision-Language
Models (VLMs) have shown great potential in medical diagnostics, particularly
in radiology, where datasets such as X-rays are paired with human-generated
diagnostic reports. However, a significant research gap exists in the
neuroimaging field, especially for conditions such as Alzheimer's disease, due
to the lack of comprehensive diagnostic reports that can be utilized for model
fine-tuning. This paper addresses this gap by generating synthetic diagnostic
reports using GPT-4o-mini on structured data from the OASIS-4 dataset, which
comprises 663 patients. Using the synthetic reports as ground truth for
training and validation, we then generated neurological reports directly from
the images in the dataset leveraging the pre-trained BiomedCLIP and T5 models.
Our proposed method achieved a BLEU-4 score of 0.1827, ROUGE-L score of 0.3719,
and METEOR score of 0.4163, revealing its potential in generating clinically
relevant and accurate diagnostic reports.",cs.AI
Trustful LLMs: Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders,"Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.",cs.AI
Tucano: Advancing Neural Text Generation for Portuguese,"Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/",cs.AI
IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems,"Adversarial examples, which are inputs deliberately perturbed with
imperceptible changes to induce model errors, have raised serious concerns for
the reliability and security of deep neural networks (DNNs). While adversarial
attacks have been extensively studied in continuous data domains such as
images, the discrete nature of text presents unique challenges. In this paper,
we propose Irony-based Adversarial Examples (IAE), a method that transforms
straightforward sentences into ironic ones to create adversarial text. This
approach exploits the rhetorical device of irony, where the intended meaning is
opposite to the literal interpretation, requiring a deeper understanding of
context to detect. The IAE method is particularly challenging due to the need
to accurately locate evaluation words, substitute them with appropriate
collocations, and expand the text with suitable ironic elements while
maintaining semantic coherence. Our research makes the following key
contributions: (1) We introduce IAE, a strategy for generating textual
adversarial examples using irony. This method does not rely on pre-existing
irony corpora, making it a versatile tool for creating adversarial text in
various NLP tasks. (2) We demonstrate that the performance of several
state-of-the-art deep learning models on sentiment analysis tasks significantly
deteriorates when subjected to IAE attacks. This finding underscores the
susceptibility of current NLP systems to adversarial manipulation through
irony. (3) We compare the impact of IAE on human judgment versus NLP systems,
revealing that humans are less susceptible to the effects of irony in text.",cs.AI
Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements,"What ethical concerns, if any, do LLM researchers have? We introduce EthiCon,
a corpus of 1,580 ethical concern statements extracted from scientific papers
published in the ACL Anthology. We extract ethical concern keywords from the
statements and show promising results in automating the concern identification
process. Through a survey, we compare the ethical concerns of the corpus to the
concerns listed by the general public and professionals in the field. Finally,
we compare our retrieved ethical concerns with existing taxonomies pointing to
gaps and future research directions.",cs.AI
Chain Association-based Attacking and Shielding Natural Language Processing Systems,"Association as a gift enables people do not have to mention something in
completely straightforward words and allows others to understand what they
intend to refer to. In this paper, we propose a chain association-based
adversarial attack against natural language processing systems, utilizing the
comprehension gap between humans and machines. We first generate a chain
association graph for Chinese characters based on the association paradigm for
building search space of potential adversarial examples. Then, we introduce an
discrete particle swarm optimization algorithm to search for the optimal
adversarial examples. We conduct comprehensive experiments and show that
advanced natural language processing models and applications, including large
language models, are vulnerable to our attack, while humans appear good at
understanding the perturbed text. We also explore two methods, including
adversarial training and associative graph-based recovery, to shield systems
from chain association-based attack. Since a few examples that use some
derogatory terms, this paper contains materials that may be offensive or
upsetting to some people.",cs.AI
Federated Learning for Discrete Optimal Transport with Large Population under Incomplete Information,"Optimal transport is a powerful framework for the efficient allocation of
resources between sources and targets. However, traditional models often
struggle to scale effectively in the presence of large and heterogeneous
populations. In this work, we introduce a discrete optimal transport framework
designed to handle large-scale, heterogeneous target populations, characterized
by type distributions. We address two scenarios: one where the type
distribution of targets is known, and one where it is unknown. For the known
distribution, we propose a fully distributed algorithm to achieve optimal
resource allocation. In the case of unknown distribution, we develop a
federated learning-based approach that enables efficient computation of the
optimal transport scheme while preserving privacy. Case studies are provided to
evaluate the performance of our learning algorithm.",cs.AI
Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices,"In recent years, Large Language Models (LLMs) through Transformer structures
have dominated many machine learning tasks, especially text processing.
However, these models require massive amounts of data for training and induce
high resource requirements, particularly in terms of the large number of
Floating Point Operations (FLOPs) and the high amounts of memory needed. To
fine-tune such a model in a parameter-efficient way, techniques like Adapter or
LoRA have been developed. However, we observe that the application of LoRA,
when used in federated learning (FL), while still being parameter-efficient, is
memory and FLOP inefficient. Based on that observation, we develop a novel
layer finetuning scheme that allows devices in cross-device FL to make use of
pretrained neural networks (NNs) while adhering to given resource constraints.
We show that our presented scheme outperforms the current state of the art when
dealing with homogeneous or heterogeneous computation and memory constraints
and is on par with LoRA regarding limited communication, thereby achieving
significantly higher accuracies in FL training.",cs.AI
PatchCTG: Patch Cardiotocography Transformer for Antepartum Fetal Health Monitoring,"Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but
traditional methods like the Dawes-Redman system are often limited by high
inter-observer variability, leading to inconsistent interpretations and
potential misdiagnoses. This paper introduces PatchCTG, a transformer-based
model specifically designed for CTG analysis, employing patch-based
tokenisation, instance normalisation and channel-independent processing to
capture essential local and global temporal dependencies within CTG signals.
PatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over
20,000 CTG traces across diverse clinical outcomes after applying the inclusion
and exclusion criteria. With extensive hyperparameter optimisation, PatchCTG
achieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at
Youden's index threshold, demonstrating adaptability to various clinical needs.
Testing across varying temporal thresholds showed robust predictive
performance, particularly with finetuning on data closer to delivery, achieving
a sensitivity of 52% and specificity of 88% for near-delivery cases. These
findings suggest the potential of PatchCTG to enhance clinical decision-making
in antepartum care by providing a reliable, objective tool for fetal health
assessment. The source code is available at
https://github.com/jaleedkhan/PatchCTG.",cs.AI
RedCode: Risky Code Execution and Generation Benchmark for Code Agents,"With the rapidly increasing capabilities and adoption of code agents for
AI-assisted coding, safety concerns, such as generating or executing risky
code, have become significant barriers to the real-world deployment of these
agents. To provide comprehensive and practical evaluations on the safety of
code agents, we propose RedCode, a benchmark for risky code execution and
generation: (1) RedCode-Exec provides challenging prompts that could lead to
risky code execution, aiming to evaluate code agents' ability to recognize and
handle unsafe code. We provide a total of 4,050 risky test cases in Python and
Bash tasks with diverse input formats including code snippets and natural text.
They covers 25 types of critical vulnerabilities spanning 8 domains (e.g.,
websites, file systems). We provide Docker environments and design
corresponding evaluation metrics to assess their execution results. (2)
RedCode-Gen provides 160 prompts with function signatures and docstrings as
input to assess whether code agents will follow instructions to generate
harmful code or software. Our empirical findings, derived from evaluating three
agent frameworks based on 19 LLMs, provide insights into code agents'
vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are
more likely to reject executing risky operations on the operating system, but
are less likely to reject executing technically buggy code, indicating high
risks. Risky operations described in natural text lead to a lower rejection
rate than those in code format. Additionally, evaluations on RedCode-Gen show
that more capable base models and agents with stronger overall coding
abilities, such as GPT4, tend to produce more sophisticated and effective
harmful software. Our findings highlight the need for stringent safety
evaluations for diverse code agents. Our dataset and code are available at
https://github.com/AI-secure/RedCode.",cs.AI
Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.",cs.AI
Automatic Album Sequencing,"Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing",cs.AI
Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows,"Real-world enterprise text-to-SQL workflows often involve complex cloud or
local data across various database systems, multiple SQL queries in various
dialects, and diverse operations from data transformation to analytics. We
introduce Spider 2.0, an evaluation framework comprising 632 real-world
text-to-SQL workflow problems derived from enterprise-level database use cases.
The databases in Spider 2.0 are sourced from real data applications, often
containing over 1,000 columns and stored in local or cloud database systems
such as BigQuery and Snowflake. We show that solving problems in Spider 2.0
frequently requires understanding and searching through database metadata,
dialect documentation, and even project-level codebases. This challenge calls
for models to interact with complex SQL workflow environments, process
extremely long contexts, perform intricate reasoning, and generate multiple SQL
queries with diverse operations, often exceeding 100 lines, which goes far
beyond traditional text-to-SQL challenges. Our evaluations indicate that based
on o1-preview, our code agent framework successfully solves only 17.0% of the
tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on
Spider 2.0 show that while language models have demonstrated remarkable
performance in code generation -- especially in prior text-to-SQL benchmarks --
they require significant improvement in order to achieve adequate performance
for real-world enterprise usage. Progress on Spider 2.0 represents crucial
steps towards developing intelligent, autonomous, code agents for real-world
enterprise settings. Our code, baseline models, and data are available at
https://spider2-sql.github.io.",cs.AI
ASER: Activation Smoothing and Error Reconstruction for Large Language Model Quantization,"Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective low-bit quantization. The limited numerical mapping makes the
quantized model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training quantization. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
quantization error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to low-bit ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art quantization
algorithms, showing potential to activation quantization, with minor overhead.",cs.AI
Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit Q-Learning,"Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In this work, we present a novel
hierarchical transformer-based approach leveraging a learned quantizer of the
space. This quantization enables the training of a simpler zone-conditioned
low-level policy and simplifies planning, which is reduced to discrete
autoregressive prediction. Among other benefits, zone-level reasoning in
planning enables explicit trajectory stitching rather than implicit stitching
based on noisy value function estimates. By combining this transformer-based
planner with recent advancements in offline RL, our proposed approach achieves
state-of-the-art results in complex long-distance navigation environments.",cs.AI
Optimizing Traffic Signal Control using High-Dimensional State Representation and Efficient Deep Reinforcement Learning,"In reinforcement learning-based (RL-based) traffic signal control (TSC),
decisions on the signal timing are made based on the available information on
vehicles at a road intersection. This forms the state representation for the RL
environment which can either be high-dimensional containing several variables
or a low-dimensional vector. Current studies suggest that using high
dimensional state representations does not lead to improved performance on TSC.
However, we argue, with experimental results, that the use of high dimensional
state representations can, in fact, lead to improved TSC performance with
improvements up to 17.9% of the average waiting time. This high-dimensional
representation is obtainable using the cost-effective vehicle-to-infrastructure
(V2I) communication, encouraging its adoption for TSC. Additionally, given the
large size of the state, we identified the need to have computational efficient
models and explored model compression via pruning.",cs.AI
SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model,"Speech enhancement plays an essential role in various applications, and the
integration of visual information has been demonstrated to bring substantial
advantages. However, the majority of current research concentrates on the
examination of facial and lip movements, which can be compromised or entirely
inaccessible in scenarios where occlusions occur or when the camera view is
distant. Whereas contextual visual cues from the surrounding environment have
been overlooked: for example, when we see a dog bark, our brain has the innate
ability to discern and filter out the barking noise. To this end, in this
paper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is
the first proposal to use rich contextual information from synchronized video
as auxiliary cues to indicate the type of noise, which eventually improves the
speech enhancement performance. Specifically, we propose the VC-S$^2$E method,
which incorporates the Conformer and Mamba modules for their complementary
strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and
AudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E
over other competitive methods. We will make the source code publicly
available. Project demo page: https://AVSEPage.github.io/",cs.AI
Unlocking Legal Knowledge with Multi-Layered Embedding-Based Retrieval,"This work addresses the challenge of capturing the complexities of legal
knowledge by proposing a multi-layered embedding-based retrieval method for
legal and legislative texts. Creating embeddings not only for individual
articles but also for their components (paragraphs, clauses) and structural
groupings (books, titles, chapters, etc), we seek to capture the subtleties of
legal information through the use of dense vectors of embeddings, representing
it at varying levels of granularity. Our method meets various information needs
by allowing the Retrieval Augmented Generation system to provide accurate
responses, whether for specific segments or entire sections, tailored to the
user's query. We explore the concepts of aboutness, semantic chunking, and
inherent hierarchy within legal texts, arguing that this method enhances the
legal information retrieval. Despite the focus being on Brazil's legislative
methods and the Brazilian Constitution, which follow a civil law tradition, our
findings should in principle be applicable across different legal systems,
including those adhering to common law traditions. Furthermore, the principles
of the proposed method extend beyond the legal domain, offering valuable
insights for organizing and retrieving information in any field characterized
by information encoded in hierarchical text.",cs.AI
No-Reference Point Cloud Quality Assessment via Graph Convolutional Network,"Three-dimensional (3D) point cloud, as an emerging visual media format, is
increasingly favored by consumers as it can provide more realistic visual
information than two-dimensional (2D) data. Similar to 2D plane images and
videos, point clouds inevitably suffer from quality degradation and information
loss through multimedia communication systems. Therefore, automatic point cloud
quality assessment (PCQA) is of critical importance. In this work, we propose a
novel no-reference PCQA method by using a graph convolutional network (GCN) to
characterize the mutual dependencies of multi-view 2D projected image contents.
The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e.,
multi-view projection, graph construction, and GCN-based quality prediction.
First, multi-view projection is performed on the test point cloud to obtain a
set of horizontally and vertically projected images. Then, a
perception-consistent graph is constructed based on the spatial relations among
different projected images. Finally, reasoning on the constructed graph is
performed by GCN to characterize the mutual dependencies and interactions
between different projected images, and aggregate feature information of
multi-view projected images for final quality prediction. Experimental results
on two publicly available benchmark databases show that our proposed GC-PCQA
can achieve superior performance than state-of-the-art quality assessment
metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.",cs.AI
Is Cognition consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding,"Multimodal large language models (MLLMs) have shown impressive capabilities
in document understanding, a rapidly growing research area with significant
industrial demand in recent years. As a multimodal task, document understanding
requires models to possess both perceptual and cognitive abilities. However,
current MLLMs often face conflicts between perception and cognition. Taking a
document VQA task (cognition) as an example, an MLLM might generate answers
that do not match the corresponding visual content identified by its OCR
(perception). This conflict suggests that the MLLM might struggle to establish
an intrinsic connection between the information it ""sees"" and what it
""understands."" Such conflicts challenge the intuitive notion that cognition is
consistent with perception, hindering the performance and explainability of
MLLMs. In this paper, we define the conflicts between cognition and perception
as Cognition and Perception (C&P) knowledge conflicts, a form of multimodal
knowledge conflicts, and systematically assess them with a focus on document
understanding. Our analysis reveals that even GPT-4o, a leading MLLM, achieves
only 68.6% C&P consistency. To mitigate the C&P knowledge conflicts, we propose
a novel method called Multimodal Knowledge Consistency Fine-tuning. This method
first ensures task-specific consistency and then connects the cognitive and
perceptual knowledge. Our method significantly reduces C&P knowledge conflicts
across all tested MLLMs and enhances their performance in both cognitive and
perceptual tasks in most scenarios.",cs.AI
Training Data for Large Language Model,"In 2022, with the release of ChatGPT, large-scale language models gained
widespread attention. ChatGPT not only surpassed previous models in terms of
parameters and the scale of its pretraining corpus but also achieved
revolutionary performance improvements through fine-tuning on a vast amount of
high-quality, human-annotated data. This progress has led enterprises and
research institutions to recognize that building smarter and more powerful
models relies on rich and high-quality datasets. Consequently, the construction
and optimization of datasets have become a critical focus in the field of
artificial intelligence. This paper summarizes the current state of pretraining
and fine-tuning data for training large-scale language models, covering aspects
such as data scale, collection methods, data types and characteristics,
processing workflows, and provides an overview of available open-source
datasets.",cs.AI
New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook,"Thanks to the explosive growth of data and the development of computational
resources, it is possible to build pre-trained models that can achieve
outstanding performance on various tasks, such as neural language processing,
computer vision, and more. Despite their powerful capabilities, pre-trained
models have also sparked attention to the emerging security challenges
associated with their real-world applications. Security and privacy issues,
such as leaking privacy information and generating harmful responses, have
seriously undermined users' confidence in these powerful models. Concerns are
growing as model performance improves dramatically. Researchers are eager to
explore the unique security and privacy issues that have emerged, their
distinguishing factors, and how to defend against them. However, the current
literature lacks a clear taxonomy of emerging attacks and defenses for
pre-trained models, which hinders a high-level and comprehensive understanding
of these questions. To fill the gap, we conduct a systematical survey on the
security risks of pre-trained models, proposing a taxonomy of attack and
defense methods based on the accessibility of pre-trained models' input and
weights in various security test scenarios. This taxonomy categorizes attacks
and defenses into No-Change, Input-Change, and Model-Change approaches. With
the taxonomy analysis, we capture the unique security and privacy issues of
pre-trained models, categorizing and summarizing existing security issues based
on their characteristics. In addition, we offer a timely and comprehensive
review of each category's strengths and limitations. Our survey concludes by
highlighting potential new research opportunities in the security and privacy
of pre-trained models.",cs.AI
World Models: The Safety Perspective,"With the proliferation of the Large Language Model (LLM), the concept of
World Models (WM) has recently attracted a great deal of attention in the AI
research community, especially in the context of AI agents. It is arguably
evolving into an essential foundation for building AI agent systems. A WM is
intended to help the agent predict the future evolution of environmental states
or help the agent fill in missing information so that it can plan its actions
and behave safely. The safety property of WM plays a key role in their
effective use in critical applications. In this work, we review and analyze the
impacts of the current state-of-the-art in WM technology from the point of view
of trustworthiness and safety based on a comprehensive survey and the fields of
application envisaged. We provide an in-depth analysis of state-of-the-art WMs
and derive technical research challenges and their impact in order to call on
the research community to collaborate on improving the safety and
trustworthiness of WM.",cs.AI
Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG,"Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000
$\times$ 100,000 pixels or more) poses a significant challenge for current
Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize
the UHR image to standard input image size, the extensive spatial and
contextual information that UHR images contain will be neglected. Otherwise,
the original size of these images often exceeds the token limits of standard
RSMLLMs, making it difficult to process the entire image and capture long-range
dependencies to answer the query based on the abundant visual context. In this
paper, we introduce ImageRAG for RS, a training-free framework to address the
complexities of analyzing UHR remote sensing imagery. By transforming UHR
remote sensing image analysis task to image's long context selection task, we
design an innovative image contextual retrieval mechanism based on the
Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's
core innovation lies in its ability to selectively retrieve and focus on the
most relevant portions of the UHR image as visual contexts that pertain to a
given query. Fast path and slow path are proposed in this framework to handle
this task efficiently and effectively. ImageRAG allows RSMLLMs to manage
extensive context and spatial information from UHR RSI, ensuring the analysis
is both accurate and efficient.",cs.AI
Data-Driven Graph Switching for Cyber-Resilient Control in Microgrids,"Distributed microgrids are conventionally dependent on communication networks
to achieve secondary control objectives. This dependence makes them vulnerable
to stealth data integrity attacks (DIAs) where adversaries may perform
manipulations via infected transmitters and repeaters to jeopardize stability.
This paper presents a physics-guided, supervised Artificial Neural Network
(ANN)-based framework that identifies communication-level cyberattacks in
microgrids by analyzing whether incoming measurements will cause abnormal
behavior of the secondary control layer. If abnormalities are detected, an
iteration through possible spanning tree graph topologies that can be used to
fulfill secondary control objectives is done. Then, a communication network
topology that would not create secondary control abnormalities is identified
and enforced for maximum stability. By altering the communication graph
topology, the framework eliminates the dependence of the secondary control
layer on inputs from compromised cyber devices helping it achieve resilience
without instability. Several case studies are provided showcasing the
robustness of the framework against False Data Injections and repeater-level
Man-in-the-Middle attacks. To understand practical feasibility, robustness is
also verified against larger microgrid sizes and in the presence of varying
noise levels. Our findings indicate that performance can be affected when
attempting scalability in the presence of noise. However, the framework
operates robustly in low-noise settings.",cs.AI
Fast Disentangled Slim Tensor Learning for Multi-view Clustering,"Tensor-based multi-view clustering has recently received significant
attention due to its exceptional ability to explore cross-view high-order
correlations. However, most existing methods still encounter some limitations.
(1) Most of them explore the correlations among different affinity matrices,
making them unscalable to large-scale data. (2) Although some methods address
it by introducing bipartite graphs, they may result in sub-optimal solutions
caused by an unstable anchor selection process. (3) They generally ignore the
negative impact of latent semantic-unrelated information in each view. To
tackle these issues, we propose a new approach termed fast Disentangled Slim
Tensor Learning (DSTL) for multi-view clustering . Instead of focusing on the
multi-view graph structures, DSTL directly explores the high-order correlations
among multi-view latent semantic representations based on matrix factorization.
To alleviate the negative influence of feature redundancy, inspired by robust
PCA, DSTL disentangles the latent low-dimensional representation into a
semantic-unrelated part and a semantic-related part for each view.
Subsequently, two slim tensors are constructed with tensor-based
regularization. To further enhance the quality of feature disentanglement, the
semantic-related representations are aligned across views through a consensus
alignment indicator. Our proposed model is computationally efficient and can be
solved effectively. Extensive experiments demonstrate the superiority and
efficiency of DSTL over state-of-the-art approaches. The code of DSTL is
available at https://github.com/dengxu-nju/DSTL.",cs.AI
AI enhanced diagnosis of Peyronies disease a novel approach using Computer Vision,"This study presents an innovative AI-driven tool for diagnosing Peyronie's
Disease (PD), a condition that affects between 0.3% and 13.1% of men worldwide.
Our method uses key point detection on both images and videos to measure penile
curvature angles, utilizing advanced computer vision techniques. This tool has
demonstrated high accuracy in identifying anatomical landmarks, validated
against conventional goniometer measurements. Traditional PD diagnosis often
involves subjective and invasive methods, which can lead to patient discomfort
and inaccuracies. Our approach offers a precise, reliable, and non-invasive
diagnostic tool to address these drawbacks. The model distinguishes between PD
and normal anatomical changes with a sensitivity of 96.7% and a specificity of
100%. This advancement represents a significant improvement in urological
diagnostics, greatly enhancing the efficacy and convenience of PD assessment
for healthcare providers and patients.",cs.AI
Spike Talk in Power Electronic Grids -- Leveraging Post Moore's Computing Laws,"Emerging distributed generation demands highly reliable and resilient
coordinating control in microgrids. To improve on these aspects, spiking neural
network is leveraged, as a grid-edge intelligence tool to establish a talkative
infrastructure, Spike Talk, expediting coordination in next-generation
microgrids without the need of communication at all. This paper unravels the
physics behind Spike Talk from the perspective of its distributed
infrastructure, which aims to address the Von Neumann Bottleneck. Relying on
inferring information via power flows in tie lines, Spike Talk allows adaptive
and flexible control and coordination itself, and features in synaptic
plasticity facilitating online and local training functionality. Preliminary
case studies are demonstrated with results, while more extensive validations
are to be included as future scopes of work.",cs.AI
"Understanding Audiovisual Deepfake Detection: Techniques, Challenges, Human Factors and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.",cs.AI
Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling,"Scheduling problems pose significant challenges in resource, industry, and
operational management. This paper addresses the Unrelated Parallel Machine
Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent
Reinforcement Learning (MARL) approach. The study introduces the Reinforcement
Learning environment and conducts empirical analyses, comparing MARL with
Single-Agent algorithms. The experiments employ various deep neural network
policies for single- and Multi-Agent approaches. Results demonstrate the
efficacy of the Maskable extension of the Proximal Policy Optimization (PPO)
algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in
Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced
scenarios, Multi-Agent approaches reveal challenges in cooperative learning but
a scalable capacity. This research contributes insights into applying MARL
techniques to scheduling optimization, emphasizing the need for algorithmic
sophistication balanced with scalability for intelligent scheduling solutions.",cs.AI
Direct Preference Optimization Using Sparse Feature-Level Constraints,"The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.",cs.AI
Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation,"Clinical rationales play a pivotal role in accurate disease diagnosis;
however, many models predominantly use discriminative methods and overlook the
importance of generating supportive rationales. Rationale distillation is a
process that transfers knowledge from large language models (LLMs) to smaller
language models (SLMs), thereby enhancing the latter's ability to break down
complex tasks. Despite its benefits, rationale distillation alone is inadequate
for addressing domain knowledge limitations in tasks requiring specialized
expertise, such as disease diagnosis. Effectively embedding domain knowledge in
SLMs poses a significant challenge. While current LLMs are primarily geared
toward processing textual data, multimodal LLMs that incorporate time series
data, especially electronic health records (EHRs), are still evolving. To
tackle these limitations, we introduce ClinRaGen, an SLM optimized for
multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a
unique knowledge-augmented attention mechanism to merge domain knowledge with
time series EHR data, utilizing a stepwise rationale distillation strategy to
produce both textual and time series-based clinical rationales. Our evaluations
show that ClinRaGen markedly improves the SLM's capability to interpret
multimodal EHR data and generate accurate clinical rationales, supporting more
reliable disease diagnosis, advancing LLM applications in healthcare, and
narrowing the performance divide between LLMs and SLMs.",cs.AI
Optimizing Service Function Chain Mapping in Network Function Virtualization through Simultaneous NF Decomposition and VNF Placement,"Network function virtualization enables network operators to implement new
services through a process called service function chain mapping. The concept
of Service Function Chain (SFC) is introduced to provide complex services,
which is an ordered set of Network Functions (NF). The network functions of an
SFC can be decomposed in several ways into some Virtual Network Functions
(VNF). Additionally, the decomposed NFs can be placed (mapped) as VNFs on
different machines on the underlying physical infrastructure. Selecting good
decompositions and good placements among the possible options greatly affects
both costs and service quality metrics. Previous research has addressed NF
decomposition and VNF placement as separate problems. However, in this paper,
we address both NF decomposition and VNF placement simultaneously as a single
problem. Since finding an optimal solution is NP-hard, we have employed
heuristic algorithms to solve the problem. Specifically, we have introduced a
multiobjective decomposition and mapping VNFs (MODMVNF) method based on the
non-dominated sorting genetic multi-objective algorithm (NSGAII) to solve the
problem. The goal is to find near-optimal decomposition and mapping on the
physical network at the same time to minimize the mapping cost and
communication latency of SFC. The comparison of the results of the proposed
method with the results obtained by solving ILP formulation of the problem as
well as the results obtained from the multi-objective particle swarm algorithm
shows the efficiency and effectiveness of the proposed method in terms of cost
and communication latency.",cs.AI
Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.",cs.AI
Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations,"Many open-ended conversations (e.g., tutoring lessons or business meetings)
revolve around pre-defined reference materials, like worksheets or meeting
bullets. To provide a framework for studying such conversation structure, we
introduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly
breaking down conversations into segments and linking each segment to the
relevant reference item. As a case study, we apply POSR to education where
effectively structuring lessons around problems is critical yet difficult. We
present LessonLink, the first dataset of real-world tutoring lessons, featuring
3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT
math problems. We define and evaluate several joint and independent approaches
for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT),
and large language models (LLMs) methods. Our results highlight that modeling
POSR as one joint task is essential: POSR methods outperform independent
segmentation and retrieval pipelines by up to +76% on joint metrics and surpass
traditional segmentation methods by up to +78% on segmentation metrics. We
demonstrate POSR's practical impact on downstream education applications,
deriving new insights on the language and time use in real-world lesson
structures.",cs.AI
Entropy Controllable Direct Preference Optimization,"In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.",cs.AI
Overhead-free User-side Recommender Systems,"Traditionally, recommendation algorithms have been designed for service
developers. But recently, a new paradigm called user-side recommender systems
has been proposed. User-side recommender systems are built and used by end
users, in sharp contrast to traditional provider-side recommender systems. Even
if the official recommender system offered by the provider is not fair, end
users can create and enjoy their own user-side recommender systems by
themselves. Although the concept of user-side recommender systems is
attractive, the problem is they require tremendous communication costs between
the user and the official system. Even the most efficient user-side recommender
systems require about 5 times more costs than provider-side recommender
systems. Such high costs hinder the adoption of user-side recommender systems.
In this paper, we propose overhead-free user-side recommender systems,
RecCycle, which realizes user-side recommender systems without any
communication overhead. The main idea of RecCycle is to recycle past
recommendation results offered by the provider's recommender systems. The
ingredients of RecCycle can be retrieved ``for free,'' and it greatly reduces
the cost of user-side recommendations. In the experiments, we confirm that
RecCycle performs as well as state-of-the-art user-side recommendation
algorithms while RecCycle reduces costs significantly.",cs.AI
A Comprehensive Survey of AI-Driven Advancements and Techniques in Automated Program Repair and Code Generation,"Bug fixing and code generation have been core research topics in software
development for many years. The recent explosive growth in Large Language
Models has completely transformed these spaces, putting in reach incredibly
powerful tools for both. In this survey, 27 recent papers have been reviewed
and split into two groups: one dedicated to Automated Program Repair (APR) and
LLM integration and the other to code generation using LLMs. The first group
consists of new methods for bug detection and repair, which include locating
semantic errors, security vulnerabilities, and runtime failure bugs. The place
of LLMs in reducing manual debugging efforts is emphasized in this work by APR
toward context-aware fixes, with innovations that boost accuracy and efficiency
in automatic debugging. The second group dwells on code generation, providing
an overview of both general-purpose LLMs fine-tuned for programming and
task-specific models. It also presents methods to improve code generation, such
as identifier-aware training, fine-tuning at the instruction level, and
incorporating semantic code structures. This survey work contrasts the
methodologies in APR and code generation to identify trends such as using LLMs,
feedback loops to enable iterative code improvement and open-source models. It
also discusses the challenges of achieving functional correctness and security
and outlines future directions for research in LLM-based software development.",cs.AI
Reinforcement Learning Framework for Quantitative Trading,"The inherent volatility and dynamic fluctuations within the financial stock
market underscore the necessity for investors to employ a comprehensive and
reliable approach that integrates risk management strategies, market trends,
and the movement trends of individual securities. By evaluating specific data,
investors can make more informed decisions. However, the current body of
literature lacks substantial evidence supporting the practical efficacy of
reinforcement learning (RL) agents, as many models have only demonstrated
success in back testing using historical data. This highlights the urgent need
for a more advanced methodology capable of addressing these challenges. There
is a significant disconnect in the effective utilization of financial
indicators to better understand the potential market trends of individual
securities. The disclosure of successful trading strategies is often restricted
within financial markets, resulting in a scarcity of widely documented and
published strategies leveraging RL. Furthermore, current research frequently
overlooks the identification of financial indicators correlated with various
market trends and their potential advantages.
  This research endeavors to address these complexities by enhancing the
ability of RL agents to effectively differentiate between positive and negative
buy/sell actions using financial indicators. While we do not address all
concerns, this paper provides deeper insights and commentary on the utilization
of technical indicators and their benefits within reinforcement learning. This
work establishes a foundational framework for further exploration and
investigation of more complex scenarios.",cs.AI
Disentangling Tabular Data towards Better One-Class Anomaly Detection,"Tabular anomaly detection under the one-class classification setting poses a
significant challenge, as it involves accurately conceptualizing ""normal""
derived exclusively from a single category to discern anomalies from normal
data variations. Capturing the intrinsic correlation among attributes within
normal samples presents one promising method for learning the concept. To do
so, the most recent effort relies on a learnable mask strategy with a
reconstruction task. However, this wisdom may suffer from the risk of producing
uniform masks, i.e., essentially nothing is masked, leading to less effective
correlation learning. To address this issue, we presume that attributes related
to others in normal samples can be divided into two non-overlapping and
correlated subsets, defined as CorrSets, to capture the intrinsic correlation
effectively. Accordingly, we introduce an innovative method that disentangles
CorrSets from normal tabular data. To our knowledge, this is a pioneering
effort to apply the concept of disentanglement for one-class anomaly detection
on tabular data. Extensive experiments on 20 tabular datasets show that our
method substantially outperforms the state-of-the-art methods and leads to an
average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.",cs.AI
Improving Grapheme-to-Phoneme Conversion through In-Context Knowledge Retrieval with Large Language Models,"Grapheme-to-phoneme (G2P) conversion is a crucial step in Text-to-Speech
(TTS) systems, responsible for mapping grapheme to corresponding phonetic
representations. However, it faces ambiguities problems where the same grapheme
can represent multiple phonemes depending on contexts, posing a challenge for
G2P conversion. Inspired by the remarkable success of Large Language Models
(LLMs) in handling context-aware scenarios, contextual G2P conversion systems
with LLMs' in-context knowledge retrieval (ICKR) capabilities are proposed to
promote disambiguation capability. The efficacy of incorporating ICKR into G2P
conversion systems is demonstrated thoroughly on the Librig2p dataset. In
particular, the best contextual G2P conversion system using ICKR outperforms
the baseline with weighted average phoneme error rate (PER) reductions of 2.0%
absolute (28.9% relative). Using GPT-4 in the ICKR system can increase of 3.5%
absolute (3.8% relative) on the Librig2p dataset.",cs.AI
EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods,"This study introduces a novel approach for EUR/USD exchange rate forecasting
that integrates deep learning, textual analysis, and particle swarm
optimization (PSO). By incorporating online news and analysis texts as
qualitative data, the proposed PSO-LSTM model demonstrates superior performance
compared to traditional econometric and machine learning models. The research
employs advanced text mining techniques, including sentiment analysis using the
RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore
the significant advantage of incorporating textual data, with the PSO-LSTM
model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH.
Ablation experiments reveal the contribution of each textual data category to
the overall forecasting performance. The study highlights the transformative
potential of artificial intelligence in finance and paves the way for future
research in real-time forecasting and the integration of alternative data
sources.",cs.AI
Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models,"Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs)
to output harmful responses, raise significant safety concerns. Among these
methods, gradient-based approaches, which use gradients to generate malicious
prompts, have been widely studied due to their high success rates in white-box
settings, where full access to the model is available. However, these methods
have notable limitations: they require white-box access, which is not always
feasible, and involve high memory usage. To address scenarios where white-box
access is unavailable, attackers often resort to transfer attacks. In transfer
attacks, malicious inputs generated using white-box models are applied to
black-box models, but this typically results in reduced attack performance. To
overcome these challenges, we propose Zer0-Jack, a method that bypasses the
need for white-box access by leveraging zeroth-order optimization. We propose
patch coordinate descent to efficiently generate malicious image inputs to
directly attack black-box MLLMs, which significantly reduces memory usage
further. Through extensive experiments, Zer0-Jack achieves a high attack
success rate across various models, surpassing previous transfer-based methods
and performing comparably with existing white-box jailbreak techniques.
Notably, Zer0-Jack achieves a 95\% attack success rate on MiniGPT-4 with the
Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its
effectiveness. Additionally, we show that Zer0-Jack can directly attack
commercial MLLMs such as GPT-4o. Codes are provided in the supplement.",cs.AI
Contrastive Language Prompting to Ease False Positives in Medical Anomaly Detection,"A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.",cs.AI
Model Stealing for Any Low-Rank Language Model,"Model stealing, where a learner tries to recover an unknown model via
carefully chosen queries, is a critical problem in machine learning, as it
threatens the security of proprietary models and the privacy of data they are
trained on. In recent years, there has been particular interest in stealing
large language models (LLMs). In this paper, we aim to build a theoretical
understanding of stealing language models by studying a simple and
mathematically tractable setting. We study model stealing for Hidden Markov
Models (HMMs), and more generally low-rank language models.
  We assume that the learner works in the conditional query model, introduced
by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient
algorithm in the conditional query model, for learning any low-rank
distribution. In other words, our algorithm succeeds at stealing any language
model whose output distribution is low-rank. This improves upon the previous
result by Kakade, Krishnamurthy, Mahajan and Zhang, which also requires the
unknown distribution to have high ""fidelity"", a property that holds only in
restricted cases. There are two key insights behind our algorithm: First, we
represent the conditional distributions at each timestep by constructing
barycentric spanners among a collection of vectors of exponentially large
dimension. Second, for sampling from our representation, we iteratively solve a
sequence of convex optimization problems that involve projection in relative
entropy to prevent compounding of errors over the length of the sequence. This
is an interesting example where, at least theoretically, allowing a machine
learning model to solve more complex problems at inference time can lead to
drastic improvements in its performance.",cs.AI
Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of Different Complexity Levels: An Empirical Analysis,"ChatGPT and other large language models (LLMs) promise to revolutionize
software development by automatically generating code from program
specifications. We assess the performance of ChatGPT's GPT-3.5-turbo model on
LeetCode, a popular platform with algorithmic coding challenges for technical
interview practice, across three difficulty levels: easy, medium, and hard. We
test three main hypotheses. First, ChatGPT solves fewer problems as difficulty
rises (Hypothesis 1). Second, prompt engineering improves ChatGPT's
performance, with greater gains on easier problems and diminishing returns on
harder ones (Hypothesis 2). Third, ChatGPT performs better in popular languages
like Python, Java, and C++ than in less common ones like Elixir, Erlang, and
Racket (Hypothesis 3). To investigate these hypotheses, we conduct automated
experiments using Python scripts to generate prompts that instruct ChatGPT to
create Python solutions. These solutions are stored and manually submitted on
LeetCode to check their correctness. For Hypothesis 1, results show the
GPT-3.5-turbo model successfully solves 92% of easy, 79% of medium, and 51% of
hard problems. For Hypothesis 2, prompt engineering yields improvements: 14-29%
for Chain of Thought Prompting, 38-60% by providing failed test cases in a
second feedback prompt, and 33-58% by switching to GPT-4. From a random subset
of problems ChatGPT solved in Python, it also solved 78% in Java, 50% in C++,
and none in Elixir, Erlang, or Racket. These findings generally validate all
three hypotheses.",cs.AI
SecEncoder: Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.",cs.AI
Fair Summarization: Bridging Quality and Diversity in Extractive Summaries,"Fairness in multi-document summarization of user-generated content remains a
critical challenge in natural language processing (NLP). Existing summarization
methods often fail to ensure equitable representation across different social
groups, leading to biased outputs. In this paper, we introduce two novel
methods for fair extractive summarization: FairExtract, a clustering-based
approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.
We evaluate these methods using Divsumm summarization dataset of White-aligned,
Hispanic, and African-American dialect tweets and compare them against relevant
baselines. The results obtained using a comprehensive set of summarization
quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well
as a fairness metric F, demonstrate that FairExtract and FairGPT achieve
superior fairness while maintaining competitive summarization quality.
Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that
integrate quality and fairness into a single evaluation framework, offering a
more nuanced understanding of the trade-offs between these objectives. This
work highlights the importance of fairness in summarization and sets a
benchmark for future research in fairness-aware NLP models.",cs.AI
TIPS: Threat Actor Informed Prioritization of Applications using SecEncoder,"This paper introduces TIPS: Threat Actor Informed Prioritization using
SecEncoder, a specialized language model for security. TIPS combines the
strengths of both encoder and decoder language models to detect and prioritize
compromised applications. By integrating threat actor intelligence, TIPS
enhances the accuracy and relevance of its detections. Extensive experiments
with a real-world benchmark dataset of applications demonstrate TIPS's high
efficacy, achieving an F-1 score of 0.90 in identifying malicious applications.
Additionally, in real-world scenarios, TIPS significantly reduces the backlog
of investigations for security analysts by 87%, thereby streamlining the threat
response process and improving overall security posture.",cs.AI
LLM App Squatting and Cloning,"Impersonation tactics, such as app squatting and app cloning, have posed
longstanding challenges in mobile app stores, where malicious actors exploit
the names and reputations of popular apps to deceive users. With the rapid
growth of Large Language Model (LLM) stores like GPT Store and FlowGPT, these
issues have similarly surfaced, threatening the integrity of the LLM app
ecosystem. In this study, we present the first large-scale analysis of LLM app
squatting and cloning using our custom-built tool, LLMappCrazy. LLMappCrazy
covers 14 squatting generation techniques and integrates Levenshtein distance
and BERT-based semantic analysis to detect cloning by analyzing app functional
similarities. Using this tool, we generated variations of the top 1000 app
names and found over 5,000 squatting apps in the dataset. Additionally, we
observed 3,509 squatting apps and 9,575 cloning cases across six major
platforms. After sampling, we find that 18.7% of the squatting apps and 4.9% of
the cloning apps exhibited malicious behavior, including phishing, malware
distribution, fake content dissemination, and aggressive ad injection.",cs.AI
An Attack Traffic Identification Method Based on Temporal Spectrum,"To address the issues of insufficient robustness, unstable features, and data
noise interference in existing network attack detection and identification
models, this paper proposes an attack traffic detection and identification
method based on temporal spectrum. First, traffic data is segmented by a
sliding window to construct a feature sequence and a corresponding label
sequence for network traffic. Next, the proposed spectral label generation
methods, SSPE and COAP, are applied to transform the label sequence into
spectral labels and the feature sequence into temporal features. Spectral
labels and temporal features are used to capture and represent behavioral
patterns of attacks. Finally, the constructed temporal features and spectral
labels are used to train models, which subsequently detects and identifies
network attack behaviors. Experimental results demonstrate that compared to
traditional methods, models trained with the SSPE or COAP method improve
identification accuracy by 10%, and exhibit strong robustness, particularly in
noisy environments.",cs.AI
FM-TS: Flow Matching for Time Series Generation,"Time series generation has emerged as an essential tool for analyzing
temporal data across numerous fields. While diffusion models have recently
gained significant attention in generating high-quality time series, they tend
to be computationally demanding and reliant on complex stochastic processes. To
address these limitations, we introduce FM-TS, a rectified Flow Matching-based
framework for Time Series generation, which simplifies the time series
generation process by directly optimizing continuous trajectories. This
approach avoids the need for iterative sampling or complex noise schedules
typically required in diffusion-based models. FM-TS is more efficient in terms
of training and inference. Moreover, FM-TS is highly adaptive, supporting both
conditional and unconditional time series generation. Notably, through our
novel inference design, the model trained in an unconditional setting can
seamlessly generalize to conditional tasks without the need for retraining.
Extensive benchmarking across both settings demonstrates that FM-TS
consistently delivers superior performance compared to existing approaches
while being more efficient in terms of training and inference. For instance, in
terms of discriminative score, FM-TS achieves 0.005, 0.019, 0.011, 0.005,
0.053, and 0.106 on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI
unconditional time series datasets, respectively, significantly outperforming
the second-best method which achieves 0.006, 0.067, 0.061, 0.008, 0.122, and
0.167 on the same datasets. We have achieved superior performance in solar
forecasting and MuJoCo imputation tasks, significantly enhanced by our
innovative $t$ power sampling method. The code is available at
https://github.com/UNITES-Lab/FMTS.",cs.AI
LAuReL: Learned Augmented Residual Layer,"One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.",cs.AI
Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling,"Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.",cs.AI
IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark,"Recent evaluations of LLMs on coreference resolution have revealed that
traditional output formats and evaluation metrics do not fully capture the
models' referential understanding. To address this, we introduce IdentifyMe, a
new benchmark for mention resolution presented in a multiple-choice question
(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long
narratives and employs heuristics to exclude easily identifiable mentions,
creating a more challenging task. The benchmark also consists of a curated
mixture of different mention types and corresponding entities, allowing for a
fine-grained analysis of model performance. We evaluate both closed- and open
source LLMs on IdentifyMe and observe a significant performance gap (20-30%)
between the state-of-the-art sub-10B open models vs. closed ones. We observe
that pronominal mentions, which have limited surface information, are typically
much harder for models to resolve than nominal mentions. Additionally, we find
that LLMs often confuse entities when their mentions overlap in nested
structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,
highlighting the strong referential capabilities of state-of-the-art LLMs while
also indicating room for further improvement.",cs.AI
BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including
generation of code snippets, but often struggle with generating code for
complex Machine Learning (ML) tasks. Although existing LLM single-agent based
systems give varying performance depending on the task complexity, they purely
rely on larger and expensive models such as GPT-4. Our investigation reveals
that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama
perform far worse than GPT-4 in a single-agent setting. With the motivation of
developing a cost-efficient LLM based solution for solving ML tasks, we propose
an LLM Multi-Agent based system which leverages combination of experts using
profiling, efficient retrieval of past observations, LLM cascades, and
ask-the-expert calls. Through empirical analysis on ML engineering tasks in the
MLAgentBench benchmark, we demonstrate the effectiveness of our system, using
no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and
expert to serve occasional ask-the-expert calls for planning. With 94.2\%
reduction in the cost (from \$0.931 per run cost averaged over all tasks for
GPT-4 single agent system to \$0.054), our system is able to yield better
average success rate of 32.95\% as compared to GPT-4 single-agent system
yielding 22.72\% success rate averaged over all the tasks of MLAgentBench.",cs.AI
BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions,"We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that
bridges the gap between descriptive synthetic captions and factual web-scale
alt-text. KALE augments synthetic dense image captions with web-scale alt-text
to generate factually grounded image captions. Our two-stage approach leverages
large vision-language models and language models to create knowledge-augmented
captions, which are then used to train a specialized VLM for scaling up the
dataset. We train vision-language models on KALE and demonstrate improvements
on vision-language tasks. Our experiments show the utility of KALE for training
more capable and knowledgeable multimodal models. We release the KALE dataset
at https://huggingface.co/datasets/Salesforce/blip3-kale",cs.AI
Research on fault diagnosis of nuclear power first-second circuit based on hierarchical multi-granularity classification network,"The safe and reliable operation of complex electromechanical systems in
nuclear power plants is crucial for the safe production of nuclear power plants
and their nuclear power unit. Therefore, accurate and timely fault diagnosis of
nuclear power systems is of great significance for ensuring the safe and
reliable operation of nuclear power plants. The existing fault diagnosis
methods mainly target a single device or subsystem, making it difficult to
analyze the inherent connections and mutual effects between different types of
faults at the entire unit level. This article uses the AP1000 full-scale
simulator to simulate the important mechanical component failures of some key
systems in the primary and secondary circuits of nuclear power units, and
constructs a fault dataset. Meanwhile, a hierarchical multi granularity
classification fault diagnosis model based on the EfficientNet large model is
proposed, aiming to achieve hierarchical classification of nuclear power
faults. The results indicate that the proposed fault diagnosis model can
effectively classify faults in different circuits and system components of
nuclear power units into hierarchical categories. However, the fault dataset in
this study was obtained from a simulator, which may introduce additional
information due to parameter redundancy, thereby affecting the diagnostic
performance of the model.",cs.AI
"Optimizing Data Delivery: Insights from User Preferences on Visuals, Tables, and Text","In this work, we research user preferences to see a chart, table, or text
given a question asked by the user. This enables us to understand when it is
best to show a chart, table, or text to the user for the specific question. For
this, we conduct a user study where users are shown a question and asked what
they would prefer to see and used the data to establish that a user's personal
traits does influence the data outputs that they prefer. Understanding how user
characteristics impact a user's preferences is critical to creating data tools
with a better user experience. Additionally, we investigate to what degree an
LLM can be used to replicate a user's preference with and without user
preference data. Overall, these findings have significant implications
pertaining to the development of data tools and the replication of human
preferences using LLMs. Furthermore, this work demonstrates the potential use
of LLMs to replicate user preference data which has major implications for
future user modeling and personalization research.",cs.AI
The Effect of Scheduling and Preemption on the Efficiency of LLM Inference Serving,"The growing usage of Large Language Models (LLMs) highlights the demands and
challenges in scalable LLM inference systems, affecting deployment and
development processes. On the deployment side, there is a lack of comprehensive
analysis on the conditions under which a particular scheduler performs better
or worse, with performance varying substantially across different schedulers,
hardware, models, and workloads. Manually testing each configuration on GPUs
can be prohibitively expensive. On the development side, unpredictable
performance and unknown upper limits can lead to inconclusive trial-and-error
processes, consuming resources on ideas that end up ineffective. To address
these challenges, we introduce INFERMAX, an analytical framework that uses
inference cost models to compare various schedulers, including an optimal
scheduler formulated as a constraint satisfaction problem (CSP) to establish an
upper bound on performance. Our framework offers in-depth analysis and raises
essential questions, challenging assumptions and exploring opportunities for
more efficient scheduling. Notably, our findings indicate that preempting
requests can reduce GPU costs by 30% compared to avoiding preemptions at all.
We believe our methods and insights will facilitate the cost-effective
deployment and development of scalable, efficient inference systems and pave
the way for cost-based scheduling.",cs.AI
Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions,"In today's Function-as-a-Service offerings, a programmer is usually
responsible for configuring function memory for its successful execution, which
allocates proportional function resources such as CPU and network. However,
right-sizing the function memory force developers to speculate performance and
make ad-hoc configuration decisions. Recent research has highlighted that a
function's input characteristics, such as input size, type and number of
inputs, significantly impact its resource demand, run-time performance and
costs with fluctuating workloads. This correlation further makes memory
configuration a non-trivial task. On that account, an input-aware function
memory allocator not only improves developer productivity by completely hiding
resource-related decisions but also drives an opportunity to reduce resource
wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we
present MemFigLess, a serverless solution that estimates the memory requirement
of a serverless function with input-awareness. The framework executes function
profiling in an offline stage and trains a multi-output Random Forest
Regression model on the collected metrics to invoke input-aware optimal
configurations. We evaluate our work with the state-of-the-art approaches on
AWS Lambda service to find that MemFigLess is able to capture the input-aware
resource relationships and allocate upto 82% less resources and save up to 87%
run-time costs.",cs.AI
Automatically Detecting Online Deceptive Patterns in Real-time,"Deceptive patterns (DPs) in digital interfaces manipulate users into making
unintended decisions, exploiting cognitive biases and psychological
vulnerabilities. These patterns have become ubiquitous across various digital
platforms. While efforts to mitigate DPs have emerged from legal and technical
perspectives, a significant gap in usable solutions that empower users to
identify and make informed decisions about DPs in real-time remains. In this
work, we introduce AutoBot, an automated, deceptive pattern detector that
analyzes websites' visual appearances using machine learning techniques to
identify and notify users of DPs in real-time. AutoBot employs a two-staged
pipeline that processes website screenshots, identifying interactable elements
and extracting textual features without relying on HTML structure. By
leveraging a custom language model, AutoBot understands the context surrounding
these elements to determine the presence of deceptive patterns. We implement
AutoBot as a lightweight Chrome browser extension that performs all analyses
locally, minimizing latency and preserving user privacy. Through extensive
evaluation, we demonstrate AutoBot's effectiveness in enhancing users' ability
to navigate digital environments safely while providing a valuable tool for
regulators to assess and enforce compliance with DP regulations.",cs.AI
Evaluating Detection Thresholds: The Impact of False Positives and Negatives on Super-Resolution Ultrasound Localization Microscopy,"Super-resolution ultrasound imaging with ultrasound localization microscopy
(ULM) offers a high-resolution view of microvascular structures. Yet, ULM image
quality heavily relies on precise microbubble (MB) detection. Despite the
crucial role of localization algorithms, there has been limited focus on the
practical pitfalls in MB detection tasks such as setting the detection
threshold. This study examines how False Positives (FPs) and False Negatives
(FNs) affect ULM image quality by systematically adding controlled detection
errors to simulated data. Results indicate that while both FP and FN rates
impact Peak Signal-to-Noise Ratio (PSNR) similarly, increasing FP rates from
0\% to 20\% decreases Structural Similarity Index (SSIM) by 7\%, whereas same
FN rates cause a greater drop of around 45\%. Moreover, dense MB regions are
more resilient to detection errors, while sparse regions show high sensitivity,
showcasing the need for robust MB detection frameworks to enhance
super-resolution imaging.",cs.AI
Predicting BWR Criticality with Data-Driven Machine Learning Model,"One of the challenges in operating nuclear power plants is to decide the
amount of fuel needed in a cycle. Large-scale nuclear power plants are designed
to operate at base load, meaning that they are expected to always operate at
full power. Economically, a nuclear power plant should burn enough fuel to
maintain criticality until the end of a cycle (EOC). If the reactor goes
subcritical before the end of a cycle, it may result in early coastdown as the
fuel in the core is already depleted. On contrary, if the reactor still has
significant excess reactivity by the end of a cycle, the remaining fuels will
remain unused. In both cases, the plant may lose a significant amount of money.
This work proposes an innovative method based on a data-driven deep learning
model to estimate the excess criticality of a boiling water reactor.",cs.AI
Controllable Context Sensitivity and the Knob Behind It,"When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.",cs.AI
Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews,"With the increasing proliferation of mobile applications in our everyday
experiences, the concerns surrounding ethics have surged significantly. Users
generally communicate their feedback, report issues, and suggest new
functionalities in application (app) reviews, frequently emphasizing safety,
privacy, and accountability concerns. Incorporating these reviews is essential
to developing successful products. However, app reviews related to ethical
concerns generally use domain-specific language and are expressed using a more
varied vocabulary. Thus making automated ethical concern-related app review
extraction a challenging and time-consuming effort.
  This study proposes a novel Natural Language Processing (NLP) based approach
that combines Natural Language Inference (NLI), which provides a deep
comprehension of language nuances, and a decoder-only (LLaMA-like) Large
Language Model (LLM) to extract ethical concern-related app reviews at scale.
Utilizing 43,647 app reviews from the mental health domain, the proposed
methodology 1) Evaluates four NLI models to extract potential privacy reviews
and compares the results of domain-specific privacy hypotheses with generic
privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to
privacy concerns; and 3) Uses the best NLI and LLM models further to extract
new privacy reviews from the dataset. Results show that the
DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses
yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the
classification of app reviews. Then, using NLI+LLM, an additional 1,008 new
privacy-related reviews were extracted that were not identified through the
keyword-based approach in previous research, thus demonstrating the
effectiveness of the proposed approach.",cs.AI
Data-Centric Learning Framework for Real-Time Detection of Aiming Beam in Fluorescence Lifetime Imaging Guided Surgery,"This study introduces a novel data-centric approach to improve real-time
surgical guidance using fiber-based fluorescence lifetime imaging (FLIm). A key
aspect of the methodology is the accurate detection of the aiming beam, which
is essential for localizing points used to map FLIm measurements onto the
tissue region within the surgical field. The primary challenge arises from the
complex and variable conditions encountered in the surgical environment,
particularly in Transoral Robotic Surgery (TORS). Uneven illumination in the
surgical field can cause reflections, reduce contrast, and results in
inconsistent color representation, further complicating aiming beam detection.
To overcome these challenges, an instance segmentation model was developed
using a data-centric training strategy that improves accuracy by minimizing
label noise and enhancing detection robustness. The model was evaluated on a
dataset comprising 40 in vivo surgical videos, demonstrating a median detection
rate of 85%. This performance was maintained when the model was integrated in a
clinical system, achieving a similar detection rate of 85% during TORS
procedures conducted in patients. The system's computational efficiency,
measured at approximately 24 frames per second (FPS), was sufficient for
real-time surgical guidance. This study enhances the reliability of FLIm-based
aiming beam detection in complex surgical environments, advancing the
feasibility of real-time, image-guided interventions for improved surgical
precision",cs.AI
Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set Domain Generalization,"Open-set domain generalization addresses a real-world challenge: training a
model to generalize across unseen domains (domain generalization) while also
detecting samples from unknown classes not encountered during training
(open-set recognition). However, most existing approaches tackle these issues
separately, limiting their practical applicability. To overcome this
limitation, we propose a unified framework for open-set domain generalization
by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic
consistency across different domains within the feature space, enabling more
accurate detection of OOD instances in unseen domains. Additionally, we adopt a
generative model to produce synthetic data with novel domain styles or class
labels, enhancing model robustness. Initial experiments show that our method
improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly
increasing in-distribution classification accuracy.",cs.AI
Federated Learning Client Pruning for Noisy Labels,"Federated Learning (FL) enables collaborative model training across
decentralized edge devices while preserving data privacy. However, existing FL
methods often assume clean annotated datasets, impractical for
resource-constrained edge devices. In reality, noisy labels are prevalent,
posing significant challenges to FL performance. Prior approaches attempt label
correction and robust training techniques but exhibit limited efficacy,
particularly under high noise levels. This paper introduces ClipFL (Federated
Learning Client Pruning), a novel framework addressing noisy labels from a
fresh perspective. ClipFL identifies and excludes noisy clients based on their
performance on a clean validation dataset, tracked using a Noise Candidacy
Score (NCS). The framework comprises three phases: pre-client pruning to
identify potential noisy clients and calculate their NCS, client pruning to
exclude a percentage of clients with the highest NCS, and post-client pruning
for fine-tuning the global model with standard FL on clean clients. Empirical
evaluation demonstrates ClipFL's efficacy across diverse datasets and noise
levels, achieving accurate noisy client identification, superior performance,
faster convergence, and reduced communication costs compared to
state-of-the-art FL methods. Our code is available at
https://github.com/MMorafah/ClipFL.",cs.AI
Firing Rate Models as Associative Memory: Excitatory-Inhibitory Balance for Robust Retrieval,"Firing rate models are dynamical systems widely used in applied and
theoretical neuroscience to describe local cortical dynamics in neuronal
populations. By providing a macroscopic perspective of neuronal activity, these
models are essential for investigating oscillatory phenomena, chaotic behavior,
and associative memory processes. Despite their widespread use, the application
of firing rate models to associative memory networks has received limited
mathematical exploration, and most existing studies are focused on specific
models. Conversely, well-established associative memory designs, such as
Hopfield networks, lack key biologically-relevant features intrinsic to firing
rate models, including positivity and interpretable synaptic matrices that
reflect excitatory and inhibitory interactions. To address this gap, we propose
a general framework that ensures the emergence of re-scaled memory patterns as
stable equilibria in the firing rate dynamics. Furthermore, we analyze the
conditions under which the memories are locally and globally asymptotically
stable, providing insights into constructing biologically-plausible and robust
systems for associative memory retrieval.",cs.AI
Data-Driven Analysis of AI in Medical Device Software in China: Deep Learning and General AI Trends Based on Regulatory Data,"Artificial intelligence (AI) in medical device software (MDSW) represents a
transformative clinical technology, attracting increasing attention within both
the medical community and the regulators. In this study, we leverage a
data-driven approach to automatically extract and analyze AI-enabled medical
devices (AIMD) from the National Medical Products Administration (NMPA)
regulatory database. The continued increase in publicly available regulatory
data requires scalable methods for analysis. Automation of regulatory
information screening is essential to create reproducible insights that can be
quickly updated in an ever changing medical device landscape. More than 4
million entries were assessed, identifying 2,174 MDSW registrations, including
531 standalone applications and 1,643 integrated within medical devices, of
which 43 were AI-enabled. It was shown that the leading medical specialties
utilizing AIMD include respiratory (20.5%), ophthalmology/endocrinology
(12.8%), and orthopedics (10.3%). This approach greatly improves the speed of
data extracting providing a greater ability to compare and contrast. This study
provides the first extensive, data-driven exploration of AIMD in China,
showcasing the potential of automated regulatory data analysis in understanding
and advancing the landscape of AI in medical technology.",cs.AI
Ensemble Learning for Microbubble Localization in Super-Resolution Ultrasound,"Super-resolution ultrasound (SR-US) is a powerful imaging technique for
capturing microvasculature and blood flow at high spatial resolution. However,
accurate microbubble (MB) localization remains a key challenge, as errors in
localization can propagate through subsequent stages of the super-resolution
process, affecting overall performance. In this paper, we explore the potential
of ensemble learning techniques to enhance MB localization by increasing
detection sensitivity and reducing false positives. Our study evaluates the
effectiveness of ensemble methods on both in vivo and simulated outputs of a
Deformable DEtection TRansformer (Deformable DETR) network. As a result of our
study, we are able to demonstrate the advantages of these ensemble approaches
by showing improved precision and recall in MB detection and offering insights
into their application in SR-US.",cs.AI
Warmstarting for Scaling Language Models,"Scaling model sizes to scale performance has worked remarkably well for the
current large language models paradigm. The research and empirical findings of
various scaling studies led to novel scaling results and laws that guides
subsequent research. High training costs for contemporary scales of data and
models result in a lack of thorough understanding of how to tune and arrive at
such training setups. One direction to ameliorate the cost of pretraining large
models is to warmstart the large-scale training from smaller models that are
cheaper to tune. In this work, we attempt to understand if the behavior of
optimal hyperparameters can be retained under warmstarting for scaling. We
explore simple operations that allow the application of theoretically motivated
methods of zero-shot transfer of optimal hyperparameters using {\mu}Transfer.
We investigate the aspects that contribute to the speedup in convergence and
the preservation of stable training dynamics under warmstarting with
{\mu}Transfer. We find that shrinking smaller model weights, zero-padding, and
perturbing the resulting larger model with scaled initialization from {\mu}P
enables effective warmstarting of $\mut{}$.",cs.AI
Multimodal Fusion Balancing Through Game-Theoretic Regularization,"Multimodal learning can complete the picture of information extraction by
uncovering key dependencies between data sources. However, current systems fail
to fully leverage multiple modalities for optimal performance. This has been
attributed to modality competition, where modalities strive for training
resources, leaving some underoptimized. We show that current balancing methods
struggle to train multimodal models that surpass even simple baselines, such as
ensembles. This raises the question: how can we ensure that all modalities in
multimodal training are sufficiently trained, and that learning from new
modalities consistently improves performance? This paper proposes the
Multimodal Competition Regularizer (MCR), a new loss component inspired by
mutual information (MI) decomposition designed to prevent the adverse effects
of competition in multimodal training. Our key contributions are: 1)
Introducing game-theoretic principles in multimodal learning, where each
modality acts as a player competing to maximize its influence on the final
outcome, enabling automatic balancing of the MI terms. 2) Refining lower and
upper bounds for each MI term to enhance the extraction of task-relevant unique
and shared information across modalities. 3) Suggesting latent space
permutations for conditional MI estimation, significantly improving
computational efficiency. MCR outperforms all previously suggested training
strategies and is the first to consistently improve multimodal learning beyond
the ensemble baseline, clearly demonstrating that combining modalities leads to
significant performance gains on both synthetic and large real-world datasets.",cs.AI
Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations,"While a large body of work inspects language models for biases concerning
gender, race, occupation and religion, biases of geographical nature are
relatively less explored. Some recent studies benchmark the degree to which
large language models encode geospatial knowledge. However, the impact of the
encoded geographical knowledge (or lack thereof) on real-world applications has
not been documented. In this work, we examine large language models for two
common scenarios that require geographical knowledge: (a) travel
recommendations and (b) geo-anchored story generation. Specifically, we study
four popular language models, and across about $100$K travel requests, and
$200$K story generations, we observe that travel recommendations corresponding
to poorer countries are less unique with fewer location references, and stories
from these regions more often convey emotions of hardship and sadness compared
to those from wealthier nations.",cs.AI
Harnessing Smartphone Sensors for Enhanced Road Safety: A Comprehensive Dataset and Review,"Severe collisions can result from aggressive driving and poor road
conditions, emphasizing the need for effective monitoring to ensure safety.
Smartphones, with their array of built-in sensors, offer a practical and
affordable solution for road-sensing. However, the lack of reliable,
standardized datasets has hindered progress in assessing road conditions and
driving patterns. This study addresses this gap by introducing a comprehensive
dataset derived from smartphone sensors, which surpasses existing datasets by
incorporating a diverse range of sensors including accelerometer, gyroscope,
magnetometer, GPS, gravity, orientation, and uncalibrated sensors. These
sensors capture extensive parameters such as acceleration force, gravitation,
rotation rate, magnetic field strength, and vehicle speed, providing a detailed
understanding of road conditions and driving behaviors. The dataset is designed
to enhance road safety, infrastructure maintenance, traffic management, and
urban planning. By making this dataset available to the community, the study
aims to foster collaboration, inspire further research, and facilitate the
development of innovative solutions in intelligent transportation systems.",cs.AI
X-DFS: Explainable Artificial Intelligence Guided Design-for-Security Solution Space Exploration,"Design and manufacturing of integrated circuits predominantly use a globally
distributed semiconductor supply chain involving diverse entities. The modern
semiconductor supply chain has been designed to boost production efficiency,
but is filled with major security concerns such as malicious modifications
(hardware Trojans), reverse engineering (RE), and cloning. While being
deployed, digital systems are also subject to a plethora of threats such as
power, timing, and electromagnetic (EM) side channel attacks. Many
Design-for-Security (DFS) solutions have been proposed to deal with these
vulnerabilities, and such solutions (DFS) relays on strategic modifications
(e.g., logic locking, side channel resilient masking, and dummy logic
insertion) of the digital designs for ensuring a higher level of security.
However, most of these DFS strategies lack robust formalism, are often not
human-understandable, and require an extensive amount of human expert effort
during their development/use. All of these factors make it difficult to keep up
with the ever growing number of microelectronic vulnerabilities. In this work,
we propose X-DFS, an explainable Artificial Intelligence (AI) guided DFS
solution-space exploration approach that can dramatically cut down the
mitigation strategy development/use time while enriching our understanding of
the vulnerability by providing human-understandable decision rationale. We
implement X-DFS and comprehensively evaluate it for reverse engineering threats
(SAIL, SWEEP, and OMLA) and formalize a generalized mechanism for applying
X-DFS to defend against other threats such as hardware Trojans, fault attacks,
and side channel attacks for seamless future extensions.",cs.AI
Artificial Intelligence Ecosystem for Automating Self-Directed Teaching,"This research introduces an innovative artificial intelligence-driven
educational concept designed to optimize self-directed learning through
personalized course delivery and automated teaching assistance. The system
leverages fine-tuned AI models to create an adaptive learning environment that
encompasses customized roadmaps, automated presentation generation, and
three-dimensional modeling for complex concept visualization. By integrating
real-time virtual assistance for doubt resolution, the platform addresses the
immediate educational needs of learners while promoting autonomous learning
practices. This study explores the psychological advantages of self-directed
learning and demonstrates how AI automation can enhance educational outcomes
through personalized content delivery and interactive support mechanisms. The
research contributes to the growing field of educational technology by
presenting a comprehensive framework that combines automated content
generation, visual learning aids, and intelligent tutoring to create an
efficient, scalable solution for modern educational needs. Preliminary findings
suggest that this approach not only accommodates diverse learning styles but
also strengthens student engagement and knowledge retention through its
emphasis on self-paced, independent learning methodologies.",cs.AI
The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,"Language models have shown impressive performance on tasks within their
training distribution, but often struggle with novel problems requiring complex
reasoning. We investigate the effectiveness of test-time training (TTT) --
updating model parameters temporarily during inference using a loss derived
from input data -- as a mechanism for improving models' reasoning capabilities,
using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through
systematic experimentation, we identify three crucial components for successful
TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and
augmentations (3) per-instance training. TTT significantly improves performance
on ARC tasks, achieving up to 6x improvement in accuracy compared to base
fine-tuned models; applying TTT to an 8B-parameter language model, we achieve
53% accuracy on the ARC's public validation set, improving the state-of-the-art
by nearly 25% for public and purely neural approaches. By ensembling our method
with recent program generation approaches, we get SoTA public validation
accuracy of 61.9%, matching the average human score. Our findings suggest that
explicit symbolic search is not the only path to improved abstract reasoning in
neural language models; additional test-time applied to continued training on
few-shot examples can also be extremely effective.",cs.AI
UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts,"The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and adaptability. This
paper introduces the UTMath Benchmark, which robustly evaluates the models
through extensive unit tests. It consists of 1,053 problems across 9
mathematical domains, with over 68 test cases per problem. We propose an
innovative evaluation framework inspired by unit testing in software
development, focusing on both accuracy and reliability of results. Furthermore,
we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which
encourages LLMs to perform explicit reasoning before generating code, leading
to generating more advanced solution and improved performance. Furthermore, we
are releasing not only the UTMath benchmark but also the UTMath-Train training
dataset (more than 70k samples), to support the community in further exploring
mathematical reasoning.",cs.AI
Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models,"Adding Object into images based on text instructions is a challenging task in
semantic image editing, requiring a balance between preserving the original
scene and seamlessly integrating the new object in a fitting location. Despite
extensive efforts, existing models often struggle with this balance,
particularly with finding a natural location for adding an object in complex
scenes. We introduce Add-it, a training-free approach that extends diffusion
models' attention mechanisms to incorporate information from three key sources:
the scene image, the text prompt, and the generated image itself. Our weighted
extended-attention mechanism maintains structural consistency and fine details
while ensuring natural object placement. Without task-specific fine-tuning,
Add-it achieves state-of-the-art results on both real and generated image
insertion benchmarks, including our newly constructed ""Additing Affordance
Benchmark"" for evaluating object placement plausibility, outperforming
supervised methods. Human evaluations show that Add-it is preferred in over 80%
of cases, and it also demonstrates improvements in various automated metrics.",cs.AI
Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving,"To enhance large language models (LLMs) for chemistry problem solving,
several LLM-based agents augmented with tools have been proposed, such as
ChemCrow and Coscientist. However, their evaluations are narrow in scope,
leaving a large gap in understanding the benefits of tools across diverse
chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced
chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its
performance on both specialized chemistry tasks and general chemistry
questions. Surprisingly, ChemAgent does not consistently outperform its base
LLMs without tools. Our error analysis with a chemistry expert suggests that:
For specialized chemistry tasks, such as synthesis prediction, we should
augment agents with specialized tools; however, for general chemistry questions
like those in exams, agents' ability to reason correctly with chemistry
knowledge matters more, and tool augmentation does not always help.",cs.AI
Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide
a rich source of physical knowledge about the dynamics and motions of objects
and tasks. However, video models are not grounded in the embodiment of an
agent, and do not describe how to actuate the world to reach the visual states
depicted in a video. To tackle this problem, current methods use a separate
vision-based inverse dynamic model trained on embodiment-specific data to map
image states to actions. Gathering data to train such a model is often
expensive and challenging, and this model is limited to visual settings similar
to the ones in which data are available. In this paper, we investigate how to
directly ground video models to continuous actions through self-exploration in
the embodied environment -- using generated video states as visual goals for
exploration. We propose a framework that uses trajectory level action
generation in combination with video guidance to enable an agent to solve
complex tasks without any external supervision, e.g., rewards, action labels,
or segmentation masks. We validate the proposed approach on 8 tasks in Libero,
6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual
Navigation. We show how our approach is on par with or even surpasses multiple
behavior cloning baselines trained on expert demonstrations while without
requiring any action annotations.",cs.AI
TreeCoders: Trees of Transformers,"In this paper, we introduce TreeCoders, a novel family of transformer trees.
We moved away from traditional linear transformers to complete k-ary trees.
Transformer blocks serve as nodes, and generic classifiers learn to select the
best child and route the sequence of tokens to a specific leaf. The selectors,
moved outside the transformer blocks, allow for the use of a variety of
architecture without further modifications. Furthermore, our proposed
architecture supports sparse node activation due to the logarithmic complexity
of a tree search. We validate our idea by testing a series of decoder-only tree
transformers, achieving competitive results across a diverse range of language
datasets. Our study demonstrates that the proposed tree transformer model
outperforms a size-equivalent linear transformer model 76\% of the time over a
wide range of tree architectures. Furthermore, our proposed model naturally
lends itself to distributed implementation.",cs.AI
'Explaining RL Decisions with Trajectories': A Reproducibility Study,"This work investigates the reproducibility of the paper 'Explaining RL
decisions with trajectories'. The original paper introduces a novel approach in
explainable reinforcement learning based on the attribution decisions of an
agent to specific clusters of trajectories encountered during training. We
verify the main claims from the paper, which state that (i) training on less
trajectories induces a lower initial state value, (ii) trajectories in a
cluster present similar high-level patterns, (iii) distant trajectories
influence the decision of an agent, and (iv) humans correctly identify the
attributed trajectories to the decision of the agent. We recover the
environments used by the authors based on the partial original code they
provided for one of the environments (Grid-World), and implemented the
remaining from scratch (Seaquest, HalfCheetah, Breakout and Q*Bert). While we
confirm that (i), (ii), and (iii) partially hold, we extend on the largely
qualitative experiments from the authors by introducing a quantitative metric
to further support (iii), and new experiments and visual results for (i).
Moreover, we investigate the use of different clustering algorithms and encoder
architectures to further support (ii). We could not support (iv), given the
limited extent of the original experiments. We conclude that, while some of the
claims can be supported, further investigations and experiments could be of
interest. We recognise the novelty of the work from the authors and hope that
our work paves the way for clearer and more transparent approaches.",cs.AI
OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision,"Instruction-guided image editing methods have demonstrated significant
potential by training diffusion models on automatically synthesized or manually
annotated image editing pairs. However, these methods remain far from
practical, real-life applications. We identify three primary challenges
contributing to this gap. Firstly, existing models have limited editing skills
due to the biased synthesis process. Secondly, these methods are trained with
datasets with a high volume of noise and artifacts. This is due to the
application of simple filtering methods like CLIP-score. Thirdly, all these
datasets are restricted to a single low resolution and fixed aspect ratio,
limiting the versatility to handle real-world use cases. In this paper, we
present \omniedit, which is an omnipotent editor to handle seven different
image editing tasks with any aspect ratio seamlessly. Our contribution is in
four folds: (1) \omniedit is trained by utilizing the supervision from seven
different specialist models to ensure task coverage. (2) we utilize importance
sampling based on the scores provided by large multimodal models (like GPT-4o)
instead of CLIP-score to improve the data quality. (3) we propose a new editing
architecture called EditNet to greatly boost the editing success rate, (4) we
provide images with different aspect ratios to ensure that our model can handle
any image in the wild. We have curated a test set containing images of
different aspect ratios, accompanied by diverse instructions to cover different
tasks. Both automatic evaluation and human evaluations demonstrate that
\omniedit can significantly outperform all the existing models. Our code,
dataset and model will be available at
\url{https://tiger-ai-lab.github.io/OmniEdit/}",cs.AI
The Super Weight in Large Language Models,"Recent works have shown a surprising result: a small fraction of Large
Language Model (LLM) parameter outliers are disproportionately important to the
quality of the model. LLMs contain billions of parameters, so these small
fractions, such as 0.01%, translate to hundreds of thousands of parameters. In
this work, we present an even more surprising finding: Pruning as few as a
single parameter can destroy an LLM's ability to generate text -- increasing
perplexity by 3 orders of magnitude and reducing zero-shot accuracy to
guessing. We propose a data-free method for identifying such parameters, termed
super weights, using a single forward pass through the model. We additionally
find that these super weights induce correspondingly rare and large activation
outliers, termed super activations. When preserved with high precision, super
activations can improve simple round-to-nearest quantization to become
competitive with state-of-the-art methods. For weight quantization, we
similarly find that by preserving the super weight and clipping other weight
outliers, round-to-nearest quantization can scale to much larger block sizes
than previously considered. To facilitate further research into super weights,
we provide an index of super weight coordinates for common, openly available
LLMs.",cs.AI
NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics,"Large language models (LLMs) prompted with text and audio represent the state
of the art in various auditory tasks, including speech, music, and general
audio, showing emergent abilities on unseen tasks. However, these capabilities
have yet to be fully demonstrated in bioacoustics tasks, such as detecting
animal vocalizations in large recordings, classifying rare and endangered
species, and labeling context and behavior - tasks that are crucial for
conservation, biodiversity monitoring, and the study of animal behavior. In
this work, we present NatureLM-audio, the first audio-language foundation model
specifically designed for bioacoustics. Our carefully curated training dataset
comprises text-audio pairs spanning a diverse range of bioacoustics, speech,
and music data, designed to address the challenges posed by limited annotated
datasets in the field. We demonstrate successful transfer of learned
representations from music and speech to bioacoustics, and our model shows
promising generalization to unseen taxa and tasks. Importantly, we test
NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of
the art (SotA) on several bioacoustics tasks, including zero-shot
classification of unseen species. To advance bioacoustics research, we also
open-source the code for generating training and benchmark data, as well as for
training the model.",cs.AI
Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised Domain Adaptation,"Multi-source unsupervised domain adaptation aims to leverage labeled data
from multiple source domains for training a machine learning model to
generalize well on a target domain without labels. Source domain selection
plays a crucial role in determining the model's performance. It relies on the
similarities amongst source and target domains. Nonetheless, existing work for
source domain selection often involves heavyweight computational procedures,
especially when dealing with numerous source domains and the need to identify
the best ones from them. In this paper, we introduce a framework for gradual
fine tuning (GFT) of machine learning models on multiple source domains. We
represent multiple source domains as an undirected weighted graph. We then give
a new generalization error bound for GFT along any path within the graph, which
is used to determine the optimal path corresponding to the optimal training
order. With this formulation, we introduce three lightweight graph-routing
strategies which tend to minimize the error bound. Our best strategy improves
$2.3\%$ of accuracy over the state-of-the-art on Natural Language Inference
(NLI) task and achieves competitive performance on Sentiment Analysis (SA)
task, especially a $3.9\%$ improvement on a more diverse subset of data we use
for SA.",cs.AI
Counterfactual Generation from Language Models,"Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to intervene on
these models. To understand the impact of interventions precisely, it is useful
to examine counterfactuals -- e.g., how a given sentence would have appeared
had it been generated by the model following a specific intervention. We
highlight that counterfactual reasoning is conceptually distinct from
interventions, as articulated in Pearl's causal hierarchy. Based on this
observation, we propose a framework for generating true string counterfactuals
by reformulating language models as Generalized Structural-equation. Models
using the Gumbel-max trick. This allows us to model the joint distribution over
original strings and their counterfactuals resulting from the same
instantiation of the sampling noise. We develop an algorithm based on hindsight
Gumbel sampling that allows us to infer the latent noise variables and generate
counterfactuals of observed strings. Our experiments demonstrate that the
approach produces meaningful counterfactuals while at the same time showing
that commonly used intervention techniques have considerable undesired side
effects.",cs.AI
More Expressive Attention with Negative Weights,"We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.",cs.AI
Anytime Sequential Halving in Monte-Carlo Tree Search,"Monte-Carlo Tree Search (MCTS) typically uses multi-armed bandit (MAB)
strategies designed to minimize cumulative regret, such as UCB1, as its
selection strategy. However, in the root node of the search tree, it is more
sensible to minimize simple regret. Previous work has proposed using Sequential
Halving as selection strategy in the root node, as, in theory, it performs
better with respect to simple regret. However, Sequential Halving requires a
budget of iterations to be predetermined, which is often impractical. This
paper proposes an anytime version of the algorithm, which can be halted at any
arbitrary time and still return a satisfactory result, while being designed
such that it approximates the behavior of Sequential Halving. Empirical results
in synthetic MAB problems and ten different board games demonstrate that the
algorithm's performance is competitive with Sequential Halving and UCB1 (and
their analogues in MCTS).",cs.AI
A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19,"Monitoring public sentiment via social media is potentially helpful during
health crises such as the COVID-19 pandemic. However, traditional
frequency-based, data-driven neural network-based approaches can miss newly
relevant content due to the evolving nature of language in a dynamically
evolving environment. Human-curated symbolic knowledge sources, such as
lexicons for standard language and slang terms, can potentially elevate social
media signals in evolving language. We introduce a neurosymbolic method that
integrates neural networks with symbolic knowledge sources, enhancing the
detection and interpretation of mental health-related tweets relevant to
COVID-19. Our method was evaluated using a corpus of large datasets
(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news
articles) and multiple knowledge graphs. This method dynamically adapts to
evolving language, outperforming purely data-driven models with an F1 score
exceeding 92\%. This approach also showed faster adaptation to new data and
lower computational demands than fine-tuning pre-trained large language models
(LLMs). This study demonstrates the benefit of neurosymbolic methods in
interpreting text in a dynamic environment for tasks such as health
surveillance.",cs.AI
RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration,"This study investigates the efficacy of Multi-Agent Systems in eliciting
cross-agent communication and enhancing collective intelligence through group
decision-making in a decentralized setting. Unlike centralized mechanisms,
where a fixed hierarchy governs social choice, decentralized group
decision-making allows agents to engage in joint deliberation. Our research
focuses on the dynamics of communication and decision-making within various
social choice methods. By applying different voting rules in various
environments, we find that moderate decision flexibility yields better
outcomes. Additionally, exploring the linguistic features of agent-to-agent
conversations reveals indicators of effective collaboration, offering insights
into communication patterns that facilitate or hinder collaboration. Finally,
we propose various methods for determining the optimal stopping point in
multi-agent collaborations based on linguistic cues. Our findings contribute to
a deeper understanding of how decentralized decision-making and group
conversation shape multi-agent collaboration, with implications for the design
of more effective MAS environments.",cs.AI
HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals,"Task-Oriented Dialogue (TOD) systems assist users in completing tasks through
natural language interactions, often relying on a single-layered workflow
structure for slot-filling in public tasks, such as hotel bookings. However, in
enterprise environments, which involve rich domain-specific knowledge, TOD
systems face challenges due to task complexity and the lack of standardized
documentation. In this work, we introduce HierTOD, an enterprise TOD system
driven by hierarchical goals and can support composite workflows. By focusing
on goal-driven interactions, our system serves a more proactive role,
facilitating mixed-initiative dialogue and improving task completion. Equipped
with components for natural language understanding, composite goal retriever,
dialogue management, and response generation, backed by a well-organized data
service with domain knowledge base and retrieval engine, HierTOD delivers
efficient task assistance. Furthermore, our system implementation unifies two
TOD paradigms: slot-filling for information collection and step-by-step
guidance for task execution. Our human study demonstrates the effectiveness and
helpfulness of HierTOD in performing both paradigms.",cs.AI
Variational Graph Contrastive Learning,"Graph representation learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-supervised learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of graph
characteristics while controlling the distribution of generated subgraphs. We
employ optimal transport distances, including Wasserstein and
Gromov-Wasserstein distances, to effectively measure the similarity between
subgraphs, enhancing the robustness of the contrastive learning process.
Extensive experiments across multiple benchmarks demonstrate that SGEC
outperforms or presents competitive performance against state-of-the-art
approaches. Our findings provide insights into the design of SSL methods for
GRL, emphasizing the importance of the distribution of the generated
contrastive pairs.",cs.AI
Edify 3D: Scalable High-Quality 3D Asset Generation,"We introduce Edify 3D, an advanced solution designed for high-quality 3D
asset generation. Our method first synthesizes RGB and surface normal images of
the described object at multiple viewpoints using a diffusion model. The
multi-view observations are then used to reconstruct the shape, texture, and
PBR materials of the object. Our method can generate high-quality 3D assets
with detailed geometry, clean shape topologies, high-resolution textures, and
materials within 2 minutes of runtime.",cs.AI
Stronger Models are NOT Stronger Teachers for Instruction Tuning,"Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.",cs.AI
Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis,"Although text-to-image (T2I) models exhibit remarkable generation
capabilities, they frequently fail to accurately bind semantically related
objects or attributes in the input prompts; a challenge termed semantic
binding. Previous approaches either involve intensive fine-tuning of the entire
T2I model or require users or large language models to specify generation
layouts, adding complexity. In this paper, we define semantic binding as the
task of associating a given object with its attribute, termed attribute
binding, or linking it to other related sub-objects, referred to as object
binding. We introduce a novel method called Token Merging (ToMe), which
enhances semantic binding by aggregating relevant tokens into a single
composite token. This ensures that the object, its attributes and sub-objects
all share the same cross-attention map. Additionally, to address potential
confusion among main objects with complex textual prompts, we propose end token
substitution as a complementary strategy. To further refine our approach in the
initial stages of T2I generation, where layouts are determined, we incorporate
two auxiliary losses, an entropy loss and a semantic binding loss, to
iteratively update the composite token to improve the generation integrity. We
conducted extensive experiments to validate the effectiveness of ToMe,
comparing it against various existing methods on the T2I-CompBench and our
proposed GPT-4o object binding benchmark. Our method is particularly effective
in complex scenarios that involve multiple objects and attributes, which
previous methods often fail to address. The code will be publicly available at
\url{https://github.com/hutaihang/ToMe}.",cs.AI
Fast and Robust Contextual Node Representation Learning over Dynamic Graphs,"Real-world graphs grow rapidly with edge and vertex insertions over time,
motivating the problem of efficiently maintaining robust node representation
over evolving graphs. Recent efficient GNNs are designed to decouple recursive
message passing from the learning process, and favor Personalized PageRank
(PPR) as the underlying feature propagation mechanism. However, most PPR-based
GNNs are designed for static graphs, and efficient PPR maintenance remains as
an open problem. Further, there is surprisingly little theoretical
justification for the choice of PPR, despite its impressive empirical
performance.
  In this paper, we are inspired by the recent PPR formulation as an explicit
$\ell_1$-regularized optimization problem and propose a unified dynamic graph
learning framework based on sparse node-wise attention. We also present a set
of desired properties to justify the choice of PPR in STOA GNNs, and serves as
the guideline for future node attention designs. Meanwhile, we take advantage
of the PPR-equivalent optimization formulation and employ the proximal gradient
method (ISTA) to improve the efficiency of PPR-based GNNs upto 6 times.
Finally, we instantiate a simple-yet-effective model (\textsc{GoPPE}) with
robust positional encodings by maximizing PPR previously used as attention. The
model performs comparably to or better than the STOA baselines and greatly
outperforms when the initial node attributes are noisy during graph evolution,
demonstrating the effectiveness and robustness of \textsc{GoPPE}.",cs.AI
Learning Multi-Agent Collaborative Manipulation for Long-Horizon Quadrupedal Pushing,"Recently, quadrupedal locomotion has achieved significant success, but their
manipulation capabilities, particularly in handling large objects, remain
limited, restricting their usefulness in demanding real-world applications such
as search and rescue, construction, industrial automation, and room
organization. This paper tackles the task of obstacle-aware, long-horizon
pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent
reinforcement learning framework with three levels of control. The high-level
controller integrates an RRT planner and a centralized adaptive policy to
generate subgoals, while the mid-level controller uses a decentralized
goal-conditioned policy to guide the robots toward these sub-goals. A
pre-trained low-level locomotion policy executes the movement commands. We
evaluate our method against several baselines in simulation, demonstrating
significant improvements over baseline approaches, with 36.0% higher success
rates and 24.5% reduction in completion time than the best baseline. Our
framework successfully enables long-horizon, obstacle-aware manipulation tasks
like Push-Cuboid and Push-T on Go1 robots in the real world.",cs.AI
Bounded Rationality Equilibrium Learning in Mean Field Games,"Mean field games (MFGs) tractably model behavior in large agent populations.
The literature on learning MFG equilibria typically focuses on finding Nash
equilibria (NE), which assume perfectly rational agents and are hence
implausible in many realistic situations. To overcome these limitations, we
incorporate bounded rationality into MFGs by leveraging the well-known concept
of quantal response equilibria (QRE). Two novel types of MFG QRE enable the
modeling of large agent populations where individuals only noisily estimate the
true objective. We also introduce a second source of bounded rationality to
MFGs by restricting agents' planning horizon. The resulting novel receding
horizon (RH) MFGs are combined with QRE and existing approaches to model
different aspects of bounded rationality in MFGs. We formally define MFG QRE
and RH MFGs and compare them to existing equilibrium concepts such as
entropy-regularized NE. Subsequently, we design generalized fixed point
iteration and fictitious play algorithms to learn QRE and RH equilibria. After
a theoretical analysis, we give different examples to evaluate the capabilities
of our learning algorithms and outline practical differences between the
equilibrium concepts.",cs.AI
A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs,"As modern web services increasingly rely on REST APIs, their thorough testing
has become crucial. Furthermore, the advent of REST API specifications such as
the OpenAPI Specification has led to the emergence of many black-box REST API
testing tools. However, these tools often focus on individual test elements in
isolation (e.g., APIs, parameters, values), resulting in lower coverage and
less effectiveness in detecting faults (i.e., 500 response codes). To address
these limitations, we present AutoRestTest, the first black-box framework to
adopt a dependency-embedded multi-agent approach for REST API testing,
integrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property
Dependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats
REST API testing as a separable problem, where four agents -- API, dependency,
parameter, and value -- collaborate to optimize API exploration. LLMs handle
domain-specific value restrictions, the SPDG model simplifies the search space
for dependencies using a similarity score between API operations, and MARL
dynamically optimizes the agents' behavior. Evaluated on 12 real-world REST
services, AutoRestTest outperforms the four leading black-box REST API testing
tools, including those assisted by RESTGPT (which augments realistic test
inputs using LLMs), in terms of code coverage, operation coverage, and fault
detection. Notably, AutoRestTest is the only tool able to identify an internal
server error in Spotify. Our ablation study underscores the significant
contributions of the agent learning, SPDG, and LLM components.",cs.AI
Towards Characterizing Cyber Networks with Large Language Models,"Threat hunting analyzes large, noisy, high-dimensional data to find sparse
adversarial behavior. We believe adversarial activities, however they are
disguised, are extremely difficult to completely obscure in high dimensional
space. In this paper, we employ these latent features of cyber data to find
anomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM
was trained on Zeek network traffic logs from both a real-world production
network and an from Internet of Things (IoT) cybersecurity testbed. The model
is deliberately overtrained on a sliding window of data to characterize each
window closely. We use the Adjusted Rand Index (ARI) to comparing the k-means
clustering of CLEM output to expert labeling of the embeddings. Our approach
demonstrates that there is promise in using natural language modeling to
understand cyber data.",cs.AI
OCMDP: Observation-Constrained Markov Decision Process,"In many practical applications, decision-making processes must balance the
costs of acquiring information with the benefits it provides. Traditional
control systems often assume full observability, an unrealistic assumption when
observations are expensive. We tackle the challenge of simultaneously learning
observation and control strategies in such cost-sensitive environments by
introducing the Observation-Constrained Markov Decision Process (OCMDP), where
the policy influences the observability of the true state. To manage the
complexity arising from the combined observation and control actions, we
develop an iterative, model-free deep reinforcement learning algorithm that
separates the sensing and control components of the policy. This decomposition
enables efficient learning in the expanded action space by focusing on when and
what to observe, as well as determining optimal control actions, without
requiring knowledge of the environment's dynamics. We validate our approach on
a simulated diagnostic task and a realistic healthcare environment using
HeartPole. Given both scenarios, the experimental results demonstrate that our
model achieves a substantial reduction in observation costs on average,
significantly outperforming baseline methods by a notable margin in efficiency.",cs.AI
To Train or Not to Train: Balancing Efficiency and Training Cost in Deep Reinforcement Learning for Mobile Edge Computing,"Artificial Intelligence (AI) is a key component of 6G networks, as it enables
communication and computing services to adapt to end users' requirements and
demand patterns. The management of Mobile Edge Computing (MEC) is a meaningful
example of AI application: computational resources available at the network
edge need to be carefully allocated to users, whose jobs may have different
priorities and latency requirements. The research community has developed
several AI algorithms to perform this resource allocation, but it has neglected
a key aspect: learning is itself a computationally demanding task, and
considering free training results in idealized conditions and performance in
simulations. In this work, we consider a more realistic case in which the cost
of learning is specifically accounted for, presenting a new algorithm to
dynamically select when to train a Deep Reinforcement Learning (DRL) agent that
allocates resources. Our method is highly general, as it can be directly
applied to any scenario involving a training overhead, and it can approach the
same performance as an ideal learning agent even under realistic training
conditions.",cs.AI
StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification,"Existing large vision-language models (LVLMs) are largely limited to
processing short, seconds-long videos and struggle with generating coherent
descriptions for extended video spanning minutes or more. Long video
description introduces new challenges, such as plot-level consistency across
descriptions. To address these, we figure out audio-visual character
identification, matching character names to each dialogue, as a key factor. We
propose StoryTeller, a system for generating dense descriptions of long videos,
incorporating both low-level visual concepts and high-level plot information.
StoryTeller uses a multimodal large language model that integrates visual,
audio, and text modalities to perform audio-visual character identification on
minute-long video clips. The results are then fed into a LVLM to enhance
consistency of video description. We validate our approach on movie description
tasks and introduce MovieStory101, a dataset with dense descriptions for
three-minute movie clips. To evaluate long video descriptions, we create
MovieQA, a large set of multiple-choice questions for the MovieStory101 test
set. We assess descriptions by inputting them into GPT-4 to answer these
questions, using accuracy as an automatic evaluation metric. Experiments show
that StoryTeller outperforms all open and closed-source baselines on MovieQA,
achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and
demonstrating a +15.56% advantage in human side-by-side evaluations.
Additionally, incorporating audio-visual character identification from
StoryTeller improves the performance of all video description models, with
Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,
respectively, in accuracy on MovieQA.",cs.AI
An Interpretable X-ray Style Transfer via Trainable Local Laplacian Filter,"Radiologists have preferred visual impressions or 'styles' of X-ray images
that are manually adjusted to their needs to support their diagnostic
performance. In this work, we propose an automatic and interpretable X-ray
style transfer by introducing a trainable version of the Local Laplacian Filter
(LLF). From the shape of the LLF's optimized remap function, the
characteristics of the style transfer can be inferred and reliability of the
algorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray
style features by replacing the remap function with a Multi-Layer Perceptron
(MLP) and adding a trainable normalization layer. We demonstrate the
effectiveness of the proposed method by transforming unprocessed mammographic
X-ray images into images that match the style of target mammograms and achieve
a Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline
LLF style transfer method from Aubry et al.",cs.AI
Universal Response and Emergence of Induction in LLMs,"While induction is considered a key mechanism for in-context learning in
LLMs, understanding its precise circuit decomposition beyond toy models remains
elusive. Here, we study the emergence of induction behavior within LLMs by
probing their response to weak single-token perturbations of the residual
stream. We find that LLMs exhibit a robust, universal regime in which their
response remains scale-invariant under changes in perturbation strength,
thereby allowing us to quantify the build-up of token correlations throughout
the model. By applying our method, we observe signatures of induction behavior
within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across
all models, we find that these induction signatures gradually emerge within
intermediate layers and identify the relevant model sections composing this
behavior. Our results provide insights into the collective interplay of
components within LLMs and serve as a benchmark for large-scale circuit
analysis.",cs.AI
On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models,"The pretraining and fine-tuning approach has become the leading technique for
various NLP applications. However, recent studies reveal that fine-tuning data,
due to their sensitive nature, domain-specific characteristics, and
identifiability, pose significant privacy concerns. To help develop more
privacy-resilient fine-tuning models, we introduce a novel active privacy
auditing framework, dubbed Parsing, designed to identify and quantify privacy
leakage risks during the supervised fine-tuning (SFT) of language models (LMs).
The framework leverages improved white-box membership inference attacks (MIAs)
as the core technology, utilizing novel learning objectives and a two-stage
pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the
exposure of privacy risks. Additionally, we have improved the effectiveness of
MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our
research aims to provide the SFT community of LMs with a reliable, ready-to-use
privacy auditing tool, and to offer valuable insights into safeguarding privacy
during the fine-tuning process. Experimental results confirm the framework's
efficiency across various models and tasks, emphasizing notable privacy
concerns in the fine-tuning process. Project code available for
https://anonymous.4open.science/r/PARSING-4817/.",cs.AI
Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training,"Network pruning is a set of computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has focused on pruning and re-training, which
nowadays is inconvenient due to the vast amount of pre-trained models, which
are in any case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs, that
modifies the block-wise and row-wise sparsity ratios to maximize the
\emph{neuron alignment} among activations. Moreover, differently from existing
methods, our approach adaptively selects the best parameters for the block-wise
and row-wise sparsity ratios w.r.t. to the model and the desired sparsity
(given as input), and requires \emph{no re-training}. We test our method on 4
different LLM families and 3 different sparsity ratios, showing how it
consistently outperforms the latest state-of-the-art techniques. The code is
available at https://github.com/eliacunegatti/NeuroAL.",cs.AI
Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"AI companions based on large language models can role-play and converse very
naturally. When value conflicts arise between the AI companion and the user, it
may offend or upset the user. Yet, little research has examined such conflicts.
We first conducted a formative study that analyzed 151 user complaints about
conflicts with AI companions, providing design implications for our study.
Based on these, we created Minion, a technology probe to help users resolve
human-AI value conflicts. Minion applies a user-empowerment intervention method
that provides suggestions by combining expert-driven and user-driven conflict
resolution strategies. We conducted a technology probe study, creating 40 value
conflict scenarios on Character.AI and Talkie. 22 participants completed 274
tasks and successfully resolved conflicts 94.16% of the time. We summarize user
responses, preferences, and needs in resolving value conflicts, and propose
design implications to reduce conflicts and empower users to resolve them more
effectively.",cs.AI
Designing Reliable Experiments with Generative Agent-Based Modeling: A Comprehensive Guide Using Concordia by Google DeepMind,"In social sciences, researchers often face challenges when conducting
large-scale experiments, particularly due to the simulations' complexity and
the lack of technical expertise required to develop such frameworks.
Agent-Based Modeling (ABM) is a computational approach that simulates agents'
actions and interactions to evaluate how their behaviors influence the
outcomes. However, the traditional implementation of ABM can be demanding and
complex. Generative Agent-Based Modeling (GABM) offers a solution by enabling
scholars to create simulations where AI-driven agents can generate complex
behaviors based on underlying rules and interactions. This paper introduces a
framework for designing reliable experiments using GABM, making sophisticated
simulation techniques more accessible to researchers across various fields. We
provide a step-by-step guide for selecting appropriate tools, designing the
model, establishing experimentation protocols, and validating results.",cs.AI
Evaluating the Accuracy of Chatbots in Financial Literature,"We evaluate the reliability of two chatbots, ChatGPT (4o and o1-preview
versions), and Gemini Advanced, in providing references on financial literature
and employing novel methodologies. Alongside the conventional binary approach
commonly used in the literature, we developed a nonbinary approach and a
recency measure to assess how hallucination rates vary with how recent a topic
is. After analyzing 150 citations, ChatGPT-4o had a hallucination rate of 20.0%
(95% CI, 13.6%-26.4%), while the o1-preview had a hallucination rate of 21.3%
(95% CI, 14.8%-27.9%). In contrast, Gemini Advanced exhibited higher
hallucination rates: 76.7% (95% CI, 69.9%-83.4%). While hallucination rates
increased for more recent topics, this trend was not statistically significant
for Gemini Advanced. These findings emphasize the importance of verifying
chatbot-provided references, particularly in rapidly evolving fields.",cs.AI
UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction,"Beyond-triple fact representations including hyper-relational facts with
auxiliary key-value pairs, temporal facts with additional timestamps, and
nested facts implying relationships between facts, are gaining significant
attention. However, existing link prediction models are usually designed for
one specific type of facts, making it difficult to generalize to other fact
representations. To overcome this limitation, we propose a Unified Hierarchical
Representation learning framework (UniHR) for unified knowledge graph link
prediction. It consists of a unified Hierarchical Data Representation (HiDR)
module and a unified Hierarchical Structure Learning (HiSL) module as graph
encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested
factual KGs into triple-based representations. Then HiSL incorporates
intra-fact and inter-fact message passing, focusing on enhancing the semantic
information within individual facts and enriching the structural information
between facts. Experimental results across 7 datasets from 3 types of KGs
demonstrate that our UniHR outperforms baselines designed for one specific kind
of KG, indicating strong generalization capability of HiDR form and the
effectiveness of HiSL module. Code and data are available at
https://github.com/Lza12a/UniHR.",cs.AI
Leveraging LSTM for Predictive Modeling of Satellite Clock Bias,"Satellite clock bias prediction plays a crucial role in enhancing the
accuracy of satellite navigation systems. In this paper, we propose an approach
utilizing Long Short-Term Memory (LSTM) networks to predict satellite clock
bias. We gather data from the PRN 8 satellite of the Galileo and preprocess it
to obtain a single difference sequence, crucial for normalizing the data.
Normalization allows resampling of the data, ensuring that the predictions are
equidistant and complete. Our methodology involves training the LSTM model on
varying lengths of datasets, ranging from 7 days to 31 days. We employ a
training set consisting of two days' worth of data in each case. Our LSTM model
exhibits exceptional accuracy, with a Root Mean Square Error (RMSE) of 2.11
$\times$ 10$^{-11}$. Notably, our approach outperforms traditional methods used
for similar time-series forecasting projects, being 170 times more accurate
than RNN, 2.3 $\times$ 10$^7$ times more accurate than MLP, and 1.9 $\times$
10$^4$ times more accurate than ARIMA. This study holds significant potential
in enhancing the accuracy and efficiency of low-power receivers used in various
devices, particularly those requiring power conservation. By providing more
accurate predictions of satellite clock bias, the findings of this research can
be integrated into the algorithms of such devices, enabling them to function
with heightened precision while conserving power. Improved accuracy in clock
bias predictions ensures that low-power receivers can maintain optimal
performance levels, thereby enhancing the overall reliability and effectiveness
of satellite navigation systems. Consequently, this advancement holds promise
for a wide range of applications, including remote areas, IoT devices, wearable
technology, and other devices where power efficiency and navigation accuracy
are paramount.",cs.AI
A neural-network based anomaly detection system and a safety protocol to protect vehicular network,"This thesis addresses the use of Cooperative Intelligent Transport Systems
(CITS) to improve road safety and efficiency by enabling vehicle-to-vehicle
communication, highlighting the importance of secure and accurate data
exchange. To ensure safety, the thesis proposes a Machine Learning-based
Misbehavior Detection System (MDS) using Long Short-Term Memory (LSTM) networks
to detect and mitigate incorrect or misleading messages within vehicular
networks. Trained offline on the VeReMi dataset, the detection model is tested
in real-time within a platooning scenario, demonstrating that it can prevent
nearly all accidents caused by misbehavior by triggering a defense protocol
that dissolves the platoon if anomalies are detected. The results show that
while the system can accurately detect general misbehavior, it struggles to
label specific types due to varying traffic conditions, implying the difficulty
of creating a universally adaptive protocol. However, the thesis suggests that
with more data and further refinement, this MDS could be implemented in
real-world CITS, enhancing driving safety by mitigating risks from misbehavior
in cooperative driving networks.",cs.AI
Permutative redundancy and uncertainty of the objective in deep learning,"Implications of uncertain objective functions and permutative symmetry of
traditional deep learning architectures are discussed. It is shown that
traditional architectures are polluted by an astronomical number of equivalent
global and local optima. Uncertainty of the objective makes local optima
unattainable, and, as the size of the network grows, the global optimization
landscape likely becomes a tangled web of valleys and ridges. Some remedies
which reduce or eliminate ghost optima are discussed including forced
pre-pruning, re-ordering, ortho-polynomial activations, and modular
bio-inspired architectures.",cs.AI
Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching,"In inverse reinforcement learning (IRL), an agent seeks to replicate expert
demonstrations through interactions with the environment. Traditionally, IRL is
treated as an adversarial game, where an adversary searches over reward models,
and a learner optimizes the reward through repeated RL procedures. This
game-solving approach is both computationally expensive and difficult to
stabilize. In this work, we propose a novel approach to IRL by direct policy
optimization: exploiting a linear factorization of the return as the inner
product of successor features and a reward vector, we design an IRL algorithm
by policy gradient descent on the gap between the learner and expert features.
Our non-adversarial method does not require learning a reward function and can
be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our
approach works in state-only settings without expert action labels, a setting
which behavior cloning (BC) cannot solve. Empirical results demonstrate that
our method learns from as few as a single expert demonstration and achieves
improved performance on various control tasks.",cs.AI
Estimating Causal Effects in Partially Directed Parametric Causal Factor Graphs,"Lifting uses a representative of indistinguishable individuals to exploit
symmetries in probabilistic relational models, denoted as parametric factor
graphs, to speed up inference while maintaining exact answers. In this paper,
we show how lifting can be applied to causal inference in partially directed
graphs, i.e., graphs that contain both directed and undirected edges to
represent causal relationships between random variables. We present partially
directed parametric causal factor graphs (PPCFGs) as a generalisation of
previously introduced parametric causal factor graphs, which require a fully
directed graph. We further show how causal inference can be performed on a
lifted level in PPCFGs, thereby extending the applicability of lifted causal
inference to a broader range of models requiring less prior knowledge about
causal relationships.",cs.AI
Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind,"The adaptation to users' preferences and the ability to infer and interpret
humans' beliefs and intents, which is known as the Theory of Mind (ToM), are
two crucial aspects for achieving effective human-robot collaboration. Despite
its importance, very few studies have investigated the impact of adaptive
robots with ToM abilities. In this work, we present an exploratory comparative
study to investigate how social robots equipped with ToM abilities impact
users' performance and perception. We design a two-layer architecture. The
Q-learning agent on the first layer learns the robot's higher-level behaviour.
On the second layer, a heuristic-based ToM infers the user's intended strategy
and is responsible for implementing the robot's assistance, as well as
providing the motivation behind its choice. We conducted a user study in a
real-world setting, involving 56 participants who interacted with either an
adaptive robot capable of ToM, or with a robot lacking such abilities. Our
findings suggest that participants in the ToM condition performed better,
accepted the robot's assistance more often, and perceived its ability to adapt,
predict and recognise their intents to a higher degree. Our preliminary
insights could inform future research and pave the way for designing more
complex computation architectures for adaptive behaviour with ToM capabilities.",cs.AI
Which PPML Would a User Choose? A Structured Decision Support Framework for Developers to Rank PPML Techniques Based on User Acceptance Criteria,"Using Privacy-Enhancing Technologies (PETs) for machine learning often
influences the characteristics of a machine learning approach, e.g., the needed
computational power, timing of the answers or how the data can be utilized.
When designing a new service, the developer faces the problem that some
decisions require a trade-off. For example, the use of a PET may cause a delay
in the responses or adding noise to the data to improve the users' privacy
might have a negative impact on the accuracy of the machine learning approach.
As of now, there is no structured way how the users' perception of a machine
learning based service can contribute to the selection of Privacy Preserving
Machine Learning (PPML) methods. This is especially a challenge since one
cannot assume that users have a deep technical understanding of these
technologies. Therefore, they can only be asked about certain attributes that
they can perceive when using the service and not directly which PPML they
prefer.
  This study introduces a decision support framework with the aim of supporting
the selection of PPML technologies based on user preferences. Based on prior
work analysing User Acceptance Criteria (UAC), we translate these criteria into
differentiating characteristics for various PPML techniques. As a final result,
we achieve a technology ranking based on the User Acceptance Criteria while
providing technology insights for the developers. We demonstrate its
application using the use case of classifying privacy-relevant information.
  Our contribution consists of the decision support framework which consists of
a process to connect PPML technologies with UAC, a process for evaluating the
characteristics that separate PPML techniques, and a ranking method to evaluate
the best PPML technique for the use case.",cs.AI
Token2Wave,"This paper provides an in-depth analysis of Token2Wave, a novel token
representation method derived from the Wave Network, designed to capture both
global and local semantics of input text through wave-inspired complex vectors.
In Token2Wave, each token is represented with a magnitude component, capturing
the global semantics of the entire input text, and a phase component, encoding
the relationships between individual tokens and the global semantics. Building
on prior research that demonstrated the effectiveness of wave-like operations,
such as interference and modulation, during forward propagation, this study
investigates the convergence behavior, backpropagation characteristics, and
embedding independence within the Token2Wave framework. A detailed
computational complexity analysis shows that Token2Wave can significantly
reduce video memory usage and training time compared to BERT. Gradient
comparisons for the [CLS] token, total input text, and classifier parameters
further highlight Token2Wave's unique characteristics. This research offers new
insights into wave-based token representations, demonstrating their potential
to enable efficient and computationally friendly language model architectures.",cs.AI
Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration,"Learning diverse and high-performance behaviors from a limited set of
demonstrations is a grand challenge. Traditional imitation learning methods
usually fail in this task because most of them are designed to learn one
specific behavior even with multiple demonstrations. Therefore, novel
techniques for quality diversity imitation learning are needed to solve the
above challenge. This work introduces Wasserstein Quality Diversity Imitation
Learning (WQDIL), which 1) improves the stability of imitation learning in the
quality diversity setting with latent adversarial training based on a
Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting issue
using a measure-conditioned reward function with a single-step archive
exploration bonus. Empirically, our method significantly outperforms
state-of-the-art IL methods, achieving near-expert or beyond-expert QD
performance on the challenging continuous control tasks derived from MuJoCo
environments.",cs.AI
ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis,"Recently, token-based generation have demonstrated their effectiveness in
image synthesis. As a representative example, non-autoregressive Transformers
(NATs) can generate decent-quality images in a few steps. NATs perform
generation in a progressive manner, where the latent tokens of a resulting
image are incrementally revealed. At each step, the unrevealed image regions
are padded with mask tokens and inferred by NAT. In this paper, we delve into
the mechanisms behind the effectiveness of NATs and uncover two important
patterns that naturally emerge from NATs: Spatially (within a step), although
mask and visible tokens are processed uniformly by NATs, the interactions
between them are highly asymmetric. In specific, mask tokens mainly gather
information for decoding, while visible tokens tend to primarily provide
information, and their deep representations can be built only upon themselves.
Temporally (across steps), the interactions between adjacent generation steps
mostly concentrate on updating the representations of a few critical tokens,
while the computation for the majority of tokens is generally repetitive.
Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that
explicitly encourages these critical interactions inherent in NATs. At the
spatial level, we disentangle the computations of visible and mask tokens by
encoding visible tokens independently, while decoding mask tokens conditioned
on the fully encoded visible tokens. At the temporal level, we prioritize the
computation of the critical tokens at each step, while maximally reusing
previously computed token representations to supplement necessary information.
ENAT improves the performance of NATs notably with significantly reduced
computational cost. Experiments on ImageNet-256, ImageNet-512 and MS-COCO
validate the effectiveness of ENAT. Code is available at
https://github.com/LeapLabTHU/ENAT.",cs.AI
Electroencephalogram-based Multi-class Decoding of Attended Speakers' Direction with Audio Spatial Spectrum,"Decoding the directional focus of an attended speaker from listeners'
electroencephalogram (EEG) signals is essential for developing brain-computer
interfaces to improve the quality of life for individuals with hearing
impairment. Previous works have concentrated on binary directional focus
decoding, i.e., determining whether the attended speaker is on the left or
right side of the listener. However, a more precise decoding of the exact
direction of the attended speaker is necessary for effective speech processing.
Additionally, audio spatial information has not been effectively leveraged,
resulting in suboptimal decoding results. In this paper, we observe that, on
our recently presented dataset with 15-class directional focus, models relying
exclusively on EEG inputs exhibits significantly lower accuracy when decoding
the directional focus in both leave-one-subject-out and leave-one-trial-out
scenarios. By integrating audio spatial spectra with EEG features, the decoding
accuracy can be effectively improved. We employ the CNN, LSM-CNN, and
EEG-Deformer models to decode the directional focus from listeners' EEG signals
with the auxiliary audio spatial spectra. The proposed Sp-Aux-Deformer model
achieves notable 15-class decoding accuracies of 57.48% and 61.83% in
leave-one-subject-out and leave-one-trial-out scenarios, respectively.",cs.AI
Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive DOA Sensing via a Green Massive H2AD MIMO Receiver,"Most existing DOA estimation methods assume ideal source incident angles with
minimal noise. Moreover, directly using pre-estimated angles to calculate
weighted coefficients can lead to performance loss. Thus, a green multi-modal
(MM) fusion DOA framework is proposed to realize a more practical, low-cost and
high time-efficiency DOA estimation for a H$^2$AD array. Firstly, two more
efficient clustering methods, global maximum cos\_similarity clustering
(GMaxCS) and global minimum distance clustering (GMinD), are presented to infer
more precise true solutions from the candidate solution sets. Based on this, an
iteration weighted fusion (IWF)-based method is introduced to iteratively
update weighted fusion coefficients and the clustering center of the true
solution classes by using the estimated values. Particularly, the coarse DOA
calculated by fully digital (FD) subarray, serves as the initial cluster
center. The above process yields two methods called MM-IWF-GMaxCS and
MM-IWF-GMinD. To further provide a higher-accuracy DOA estimation, a fusion
network (fusionNet) is proposed to aggregate the inferred two-part true angles
and thus generates two effective approaches called MM-fusionNet-GMaxCS and
MM-fusionNet-GMinD. The simulation outcomes show the proposed four approaches
can achieve the ideal DOA performance and the CRLB. Meanwhile, proposed
MM-fusionNet-GMaxCS and MM-fusionNet-GMinD exhibit superior DOA performance
compared to MM-IWF-GMaxCS and MM-IWF-GMinD, especially in extremely-low SNR
range.",cs.AI
Slowing Down Forgetting in Continual Learning,"A common challenge in continual learning (CL) is catastrophic forgetting,
where the performance on old tasks drops after new, additional tasks are
learned. In this paper, we propose a novel framework called ReCL to slow down
forgetting in CL. Our framework exploits an implicit bias of gradient-based
neural networks due to which these converge to margin maximization points. Such
convergence points allow us to reconstruct old data from previous tasks, which
we then combine with the current training data. Our framework is flexible and
can be applied on top of existing, state-of-the-art CL methods to slow down
forgetting. We further demonstrate the performance gain from our framework
across a large series of experiments, including different CL scenarios (class
incremental, domain incremental, task incremental learning) different datasets
(MNIST, CIFAR10), and different network architectures. Across all experiments,
we find large performance gains through ReCL. To the best of our knowledge, our
framework is the first to address catastrophic forgetting by leveraging models
in CL as their own memory buffers.",cs.AI
Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI,"Segmentation of cardiac magnetic resonance images (MRI) is crucial for the
analysis and assessment of cardiac function, helping to diagnose and treat
various cardiovascular diseases. Most recent techniques rely on deep learning
and usually require an extensive amount of labeled data. To overcome this
problem, few-shot learning has the capability of reducing data dependency on
labeled data. In this work, we introduce a new method that merges few-shot
learning with a U-Net architecture and Gaussian Process Emulators (GPEs),
enhancing data integration from a support set for improved performance. GPEs
are trained to learn the relation between the support images and the
corresponding masks in latent space, facilitating the segmentation of unseen
query images given only a small labeled support set at inference. We test our
model with the M&Ms-2 public dataset to assess its ability to segment the heart
in cardiac magnetic resonance imaging from different orientations, and compare
it with state-of-the-art unsupervised and few-shot methods. Our architecture
shows higher DICE coefficients compared to these methods, especially in the
more challenging setups where the size of the support set is considerably
small.",cs.AI
LongSafetyBench: Long-Context LLMs Struggle with Safety Issues,"With the development of large language models (LLMs), the sequence length of
these models continues to increase, drawing significant attention to
long-context language models. However, the evaluation of these models has been
primarily limited to their capabilities, with a lack of research focusing on
their safety. Existing work, such as ManyShotJailbreak, has to some extent
demonstrated that long-context language models can exhibit safety concerns.
However, the methods used are limited and lack comprehensiveness. In response,
we introduce \textbf{LongSafetyBench}, the first benchmark designed to
objectively and comprehensively evaluate the safety of long-context models.
LongSafetyBench consists of 10 task categories, with an average length of
41,889 words. After testing eight long-context language models on
LongSafetyBench, we found that existing models generally exhibit insufficient
safety capabilities. The proportion of safe responses from most mainstream
long-context LLMs is below 50\%. Moreover, models' safety performance in
long-context scenarios does not always align with that in short-context
scenarios. Further investigation revealed that long-context models tend to
overlook harmful content within lengthy texts. We also proposed a simple yet
effective solution, allowing open-source models to achieve performance
comparable to that of top-tier closed-source models. We believe that
LongSafetyBench can serve as a valuable benchmark for evaluating the safety
capabilities of long-context language models. We hope that our work will
encourage the broader community to pay attention to the safety of long-context
models and contribute to the development of solutions to improve the safety of
long-context LLMs.",cs.AI
GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs,"Graph-based patterns are extensively employed and favored by practitioners
within industrial companies due to their capacity to represent the behavioral
attributes and topological relationships among users, thereby offering enhanced
interpretability in comparison to black-box models commonly utilized for
classification and recognition tasks. For instance, within the scenario of
transaction risk management, a graph pattern that is characteristic of a
particular risk category can be readily employed to discern transactions
fraught with risk, delineate networks of criminal activity, or investigate the
methodologies employed by fraudsters. Nonetheless, graph data in industrial
settings is often characterized by its massive scale, encompassing data sets
with millions or even billions of nodes, making the manual extraction of graph
patterns not only labor-intensive but also necessitating specialized knowledge
in particular domains of risk. Moreover, existing methodologies for mining
graph patterns encounter significant obstacles when tasked with analyzing
large-scale attributed graphs. In this work, we introduce GraphRPM, an
industry-purpose parallel and distributed risk pattern mining framework on
large attributed graphs. The framework incorporates a novel edge-involved graph
isomorphism network alongside optimized operations for parallel graph
computation, which collectively contribute to a considerable reduction in
computational complexity and resource expenditure. Moreover, the intelligent
filtration of efficacious risky graph patterns is facilitated by the proposed
evaluation metrics. Comprehensive experimental evaluations conducted on
real-world datasets of varying sizes substantiate the capability of GraphRPM to
adeptly address the challenges inherent in mining patterns from large-scale
industrial attributed graphs, thereby underscoring its substantial value for
industrial deployment.",cs.AI
Multi-Modal interpretable automatic video captioning,"Video captioning aims to describe video contents using natural language
format that involves understanding and interpreting scenes, actions and events
that occurs simultaneously on the view. Current approaches have mainly
concentrated on visual cues, often neglecting the rich information available
from other important modality of audio information, including their
inter-dependencies. In this work, we introduce a novel video captioning method
trained with multi-modal contrastive loss that emphasizes both multi-modal
integration and interpretability. Our approach is designed to capture the
dependency between these modalities, resulting in more accurate, thus pertinent
captions. Furthermore, we highlight the importance of interpretability,
employing multiple attention mechanisms that provide explanation into the
model's decision-making process. Our experimental results demonstrate that our
proposed method performs favorably against the state-of the-art models on
commonly used benchmark datasets of MSR-VTT and VATEX.",cs.AI
AI-Native Multi-Access Future Networks -- The REASON Architecture,"The development of the sixth generation of communication networks (6G) has
been gaining momentum over the past years, with a target of being introduced by
2030. Several initiatives worldwide are developing innovative solutions and
setting the direction for the key features of these networks. Some common
emerging themes are the tight integration of AI, the convergence of multiple
access technologies and sustainable operation, aiming to meet stringent
performance and societal requirements. To that end, we are introducing REASON -
Realising Enabling Architectures and Solutions for Open Networks. The REASON
project aims to address technical challenges in future network deployments,
such as E2E service orchestration, sustainability, security and trust
management, and policy management, utilising AI-native principles, considering
multiple access technologies and cloud-native solutions.
  This paper presents REASON's architecture and the identified requirements for
future networks. The architecture is meticulously designed for modularity,
interoperability, scalability, simplified troubleshooting, flexibility, and
enhanced security, taking into consideration current and future standardisation
efforts, and the ease of implementation and training. It is structured into
four horizontal layers: Physical Infrastructure, Network Service, Knowledge,
and End-User Application, complemented by two vertical layers: Management and
Orchestration, and E2E Security. This layered approach ensures a robust,
adaptable framework to support the diverse and evolving requirements of 6G
networks, fostering innovation and facilitating seamless integration of
advanced technologies.",cs.AI
Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to
employ reasoning according to commonsense. Previous studies predominantly
employ an extracting-and-modeling paradigm to harness the information in KG,
which first extracts relevant subgraphs based on pre-defined rules and then
proceeds to design various strategies aiming to improve the representations and
fusion of the extracted structural knowledge. Despite their effectiveness,
there are still two challenges. On one hand, subgraphs extracted by rule-based
methods may have the potential to overlook critical nodes and result in
uncontrollable subgraph size. On the other hand, the misalignment between graph
and text modalities undermines the effectiveness of knowledge fusion,
ultimately impacting the task performance. To deal with the problems above, we
propose a novel framework: \textbf{S}ubgraph R\textbf{E}trieval Enhanced by
Gra\textbf{P}h-\textbf{T}ext \textbf{A}lignment, named \textbf{SEPTA}. Firstly,
we transform the knowledge graph into a database of subgraph vectors and
propose a BFS-style subgraph sampling strategy to avoid information loss,
leveraging the analogy between BFS and the message-passing mechanism. In
addition, we propose a bidirectional contrastive learning approach for
graph-text alignment, which effectively enhances both subgraph retrieval and
knowledge fusion. Finally, all the retrieved information is combined for
reasoning in the prediction module. Extensive experiments on five datasets
demonstrate the effectiveness and robustness of our framework.",cs.AI
Computable Model-Independent Bounds for Adversarial Quantum Machine Learning,"By leveraging the principles of quantum mechanics, QML opens doors to novel
approaches in machine learning and offers potential speedup. However, machine
learning models are well-documented to be vulnerable to malicious
manipulations, and this susceptibility extends to the models of QML. This
situation necessitates a thorough understanding of QML's resilience against
adversarial attacks, particularly in an era where quantum computing
capabilities are expanding. In this regard, this paper examines
model-independent bounds on adversarial performance for QML. To the best of our
knowledge, we introduce the first computation of an approximate lower bound for
adversarial error when evaluating model resilience against sophisticated
quantum-based adversarial attacks. Experimental results are compared to the
computed bound, demonstrating the potential of QML models to achieve high
robustness. In the best case, the experimental error is only 10% above the
estimated bound, offering evidence of the inherent robustness of quantum
models. This work not only advances our theoretical understanding of quantum
model resilience but also provides a precise reference bound for the future
development of robust QML algorithms.",cs.AI
"Enhancing Phishing Detection through Feature Importance Analysis and Explainable AI: A Comparative Study of CatBoost, XGBoost, and EBM Models","Phishing attacks remain a persistent threat to online security, demanding
robust detection methods. This study investigates the use of machine learning
to identify phishing URLs, emphasizing the crucial role of feature selection
and model interpretability for improved performance. Employing Recursive
Feature Elimination, the research pinpointed key features like ""length_url,""
""time_domain_activation"" and ""Page_rank"" as strong indicators of phishing
attempts. The study evaluated various algorithms, including CatBoost, XGBoost,
and Explainable Boosting Machine, assessing their robustness and scalability.
XGBoost emerged as highly efficient in terms of runtime, making it well-suited
for large datasets. CatBoost, on the other hand, demonstrated resilience by
maintaining high accuracy even with reduced features. To enhance transparency
and trustworthiness, Explainable AI techniques, such as SHAP, were employed to
provide insights into feature importance. The study's findings highlight that
effective feature selection and model interpretability can significantly
bolster phishing detection systems, paving the way for more efficient and
adaptable defenses against evolving cyber threats",cs.AI
Scientific machine learning in ecological systems: A study on the predator-prey dynamics,"In this study, we apply two pillars of Scientific Machine Learning: Neural
Ordinary Differential Equations (Neural ODEs) and Universal Differential
Equations (UDEs) to the Lotka Volterra Predator Prey Model, a fundamental
ecological model describing the dynamic interactions between predator and prey
populations. The Lotka-Volterra model is critical for understanding ecological
dynamics, population control, and species interactions, as it is represented by
a system of differential equations. In this work, we aim to uncover the
underlying differential equations without prior knowledge of the system,
relying solely on training data and neural networks. Using robust modeling in
the Julia programming language, we demonstrate that both Neural ODEs and UDEs
can be effectively utilized for prediction and forecasting of the
Lotka-Volterra system. More importantly, we introduce the forecasting breakdown
point: the time at which forecasting fails for both Neural ODEs and UDEs. We
observe how UDEs outperform Neural ODEs by effectively recovering the
underlying dynamics and achieving accurate forecasting with significantly less
training data. Additionally, we introduce Gaussian noise of varying magnitudes
(from mild to high) to simulate real-world data perturbations and show that
UDEs exhibit superior robustness, effectively recovering the underlying
dynamics even in the presence of noisy data, while Neural ODEs struggle with
high levels of noise. Through extensive hyperparameter optimization, we offer
insights into neural network architectures, activation functions, and
optimizers that yield the best results. This study opens the door to applying
Scientific Machine Learning frameworks for forecasting tasks across a wide
range of ecological and scientific domains.",cs.AI
Evaluating Large Language Models on Financial Report Summarization: An Empirical Study,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
versatility across various applications, including natural language
understanding, domain-specific knowledge tasks, etc. However, applying LLMs to
complex, high-stakes domains like finance requires rigorous evaluation to
ensure reliability, accuracy, and compliance with industry standards. To
address this need, we conduct a comprehensive and comparative study on three
state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their
effectiveness in generating automated financial reports. Our primary motivation
is to explore how these models can be harnessed within finance, a field
demanding precision, contextual relevance, and robustness against erroneous or
misleading information. By examining each model's capabilities, we aim to
provide an insightful assessment of their strengths and limitations. Our paper
offers benchmarks for financial report analysis, encompassing proposed metrics
such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative
evaluation framework that integrates both quantitative metrics (e.g.,
precision, recall) and qualitative analyses (e.g., contextual fit, consistency)
to provide a holistic view of each model's output quality. Additionally, we
make our financial dataset publicly available, inviting researchers and
practitioners to leverage, scrutinize, and enhance our findings through broader
community engagement and collaborative improvement. Our dataset is available on
huggingface.",cs.AI
"1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs","This paper presents a detailed system description of our entry for the
CHiPSAL 2025 shared task, focusing on language detection, hate speech
identification, and target detection in Devanagari script languages. We
experimented with a combination of large language models and their ensembles,
including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like
focal loss to address challenges in the natural understanding of Devanagari
languages, such as multilingual processing and class imbalance. Our approach
achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804
for Sub-tasks A, B, and C respectively. This work provides insights into the
effectiveness of transformer models in tasks with domain-specific and
linguistic challenges, as well as areas for potential improvement in future
iterations.",cs.AI
LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models,"In this paper, we propose a novel LLM-Neo framework that efficiently
transfers knowledge from a large language model (LLM) teacher to a compact
student. Initially, we revisit the knowledge distillation (KD) and low-rank
adaption (LoRA), and argue that they share the same paradigm. Inspired by this
observation, we explore the strategy that combines LoRA and KD to enhance the
efficiency of knowledge transfer. We first summarize some guidelines for this
design and further develop the LLM-Neo. Experimental results on compressing
Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further
analysis demonstrates the robustness of the proposed LLM-Neo on variants of
LoRA. The trained models have been available at
\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this
repository}.",cs.AI
Learning Interpretable Network Dynamics via Universal Neural Symbolic Regression,"Discovering governing equations of complex network dynamics is a fundamental
challenge in contemporary science with rich data, which can uncover the
mysterious patterns and mechanisms of the formation and evolution of complex
phenomena in various fields and assist in decision-making. In this work, we
develop a universal computational tool that can automatically, efficiently, and
accurately learn the symbolic changing patterns of complex system states by
combining the excellent fitting ability from deep learning and the equation
inference ability from pre-trained symbolic regression. We conduct intensive
experimental verifications on more than ten representative scenarios from
physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the
outstanding effectiveness and efficiency of our tool by comparing with the
state-of-the-art symbolic regression techniques for network dynamics. The
application to real-world systems including global epidemic transmission and
pedestrian movements has verified its practical applicability. We believe that
our tool can serve as a universal solution to dispel the fog of hidden
mechanisms of changes in complex phenomena, advance toward interpretability,
and inspire more scientific discoveries.",cs.AI
Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs,"There is a growing interest in training domain-expert LLMs that excel in
specific technical fields compared to their general-purpose instruction-tuned
counterparts. However, these expert models often experience a loss in their
safety abilities in the process, making them capable of generating harmful
content. As a solution, we introduce an efficient and effective merging-based
alignment method called \textsc{MergeAlign} that interpolates the domain and
alignment vectors, creating safer domain-specific models while preserving their
utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in
medicine and finance, obtaining substantial alignment improvements with minimal
to no degradation on domain-specific benchmarks. We study the impact of model
merging through model similarity metrics and contributions of individual models
being merged. We hope our findings open new research avenues and inspire more
efficient development of safe expert LLMs.",cs.AI
Generative midtended cognition and Artificial Intelligence. Thinging with thinging things,"This paper introduces the concept of ``generative midtended cognition'',
exploring the integration of generative AI with human cognition. The term
""generative"" reflects AI's ability to iteratively produce structured outputs,
while ""midtended"" captures the potential hybrid (human-AI) nature of the
process. It stands between traditional conceptions of intended creation,
understood directed from within, and extended processes that bring
exo-biological processes into the creative process. We examine current
generative technologies (based on multimodal transformer architectures typical
of large language models like ChatGPT), to explain how they can transform human
cognitive agency beyond what standard theories of extended cognition can
capture. We suggest that the type of cognitive activity typical of the coupling
between a human and generative technologies is closer (but not equivalent) to
social cognition than to classical extended cognitive paradigms. Yet, it
deserves a specific treatment. We provide an explicit definition of generative
midtended cognition in which we treat interventions by AI systems as
constitutive of the agent's intentional creative processes. Furthermore, we
distinguish two dimensions of generative hybrid creativity: 1. Width: captures
the sensitivity of the context of the generative process (from the single
letter to the whole historical and surrounding data), 2. Depth: captures the
granularity of iteration loops involved in the process. Generative midtended
cognition stands in the middle depth between conversational forms of cognition
in which complete utterances or creative units are exchanged, and
micro-cognitive (e.g. neural) subpersonal processes. Finally, the paper
discusses the potential risks and benefits of widespread generative AI
adoption, including the challenges of authenticity, generative power asymmetry,
and creative boost or atrophy.",cs.AI
JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset,"Learning-based image compression methods have improved in recent years and
started to outperform traditional codecs. However, neural-network approaches
can unexpectedly introduce visual artifacts in some images. We therefore
propose methods to separately detect three types of artifacts (texture and
boundary degradation, color change, and text corruption), to localize the
affected regions, and to quantify the artifact strength. We consider only those
regions that exhibit distortion due solely to the neural compression but that a
traditional codec recovers successfully at a comparable bitrate. We employed
our methods to collect artifacts for the JPEG AI verification model with
respect to HM-18.0, the H.265 reference software. We processed about 350,000
unique images from the Open Images dataset using different compression-quality
parameters; the result is a dataset of 46,440 artifacts validated through
crowd-sourced subjective assessment. Our proposed dataset and methods are
valuable for testing neural-network-based image codecs, identifying bugs in
these codecs, and enhancing their performance. We make source code of the
methods and the dataset publicly available.",cs.AI
AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant,"The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as ""hallucination"". Initial retrieval-augmented
generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.",cs.AI
LA4SR: illuminating the dark proteome with generative AI,"AI language models (LMs) show promise for biological sequence analysis. We
re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba,
ranging from 70M to 12B parameters) for microbial sequence classification. The
models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the
recall of BLASTP. They effectively classified the algal dark proteome -
uncharacterized proteins comprising about 65% of total proteins - validated on
new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger
(>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2%
of available data, rapidly achieving strong generalization capacity. High
accuracy was achieved when training data had intact or scrambled terminal
information, demonstrating robust generalization to incomplete sequences.
Finally, we provide custom AI explainability software tools for attributing
amino acid patterns to AI generative processes and interpret their outputs in
evolutionary and biophysical contexts.",cs.AI
Evolving Efficient Genetic Encoding for Deep Spiking Neural Networks,"By exploiting discrete signal processing and simulating brain neuron
communication, Spiking Neural Networks (SNNs) offer a low-energy alternative to
Artificial Neural Networks (ANNs). However, existing SNN models, still face
high computational costs due to the numerous time steps as well as network
depth and scale. The tens of billions of neurons and trillions of synapses in
the human brain are developed from only 20,000 genes, which inspires us to
design an efficient genetic encoding strategy that dynamic evolves to regulate
large-scale deep SNNs at low cost. Therefore, we first propose a genetically
scaled SNN encoding scheme that incorporates globally shared genetic
interactions to indirectly optimize neuronal encoding instead of weight, which
obviously brings about reductions in parameters and energy consumption. Then, a
spatio-temporal evolutionary framework is designed to optimize the inherently
initial wiring rules. Two dynamic regularization operators in the fitness
function evolve the neuronal encoding to a suitable distribution and enhance
information quality of the genetic interaction respectively, substantially
accelerating evolutionary speed and improving efficiency. Experiments show that
our approach compresses parameters by approximately 50\% to 80\%, while
outperforming models on the same architectures by 0.21\% to 4.38\% on CIFAR-10,
CIFAR-100 and ImageNet. In summary, the consistent trends of the proposed
genetically encoded spatio-temporal evolution across different datasets and
architectures highlight its significant enhancements in terms of efficiency,
broad scalability and robustness, demonstrating the advantages of the
brain-inspired evolutionary genetic coding for SNN optimization.",cs.AI
ScaleKD: Strong Vision Transformers Could Be Excellent Teachers,"In this paper, we question if well pre-trained vision transformer (ViT)
models could be used as teachers that exhibit scalable properties to advance
cross architecture knowledge distillation (KD) research, in the context of
using large-scale datasets for evaluation. To make this possible, our analysis
underlines the importance of seeking effective strategies to align (1) feature
computing paradigm differences, (2) model scale differences, and (3) knowledge
density differences. By combining three coupled components namely cross
attention projector, dual-view feature mimicking and teacher parameter
perception tailored to address the above problems, we present a simple and
effective KD method, called ScaleKD. Our method can train student backbones
that span across a variety of convolutional neural network (CNN), multi-layer
perceptron (MLP), and ViT architectures on image classification datasets,
achieving state-of-the-art distillation performance. For instance, taking a
well pre-trained Swin-L as the teacher model, our method gets
75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% top-1 accuracies for
MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16
models trained on ImageNet-1K dataset from scratch, showing
3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% absolute gains to the
individually trained counterparts. Intriguingly, when scaling up the size of
teacher models or their pre-training datasets, our method showcases the desired
scalable properties, bringing increasingly larger gains to student models. The
student backbones trained by our method transfer well on downstream MS-COCO and
ADE20K datasets. More importantly, our method could be used as a more efficient
alternative to the time-intensive pre-training paradigm for any target student
model if a strong pre-trained ViT is available, reducing the amount of viewed
training samples up to 195x.",cs.AI
QuadWBG: Generalizable Quadrupedal Whole-Body Grasping,"Legged robots with advanced manipulation capabilities have the potential to
significantly improve household duties and urban maintenance. Despite
considerable progress in developing robust locomotion and precise manipulation
methods, seamlessly integrating these into cohesive whole-body control for
real-world applications remains challenging. In this paper, we present a
modular framework for robust and generalizable whole-body loco-manipulation
controller based on a single arm-mounted camera. By using reinforcement
learning (RL), we enable a robust low-level policy for command execution over 5
dimensions (5D) and a grasp-aware high-level policy guided by a novel metric,
Generalized Oriented Reachability Map (GORM). The proposed system achieves
state-of-the-art one-time grasping accuracy of 89% in the real world, including
challenging tasks such as grasping transparent objects. Through extensive
simulations and real-world experiments, we demonstrate that our system can
effectively manage a large workspace, from floor level to above body height,
and perform diverse whole-body loco-manipulation tasks.",cs.AI
MP-PINN: A Multi-Phase Physics-Informed Neural Network for Epidemic Forecasting,"Forecasting temporal processes such as virus spreading in epidemics often
requires more than just observed time-series data, especially at the beginning
of a wave when data is limited. Traditional methods employ mechanistic models
like the SIR family, which make strong assumptions about the underlying
spreading process, often represented as a small set of compact differential
equations. Data-driven methods such as deep neural networks make no such
assumptions and can capture the generative process in more detail, but fail in
long-term forecasting due to data limitations. We propose a new hybrid method
called MP-PINN (Multi-Phase Physics-Informed Neural Network) to overcome the
limitations of these two major approaches. MP-PINN instils the spreading
mechanism into a neural network, enabling the mechanism to update in phases
over time, reflecting the dynamics of the epidemics due to policy
interventions. Experiments on COVID-19 waves demonstrate that MP-PINN achieves
superior performance over pure data-driven or model-driven approaches for both
short-term and long-term forecasting.",cs.AI
Machine vision-aware quality metrics for compressed image and video assessment,"A main goal in developing video-compression algorithms is to enhance
human-perceived visual quality while maintaining file size. But modern
video-analysis efforts such as detection and recognition, which are integral to
video surveillance and autonomous vehicles, involve so much data that they
necessitate machine-vision processing with minimal human intervention. In such
cases, the video codec must be optimized for machine vision. This paper
explores the effects of compression on detection and recognition algorithms
(objects, faces, and license plates) and introduces novel full-reference
image/video-quality metrics for each task, tailored to machine vision.
Experimental results indicate our proposed metrics correlate better with the
machine-vision results for the respective tasks than do existing
image/video-quality metrics.",cs.AI
A Text Classification Model Combining Adversarial Training with Pre-trained Language Model and neural networks: A Case Study on Telecom Fraud Incident Texts,"Front-line police officers often categorize all police call reported cases of
Telecom Fraud into 14 subcategories to facilitate targeted prevention measures,
such as precise public education. However, the associated data is characterized
by its large volume, diverse information content, and variations in expression.
Currently, there is a lack of efficient and accurate intelligent models to
replace manual classification, which, while precise, is relatively inefficient.
To address these challenges, this paper proposes a text classification model
that combines adversarial training with Pre-trained Language Model and neural
networks. The Linguistically-motivated Pre-trained Language Model model
extracts three types of language features and then utilizes the Fast Gradient
Method algorithm to perturb the generated embedding layer. Subsequently, the
Bi-directional Long Short-Term Memory and Convolutional Neural Networks
networks extract contextual syntactic information and local semantic
information, respectively. The model achieved an 83.9% classification accuracy
when trained on a portion of telecom fraud case data provided by the
operational department. The model established in this paper has been deployed
in the operational department, freeing up a significant amount of manpower and
improving the department's efficiency in combating Telecom Fraud crimes.
Furthermore, considering the universality of the model established in this
paper, other application scenarios await further exploration.",cs.AI
PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing,"Code Large Language Models (Code LLMs), such as Code llama and
DeepSeek-Coder, have demonstrated exceptional performance in the code
generation tasks. However, most existing models focus on the abilities of
generating correct code, but often struggle with bug repair. We introduce a
suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are
mainly consisted of two parts: A Progressive Dataset Construction (PDC) from
scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data
expansion methods from the perspectives of breadth first and depth first
respectively. DM-SFT introduces an efficient bug-fixing supervised learning
approach, which effectively reduce the total training steps and mitigate the
""disorientation"" in SQL code bug-fixing training. In our evaluation, the code
LLM models trained with two methods have exceeds all current best performing
model which size is much larger.",cs.AI
Research on an intelligent fault diagnosis method for nuclear power plants based on ETCN-SSA combined algorithm,"Utilizing fault diagnosis methods is crucial for nuclear power professionals
to achieve efficient and accurate fault diagnosis for nuclear power plants
(NPPs). The performance of traditional methods is limited by their dependence
on complex feature extraction and skilled expert knowledge, which can be
time-consuming and subjective. This paper proposes a novel intelligent fault
diagnosis method for NPPs that combines enhanced temporal convolutional network
(ETCN) with sparrow search algorithm (SSA). ETCN utilizes temporal
convolutional network (TCN), self-attention (SA) mechanism and residual block
for enhancing performance. ETCN excels at extracting local features and
capturing time series information, while SSA adaptively optimizes its
hyperparameters for superior performance. The proposed method's performance is
experimentally verified on a CPR1000 simulation dataset. Compared to other
advanced intelligent fault diagnosis methods, the proposed one demonstrates
superior performance across all evaluation metrics. This makes it a promising
tool for NPP intelligent fault diagnosis, ultimately enhancing operational
reliability.",cs.AI
KLCBL: An Improved Police Incident Classification Model,"Police incident data is crucial for public security intelligence, yet
grassroots agencies struggle with efficient classification due to manual
inefficiency and automated system limitations, especially in telecom and online
fraud cases. This research proposes a multichannel neural network model, KLCBL,
integrating Kolmogorov-Arnold Networks (KAN), a linguistically enhanced text
preprocessing approach (LERT), Convolutional Neural Network (CNN), and
Bidirectional Long Short-Term Memory (BiLSTM) for police incident
classification. Evaluated with real data, KLCBL achieved 91.9% accuracy,
outperforming baseline models. The model addresses classification challenges,
enhances police informatization, improves resource allocation, and offers broad
applicability to other classification tasks.",cs.AI
Dockformer: A transformer-based molecular docking paradigm for large-scale virtual screening,"Molecular docking enables virtual screening of compound libraries to identify
potential ligands that target proteins of interest, a crucial step in drug
development; however, as the size of the compound library increases, the
computational complexity of traditional docking models increases. Deep learning
algorithms can provide data-driven research and development models to increase
the speed of the docking process. Unfortunately, few models can achieve
superior screening performance compared to that of traditional models.
Therefore, a novel deep learning-based docking approach named Dockformer is
introduced in this study. Dockformer leverages multimodal information to
capture the geometric topology and structural knowledge of molecules and can
directly generate binding conformations with the corresponding confidence
measures in an end-to-end manner. The experimental results show that Dockformer
achieves success rates of 90.53\% and 82.71\% on the PDBbind core set and
PoseBusters benchmarks, respectively, and more than a 100-fold increase in the
inference process speed, outperforming almost all state-of-the-art docking
methods. In addition, the ability of Dockformer to identify the main protease
inhibitors of coronaviruses is demonstrated in a real-world virtual screening
scenario. Considering its high docking accuracy and screening efficiency,
Dockformer can be regarded as a powerful and robust tool in the field of drug
design.",cs.AI
Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data,"Current forecasting approaches are largely unimodal and ignore the rich
textual data that often accompany the time series due to lack of well-curated
multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a
carefully curated, time-aligned text and time dataset for multimodal
forecasting. Our dataset is composed of sequences of numbers and text aligned
to timestamps, and includes data from two different domains: climate science
and healthcare. Our data is a significant contribution to the rare selection of
available multimodal datasets. We also propose the Hybrid Multi-Modal
Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and
time series data using shared embeddings. However, contrary to our
expectations, our Hybrid-MMF model does not outperform existing baselines in
our experiments. This negative result highlights the challenges inherent in
multimodal forecasting. Our code and data are available at
https://github.com/Rose-STL-Lab/Multimodal_ Forecasting.",cs.AI
On the Principles of ReLU Networks with One Hidden Layer,"A neural network with one hidden layer or a two-layer network (regardless of
the input layer) is the simplest feedforward neural network, whose mechanism
may be the basis of more general network architectures. However, even to this
type of simple architecture, it is also a ``black box''; that is, it remains
unclear how to interpret the mechanism of its solutions obtained by the
back-propagation algorithm and how to control the training process through a
deterministic way. This paper systematically studies the first problem by
constructing universal function-approximation solutions. It is shown that, both
theoretically and experimentally, the training solution for the one-dimensional
input could be completely understood, and that for a higher-dimensional input
can also be well interpreted to some extent. Those results pave the way for
thoroughly revealing the black box of two-layer ReLU networks and advance the
understanding of deep ReLU networks.",cs.AI
Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy,"Chatbots or conversational agents (CAs) are increasingly used to improve
access to digital psychotherapy. Many current systems rely on rigid, rule-based
designs, heavily dependent on expert-crafted dialogue scripts for guiding
therapeutic conversations. Although recent advances in large language models
(LLMs) offer the potential for more flexible interactions, their lack of
controllability and transparency poses significant challenges in sensitive
areas like psychotherapy. In this work, we explored how aligning LLMs with
expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our
comparative study showed that LLMs aligned with expert-crafted scripts through
prompting and fine-tuning significantly outperformed both pure LLMs and
rule-based chatbots, achieving a more effective balance between dialogue
flexibility and adherence to therapeutic principles. Building on findings, we
proposed ``Script-Strategy Aligned Generation (SSAG)'', a flexible alignment
approach that reduces reliance on fully scripted content while enhancing LLMs'
therapeutic adherence and controllability. In a 10-day field study, SSAG
demonstrated performance comparable to full script alignment and outperformed
rule-based chatbots, empirically supporting SSAG as an efficient approach for
aligning LLMs with domain expertise. Our work advances LLM applications in
psychotherapy by providing a controllable, adaptable, and scalable solution for
digital interventions, reducing reliance on expert effort. It also provides a
collaborative framework for domain experts and developers to efficiently build
expertise-aligned chatbots, broadening access to psychotherapy and behavioral
interventions.",cs.AI
"Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models","Presenting users with diverse responses from foundation models is crucial for
enhancing user experience and accommodating varying preferences. However,
generating multiple high-quality and diverse responses without sacrificing
accuracy remains a challenge, especially when using greedy sampling. In this
work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that
leverages the abundant synthetic data available in many domains to elicit
diverse responses from foundation models. By leveraging signal provided by data
attribution methods such as influence functions, SPA partitions data into
subsets, each targeting unique aspects of the data, and trains multiple model
adaptations optimized for these subsets. Experimental results demonstrate the
effectiveness of our approach in diversifying foundation model responses while
maintaining high quality, showcased through the HumanEval and MBPP tasks in the
code generation domain and several tasks in the natural language understanding
domain, highlighting its potential to enrich user experience across various
applications.",cs.AI
DiffSR: Learning Radar Reflectivity Synthesis via Diffusion Model from Satellite Observations,"Weather radar data synthesis can fill in data for areas where ground
observations are missing. Existing methods often employ reconstruction-based
approaches with MSE loss to reconstruct radar data from satellite observation.
However, such methods lead to over-smoothing, which hinders the generation of
high-frequency details or high-value observation areas associated with
convective weather. To address this issue, we propose a two-stage
diffusion-based method called DiffSR. We first pre-train a reconstruction model
on global-scale data to obtain radar estimation and then synthesize radar
reflectivity by combining radar estimation results with satellite data as
conditions for the diffusion model. Extensive experiments show that our method
achieves state-of-the-art (SOTA) results, demonstrating the ability to generate
high-frequency details and high-value areas.",cs.AI
Ambient AI Scribing Support: Comparing the Performance of Specialized AI Agentic Architecture to Leading Foundational Models,"This study compares Sporo Health's AI Scribe, a proprietary model fine-tuned
for medical scribing, with various LLMs (GPT-4o, GPT-3.5, Gemma-9B, and
Llama-3.2-3B) in clinical documentation. We analyzed de-identified patient
transcripts from partner clinics, using clinician-provided SOAP notes as the
ground truth. Each model generated SOAP summaries using zero-shot prompting,
with performance assessed via recall, precision, and F1 scores. Sporo
outperformed all models, achieving the highest recall (73.3%), precision
(78.6%), and F1 score (75.3%) with the lowest performance variance.
Statistically significant differences (p < 0.05) were found between Sporo and
the other models, with post-hoc tests showing significant improvements over
GPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to
10%, the difference was not statistically significant (p = 0.25). Clinical user
satisfaction, measured with a modified PDQI-9 inventory, favored Sporo.
Evaluations indicated Sporo's outputs were more accurate and relevant. This
highlights the potential of Sporo's multi-agentic architecture to improve
clinical workflows.",cs.AI
Anytime Probabilistically Constrained Provably Convergent Online Belief Space Planning,"Taking into account future risk is essential for an autonomously operating
robot to find online not only the best but also a safe action to execute. In
this paper, we build upon the recently introduced formulation of probabilistic
belief-dependent constraints. We present an anytime approach employing the
Monte Carlo Tree Search (MCTS) method in continuous domains. Unlike previous
approaches, our method assures safety anytime with respect to the currently
expanded search tree without relying on the convergence of the search. We prove
convergence in probability with an exponential rate of a version of our
algorithms and study proposed techniques via extensive simulations. Even with a
tiny number of tree queries, the best action found by our approach is much
safer than the baseline. Moreover, our approach constantly finds better than
the baseline action in terms of objective. This is because we revise the values
and statistics maintained in the search tree and remove from them the
contribution of the pruned actions.",cs.AI
Model Fusion through Bayesian Optimization in Language Model Fine-Tuning,"Fine-tuning pre-trained models for downstream tasks is a widely adopted
technique known for its adaptability and reliability across various domains.
Despite its conceptual simplicity, fine-tuning entails several troublesome
engineering choices, such as selecting hyperparameters and determining
checkpoints from an optimization trajectory. To tackle the difficulty of
choosing the best model, one effective solution is model fusion, which combines
multiple models in a parameter space. However, we observe a large discrepancy
between loss and metric landscapes during the fine-tuning of pre-trained
language models. Building on this observation, we introduce a novel model
fusion technique that optimizes both the desired metric and loss through
multi-objective Bayesian optimization. In addition, to effectively select
hyperparameters, we establish a two-stage procedure by integrating Bayesian
optimization processes into our framework. Experiments across various
downstream tasks show considerable performance improvements using our Bayesian
optimization-guided method.",cs.AI
Autonomous Droplet Microfluidic Design Framework with Large Language Models,"Droplet-based microfluidic devices have substantial promise as cost-effective
alternatives to current assessment tools in biological research. Moreover,
machine learning models that leverage tabular data, including input design
parameters and their corresponding efficiency outputs, are increasingly
utilised to automate the design process of these devices and to predict their
performance. However, these models fail to fully leverage the data presented in
the tables, neglecting crucial contextual information, including column
headings and their associated descriptions. This study presents
MicroFluidic-LLMs, a framework designed for processing and feature extraction,
which effectively captures contextual information from tabular data formats.
MicroFluidic-LLMs overcomes processing challenges by transforming the content
into a linguistic format and leveraging pre-trained large language models
(LLMs) for analysis. We evaluate our MicroFluidic-LLMs framework on 11
prediction tasks, covering aspects such as geometry, flow conditions, regimes,
and performance, utilising a publicly available dataset on flow-focusing
droplet microfluidics. We demonstrate that our MicroFluidic-LLMs framework can
empower deep neural network models to be highly effective and straightforward
while minimising the need for extensive data preprocessing. Moreover, the
exceptional performance of deep neural network models, particularly when
combined with advanced natural language processing models such as DistilBERT
and GPT-2, reduces the mean absolute error in the droplet diameter and
generation rate by nearly 5- and 7-fold, respectively, and enhances the regime
classification accuracy by over 4%, compared with the performance reported in a
previous study. This study lays the foundation for the huge potential
applications of LLMs and machine learning in a wider spectrum of microfluidic
applications.",cs.AI
High-Frequency Enhanced Hybrid Neural Representation for Video Compression,"Neural Representations for Videos (NeRV) have simplified the video codec
process and achieved swift decoding speeds by encoding video content into a
neural network, presenting a promising solution for video compression. However,
existing work overlooks the crucial issue that videos reconstructed by these
methods lack high-frequency details. To address this problem, this paper
introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our
method focuses on leveraging high-frequency information to improve the
synthesis of fine details by the network. Specifically, we design a wavelet
high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD)
blocks to generate high-frequency feature embeddings. Next, we design the
High-Frequency Feature Modulation (HFM) block, which leverages the extracted
high-frequency embeddings to enhance the fitting process of the decoder.
Finally, with the refined Harmonic decoder block and a Dynamic Weighted
Frequency Loss, we further reduce the potential loss of high-frequency
information. Experiments on the Bunny and UVG datasets demonstrate that our
method outperforms other methods, showing notable improvements in detail
preservation and compression performance.",cs.AI
WDMoE: Wireless Distributed Mixture of Experts for Large Language Models,"Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but the role of wireless networks in
supporting LLMs has not been thoroughly explored. In this paper, we propose a
wireless distributed Mixture of Experts (WDMoE) architecture to enable
collaborative deployment of LLMs across edge servers at the base station (BS)
and mobile devices in wireless networks. Specifically, we decompose the MoE
layer in LLMs by placing the gating network and the preceding neural network
layer at BS, while distributing the expert networks among the devices. This
deployment leverages the parallel inference capabilities of expert networks on
mobile devices, effectively utilizing the limited computing and caching
resources of these devices. Accordingly, we develop a performance metric for
WDMoE-based LLMs, which accounts for both model capability and latency. To
minimize the latency while maintaining accuracy, we jointly optimize expert
selection and bandwidth allocation based on the performance metric. Moreover,
we build a hardware testbed using NVIDIA Jetson kits to validate the
effectiveness of WDMoE. Both theoretical simulations and practical hardware
experiments demonstrate that the proposed method can significantly reduce the
latency without compromising LLM performance.",cs.AI
What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance,"We explore the impact of pre-training data composition on the performance of
small language models in a sample-efficient setting. Using datasets limited to
10 million words, we evaluate several dataset sources, including child-directed
speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and
a mix of these (Mix) across different model sizes ranging from 18 million to
705 million parameters. Our experiments show that smaller models (e.g.,
GPT2-97M, GPT2-705M, Llama-360M) perform better when trained on more complex
and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories
datasets underperformed across all model sizes. These findings suggest that the
optimal dataset for sample efficient training depends on the model size, and
that neither child-directed speech nor simplified stories are optimal for
language models of all sizes. We highlight the importance of considering both
dataset composition and model capacity for effective sample efficient language
model training.",cs.AI
Adversarial Detection with a Dynamically Stable System,"Adversarial detection is designed to identify and reject maliciously crafted
adversarial examples(AEs) which are generated to disrupt the classification of
target models.
  Presently, various input transformation-based methods have been developed on
adversarial example detection, which typically rely on empirical experience and
lead to unreliability against new attacks.
  To address this issue, we propose and conduct a Dynamically Stable System
(DSS), which can effectively detect the adversarial examples from normal
examples according to the stability of input examples.
  Particularly, in our paper, the generation of adversarial examples is
considered as the perturbation process of a Lyapunov dynamic system, and we
propose an example stability mechanism, in which a novel control term is added
in adversarial example generation to ensure that the normal examples can
achieve dynamic stability while the adversarial examples cannot achieve the
stability.
  Then, based on the proposed example stability mechanism, a Dynamically Stable
System (DSS) is proposed, which can utilize the disruption and restoration
actions to determine the stability of input examples and detect the adversarial
examples through changes in the stability of the input examples.
  In comparison with existing methods in three benchmark datasets(MNIST,
CIFAR10, and CIFAR100), our evaluation results show that our proposed DSS can
achieve ROC-AUC values of 99.83%, 97.81% and 94.47%, surpassing the
state-of-the-art(SOTA) values of 97.35%, 91.10% and 93.49% in the other 7
methods.",cs.AI
An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning,"Incremental graph learning has gained significant attention for its ability
to address the catastrophic forgetting problem in graph representation
learning. However, traditional methods often rely on a large number of labels
for node classification, which is impractical in real-world applications. This
makes few-shot incremental learning on graphs a pressing need. Current methods
typically require extensive training samples from meta-learning to build memory
and perform intensive fine-tuning of GNN parameters, leading to high memory
consumption and potential loss of previously learned knowledge. To tackle these
challenges, we introduce Mecoin, an efficient method for building and
maintaining memory. Mecoin employs Structured Memory Units to cache prototypes
of learned categories, as well as Memory Construction Modules to update these
prototypes for new categories through interactions between the nodes and the
cached prototypes. Additionally, we have designed a Memory Representation
Adaptation Module to store probabilities associated with each class prototype,
reducing the need for parameter fine-tuning and lowering the forgetting rate.
When a sample matches its corresponding class prototype, the relevant
probabilities are retrieved from the MRaM. Knowledge is then distilled back
into the GNN through a Graph Knowledge Distillation Module, preserving the
model's memory. We analyze the effectiveness of Mecoin in terms of
generalization error and explore the impact of different distillation
strategies on model performance through experiments and VC-dimension analysis.
Compared to other related works, Mecoin shows superior performance in accuracy
and forgetting rate. Our code is publicly available on the
https://github.com/Arvin0313/Mecoin-GFSCIL.git .",cs.AI
Renaissance: Investigating the Pretraining of Vision-Language Encoders,"In the past several years there has been an explosion of available models for
vision-language tasks. Unfortunately, the literature still leaves open a number
of questions related to best practices in designing and training such models.
In this paper we seek to answer several questions related to the pretraining of
vision-language encoders through meta-analysis. In our first set of
experiments, we show that we can save significant compute at no cost to
downstream performance, by freezing large parts of vision-language models
during pretraining. In our second set of experiments we examine the effect of
basing a VL transformer on a vision model versus a text model. Additionally, we
introduce a VL modeling platform called Renaissance that we use to conduct all
of the experiments. This program offers a great deal of flexibility in
creating, training and evaluating transformer encoders for VL modeling. The
source code for Renaissance can be found at
https://github.com/bsu-slim/renaissance.",cs.AI
Explore the Reasoning Capability of LLMs in the Chess Testbed,"Reasoning is a central capability of human intelligence. In recent years,
with the advent of large-scale datasets, pretrained large language models have
emerged with new capabilities, including reasoning. However, these models still
struggle with long-term, complex reasoning tasks, such as playing chess. Based
on the observation that expert chess players employ a dual approach combining
long-term strategic play with short-term tactical play along with language
explanation, we propose improving the reasoning capability of large language
models in chess by integrating annotated strategy and tactic. Specifically, we
collect a dataset named MATE, which consists of 1 million chess positions with
candidate moves annotated by chess experts for strategy and tactics. We
finetune the LLaMA-3-8B model and compare it against state-of-the-art
commercial language models in the task of selecting better chess moves. Our
experiments show that our models perform better than GPT, Claude, and Gemini
models. We find that language explanations can enhance the reasoning capability
of large language models.",cs.AI
Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data,"When training deep neural networks, a model's generalization error is often
observed to follow a power scaling law dependent both on the model size and the
data size. Perhaps the best known example of such scaling laws are for
transformer-based large language models, where networks with billions of
parameters are trained on trillions of tokens of text. Yet, despite sustained
widespread interest, a rigorous understanding of why transformer scaling laws
exist is still missing. To answer this question, we establish novel statistical
estimation and mathematical approximation theories for transformers when the
input data are concentrated on a low-dimensional manifold. Our theory predicts
a power law between the generalization error and both the training data size
and the network size for transformers, where the power depends on the intrinsic
dimension $d$ of the training data. Notably, the constructed model architecture
is shallow, requiring only logarithmic depth in $d$. By leveraging
low-dimensional data structures under a manifold hypothesis, we are able to
explain transformer scaling laws in a way which respects the data geometry.
Moreover, we test our theory with empirical observation by training LLMs on
natural language datasets. We find the observed empirical data scaling laws
closely agree with our theoretical predictions. Taken together, these results
rigorously show the intrinsic dimension of data to be a crucial quantity
affecting transformer scaling laws in both theory and practice.",cs.AI
Predicting Country Instability Using Bayesian Deep Learning and Random Forest,"Country instability is a global issue, with unpredictably high levels of
instability thwarting socio-economic growth and possibly causing a slew of
negative consequences. As a result, uncertainty prediction models for a country
are becoming increasingly important in the real world, and they are expanding
to provide more input from 'big data' collections, as well as the
interconnectedness of global economies and social networks. This has culminated
in massive volumes of qualitative data from outlets like television, print,
digital, and social media, necessitating the use of artificial intelligence
(AI) tools like machine learning to make sense of it all and promote predictive
precision [1]. The Global Database of Activities, Voice, and Tone (GDELT
Project) records broadcast, print, and web news in over 100 languages every
second of every day, identifying the people, locations, organisations, counts,
themes, outlets, and events that propel our global community and offering a
free open platform for computation on the entire world. The main goal of our
research is to investigate how, when our data grows more voluminous and
fine-grained, we can conduct a more complex methodological analysis of
political conflict. The GDELT dataset, which was released in 2012, is the first
and potentially the most technologically sophisticated publicly accessible
dataset on political conflict.",cs.AI
Exploring social bots: A feature-based approach to improve bot detection in social networks,"The importance of social media in our daily lives has unfortunately led to an
increase in the spread of misinformation, political messages and malicious
links. One of the most popular ways of carrying out those activities is using
automated accounts, also known as bots, which makes the detection of such
accounts a necessity. This paper addresses that problem by investigating
features based on the user account profile and its content, aiming to
understand the relevance of each feature as a basis for improving future bot
detectors. Through an exhaustive process of research, inference and feature
selection, we are able to surpass the state of the art on several metrics using
classical machine learning algorithms and identify the types of features that
are most important in detecting automated accounts.",cs.AI
A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning,"Recent regulatory proposals for artificial intelligence emphasize fairness
requirements for machine learning models. However, precisely defining the
appropriate measure of fairness is challenging due to philosophical, cultural
and political contexts. Biases can infiltrate machine learning models in
complex ways depending on the model's context, rendering a single common metric
of fairness insufficient. This ambiguity highlights the need for criteria to
guide the selection of context-aware measures, an issue of increasing
importance given the proliferation of ever tighter regulatory requirements. To
address this, we developed a flowchart to guide the selection of contextually
appropriate fairness measures. Twelve criteria were used to formulate the
flowchart. This included consideration of model assessment criteria, model
selection criteria, and data bias. We also review fairness literature in the
context of machine learning and link it to core regulatory instruments to
assist policymakers, AI developers, researchers, and other stakeholders in
appropriately addressing fairness concerns and complying with relevant
regulatory requirements.",cs.AI
MEANT: Multimodal Encoder for Antecedent Information,"The stock market provides a rich well of information that can be split across
modalities, making it an ideal candidate for multimodal evaluation. Multimodal
data plays an increasingly important role in the development of machine
learning and has shown to positively impact performance. But information can do
more than exist across modes -- it can exist across time. How should we attend
to temporal data that consists of multiple information types? This work
introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information
and (ii) a new dataset called TempStock, which consists of price, Tweets, and
graphical data with over a million Tweets from all of the companies in the S&P
500 Index. We find that MEANT improves performance on existing baselines by
over 15%, and that the textual information affects performance far more than
the visual information on our time-dependent task from our ablation study.",cs.AI
Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned
from labeled source domains to improve performance on the unlabeled target
domains. While Convolutional Neural Networks (CNNs) have been dominant in
previous UDA methods, recent research has shown promise in applying Vision
Transformers (ViTs) to this task. In this study, we propose a novel Feature
Fusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in
UDA tasks. Our method introduces two key innovations: First, we introduce a
patch discriminator to evaluate the transferability of patches, generating a
transferability matrix. We integrate this matrix into self-attention, directing
the model to focus on transferable patches. Second, we propose a feature fusion
technique to fuse embeddings in the latent space, enabling each embedding to
incorporate information from all others, thereby improving generalization.
These two components work in synergy to enhance feature representation
learning. Extensive experiments on widely used benchmarks demonstrate that our
method significantly improves UDA performance, achieving state-of-the-art
(SOTA) results.",cs.AI
Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.",stat.ML
Doubly Robust Regression Discontinuity Designs,"This study introduces a doubly robust (DR) estimator for regression
discontinuity (RD) designs. In RD designs, treatment effects are estimated in a
quasi-experimental setting where treatment assignment depends on whether a
running variable surpasses a predefined cutoff. A common approach in RD
estimation is to apply nonparametric regression methods, such as local linear
regression. In such an approach, the validity relies heavily on the consistency
of nonparametric estimators and is limited by the nonparametric convergence
rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we
propose the DR-RD estimator, which combines two distinct estimators for the
conditional expected outcomes. If either of these estimators is consistent, the
treatment effect estimator remains consistent. Furthermore, due to the
debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if
both regression estimators satisfy certain mild conditions, which also
simplifies statistical inference.",stat.ML
Tukey g-and-h neural network regression for non-Gaussian data,"This paper addresses non-Gaussian regression with neural networks via the use
of the Tukey g-and-h distribution.The Tukey g-and-h transform is a flexible
parametric transform with two parameters $g$ and $h$ which, when applied to a
standard normal random variable, introduces both skewness and kurtosis,
resulting in a distribution commonly called the Tukey g-and-h distribution.
Specific values of $g$ and $h$ produce good approximations to other families of
distributions, such as the Cauchy and student-t distributions. The flexibility
of the Tukey g-and-h distribution has driven its popularity in the statistical
community, in applied sciences and finance. In this work we consider the
training of a neural network to predict the parameters of a Tukey g-and-h
distribution in a regression framework via the minimization of the
corresponding negative log-likelihood, despite the latter having no closed-form
expression. We demonstrate the efficiency of our procedure in simulated
examples and apply our method to a real-world dataset of global crop yield for
several types of crops. Finally, we show how we can carry out a goodness-of-fit
analysis between the predicted distributions and the test data. A Pytorch
implementation is made available on Github and as a Pypi package.",stat.ML
Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of Factored-POMDPs,"Learning representations of underlying environmental dynamics from partial
observations is a critical challenge in machine learning. In the context of
Partially Observable Markov Decision Processes (POMDPs), state representations
are often inferred from the history of past observations and actions. We
demonstrate that incorporating future information is essential to accurately
capture causal dynamics and enhance state representations. To address this, we
introduce a Dynamical Variational Auto-Encoder (DVAE) designed to learn causal
Markovian dynamics from offline trajectories in a POMDP. Our method employs an
extended hindsight framework that integrates past, current, and multi-step
future information within a factored-POMDP setting. Empirical results reveal
that this approach uncovers the causal graph governing hidden state transitions
more effectively than history-based and typical hindsight-based models.",stat.ML
Quasi-Bayes empirical Bayes: a sequential approach to the Poisson compound decision problem,"The Poisson compound decision problem is a classical problem in statistics,
for which parametric and nonparametric empirical Bayes methodologies are
available to estimate the Poisson's means in static or batch domains. In this
paper, we consider the Poisson compound decision problem in a streaming or
online domain. By relying on a quasi-Bayesian approach, often referred to as
Newton's algorithm, we obtain sequential Poisson's mean estimates that are of
easy evaluation, computationally efficient and with a constant computational
cost as data increase, which is desirable for streaming data. Large sample
asymptotic properties of the proposed estimates are investigated, also
providing frequentist guarantees in terms of a regret analysis. We validate
empirically our methodology, both on synthetic and real data, comparing against
the most popular alternatives.",stat.ML
Decision Feedback In-Context Symbol Detection over Block-Fading Channels,"Pre-trained Transformers, through in-context learning (ICL), have
demonstrated exceptional capabilities to adapt to new tasks using example
prompts \textit{without model update}. Transformer-based wireless receivers,
where prompts consist of the pilot data in the form of transmitted and received
signal pairs, have shown high estimation accuracy when pilot data are abundant.
However, pilot information is often costly and limited in practice. In this
work, we propose the \underline{DE}cision \underline{F}eedback
\underline{IN}-Cont\underline{E}xt \underline{D}etection (DEFINED) solution as
a new wireless receiver design, which bypasses channel estimation and directly
performs symbol detection using the (sometimes extremely) limited pilot data.
The key innovation in DEFINED is the proposed decision feedback mechanism in
ICL, where we sequentially incorporate the detected symbols into the prompts to
improve the detections for subsequent symbols. Extensive experiments across a
broad range of wireless communication settings demonstrate that DEFINED
achieves significant performance improvements, in some cases only needing a
single pilot pair.",stat.ML
Exogenous Randomness Empowering Random Forests,"We offer theoretical and empirical insights into the impact of exogenous
randomness on the effectiveness of random forests with tree-building rules
independent of training data. We formally introduce the concept of exogenous
randomness and identify two types of commonly existing randomness: Type I from
feature subsampling, and Type II from tie-breaking in tree-building processes.
We develop non-asymptotic expansions for the mean squared error (MSE) for both
individual trees and forests and establish sufficient and necessary conditions
for their consistency. In the special example of the linear regression model
with independent features, our MSE expansions are more explicit, providing more
understanding of the random forests' mechanisms. It also allows us to derive an
upper bound on the MSE with explicit consistency rates for trees and forests.
Guided by our theoretical findings, we conduct simulations to further explore
how exogenous randomness enhances random forest performance. Our findings
unveil that feature subsampling reduces both the bias and variance of random
forests compared to individual trees, serving as an adaptive mechanism to
balance bias and variance. Furthermore, our results reveal an intriguing
phenomenon: the presence of noise features can act as a ""blessing"" in enhancing
the performance of random forests thanks to feature subsampling.",stat.ML
Model Stealing for Any Low-Rank Language Model,"Model stealing, where a learner tries to recover an unknown model via
carefully chosen queries, is a critical problem in machine learning, as it
threatens the security of proprietary models and the privacy of data they are
trained on. In recent years, there has been particular interest in stealing
large language models (LLMs). In this paper, we aim to build a theoretical
understanding of stealing language models by studying a simple and
mathematically tractable setting. We study model stealing for Hidden Markov
Models (HMMs), and more generally low-rank language models.
  We assume that the learner works in the conditional query model, introduced
by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient
algorithm in the conditional query model, for learning any low-rank
distribution. In other words, our algorithm succeeds at stealing any language
model whose output distribution is low-rank. This improves upon the previous
result by Kakade, Krishnamurthy, Mahajan and Zhang, which also requires the
unknown distribution to have high ""fidelity"", a property that holds only in
restricted cases. There are two key insights behind our algorithm: First, we
represent the conditional distributions at each timestep by constructing
barycentric spanners among a collection of vectors of exponentially large
dimension. Second, for sampling from our representation, we iteratively solve a
sequence of convex optimization problems that involve projection in relative
entropy to prevent compounding of errors over the length of the sequence. This
is an interesting example where, at least theoretically, allowing a machine
learning model to solve more complex problems at inference time can lead to
drastic improvements in its performance.",stat.ML
Collaborative and Federated Black-box Optimization: A Bayesian Optimization Perspective,"We focus on collaborative and federated black-box optimization (BBOpt), where
agents optimize their heterogeneous black-box functions through collaborative
sequential experimentation. From a Bayesian optimization perspective, we
address the fundamental challenges of distributed experimentation,
heterogeneity, and privacy within BBOpt, and propose three unifying frameworks
to tackle these issues: (i) a global framework where experiments are centrally
coordinated, (ii) a local framework that allows agents to make decisions based
on minimal shared information, and (iii) a predictive framework that enhances
local surrogates through collaboration to improve decision-making. We
categorize existing methods within these frameworks and highlight key open
questions to unlock the full potential of federated BBOpt. Our overarching goal
is to shift federated learning from its predominantly descriptive/predictive
paradigm to a prescriptive one, particularly in the context of BBOpt - an
inherently sequential decision-making problem.",stat.ML
Robust Offline Reinforcement Learning for Non-Markovian Decision Processes,"Distributionally robust offline reinforcement learning (RL) aims to find a
policy that performs the best under the worst environment within an uncertainty
set using an offline dataset collected from a nominal model. While recent
advances in robust RL focus on Markov decision processes (MDPs), robust
non-Markovian RL is limited to planning problem where the transitions in the
uncertainty set are known. In this paper, we study the learning problem of
robust offline non-Markovian RL. Specifically, when the nominal model admits a
low-rank structure, we propose a new algorithm, featuring a novel dataset
distillation and a lower confidence bound (LCB) design for robust values under
different types of the uncertainty set. We also derive new dual forms for these
robust values in non-Markovian RL, making our algorithm more amenable to
practical implementation. By further introducing a novel type-I concentrability
coefficient tailored for offline low-rank non-Markovian decision processes, we
prove that our algorithm can find an $\epsilon$-optimal robust policy using
$O(1/\epsilon^2)$ offline samples. Moreover, we extend our algorithm to the
case when the nominal model does not have specific structure. With a new
type-II concentrability coefficient, the extended algorithm also enjoys
polynomial sample efficiency under all different types of the uncertainty set.",stat.ML
Quantifying Knowledge Distillation Using Partial Information Decomposition,"Knowledge distillation provides an effective method for deploying complex
machine learning models in resource-constrained environments. It typically
involves training a smaller student model to emulate either the probabilistic
outputs or the internal feature representations of a larger teacher model. By
doing so, the student model often achieves substantially better performance on
a downstream task compared to when it is trained independently. Nevertheless,
the teacher's internal representations can also encode noise or additional
information that may not be relevant to the downstream task. This observation
motivates our primary question: What are the information-theoretic limits of
knowledge transfer? To this end, we leverage a body of work in information
theory called Partial Information Decomposition (PID) to quantify the
distillable and distilled knowledge of a teacher's representation corresponding
to a given student and a downstream task. Moreover, we demonstrate that this
metric can be practically used in distillation to address challenges caused by
the complexity gap between the teacher and the student representations.",stat.ML
Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources,"Machine learning is increasingly used to select which individuals receive
limited-resource interventions in domains such as human services, education,
development, and more. However, it is often not apparent what the right
quantity is for models to predict. In particular, policymakers rarely have
access to data from a randomized controlled trial (RCT) that would enable
accurate estimates of treatment effects -- which individuals would benefit more
from the intervention. Observational data is more likely to be available,
creating a substantial risk of bias in treatment effect estimates.
Practitioners instead commonly use a technique termed ""risk-based targeting""
where the model is just used to predict each individual's status quo outcome
(an easier, non-causal task). Those with higher predicted risk are offered
treatment. There is currently almost no empirical evidence to inform which
choices lead to the most effect machine learning-informed targeting strategies
in social domains. In this work, we use data from 5 real-world RCTs in a
variety of domains to empirically assess such choices. We find that risk-based
targeting is almost always inferior to targeting based on even biased estimates
of treatment effects. Moreover, these results hold even when the policymaker
has strong normative preferences for assisting higher-risk individuals. Our
results imply that, despite the widespread use of risk prediction models in
applied settings, practitioners may be better off incorporating even weak
evidence about heterogeneous causal effects to inform targeting.",stat.ML
PICZL: Image-based Photometric Redshifts for AGN,"Computing photo-z for AGN is challenging, primarily due to the interplay of
relative emissions associated with the SMBH and its host galaxy. SED fitting
methods, effective in pencil-beam surveys, face limitations in all-sky surveys
with fewer bands available, lacking the ability to capture the AGN contribution
to the SED accurately. This limitation affects the many 10s of millions of AGN
clearly singled out and identified by SRG/eROSITA. Our goal is to significantly
enhance photometric redshift performance for AGN in all-sky surveys while
avoiding the need to merge multiple data sets. Instead, we employ readily
available data products from the 10th Data Release of the Imaging Legacy Survey
for DESI, covering > 20,000 deg$^{2}$ with deep images and catalog-based
photometry in the grizW1-W4 bands. We introduce PICZL, a machine-learning
algorithm leveraging an ensemble of CNNs. Utilizing a cross-channel approach,
the algorithm integrates distinct SED features from images with those obtained
from catalog-level data. Full probability distributions are achieved via the
integration of Gaussian mixture models. On a validation sample of 8098 AGN,
PICZL achieves a variance $\sigma_{\textrm{NMAD}}$ of 4.5% with an outlier
fraction $\eta$ of 5.6%, outperforming previous attempts to compute accurate
photo-z for AGN using ML. We highlight that the model's performance depends on
many variables, predominantly the depth of the data. A thorough evaluation of
these dependencies is presented in the paper. Our streamlined methodology
maintains consistent performance across the entire survey area when accounting
for differing data quality. The same approach can be adopted for future deep
photometric surveys such as LSST and Euclid, showcasing its potential for
wide-scale realisation. With this paper, we release updated photo-z (including
errors) for the XMM-SERVS W-CDF-S, ELAIS-S1 and LSS fields.",stat.ML
Constructing Gaussian Processes via Samplets,"Gaussian Processes face two primary challenges: constructing models for large
datasets and selecting the optimal model. This master's thesis tackles these
challenges in the low-dimensional case. We examine recent convergence results
to identify models with optimal convergence rates and pinpoint essential
parameters. Utilizing this model, we propose a Samplet-based approach to
efficiently construct and train the Gaussian Processes, reducing the cubic
computational complexity to a log-linear scale. This method facilitates optimal
regression while maintaining efficient performance.",stat.ML
Conditional simulation via entropic optimal transport: Toward non-parametric estimation of conditional Brenier maps,"Conditional simulation is a fundamental task in statistical modeling:
Generate samples from the conditionals given finitely many data points from a
joint distribution. One promising approach is to construct conditional Brenier
maps, where the components of the map pushforward a reference distribution to
conditionals of the target. While many estimators exist, few, if any, come with
statistical or algorithmic guarantees. To this end, we propose a non-parametric
estimator for conditional Brenier maps based on the computational scalability
of \emph{entropic} optimal transport. Our estimator leverages a result of
Carlier et al. (2010), which shows that optimal transport maps under a rescaled
quadratic cost asymptotically converge to conditional Brenier maps; our
estimator is precisely the entropic analogues of these converging maps. We
provide heuristic justifications for choosing the scaling parameter in the cost
as a function of the number of samples by fully characterizing the Gaussian
setting. We conclude by comparing the performance of the estimator to other
machine learning and non-parametric approaches on benchmark datasets and
Bayesian inference problems.",stat.ML
General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization,"This work investigates the effectiveness of schedule-free methods, developed
by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings,
inspired by their remarkable empirical success in training neural networks.
Specifically, we show that schedule-free SGD achieves optimal iteration
complexity for nonsmooth, nonconvex optimization problems. Our proof begins
with the development of a general framework for online-to-nonconvex conversion,
which converts a given online learning algorithm into an optimization algorithm
for nonconvex losses. Our general framework not only recovers existing
conversions but also leads to two novel conversion schemes. Notably, one of
these new conversions corresponds directly to schedule-free SGD, allowing us to
establish its optimality. Additionally, our analysis provides valuable insights
into the parameter choices for schedule-free SGD, addressing a theoretical gap
that the convex theory cannot explain.",stat.ML
Unified Bayesian representation for high-dimensional multi-modal biomedical data for small-sample classification,"We present BALDUR, a novel Bayesian algorithm designed to deal with
multi-modal datasets and small sample sizes in high-dimensional settings while
providing explainable solutions. To do so, the proposed model combines within a
common latent space the different data views to extract the relevant
information to solve the classification task and prune out the
irrelevant/redundant features/data views. Furthermore, to provide generalizable
solutions in small sample size scenarios, BALDUR efficiently integrates dual
kernels over the views with a small sample-to-feature ratio. Finally, its
linear nature ensures the explainability of the model outcomes, allowing its
use for biomarker identification. This model was tested over two different
neurodegeneration datasets, outperforming the state-of-the-art models and
detecting features aligned with markers already described in the scientific
literature.",stat.ML
Causal-discovery-based root-cause analysis and its application in time-series prediction error diagnosis,"Recent rapid advancements of machine learning have greatly enhanced the
accuracy of prediction models, but most models remain ""black boxes"", making
prediction error diagnosis challenging, especially with outliers. This lack of
transparency hinders trust and reliability in industrial applications.
Heuristic attribution methods, while helpful, often fail to capture true causal
relationships, leading to inaccurate error attributions. Various root-cause
analysis methods have been developed using Shapley values, yet they typically
require predefined causal graphs, limiting their applicability for prediction
errors in machine learning models. To address these limitations, we introduce
the Causal-Discovery-based Root-Cause Analysis (CD-RCA) method that estimates
causal relationships between the prediction error and the explanatory
variables, without needing a pre-defined causal graph. By simulating synthetic
error data, CD-RCA can identify variable contributions to outliers in
prediction errors by Shapley values. Extensive simulations show CD-RCA
outperforms current heuristic attribution methods, and a sensitivity analysis
reveals new patterns where Shapley values may misattribute errors, paving the
way for more accurate error attribution methods.",stat.ML
SPARTAN: A Sparse Transformer Learning Local Causation,"Causal structures play a central role in world models that flexibly adapt to
changes in the environment. While recent works motivate the benefits of
discovering local causal graphs for dynamics modelling, in this work we
demonstrate that accurately capturing these relationships in complex settings
remains challenging for the current state-of-the-art. To remedy this
shortcoming, we postulate that sparsity is a critical ingredient for the
discovery of such local causal structures. To this end we present the SPARse
TrANsformer World model (SPARTAN), a Transformer-based world model that learns
local causal structures between entities in a scene. By applying sparsity
regularisation on the attention pattern between object-factored tokens, SPARTAN
identifies sparse local causal models that accurately predict future object
states. Furthermore, we extend our model to capture sparse interventions with
unknown targets on the dynamics of the environment. This results in a highly
interpretable world model that can efficiently adapt to changes. Empirically,
we evaluate SPARTAN against the current state-of-the-art in object-centric
world models on observation-based environments and demonstrate that our model
can learn accurate local causal graphs and achieve significantly improved
few-shot adaptation to changes in the dynamics of the environment as well as
robustness against removing irrelevant distractors.",stat.ML
WassFFed: Wasserstein Fair Federated Learning,"Federated Learning (FL) employs a training approach to address scenarios
where users' data cannot be shared across clients. Achieving fairness in FL is
imperative since training data in FL is inherently geographically distributed
among diverse user groups. Existing research on fairness predominantly assumes
access to the entire training data, making direct transfer to FL challenging.
However, the limited existing research on fairness in FL does not effectively
address two key challenges, i.e., (CH1) Current methods fail to deal with the
inconsistency between fair optimization results obtained with surrogate
functions and fair classification results. (CH2) Directly aggregating local
fair models does not always yield a globally fair model due to non Identical
and Independent data Distributions (non-IID) among clients. To address these
challenges, we propose a Wasserstein Fair Federated Learning framework, namely
WassFFed. To tackle CH1, we ensure that the outputs of local models, rather
than the loss calculated with surrogate functions or classification results
with a threshold, remain independent of various user groups. To resolve CH2, we
employ a Wasserstein barycenter calculation of all local models' outputs for
each user group, bringing local model outputs closer to the global output
distribution to ensure consistency between the global model and local models.
We conduct extensive experiments on three real-world datasets, demonstrating
that WassFFed outperforms existing approaches in striking a balance between
accuracy and fairness.",stat.ML
Effect sizes as a statistical feature-selector-based learning to detect breast cancer,"Breast cancer detection is still an open research field, despite a tremendous
effort devoted to work in this area. Effect size is a statistical concept that
measures the strength of the relationship between two variables on a numeric
scale. Feature selection is widely used to reduce the dimensionality of data by
selecting only a subset of predictor variables to improve a learning model. In
this work, an algorithm and experimental results demonstrate the feasibility of
developing a statistical feature-selector-based learning tool capable of
reducing the data dimensionality using parametric effect size measures from
features extracted from cell nuclei images. The SVM classifier with a linear
kernel as a learning tool achieved an accuracy of over 90%. These excellent
results suggest that the effect size is within the standards of the
feature-selector methods",stat.ML
Scientific machine learning in ecological systems: A study on the predator-prey dynamics,"In this study, we apply two pillars of Scientific Machine Learning: Neural
Ordinary Differential Equations (Neural ODEs) and Universal Differential
Equations (UDEs) to the Lotka Volterra Predator Prey Model, a fundamental
ecological model describing the dynamic interactions between predator and prey
populations. The Lotka-Volterra model is critical for understanding ecological
dynamics, population control, and species interactions, as it is represented by
a system of differential equations. In this work, we aim to uncover the
underlying differential equations without prior knowledge of the system,
relying solely on training data and neural networks. Using robust modeling in
the Julia programming language, we demonstrate that both Neural ODEs and UDEs
can be effectively utilized for prediction and forecasting of the
Lotka-Volterra system. More importantly, we introduce the forecasting breakdown
point: the time at which forecasting fails for both Neural ODEs and UDEs. We
observe how UDEs outperform Neural ODEs by effectively recovering the
underlying dynamics and achieving accurate forecasting with significantly less
training data. Additionally, we introduce Gaussian noise of varying magnitudes
(from mild to high) to simulate real-world data perturbations and show that
UDEs exhibit superior robustness, effectively recovering the underlying
dynamics even in the presence of noisy data, while Neural ODEs struggle with
high levels of noise. Through extensive hyperparameter optimization, we offer
insights into neural network architectures, activation functions, and
optimizers that yield the best results. This study opens the door to applying
Scientific Machine Learning frameworks for forecasting tasks across a wide
range of ecological and scientific domains.",stat.ML
Generative Feature Training of Thin 2-Layer Networks,"We consider the approximation of functions by 2-layer neural networks with a
small number of hidden weights based on the squared loss and small datasets.
Due to the highly non-convex energy landscape, gradient-based training often
suffers from local minima. As a remedy, we initialize the hidden weights with
samples from a learned proposal distribution, which we parameterize as a deep
generative model. To train this model, we exploit the fact that with fixed
hidden weights, the optimal output weights solve a linear equation. After
learning the generative model, we refine the sampled weights with a
gradient-based post-processing in the latent space. Here, we also include a
regularization scheme to counteract potential noise. Finally, we demonstrate
the effectiveness of our approach by numerical examples.",stat.ML
Optimized Quality of Service prediction in FSO Links over South Africa using Ensemble Learning,"Fibre optic communication system is expected to increase exponentially in
terms of application due to the numerous advantages over copper wires. The
optical network evolution presents several advantages such as over
long-distance, low-power requirement, higher carrying capacity and high
bandwidth among others Such network bandwidth surpasses methods of transmission
that include copper cables and microwaves. Despite these benefits, free-space
optical communications are severely impacted by harsh weather situations like
mist, precipitation, blizzard, fume, soil, and drizzle debris in the
atmosphere, all of which have an impact on the Quality of Service (QoS)
rendered by the systems. The primary goal of this article is to optimize the
QoS using the ensemble learning models Random Forest, ADaBoost Regression,
Stacking Regression, Gradient Boost Regression, and Multilayer Neural Network.
To accomplish the stated goal, meteorological data, visibility, wind speed, and
altitude were obtained from the South Africa Weather Services archive during a
ten-year period (2010 to 2019) at four different locations: Polokwane,
Kimberley, Bloemfontein, and George. We estimated the data rate, power
received, fog-induced attenuation, bit error rate and power penalty using the
collected and processed data. The RMSE and R-squared values of the model across
all the study locations, Polokwane, Kimberley, Bloemfontein, and George, are
0.0073 and 0.9951, 0.0065 and 0.9998, 0.0060 and 0.9941, and 0.0032 and 0.9906,
respectively. The result showed that using ensemble learning techniques in
transmission modeling can significantly enhance service quality and meet
customer service level agreements and ensemble method was successful in
efficiently optimizing the signal to noise ratio, which in turn enhanced the
QoS at the point of reception.",stat.ML
Methane projections from Canada's oil sands tailings using scientific deep learning reveal significant underestimation,"Bitumen extraction for the production of synthetic crude oil in Canada's
Athabasca Oil Sands industry has recently come under spotlight for being a
significant source of greenhouse gas emission. A major cause of concern is
methane, a greenhouse gas produced by the anaerobic biodegradation of
hydrocarbons in oil sands residues, or tailings, stored in settle basins
commonly known as oil sands tailing ponds. In order to determine the methane
emitting potential of these tailing ponds and have future methane projections,
we use real-time weather data, mechanistic models developed from laboratory
controlled experiments, and industrial reports to train a physics constrained
machine learning model. Our trained model can successfully identify the
directions of active ponds and estimate their emission levels, which are
generally hard to obtain due to data sampling restrictions. We found that each
active oil sands tailing pond could emit between 950 to 1500 tonnes of methane
per year, whose environmental impact is equivalent to carbon dioxide emissions
from at least 6000 gasoline powered vehicles. Although abandoned ponds are
often presumed to have insignificant emissions, our findings indicate that
these ponds could become active over time and potentially emit up to 1000
tonnes of methane each year. Taking an average over all datasets that was used
in model training, we estimate that emissions around major oil sands regions
would need to be reduced by approximately 12% over a year, to reduce the
average methane concentrations to 2005 levels.",stat.ML
Learning a Single Neuron Robustly to Distributional Shifts and Adversarial Label Noise,"We study the problem of learning a single neuron with respect to the
$L_2^2$-loss in the presence of adversarial distribution shifts, where the
labels can be arbitrary, and the goal is to find a ``best-fit'' function. More
precisely, given training samples from a reference distribution
$\mathcal{p}_0$, the goal is to approximate the vector $\mathbf{w}^*$ which
minimizes the squared loss with respect to the worst-case distribution that is
close in $\chi^2$-divergence to $\mathcal{p}_{0}$. We design a computationally
efficient algorithm that recovers a vector $ \hat{\mathbf{w}}$ satisfying
$\mathbb{E}_{\mathcal{p}^*} (\sigma(\hat{\mathbf{w}} \cdot \mathbf{x}) - y)^2
\leq C \, \mathbb{E}_{\mathcal{p}^*} (\sigma(\mathbf{w}^* \cdot \mathbf{x}) -
y)^2 + \epsilon$, where $C>1$ is a dimension-independent constant and
$(\mathbf{w}^*, \mathcal{p}^*)$ is the witness attaining the min-max risk
$\min_{\mathbf{w}~:~\|\mathbf{w}\| \leq W} \max_{\mathcal{p}}
\mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{p}} (\sigma(\mathbf{w} \cdot
\mathbf{x}) - y)^2 - \nu \chi^2(\mathcal{p}, \mathcal{p}_0)$. Our algorithm
follows a primal-dual framework and is designed by directly bounding the risk
with respect to the original, nonconvex $L_2^2$ loss. From an optimization
standpoint, our work opens new avenues for the design of primal-dual algorithms
under structured nonconvexity.",stat.ML
Shedding Light on Problems with Hyperbolic Graph Learning,"Recent papers in the graph machine learning literature have introduced a
number of approaches for hyperbolic representation learning. The asserted
benefits are improved performance on a variety of graph tasks, node
classification and link prediction included. Claims have also been made about
the geometric suitability of particular hierarchical graph datasets to
representation in hyperbolic space. Despite these claims, our work makes a
surprising discovery: when simple Euclidean models with comparable numbers of
parameters are properly trained in the same environment, in most cases, they
perform as well, if not better, than all introduced hyperbolic graph
representation learning models, even on graph datasets previously claimed to be
the most hyperbolic as measured by Gromov $\delta$-hyperbolicity (i.e., perfect
trees). This observation gives rise to a simple question: how can this be? We
answer this question by taking a careful look at the field of hyperbolic graph
representation learning as it stands today, and find that a number of papers
fail to diligently present baselines, make faulty modelling assumptions when
constructing algorithms, and use misleading metrics to quantify geometry of
graph datasets. We take a closer look at each of these three problems,
elucidate the issues, perform an analysis of methods, and introduce a
parametric family of benchmark datasets to ascertain the applicability of
(hyperbolic) graph neural networks.",stat.ML
Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data,"When training deep neural networks, a model's generalization error is often
observed to follow a power scaling law dependent both on the model size and the
data size. Perhaps the best known example of such scaling laws are for
transformer-based large language models, where networks with billions of
parameters are trained on trillions of tokens of text. Yet, despite sustained
widespread interest, a rigorous understanding of why transformer scaling laws
exist is still missing. To answer this question, we establish novel statistical
estimation and mathematical approximation theories for transformers when the
input data are concentrated on a low-dimensional manifold. Our theory predicts
a power law between the generalization error and both the training data size
and the network size for transformers, where the power depends on the intrinsic
dimension $d$ of the training data. Notably, the constructed model architecture
is shallow, requiring only logarithmic depth in $d$. By leveraging
low-dimensional data structures under a manifold hypothesis, we are able to
explain transformer scaling laws in a way which respects the data geometry.
Moreover, we test our theory with empirical observation by training LLMs on
natural language datasets. We find the observed empirical data scaling laws
closely agree with our theoretical predictions. Taken together, these results
rigorously show the intrinsic dimension of data to be a crucial quantity
affecting transformer scaling laws in both theory and practice.",stat.ML
Few measurement shots challenge generalization in learning to classify entanglement,"The ability to extract general laws from a few known examples depends on the
complexity of the problem and on the amount of training data. In the quantum
setting, the learner's generalization performance is further challenged by the
destructive nature of quantum measurements that, together with the no-cloning
theorem, limits the amount of information that can be extracted from each
training sample. In this paper we focus on hybrid quantum learning techniques
where classical machine-learning methods are paired with quantum algorithms and
show that, in some settings, the uncertainty coming from a few measurement
shots can be the dominant source of errors. We identify an instance of this
possibly general issue by focusing on the classification of maximally entangled
vs. separable states, showing that this toy problem becomes challenging for
learners unaware of entanglement theory. Finally, we introduce an estimator
based on classical shadows that performs better in the big data, few copy
regime. Our results show that the naive application of classical
machine-learning methods to the quantum setting is problematic, and that a
better theoretical foundation of quantum learning is required.",stat.ML
An Energy-Based Self-Adaptive Learning Rate for Stochastic Gradient Descent: Enhancing Unconstrained Optimization with VAV method,"Optimizing the learning rate remains a critical challenge in machine
learning, essential for achieving model stability and efficient convergence.
The Vector Auxiliary Variable (VAV) algorithm introduces a novel energy-based
self-adjustable learning rate optimization method designed for unconstrained
optimization problems. It incorporates an auxiliary variable $r$ to facilitate
efficient energy approximation without backtracking while adhering to the
unconditional energy dissipation law. Notably, VAV demonstrates superior
stability with larger learning rates and achieves faster convergence in the
early stage of the training process. Comparative analyses demonstrate that VAV
outperforms Stochastic Gradient Descent (SGD) across various tasks. This paper
also provides rigorous proof of the energy dissipation law and establishes the
convergence of the algorithm under reasonable assumptions. Additionally, $r$
acts as an empirical lower bound of the training loss in practice, offering a
novel scheduling approach that further enhances algorithm performance.",stat.ML
Learning Loss Landscapes in Preference Optimization,"We present an empirical study investigating how specific properties of
preference datasets, such as mixed-quality or noisy data, affect the
performance of Preference Optimization (PO) algorithms. Our experiments,
conducted in MuJoCo environments, reveal several scenarios where
state-of-the-art PO methods experience significant drops in performance. To
address this issue, we introduce a novel PO framework based on mirror descent,
which can recover existing methods like Direct Preference Optimization (DPO)
and Odds-Ratio Preference Optimization (ORPO) for specific choices of the
mirror map. Within this framework, we employ evolutionary strategies to
discover new loss functions capable of handling the identified problematic
scenarios. These new loss functions lead to significant performance
improvements over DPO and ORPO across several tasks. Additionally, we
demonstrate the generalization capability of our approach by applying the
discovered loss functions to fine-tuning large language models using
mixed-quality data, where they outperform ORPO.",stat.ML
Understanding the Role of Equivariance in Self-supervised Learning,"Contrastive learning has been a leading paradigm for self-supervised
learning, but it is widely observed that it comes at the price of sacrificing
useful features (\eg colors) by being invariant to data augmentations. Given
this limitation, there has been a surge of interest in equivariant
self-supervised learning (E-SSL) that learns features to be augmentation-aware.
However, even for the simplest rotation prediction method, there is a lack of
rigorous understanding of why, when, and how E-SSL learns useful features for
downstream tasks. To bridge this gap between practice and theory, we establish
an information-theoretic perspective to understand the generalization ability
of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL
that creates a synergy between the equivariant and classification tasks. This
synergy effect encourages models to extract class-relevant features to improve
its equivariant prediction, which, in turn, benefits downstream tasks requiring
semantic features. Based on this perspective, we theoretically analyze the
influence of data transformations and reveal several principles for practical
designs of E-SSL. Our theory not only aligns well with existing E-SSL methods
but also sheds light on new directions by exploring the benefits of model
equivariance. We believe that a theoretically grounded understanding on the
role of equivariance would inspire more principled and advanced designs in this
field. Code is available at https://github.com/kaotty/Understanding-ESSL.",stat.ML
Individual Regret in Cooperative Stochastic Multi-Armed Bandits,"We study the regret in stochastic Multi-Armed Bandits (MAB) with multiple
agents that communicate over an arbitrary connected communication graph. We
show a near-optimal individual regret bound of $\tilde{O}(\sqrt{AT/m}+A)$,
where $A$ is the number of actions, $T$ the time horizon, and $m$ the number of
agents. In particular, assuming a sufficient number of agents, we achieve a
regret bound of $\tilde{O}(A)$, which is independent of the sub-optimality gaps
and the diameter of the communication graph. To the best of our knowledge, our
study is the first to show an individual regret bound in cooperative stochastic
MAB that is independent of the graph's diameter and applicable to
non-fully-connected communication graphs.",stat.ML
Neuro-Symbolic Rule Lists,"Machine learning models deployed in sensitive areas such as healthcare must
be interpretable to ensure accountability and fairness. Rule lists (if Age < 35
$\wedge$ Priors > 0 then Recidivism = True, else if Next Condition . . . )
offer full transparency, making them well-suited for high-stakes decisions.
However, learning such rule lists presents significant challenges. Existing
methods based on combinatorial optimization require feature pre-discretization
and impose restrictions on rule size. Neuro-symbolic methods use more scalable
continuous optimization yet place similar pre-discretization constraints and
suffer from unstable optimization. To address the existing limitations, we
introduce NeuRules, an end-to-end trainable model that unifies discretization,
rule learning, and rule order into a single differentiable framework. We
formulate a continuous relaxation of the rule list learning problem that
converges to a strict rule list through temperature annealing. NeuRules learns
both the discretizations of individual features, as well as their combination
into conjunctive rules without any pre-processing or restrictions. Extensive
experiments demonstrate that NeuRules consistently outperforms both
combinatorial and neuro-symbolic methods, effectively learning simple and
complex rules, as well as their order, across a wide range of datasets.",stat.ML
Locally Adaptive One-Class Classifier Fusion with Dynamic $\ell$p-Norm Constraints for Robust Anomaly Detection,"This paper presents a novel approach to one-class classifier fusion through
locally adaptive learning with dynamic $\ell$p-norm constraints. We introduce a
framework that dynamically adjusts fusion weights based on local data
characteristics, addressing fundamental challenges in ensemble-based anomaly
detection. Our method incorporates an interior-point optimization technique
that significantly improves computational efficiency compared to traditional
Frank-Wolfe approaches, achieving up to 19-fold speed improvements in complex
scenarios. The framework is extensively evaluated on standard UCI benchmark
datasets and specialized temporal sequence datasets, demonstrating superior
performance across diverse anomaly types. Statistical validation through
Skillings-Mack tests confirms our method's significant advantages over existing
approaches, with consistent top rankings in both pure and non-pure learning
scenarios. The framework's ability to adapt to local data patterns while
maintaining computational efficiency makes it particularly valuable for
real-time applications where rapid and accurate anomaly detection is crucial.",stat.ML
Local vs. Global Models for Hierarchical Forecasting,"Hierarchical time series forecasting plays a crucial role in decision-making
in various domains while presenting significant challenges for modelling as
they involve multiple levels of aggregation, constraints, and availability of
information. This study explores the influence of distinct information
utilisation on the accuracy of hierarchical forecasts, proposing and evaluating
locals and a range of Global Forecasting Models (GFMs). In contrast to local
models, which forecast each series independently, we develop GFMs to exploit
cross-series and cross-hierarchies information, improving both forecasting
performance and computational efficiency. We employ reconciliation methods to
ensure coherency in forecasts and use the Mean Absolute Scaled Error (MASE) and
Multiple Comparisons with the Best (MCB) tests to assess statistical
significance. The findings indicate that GFMs possess significant advantages
for hierarchical forecasting, providing more accurate and computationally
efficient solutions across different levels in a hierarchy. Two specific GFMs
based on LightGBM are introduced, demonstrating superior accuracy and lower
model complexity than their counterpart local models and conventional methods
such as Exponential Smoothing (ES) and Autoregressive Integrated Moving Average
(ARIMA).",stat.ML
Stabilized Inverse Probability Weighting via Isotonic Calibration,"Inverse weighting with an estimated propensity score is widely used by
estimation methods in causal inference to adjust for confounding bias. However,
directly inverting propensity score estimates can lead to instability, bias,
and excessive variability due to large inverse weights, especially when
treatment overlap is limited. In this work, we propose a post-hoc calibration
algorithm for inverse propensity weights that generates well-calibrated,
stabilized weights from user-supplied, cross-fitted propensity score estimates.
Our approach employs a variant of isotonic regression with a loss function
specifically tailored to the inverse propensity weights. Through theoretical
analysis and empirical studies, we demonstrate that isotonic calibration
improves the performance of doubly robust estimators of the average treatment
effect.",stat.ML
Regret Minimization and Statistical Inference in Online Decision Making with High-dimensional Covariates,"This paper investigates regret minimization, statistical inference, and their
interplay in high-dimensional online decision-making based on the sparse linear
context bandit model. We integrate the $\varepsilon$-greedy bandit algorithm
for decision-making with a hard thresholding algorithm for estimating sparse
bandit parameters and introduce an inference framework based on a debiasing
method using inverse propensity weighting. Under a margin condition, our method
achieves either $O(T^{1/2})$ regret or classical $O(T^{1/2})$-consistent
inference, indicating an unavoidable trade-off between exploration and
exploitation. If a diverse covariate condition holds, we demonstrate that a
pure-greedy bandit algorithm, i.e., exploration-free, combined with a debiased
estimator based on average weighting can simultaneously achieve optimal $O(\log
T)$ regret and $O(T^{1/2})$-consistent inference. We also show that a simple
sample mean estimator can provide valid inference for the optimal policy's
value. Numerical simulations and experiments on Warfarin dosing data validate
the effectiveness of our methods.",stat.ML
Amortized Bayesian Local Interpolation NetworK: Fast covariance parameter estimation for Gaussian Processes,"Gaussian processes (GPs) are a ubiquitous tool for geostatistical modeling
with high levels of flexibility and interpretability, and the ability to make
predictions at unseen spatial locations through a process called Kriging.
Estimation of Kriging weights relies on the inversion of the process'
covariance matrix, creating a computational bottleneck for large spatial
datasets. In this paper, we propose an Amortized Bayesian Local Interpolation
NetworK (A-BLINK) for fast covariance parameter estimation, which uses two
pre-trained deep neural networks to learn a mapping from spatial location
coordinates and covariance function parameters to Kriging weights and the
spatial variance, respectively. The fast prediction time of these networks
allows us to bypass the matrix inversion step, creating large computational
speedups over competing methods in both frequentist and Bayesian settings, and
also provides full posterior inference and predictions using Markov chain Monte
Carlo sampling methods. We show significant increases in computational
efficiency over comparable scalable GP methodology in an extensive simulation
study with lower parameter estimation error. The efficacy of our approach is
also demonstrated using a temperature dataset of US climate normals for
1991--2020 based on over 7,000 weather stations.",stat.ML
Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds,"The PAC-Bayesian framework has significantly advanced our understanding of
statistical learning, particularly in majority voting methods. However, its
application to multi-view learning remains underexplored. In this paper, we
extend PAC-Bayesian theory to the multi-view setting, introducing novel
PAC-Bayesian bounds based on R\'enyi divergence. These bounds improve upon
traditional Kullback-Leibler divergence and offer more refined complexity
measures. We further propose first and second-order oracle PAC-Bayesian bounds,
along with an extension of the C-bound for multi-view learning. To ensure
practical applicability, we develop efficient optimization algorithms with
self-bounding properties.",stat.ML
RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks,"Parallel-in-time (PinT) techniques have been proposed to solve systems of
time-dependent differential equations by parallelizing the temporal domain.
Among them, Parareal computes the solution sequentially using an inaccurate
(fast) solver, and then ""corrects"" it using an accurate (slow) integrator that
runs in parallel across temporal subintervals. This work introduces
RandNet-Parareal, a novel method to learn the discrepancy between the coarse
and fine solutions using random neural networks (RandNets). RandNet-Parareal
achieves speed gains up to x125 and x22 compared to the fine solver run
serially and Parareal, respectively. Beyond theoretical guarantees of RandNets
as universal approximators, these models are quick to train, allowing the PinT
solution of partial differential equations on a spatial mesh of up to $10^5$
points with minimal overhead, dramatically increasing the scalability of
existing PinT approaches. RandNet-Parareal's numerical performance is
illustrated on systems of real-world significance, such as the viscous Burgers'
equation, the Diffusion-Reaction equation, the two- and three-dimensional
Brusselator, and the shallow water equation.",stat.ML
Weak to Strong Learning from Aggregate Labels,"In learning from aggregate labels, the training data consists of sets or
""bags"" of feature-vectors (instances) along with an aggregate label for each
bag derived from the (usually {0,1}-valued) labels of its instances. In
learning from label proportions (LLP), the aggregate label is the average of
the bag's instance labels, whereas in multiple instance learning (MIL) it is
the OR. The goal is to train an instance-level predictor, typically achieved by
fitting a model on the training data, in particular one that maximizes the
accuracy which is the fraction of satisfied bags i.e., those on which the
predicted labels are consistent with the aggregate label. A weak learner has at
a constant accuracy < 1 on the training bags, while a strong learner's accuracy
can be arbitrarily close to 1. We study the problem of using a weak learner on
such training bags with aggregate labels to obtain a strong learner, analogous
to supervised learning for which boosting algorithms are known. Our first
result shows the impossibility of boosting in LLP using weak classifiers of any
accuracy < 1 by constructing a collection of bags for which such weak learners
(for any weight assignment) exist, while not admitting any strong learner. A
variant of this construction also rules out boosting in MIL for a non-trivial
range of weak learner accuracy. In the LLP setting however, we show that a weak
learner (with small accuracy) on large enough bags can in fact be used to
obtain a strong learner for small bags, in polynomial time. We also provide
more efficient, sampling based variant of our procedure with probabilistic
guarantees which are empirically validated on three real and two synthetic
datasets. Our work is the first to theoretically study weak to strong learning
from aggregate labels, with an algorithm to achieve the same for LLP, while
proving the impossibility of boosting for both LLP and MIL.",stat.ML
Variational Bayes Portfolio Construction,"Portfolio construction is the science of balancing reward and risk; it is at
the core of modern finance. In this paper, we tackle the question of optimal
decision-making within a Bayesian paradigm, starting from a decision-theoretic
formulation. Despite the inherent intractability of the optimal decision in any
interesting scenarios, we manage to rewrite it as a saddle-point problem.
Leveraging the literature on variational Bayes (VB), we propose a relaxation of
the original problem. This novel methodology results in an efficient algorithm
that not only performs well but is also provably convergent. Furthermore, we
provide theoretical results on the statistical consistency of the resulting
decision with the optimal Bayesian decision. Using real data, our proposal
significantly enhances the speed and scalability of portfolio selection
problems. We benchmark our results against state-of-the-art algorithms, as well
as a Monte Carlo algorithm targeting the optimal decision.",stat.ML
Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization,"In the field of non-invasive medical imaging, radiomic features are utilized
to measure tumor characteristics. However, these features can be affected by
the techniques used to discretize the images, ultimately impacting the accuracy
of diagnosis. To investigate the influence of various image discretization
methods on diagnosis, it is common practice to evaluate multiple discretization
strategies individually. This approach often leads to redundant and
time-consuming tasks such as training predictive models and fine-tuning
hyperparameters separately. This study examines the feasibility of employing
multi-task Bayesian optimization to accelerate the hyperparameters search for
classifying benign and malignant pulmonary nodules using RBF SVM. Our findings
suggest that multi-task Bayesian optimization significantly accelerates the
search for hyperparameters in comparison to a single-task approach. To the best
of our knowledge, this is the first investigation to utilize multi-task
Bayesian optimization in a critical medical context.",stat.ML
Deep Nonparametric Conditional Independence Tests for Images,"Conditional independence tests (CITs) test for conditional dependence between
random variables. As existing CITs are limited in their applicability to
complex, high-dimensional variables such as images, we introduce deep
nonparametric CITs (DNCITs). The DNCITs combine embedding maps, which extract
feature representations of high-dimensional variables, with nonparametric CITs
applicable to these feature representations. For the embedding maps, we derive
general properties on their parameter estimators to obtain valid DNCITs and
show that these properties include embedding maps learned through (conditional)
unsupervised or transfer learning. For the nonparametric CITs, appropriate
tests are selected and adapted to be applicable to feature representations.
Through simulations, we investigate the performance of the DNCITs for different
embedding maps and nonparametric CITs under varying confounder dimensions and
confounder relationships. We apply the DNCITs to brain MRI scans and behavioral
traits, given confounders, of healthy individuals from the UK Biobank (UKB),
confirming null results from a number of ambiguous personality neuroscience
studies with a larger data set and with our more powerful tests. In addition,
in a confounder control study, we apply the DNCITs to brain MRI scans and a
confounder set to test for sufficient confounder control, leading to a
potential reduction in the confounder dimension under improved confounder
control compared to existing state-of-the-art confounder control studies for
the UKB. Finally, we provide an R package implementing the DNCITs.",stat.ML
Mutual-energy inner product optimization method for constructing feature coordinates and image classification in Machine Learning,"As a key task in machine learning, data classification is essentially to find
a suitable coordinate system to represent data features of different classes of
samples. This paper proposes the mutual-energy inner product optimization
method for constructing a feature coordinate system. First, by analyzing the
solution space and eigenfunctions of partial differential equations describing
a non-uniform membrane, the mutual-energy inner product is defined. Second, by
expressing the mutual-energy inner product as a series of eigenfunctions, it
shows a significant advantage of enhancing low-frequency features and
suppressing high-frequency noise, compared with the Euclidean inner product.
And then, a mutual-energy inner product optimization model is built to extract
data features, and convexity and concavity properties of its objective function
are discussed. Next, by combining the finite element method, a stable and
efficient sequential linearization algorithm is constructed to solve the
optimization model. This algorithm only solves equations including positive
definite symmetric matrix and linear programming with a few constraints, and
its vectorized implementation is discussed. Finally, the mutual-energy inner
product optimization method is used to construct feature coordinates, and
multi-class Gaussian classifiers are trained on the MINST training set. Good
prediction results of Gaussian classifiers are achieved on the MINST test set.",stat.ML
Model Selection for Average Reward RL with Application to Utility Maximization in Repeated Games,"In standard RL, a learner attempts to learn an optimal policy for a Markov
Decision Process whose structure (e.g. state space) is known. In online model
selection, a learner attempts to learn an optimal policy for an MDP knowing
only that it belongs to one of $M >1$ model classes of varying complexity.
Recent results have shown that this can be feasibly accomplished in episodic
online RL. In this work, we propose $\mathsf{MRBEAR}$, an online model
selection algorithm for the average reward RL setting. The regret of the
algorithm is in $\tilde O(M C_{m^*}^2 \mathsf{B}_{m^*}(T,\delta))$ where
$C_{m^*}$ represents the complexity of the simplest well-specified model class
and $\mathsf{B}_{m^*}(T,\delta)$ is its corresponding regret bound. This result
shows that in average reward RL, like the episodic online RL, the additional
cost of model selection scales only linearly in $M$, the number of model
classes. We apply $\mathsf{MRBEAR}$ to the interaction between a learner and an
opponent in a two-player simultaneous general-sum repeated game, where the
opponent follows a fixed unknown limited memory strategy. The learner's goal is
to maximize its utility without knowing the opponent's utility function. The
interaction is over $T$ rounds with no episode or discounting which leads us to
measure the learner's performance by average reward regret. In this
application, our algorithm enjoys an opponent-complexity-dependent regret in
$\tilde O(M(\mathsf{sp}(h^*) B^{m^*} A^{m^*+1})^{\frac{3}{2}} \sqrt{T})$, where
$m^*\le M$ is the unknown memory limit of the opponent, $\mathsf{sp}(h^*)$ is
the unknown span of optimal bias induced by the opponent, and $A$ and $B$ are
the number of actions for the learner and opponent respectively. We also show
that the exponential dependency on $m^*$ is inevitable by proving a lower bound
on the learner's regret.",stat.ML
Learning Mixtures of Experts with EM,"Mixtures of Experts (MoE) are Machine Learning models that involve
partitioning the input space, with a separate ""expert"" model trained on each
partition. Recently, MoE have become popular as components in today's large
language models as a means to reduce training and inference costs. There, the
partitioning function and the experts are both learnt jointly via gradient
descent on the log-likelihood. In this paper we focus on studying the
efficiency of the Expectation Maximization (EM) algorithm for the training of
MoE models. We first rigorously analyze EM for the cases of linear or logistic
experts, where we show that EM is equivalent to Mirror Descent with unit step
size and a Kullback-Leibler Divergence regularizer. This perspective allows us
to derive new convergence results and identify conditions for local linear
convergence based on the signal-to-noise ratio (SNR). Experiments on synthetic
and (small-scale) real-world data show that EM outperforms the gradient descent
algorithm both in terms of convergence rate and the achieved accuracy.",stat.ML
Filling in Missing FX Implied Volatilities with Uncertainties: Improving VAE-Based Volatility Imputation,"Missing data is a common problem in finance and often requires methods to
fill in the gaps, or in other words, imputation. In this work, we focused on
the imputation of missing implied volatilities for FX options. Prior work has
used variational autoencoders (VAEs), a neural network-based approach, to solve
this problem; however, using stronger classical baselines such as Heston with
jumps can significantly outperform their results. We show that simple
modifications to the architecture of the VAE lead to significant imputation
performance improvements (e.g., in low missingness regimes, nearly cutting the
error by half), removing the necessity of using $\beta$-VAEs. Further, we
modify the VAE imputation algorithm in order to better handle the uncertainty
in data, as well as to obtain accurate uncertainty estimates around imputed
values.",stat.ML
Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits,"By leveraging the representation power of deep neural networks, neural upper
confidence bound (UCB) algorithms have shown success in contextual bandits. To
further balance the exploration and exploitation, we propose
Neural-$\sigma^2$-LinearUCB, a variance-aware algorithm that utilizes
$\sigma^2_t$, i.e., an upper bound of the reward noise variance at round $t$,
to enhance the uncertainty quantification quality of the UCB, resulting in a
regret performance improvement. We provide an oracle version for our algorithm
characterized by an oracle variance upper bound $\sigma^2_t$ and a practical
version with a novel estimation for this variance bound. Theoretically, we
provide rigorous regret analysis for both versions and prove that our oracle
algorithm achieves a better regret guarantee than other neural-UCB algorithms
in the neural contextual bandits setting. Empirically, our practical method
enjoys a similar computational efficiency, while outperforming state-of-the-art
techniques by having a better calibration and lower regret across multiple
standard settings, including on the synthetic, UCI, MNIST, and CIFAR-10
datasets.",stat.ML
DNAMite: Interpretable Calibrated Survival Analysis with Discretized Additive Models,"Survival analysis is a classic problem in statistics with important
applications in healthcare. Most machine learning models for survival analysis
are black-box models, limiting their use in healthcare settings where
interpretability is paramount. More recently, glass-box machine learning models
have been introduced for survival analysis, with both strong predictive
performance and interpretability. Still, several gaps remain, as no prior
glass-box survival model can produce calibrated shape functions with enough
flexibility to capture the complex patterns often found in real data. To fill
this gap, we introduce a new glass-box machine learning model for survival
analysis called DNAMite. DNAMite uses feature discretization and kernel
smoothing in its embedding module, making it possible to learn shape functions
with a flexible balance of smoothness and jaggedness. Further, DNAMite produces
calibrated shape functions that can be directly interpreted as contributions to
the cumulative incidence function. Our experiments show that DNAMite generates
shape functions closer to true shape functions on synthetic data, while making
predictions with comparable predictive performance and better calibration than
previous glass-box and black-box models.",stat.ML
On Differentially Private String Distances,"Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.",stat.ML
Aioli: A Unified Optimization Framework for Language Model Data Mixing,"Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.",stat.ML
Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data,"Representing and exploiting multivariate signals require capturing complex
relations between variables. We define a novel Graph-Dictionary signal model,
where a finite set of graphs characterizes relationships in data distribution
through a weighted sum of their Laplacians. We propose a framework to infer the
graph dictionary representation from observed data, along with a bilinear
generalization of the primal-dual splitting algorithm to solve the learning
problem. Our new formulation allows to include a priori knowledge on signal
properties, as well as on underlying graphs and their coefficients. We show the
capability of our method to reconstruct graphs from signals in multiple
synthetic settings, where our model outperforms previous baselines. Then, we
exploit graph-dictionary representations in a motor imagery decoding task on
brain activity data, where we classify imagined motion better than standard
methods relying on many more features.",stat.ML
Multi-armed Bandits with Missing Outcome,"While significant progress has been made in designing algorithms that
minimize regret in online decision-making, real-world scenarios often introduce
additional complexities, perhaps the most challenging of which is missing
outcomes. Overlooking this aspect or simply assuming random missingness
invariably leads to biased estimates of the rewards and may result in linear
regret. Despite the practical relevance of this challenge, no rigorous
methodology currently exists for systematically handling missingness,
especially when the missingness mechanism is not random. In this paper, we
address this gap in the context of multi-armed bandits (MAB) with missing
outcomes by analyzing the impact of different missingness mechanisms on
achievable regret bounds. We introduce algorithms that account for missingness
under both missing at random (MAR) and missing not at random (MNAR) models.
Through both analytical and simulation studies, we demonstrate the drastic
improvements in decision-making by accounting for missingness in these
settings.",stat.ML
Cross-validating causal discovery via Leave-One-Variable-Out,"We propose a new approach to falsify causal discovery algorithms without
ground truth, which is based on testing the causal model on a pair of variables
that has been dropped when learning the causal model. To this end, we use the
""Leave-One-Variable-Out (LOVO)"" prediction where $Y$ is inferred from $X$
without any joint observations of $X$ and $Y$, given only training data from
$X,Z_1,\dots,Z_k$ and from $Z_1,\dots,Z_k,Y$. We demonstrate that causal models
on the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often
entail conclusions on the dependencies between $X$ and $Y$, enabling this type
of prediction. The prediction error can then be estimated since the joint
distribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only
been omitted for the purpose of falsification. After presenting this graphical
method, which is applicable to general causal discovery algorithms, we
illustrate how to construct a LOVO predictor tailored towards algorithms
relying on specific a priori assumptions, such as linear additive noise models.
Simulations indicate that the LOVO prediction error is indeed correlated with
the accuracy of the causal outputs, affirming the method's effectiveness.",stat.ML
Network EM Algorithm for Gaussian Mixture Model in Decentralized Federated Learning,"We systematically study various network Expectation-Maximization (EM)
algorithms for the Gaussian mixture model within the framework of decentralized
federated learning. Our theoretical investigation reveals that directly
extending the classical decentralized supervised learning method to the EM
algorithm exhibits poor estimation accuracy with heterogeneous data across
clients and struggles to converge numerically when Gaussian components are
poorly-separated. To address these issues, we propose two novel solutions.
First, to handle heterogeneous data, we introduce a momentum network EM (MNEM)
algorithm, which uses a momentum parameter to combine information from both the
current and historical estimators. Second, to tackle the challenge of
poorly-separated Gaussian components, we develop a semi-supervised MNEM
(semi-MNEM) algorithm, which leverages partially labeled data. Rigorous
theoretical analysis demonstrates that MNEM can achieve statistical efficiency
comparable to that of the whole sample estimator when the mixture components
satisfy certain separation conditions, even in heterogeneous scenarios.
Moreover, the semi-MNEM estimator enhances the convergence speed of the MNEM
algorithm, effectively addressing the numerical convergence challenges in
poorly-separated scenarios. Extensive simulation and real data analyses are
conducted to justify our theoretical findings.",stat.ML
The sampling complexity of learning invertible residual neural networks,"In recent work it has been shown that determining a feedforward ReLU neural
network to within high uniform accuracy from point samples suffers from the
curse of dimensionality in terms of the number of samples needed. As a
consequence, feedforward ReLU neural networks are of limited use for
applications where guaranteed high uniform accuracy is required.
  We consider the question of whether the sampling complexity can be improved
by restricting the specific neural network architecture. To this end, we
investigate invertible residual neural networks which are foundational
architectures in deep learning and are widely employed in models that power
modern generative methods. Our main result shows that the residual neural
network architecture and invertibility do not help overcome the complexity
barriers encountered with simpler feedforward architectures. Specifically, we
demonstrate that the computational complexity of approximating invertible
residual neural networks from point samples in the uniform norm suffers from
the curse of dimensionality. Similar results are established for invertible
convolutional Residual neural networks.",stat.ML
ClusterGraph: a new tool for visualization and compression of multidimensional data,"Understanding the global organization of complicated and high dimensional
data is of primary interest for many branches of applied sciences. It is
typically achieved by applying dimensionality reduction techniques mapping the
considered data into lower dimensional space. This family of methods, while
preserving local structures and features, often misses the global structure of
the dataset. Clustering techniques are another class of methods operating on
the data in the ambient space. They group together points that are similar
according to a fixed similarity criteria, however unlike dimensionality
reduction techniques, they do not provide information about the global
organization of the data. Leveraging ideas from Topological Data Analysis, in
this paper we provide an additional layer on the output of any clustering
algorithm. Such data structure, ClusterGraph, provides information about the
global layout of clusters, obtained from the considered clustering algorithm.
Appropriate measures are provided to assess the quality and usefulness of the
obtained representation. Subsequently the ClusterGraph, possibly with an
appropriate structure--preserving simplification, can be visualized and used in
synergy with state of the art exploratory data analysis techniques.",stat.ML
Post-Hoc Robustness Enhancement in Graph Neural Networks with Conditional Random Fields,"Graph Neural Networks (GNNs), which are nowadays the benchmark approach in
graph representation learning, have been shown to be vulnerable to adversarial
attacks, raising concerns about their real-world applicability. While existing
defense techniques primarily concentrate on the training phase of GNNs,
involving adjustments to message passing architectures or pre-processing
methods, there is a noticeable gap in methods focusing on increasing robustness
during inference. In this context, this study introduces RobustCRF, a post-hoc
approach aiming to enhance the robustness of GNNs at the inference stage. Our
proposed method, founded on statistical relational learning using a Conditional
Random Field, is model-agnostic and does not require prior knowledge about the
underlying model architecture. We validate the efficacy of this approach across
various models, leveraging benchmark node classification datasets.",stat.ML
Discovering Latent Structural Causal Models from Spatio-Temporal Data,"Many important phenomena in scientific fields such as climate, neuroscience,
and epidemiology are naturally represented as spatiotemporal gridded data with
complex interactions. For example, in climate science, researchers aim to
uncover how large-scale events, such as the North Atlantic Oscillation (NAO)
and the Antarctic Oscillation (AAO), influence other global processes.
Inferring causal relationships from these data is a challenging problem
compounded by the high dimensionality of such data and the correlations between
spatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),
a novel framework based on variational inference, designed to explicitly model
latent time-series and their causal relationships from spatially confined modes
in the data. Our method uses an end-to-end training process that maximizes an
evidence-lower bound (ELBO) for the data likelihood. Theoretically, we show
that, under some conditions, the latent variables are identifiable up to
transformation by an invertible matrix. Empirically, we show that SPACY
outperforms state-of-the-art baselines on synthetic data, remains scalable for
large grids, and identifies key known phenomena from real-world climate data.",stat.ML
Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass,"Large language models (LMs) are typically adapted to improve performance on
new contexts (\eg text prompts that define new tasks or domains) through
fine-tuning or prompting. However, there is an accuracy compute tradeoff --
fine-tuning incurs significant training cost and prompting increases inference
overhead. We introduce $GenerativeAdapter$, an effective and efficient
adaptation method that directly maps new contexts to low-rank LM adapters,
thereby significantly reducing inference overhead with no need for finetuning.
The adapter generator is trained via self-supervised learning, and can be used
to adapt a single frozen LM for any new task simply by mapping the associated
task or domain context to a new adapter. We apply $GenerativeAdapter$ to two
pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the
adapted models in three adaption scenarios: knowledge acquisition from
documents, learning from demonstrations, and personalization for users. In
StreamingQA, our approach is effective in injecting knowledge into the LM's
parameters, achieving a 63.5% improvement in F1 score over the model with
supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K
tokens. In the MetaICL in-context learning evaluation, our method achieves an
average accuracy of $44.9$ across 26 tasks, outperforming the base model. On
MSC, our method proves to be highly competitive in memorizing user information
from conversations with a 4x reduction in computation and memory costs compared
to prompting with full conversation history. Together, these results suggest
that $GenerativeAdapter$ should allow for general adaption to a wide range of
different contexts.",stat.ML
Generating Highly Designable Proteins with Geometric Algebra Flow Matching,"We introduce a generative model for protein backbone design utilizing
geometric products and higher order message passing. In particular, we propose
Clifford Frame Attention (CFA), an extension of the invariant point attention
(IPA) architecture from AlphaFold2, in which the backbone residue frames and
geometric features are represented in the projective geometric algebra. This
enables to construct geometrically expressive messages between residues,
including higher order terms, using the bilinear operations of the algebra. We
evaluate our architecture by incorporating it into the framework of FrameFlow,
a state-of-the-art flow matching model for protein backbone generation. The
proposed model achieves high designability, diversity and novelty, while also
sampling protein backbones that follow the statistical distribution of
secondary structure elements found in naturally occurring proteins, a property
so far only insufficiently achieved by many state-of-the-art generative models.",stat.ML
Pruning the Path to Optimal Care: Identifying Systematically Suboptimal Medical Decision-Making with Inverse Reinforcement Learning,"In aims to uncover insights into medical decision-making embedded within
observational data from clinical settings, we present a novel application of
Inverse Reinforcement Learning (IRL) that identifies suboptimal clinician
actions based on the actions of their peers. This approach centers two stages
of IRL with an intermediate step to prune trajectories displaying behavior that
deviates significantly from the consensus. This enables us to effectively
identify clinical priorities and values from ICU data containing both optimal
and suboptimal clinician decisions. We observe that the benefits of removing
suboptimal actions vary by disease and differentially impact certain
demographic groups.",stat.ML
Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry,"In this work, we conduct a systematic study of stochastic saddle point
problems (SSP) and stochastic variational inequalities (SVI) under the
constraint of $(\epsilon,\delta)$-differential privacy (DP) in both Euclidean
and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in
the $\ell_p/\ell_q$ setup, $p,q\in[1,2]$. Here, we obtain a bound of
$\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the
strong SP-gap, where $n$ is the number of samples and $d$ is the dimension.
This rate is nearly optimal for any $p,q\in[1,2]$. Without additional
assumptions, such as smoothness or linearity requirements, prior work under DP
has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup).
Further, existing algorithms have each only been shown to work for specific
settings of $p$ and $q$ and under certain assumptions on the loss and the
feasible set, whereas we provide a general algorithm for DP SSPs whenever
$p,q\in[1,2]$. Our result is obtained via a novel analysis of the recursive
regularization algorithm. In particular, we develop new tools for analyzing
generalization, which may be of independent interest. Next, we turn our
attention towards SVIs with a monotone, bounded and Lipschitz operator and
consider $\ell_p$-setups, $p\in[1,2]$. Here, we provide the first analysis
which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}}
+ \frac{\sqrt{d}}{n\epsilon}\big)$. For $p-1=\Omega(1)$, this rate is near
optimal due to existing lower bounds. To obtain this result, we develop a
modified version of recursive regularization. Our analysis builds on the
techniques we develop for SSPs as well as employing additional novel components
which handle difficulties arising from adapting the recursive regularization
framework to SVIs.",stat.ML
Inverse Transition Learning: Learning Dynamics from Demonstrations,"We consider the problem of estimating the transition dynamics $T^*$ from
near-optimal expert trajectories in the context of offline model-based
reinforcement learning. We develop a novel constraint-based method, Inverse
Transition Learning, that treats the limited coverage of the expert
trajectories as a \emph{feature}: we use the fact that the expert is
near-optimal to inform our estimate of $T^*$. We integrate our constraints into
a Bayesian approach. Across both synthetic environments and real healthcare
scenarios like Intensive Care Unit (ICU) patient management in hypotension, we
demonstrate not only significant improvements in decision-making, but that our
posterior can inform when transfer will be successful.",stat.ML
Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data,"The Gaussian process (GP) is a widely used probabilistic machine learning
method for stochastic function approximation, stochastic modeling, and
analyzing real-world measurements of nonlinear processes. Unlike many other
machine learning methods, GPs include an implicit characterization of
uncertainty, making them extremely useful across many areas of science,
technology, and engineering. Traditional implementations of GPs involve
stationary kernels (also termed covariance functions) that limit their
flexibility and exact methods for inference that prevent application to data
sets with more than about ten thousand points. Modern approaches to address
stationarity assumptions generally fail to accommodate large data sets, while
all attempts to address scalability focus on approximating the Gaussian
likelihood, which can involve subjectivity and lead to inaccuracies. In this
work, we explicitly derive an alternative kernel that can discover and encode
both sparsity and nonstationarity. We embed the kernel within a fully Bayesian
GP model and leverage high-performance computing resources to enable the
analysis of massive data sets. We demonstrate the favorable performance of our
novel kernel relative to existing exact and approximate GP methods across a
variety of synthetic data examples. Furthermore, we conduct space-time
prediction based on more than one million measurements of daily maximum
temperature and verify that our results outperform state-of-the-art methods in
the Earth sciences. More broadly, having access to exact GPs that use
ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods
to truly compete with a wide variety of machine learning methods.",stat.ML
Pareto Set Identification With Posterior Sampling,"The problem of identifying the best answer among a collection of items having
real-valued distribution is well-understood.
  Despite its practical relevance for many applications, fewer works have
studied its extension when multiple and potentially conflicting metrics are
available to assess an item's quality.
  Pareto set identification (PSI) aims to identify the set of answers whose
means are not uniformly worse than another.
  This paper studies PSI in the transductive linear setting with potentially
correlated objectives.
  Building on posterior sampling in both the stopping and the sampling rules,
we propose the PSIPS algorithm that deals simultaneously with structure and
correlation without paying the computational cost of existing oracle-based
algorithms.
  Both from a frequentist and Bayesian perspective, PSIPS is asymptotically
optimal.
  We demonstrate its good empirical performance in real-world and synthetic
instances.",stat.ML
Conformalized Credal Regions for Classification with Ambiguous Ground Truth,"An open question in \emph{Imprecise Probabilistic Machine Learning} is how to
empirically derive a credal region (i.e., a closed and convex family of
probabilities on the output space) from the available data, without any prior
knowledge or assumption. In classification problems, credal regions are a tool
that is able to provide provable guarantees under realistic assumptions by
characterizing the uncertainty about the distribution of the labels. Building
on previous work, we show that credal regions can be directly constructed using
conformal methods. This allows us to provide a novel extension of classical
conformal prediction to problems with ambiguous ground truth, that is, when the
exact labels for given inputs are not exactly known. The resulting construction
enjoys desirable practical and theoretical properties: (i) conformal coverage
guarantees, (ii) smaller prediction sets (compared to classical conformal
prediction regions) and (iii) disentanglement of uncertainty sources
(epistemic, aleatoric). We empirically verify our findings on both synthetic
and real datasets.",stat.ML
Learning dynamical systems from data: Gradient-based dictionary optimization,"The Koopman operator plays a crucial role in analyzing the global behavior of
dynamical systems. Existing data-driven methods for approximating the Koopman
operator or discovering the governing equations of the underlying system
typically require a fixed set of basis functions, also called dictionary. The
optimal choice of basis functions is highly problem-dependent and often
requires domain knowledge. We present a novel gradient descent-based
optimization framework for learning suitable and interpretable basis functions
from data and show how it can be used in combination with EDMD, SINDy, and
PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of
various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's
circuit, a nonlinear heat equation, as well as protein-folding data.",stat.ML
Conjugate gradient methods for high-dimensional GLMMs,"Generalized linear mixed models (GLMMs) are a widely used tool in statistical
analysis. The main bottleneck of many computational approaches lies in the
inversion of the high dimensional precision matrices associated with the random
effects. Such matrices are typically sparse; however, the sparsity pattern
resembles a multi partite random graph, which does not lend itself well to
default sparse linear algebra techniques. Notably, we show that, for typical
GLMMs, the Cholesky factor is dense even when the original precision is sparse.
We thus turn to approximate iterative techniques, in particular to the
conjugate gradient (CG) method. We combine a detailed analysis of the spectrum
of said precision matrices with results from random graph theory to show that
CG-based methods applied to high-dimensional GLMMs typically achieve a fixed
approximation error with a total cost that scales linearly with the number of
parameters and observations. Numerical illustrations with both real and
simulated data confirm the theoretical findings, while at the same time
illustrating situations, such as nested structures, where CG-based methods
struggle.",stat.ML
Centrality Graph Shift Operators for Graph Neural Networks,"Graph Shift Operators (GSOs), such as the adjacency and graph Laplacian
matrices, play a fundamental role in graph theory and graph representation
learning. Traditional GSOs are typically constructed by normalizing the
adjacency matrix by the degree matrix, a local centrality metric. In this work,
we instead propose and study Centrality GSOs (CGSOs), which normalize adjacency
matrices by global centrality metrics such as the PageRank, $k$-core or count
of fixed length walks. We study spectral properties of the CGSOs, allowing us
to get an understanding of their action on graph signals. We confirm this
understanding by defining and running the spectral clustering algorithm based
on different CGSOs on several synthetic and real-world datasets. We furthermore
outline how our CGSO can act as the message passing operator in any Graph
Neural Network and in particular demonstrate strong performance of a variant of
the Graph Convolutional Network and Graph Attention Network using our CGSOs on
several real-world benchmark datasets.",stat.ML
Sharp Analysis for KL-Regularized Contextual Bandits and RLHF,"Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant
technique used to enhance policy optimization in reinforcement learning (RL)
and reinforcement learning from human feedback (RLHF), which forces the learned
policy to stay close to a reference policy. While the effectiveness and
necessity of KL-regularization have been empirically demonstrated in various
practical scenarios, current theoretical analysis of KL-regularized RLHF still
obtains the same $\mathcal{O}(1 / \epsilon^2)$ sample complexity as problems
without KL-regularization. To understand the fundamental distinction between
policy learning objectives with KL-regularization and ones without
KL-regularization, we are the first to theoretically demonstrate the power of
KL-regularization by providing a sharp analysis for KL-regularized contextual
bandits and RLHF, revealing an $\mathcal{O}(1 / \epsilon)$ sample complexity
when $\epsilon$ is sufficiently small.
  We further explore the role of data coverage in contextual bandits and RLHF.
While the coverage assumption is commonly employed in offline RLHF to link the
samples from the reference policy to the optimal policy, often at the cost of a
multiplicative dependence on the coverage coefficient, its impact on the sample
complexity of online RLHF remains unclear. Previous theoretical analyses of
online RLHF typically require explicit exploration and additional structural
assumptions on the reward function class. In contrast, we show that with
sufficient coverage from the reference policy, a simple two-stage mixed
sampling strategy can achieve a sample complexity with only an additive
dependence on the coverage coefficient. Our results provide a comprehensive
understanding of the roles of KL-regularization and data coverage in RLHF,
shedding light on the design of more efficient RLHF algorithms.",stat.ML
Measure-to-measure interpolation using Transformers,"Transformers are deep neural network architectures that underpin the recent
successes of large language models. Unlike more classical architectures that
can be viewed as point-to-point maps, a Transformer acts as a
measure-to-measure map implemented as specific interacting particle system on
the unit sphere: the input is the empirical measure of tokens in a prompt and
its evolution is governed by the continuity equation. In fact, Transformers are
not limited to empirical measures and can in principle process any input
measure. As the nature of data processed by Transformers is expanding rapidly,
it is important to investigate their expressive power as maps from an arbitrary
measure to another arbitrary measure. To that end, we provide an explicit
choice of parameters that allows a single Transformer to match $N$ arbitrary
input measures to $N$ arbitrary target measures, under the minimal assumption
that every pair of input-target measures can be matched by some transport map.",stat.ML
Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity,"The wider application of end-to-end learning methods to embodied
decision-making domains remains bottlenecked by their reliance on a
superabundance of training data representative of the target domain.
Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot
generalization--the goal of standard reinforcement learning (RL)--in favor of
few-shot adaptation, and thus hold promise for bridging larger generalization
gaps. While learning this meta-level adaptive behavior still requires
substantial data, efficient environment simulators approaching real-world
complexity are growing in prevalence. Even so, hand-designing sufficiently
diverse and numerous simulated training tasks for these complex domains is
prohibitively labor-intensive. Domain randomization (DR) and procedural
generation (PG), offered as solutions to this problem, require simulators to
possess carefully-defined parameters which directly translate to meaningful
task diversity--a similarly prohibitive assumption. In this work, we present
DIVA, an evolutionary approach for generating diverse training tasks in such
complex, open-ended simulators. Like unsupervised environment design (UED)
methods, DIVA can be applied to arbitrary parameterizations, but can
additionally incorporate realistically-available domain knowledge--thus
inheriting the flexibility and generality of UED, and the supervised structure
embedded in well-designed simulators exploited by DR and PG. Our empirical
results showcase DIVA's unique ability to overcome complex parameterizations
and successfully train adaptive agent behavior, far outperforming competitive
baselines from prior literature. These findings highlight the potential of such
semi-supervised environment design (SSED) approaches, of which DIVA is the
first humble constituent, to enable training in realistic simulated domains,
and produce more robust and capable adaptive agents.",stat.ML
Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.",stat.ML
Statistical-Computational Trade-offs for Greedy Recursive Partitioning Estimators,"Models based on recursive partitioning such as decision trees and their
ensembles are popular for high-dimensional regression as they can potentially
avoid the curse of dimensionality. Because empirical risk minimization (ERM) is
computationally infeasible, these models are typically trained using greedy
algorithms. Although effective in many cases, these algorithms have been
empirically observed to get stuck at local optima. We explore this phenomenon
in the context of learning sparse regression functions over $d$ binary
features, showing that when the true regression function $f^*$ does not satisfy
the so-called Merged Staircase Property (MSP), greedy training requires
$\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does
satisfy MSP, greedy training can attain small estimation error with only
$O(\log d)$ samples. This performance mirrors that of two-layer neural networks
trained with stochastic gradient descent (SGD) in the mean-field regime,
thereby establishing a head-to-head comparison between SGD-trained neural
networks and greedy recursive partitioning estimators. Furthermore, ERM-trained
recursive partitioning estimators achieve low estimation error with $O(\log d)$
samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a
statistical-computational trade-off for greedy training. Our proofs are based
on a novel interpretation of greedy recursive partitioning using stochastic
process theory and a coupling technique that may be of independent interest.",stat.ML
Deep Heuristic Learning for Real-Time Urban Pathfinding,"This paper introduces a novel approach to urban pathfinding by transforming
traditional heuristic-based algorithms into deep learning models that leverage
real-time contextual data, such as traffic and weather conditions. We propose
two methods: an enhanced A* algorithm that dynamically adjusts routes based on
current environmental conditions, and a neural network model that predicts the
next optimal path segment using historical and live data. An extensive
benchmark was conducted to compare the performance of different deep learning
models, including MLP, GRU, LSTM, Autoencoders, and Transformers. Both methods
were evaluated in a simulated urban environment in Berlin, with the neural
network model outperforming traditional methods, reducing travel times by up to
40%, while the enhanced A* algorithm achieved a 34% improvement. These results
demonstrate the potential of deep learning to optimize urban navigation in real
time, providing more adaptable and efficient routing solutions.",stat.ML
A Fundamental Accuracy--Robustness Trade-off in Regression and Classification,"We derive a fundamental trade-off between standard and adversarial risk in a
rather general situation that formalizes the following simple intuition: ""If no
(nearly) optimal predictor is smooth, adversarial robustness comes at the cost
of accuracy."" As a concrete example, we evaluate the derived trade-off in
regression with polynomial ridge functions under mild regularity conditions.",stat.ML
Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.",stat.ML
$\spadesuit$ SPADE $\spadesuit$ Split Peak Attention DEcomposition,"Demand forecasting faces challenges induced by Peak Events (PEs)
corresponding to special periods such as promotions and holidays. Peak events
create significant spikes in demand followed by demand ramp down periods.
Neural networks like MQCNN and MQT overreact to demand peaks by carrying over
the elevated PE demand into subsequent Post-Peak-Event (PPE) periods, resulting
in significantly over-biased forecasts. To tackle this challenge, we introduce
a neural forecasting model called Split Peak Attention DEcomposition, SPADE.
This model reduces the impact of PEs on subsequent forecasts by modeling
forecasting as consisting of two separate tasks: one for PEs; and the other for
the rest. Its architecture then uses masked convolution filters and a
specialized Peak Attention module. We show SPADE's performance on a worldwide
retail dataset with hundreds of millions of products. Our results reveal a
reduction in PPE degradation by 4.5% and an improvement in PE accuracy by 3.9%,
relative to current production models.",stat.ML
Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking,"We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.",stat.ML
Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems,"In this paper, we propose a novel model called Recurrent Explicit Duration
Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent
explicit duration variables into the rSLDS model. We also propose an inference
and learning scheme that involves the use of P\'olya-gamma augmentation. We
demonstrate the improved segmentation capabilities of our model on three
benchmark datasets, including two quantitative datasets and one qualitative
dataset.",stat.ML
The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model,"The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural
Bayesian nonparametric extension of the classical Hidden Markov Model for
learning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to
strengthen the self-persistence probability in the HDP-HMM. Then, disentangled
sticky HDP-HMM has been proposed to disentangle the strength of the
self-persistence prior and transition prior. However, the sticky HDP-HMM
assumes that the self-persistence probability is stationary, limiting its
expressiveness. Here, we build on previous work on sticky HDP-HMM and
disentangled sticky HDP-HMM, developing a more general model: the recurrent
sticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for
efficient inference in this model. We show that RS-HDP-HMM outperforms
disentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and
real data segmentation.",stat.ML
Graph neural networks and non-commuting operators,"Graph neural networks (GNNs) provide state-of-the-art results in a wide
variety of tasks which typically involve predicting features at the vertices of
a graph. They are built from layers of graph convolutions which serve as a
powerful inductive bias for describing the flow of information among the
vertices. Often, more than one data modality is available. This work considers
a setting in which several graphs have the same vertex set and a common
vertex-level learning task. This generalizes standard GNN models to GNNs with
several graph operators that do not commute. We may call this model graph-tuple
neural networks (GtNN).
  In this work, we develop the mathematical theory to address the stability and
transferability of GtNNs using properties of non-commuting non-expansive
operators. We develop a limit theory of graphon-tuple neural networks and use
it to prove a universal transferability theorem that guarantees that all
graph-tuple neural networks are transferable on convergent graph-tuple
sequences. In particular, there is no non-transferable energy under the
convergence we consider here. Our theoretical results extend well-known
transferability theorems for GNNs to the case of several simultaneous graphs
(GtNNs) and provide a strict improvement on what is currently known even in the
GNN case.
  We illustrate our theoretical results with simple experiments on synthetic
and real-world data. To this end, we derive a training procedure that provably
enforces the stability of the resulting model.",stat.ML
ION-C: Integration of Overlapping Networks via Constraints,"In many causal learning problems, variables of interest are often not all
measured over the same observations, but are instead distributed across
multiple datasets with overlapping variables. Tillman et al. (2008) presented
the first algorithm for enumerating the minimal equivalence class of
ground-truth DAGs consistent with all input graphs by exploiting local
independence relations, called ION. In this paper, this problem is formulated
as a more computationally efficient answer set programming (ASP) problem, which
we call ION-C, and solved with the ASP system clingo. The ION-C algorithm was
run on random synthetic graphs with varying sizes, densities, and degrees of
overlap between subgraphs, with overlap having the largest impact on runtime,
number of solution graphs, and agreement within the output set. To validate
ION-C on real-world data, we ran the algorithm on overlapping graphs learned
from data from two successive iterations of the European Social Survey (ESS),
using a procedure for conducting joint independence tests to prevent
inconsistencies in the input.",stat.ML
Debiasing Synthetic Data Generated by Deep Generative Models,"While synthetic data hold great promise for privacy protection, their
statistical analysis poses significant challenges that necessitate innovative
solutions. The use of deep generative models (DGMs) for synthetic data
generation is known to induce considerable bias and imprecision into synthetic
data analyses, compromising their inferential utility as opposed to original
data analyses. This bias and uncertainty can be substantial enough to impede
statistical convergence rates, even in seemingly straightforward analyses like
mean calculation. The standard errors of such estimators then exhibit slower
shrinkage with sample size than the typical 1 over root-$n$ rate. This
complicates fundamental calculations like p-values and confidence intervals,
with no straightforward remedy currently available. In response to these
challenges, we propose a new strategy that targets synthetic data created by
DGMs for specific data analyses. Drawing insights from debiased and targeted
machine learning, our approach accounts for biases, enhances convergence rates,
and facilitates the calculation of estimators with easily approximated large
sample variances. We exemplify our proposal through a simulation study on toy
data and two case studies on real-world data, highlighting the importance of
tailoring DGMs for targeted data analysis. This debiasing strategy contributes
to advancing the reliability and applicability of synthetic data in statistical
inference.",stat.ML
Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains,"In this work, we consider the approximation capabilities of shallow neural
networks in weighted Sobolev spaces for functions in the spectral Barron space.
The existing literature already covers several cases, in which the spectral
Barron space can be approximated well, i.e., without curse of dimensionality,
by shallow networks and several different classes of activation function. The
limitations of the existing results are mostly on the error measures that were
considered, in which the results are restricted to Sobolev spaces over a
bounded domain. We will here treat two cases that extend upon the existing
results. Namely, we treat the case with bounded domain and Muckenhoupt weights
and the case, where the domain is allowed to be unbounded and the weights are
required to decay. We first present embedding results for the more general
weighted Fourier-Lebesgue spaces in the weighted Sobolev spaces and then we
establish asymptotic approximation rates for shallow neural networks that come
without curse of dimensionality.",stat.ML
Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? Review and Intercomparison of Existing Models,"Deep Learning (DL) has shown promise for downscaling global climate change
projections under different approaches, including Perfect Prognosis (PP) and
Regional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models
are trained on observational data, so it remains an open question whether they
can plausibly extrapolate unseen conditions and changes in future emissions
scenarios. Here we focus on this problem as the main drawback for the
operationalization of these methods and present the results of 1) a literature
review to identify state-of-the-art DL models for PP downscaling and 2) an
intercomparison experiment to evaluate the performance of these models and to
assess their extrapolation capability using a common experimental framework,
taking into account the sensitivity of results to different training replicas.
We focus on minimum and maximum temperatures and precipitation over Spain, a
region with a range of climatic conditions with different influential regional
processes. We conclude with a discussion of the findings, limitations of
existing methods, and prospects for future development.",stat.ML
Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits,"Causal knowledge about the relationships among decision variables and a
reward variable in a bandit setting can accelerate the learning of an optimal
decision. Current works often assume the causal graph is known, which may not
always be available a priori. Motivated by this challenge, we focus on the
causal bandit problem in scenarios where the underlying causal graph is unknown
and may include latent confounders. While intervention on the parents of the
reward node is optimal in the absence of latent confounders, this is not
necessarily the case in general. Instead, one must consider a set of possibly
optimal arms/interventions, each being a special subset of the ancestors of the
reward node, making causal discovery beyond the parents of the reward node
essential. For regret minimization, we identify that discovering the full
causal structure is unnecessary; however, no existing work provides the
necessary and sufficient components of the causal graph. We formally
characterize the set of necessary and sufficient latent confounders one needs
to detect or learn to ensure that all possibly optimal arms are identified
correctly. We also propose a randomized algorithm for learning the causal graph
with a limited number of samples, providing a sample complexity guarantee for
any desired confidence level. In the causal bandit setup, we propose a
two-stage approach. In the first stage, we learn the induced subgraph on
ancestors of the reward, along with a necessary and sufficient subset of latent
confounders, to construct the set of possibly optimal arms. The regret incurred
during this phase scales polynomially with respect to the number of nodes in
the causal graph. The second phase involves the application of a standard
bandit algorithm, such as the UCB algorithm. We also establish a regret bound
for our two-phase approach, which is sublinear in the number of rounds.",stat.ML
GUIDE-VAE: Advancing Data Generation with User Information and Pattern Dictionaries,"Generative modelling of multi-user datasets has become prominent in science
and engineering. Generating a data point for a given user requires employing
user information, and conventional generative models, including variational
autoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, a
novel conditional generative model that leverages user embeddings to generate
user-guided data. By allowing the model to benefit from shared patterns across
users, GUIDE-VAE enhances performance in multi-user settings, even under
significant data imbalance. In addition to integrating user information,
GUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)
to improve the realism of generated samples by capturing complex feature
dependencies. While user embeddings drive performance gains, PDCC addresses
common issues such as noise and over-smoothing typically seen in VAEs.
  The proposed GUIDE-VAE was evaluated on a multi-user smart meter dataset
characterized by substantial data imbalance across users. Quantitative results
show that GUIDE-VAE performs effectively in both synthetic data generation and
missing record imputation tasks, while qualitative evaluations reveal that
GUIDE-VAE produces more plausible and less noisy data. These results establish
GUIDE-VAE as a promising tool for controlled, realistic data generation in
multi-user datasets, with potential applications across various domains
requiring user-informed modelling.",stat.ML
Improved Regret of Linear Ensemble Sampling,"In this work, we close the fundamental gap of theory and practice by
providing an improved regret bound for linear ensemble sampling. We prove that
with an ensemble size logarithmic in $T$, linear ensemble sampling can achieve
a frequentist regret bound of $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$, matching
state-of-the-art results for randomized linear bandit algorithms, where $d$ and
$T$ are the dimension of the parameter and the time horizon respectively. Our
approach introduces a general regret analysis framework for linear bandit
algorithms. Additionally, we reveal a significant relationship between linear
ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing
that LinPHE is a special case of linear ensemble sampling when the ensemble
size equals $T$. This insight allows us to derive a new regret bound of
$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ for LinPHE, independent of the number of
arms. Our contributions advance the theoretical foundation of ensemble
sampling, bringing its regret bounds in line with the best known bounds for
other randomized exploration algorithms.",stat.ML
Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data,"Online Reinforcement learning (RL) typically requires high-stakes online
interaction data to learn a policy for a target task. This prompts interest in
leveraging historical data to improve sample efficiency. The historical data
may come from outdated or related source environments with different dynamics.
It remains unclear how to effectively use such data in the target task to
provably enhance learning and sample efficiency. To address this, we propose a
hybrid transfer RL (HTRL) setting, where an agent learns in a target
environment while accessing offline data from a source environment with shifted
dynamics. We show that -- without information on the dynamics shift -- general
shifted-dynamics data, even with subtle shifts, does not reduce sample
complexity in the target environment. However, with prior information on the
degree of the dynamics shift, we design HySRL, a transfer algorithm that
achieves problem-dependent sample complexity and outperforms pure online RL.
Finally, our experimental results demonstrate that HySRL surpasses
state-of-the-art online RL baseline.",stat.ML
Multilingual hierarchical classification of job advertisements for job vacancy statistics,"The goal of this paper is to develop a multilingual classifier and
conditional probability estimator of occupation codes for online job
advertisements according in accordance with the International Standard
Classification of Occupations (ISCO) extended with the Polish Classification of
Occupations and Specializations (KZiS), which is analogous to the European
Classification of Occupations. In this paper, we utilise a range of data
sources, including a novel one, namely the Central Job Offers Database, which
is a register of all vacancies submitted to Public Employment Offices. Their
staff members code the vacancies according to the ISCO and KZiS. A hierarchical
multi-class classifier has been developed based on the transformer
architecture. The classifier begins by encoding the jobs found in
advertisements to the widest 1-digit occupational group, and then narrows the
assignment to a 6-digit occupation code. We show that incorporation of the
hierarchical structure of occupations improves prediction accuracy by 1-2
percentage points, particularly for the hand-coded online job advertisements.
Finally, a bilingual (Polish and English) and multilingual (24 languages) model
is developed based on data translated using closed and open-source software.
The open-source software is provided for the benefit of the official statistics
community, with a particular focus on international comparability.",stat.ML
Fully Automated Correlated Time Series Forecasting in Minutes,"Societal and industrial infrastructures and systems increasingly leverage
sensors that emit correlated time series. Forecasting of future values of such
time series based on recorded historical values has important benefits.
Automatically designed models achieve higher accuracy than manually designed
models. Given a forecasting task, which includes a dataset and a forecasting
horizon, automated design methods automatically search for an optimal
forecasting model for the task in a manually designed search space, and then
train the identified model using the dataset to enable the forecasting.
Existing automated methods face three challenges. First, the search space is
constructed by human experts, rending the methods only semi-automated and
yielding search spaces prone to subjective biases. Second, it is time consuming
to search for an optimal model. Third, training the identified model for a new
task is also costly. These challenges limit the practicability of automated
methods in real-world settings. To contend with the challenges, we propose a
fully automated and highly efficient correlated time series forecasting
framework where the search and training can be done in minutes. The framework
includes a data-driven, iterative strategy to automatically prune a large
search space to obtain a high-quality search space for a new forecasting task.
It includes a zero-shot search strategy to efficiently identify the optimal
model in the customized search space. And it includes a fast parameter
adaptation strategy to accelerate the training of the identified model.
Experiments on seven benchmark datasets offer evidence that the framework is
capable of state-of-the-art accuracy and is much more efficient than existing
methods.",stat.ML
Variational Inference on the Boolean Hypercube with the Quantum Entropy,"In this paper, we derive variational inference upper-bounds on the
log-partition function of pairwise Markov random fields on the Boolean
hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We
then propose an efficient algorithm to compute these bounds based on
primal-dual optimization. An improvement of these bounds through the use of
''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and
we present a greedy algorithm to select among these relaxations. We carry
extensive numerical experiments and compare with state-of-the-art methods for
this inference problem.",stat.ML
Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.",stat.ML
"Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training Pipelines via Memoization-Awareness","The training or fine-tuning of machine learning, vision, and language models
is often implemented as a pipeline: a sequence of stages encompassing data
preparation, model training and evaluation. In this paper, we exploit pipeline
structures to reduce the cost of hyperparameter tuning for model
training/fine-tuning, which is particularly valuable for language models given
their high costs in GPU-days. We propose a ""memoization-aware"" Bayesian
Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline
caching system, allowing it to evaluate significantly more hyperparameter
candidates per GPU-day than other tuning algorithms. The result is
better-quality hyperparameters in the same amount of search time, or
equivalently, reduced search time to reach the same hyperparameter quality. In
our benchmarks on machine learning (model ensembles), vision (convolutional
architecture) and language (T5 architecture) pipelines, we compare EEIPU
against recent BO algorithms: EEIPU produces an average of $103\%$ more
hyperparameter candidates (within the same budget), and increases the
validation metric by an average of $108\%$ more than other algorithms (where
the increase is measured starting from the end of warm-up iterations).",stat.ML
A Subsampling Based Neural Network for Spatial Data,"The application of deep neural networks in geospatial data has become a
trending research problem in the present day. A significant amount of
statistical research has already been introduced, such as generalized least
square optimization by incorporating spatial variance-covariance matrix,
considering basis functions in the input nodes of the neural networks, and so
on. However, for lattice data, there is no available literature about the
utilization of asymptotic analysis of neural networks in regression for spatial
data. This article proposes a consistent localized two-layer deep neural
network-based regression for spatial data. We have proved the consistency of
this deep neural network for bounded and unbounded spatial domains under a
fixed sampling design of mixed-increasing spatial regions. We have proved that
its asymptotic convergence rate is faster than that of \cite{zhan2024neural}'s
neural network and an improved generalization of \cite{shen2023asymptotic}'s
neural network structure. We empirically observe the rate of convergence of
discrepancy measures between the empirical probability distribution of observed
and predicted data, which will become faster for a less smooth spatial surface.
We have applied our asymptotic analysis of deep neural networks to the
estimation of the monthly average temperature of major cities in the USA from
its satellite image. This application is an effective showcase of non-linear
spatial regression. We demonstrate our methodology with simulated lattice data
in various scenarios.",stat.ML
Designing a Linearized Potential Function in Neural Network Optimization Using Csiszár Type of Tsallis Entropy,"In recent years, learning for neural networks can be viewed as optimization
in the space of probability measures. To obtain the exponential convergence to
the optimizer, the regularizing term based on Shannon entropy plays an
important role. Even though an entropy function heavily affects convergence
results, there is almost no result on its generalization, because of the
following two technical difficulties: one is the lack of sufficient condition
for generalized logarithmic Sobolev inequality, and the other is the
distributional dependence of the potential function within the gradient flow
equation. In this paper, we establish a framework that utilizes a linearized
potential function via Csisz\'{a}r type of Tsallis entropy, which is one of the
generalized entropies. We also show that our new framework enable us to derive
an exponential convergence result.",stat.ML
Utilizing RNN for Real-time Cryptocurrency Price Prediction and Trading Strategy Optimization,"This study explores the use of Recurrent Neural Networks (RNN) for real-time
cryptocurrency price prediction and optimized trading strategies. Given the
high volatility of the cryptocurrency market, traditional forecasting models
often fall short. By leveraging RNNs' capability to capture long-term patterns
in time-series data, this research aims to improve accuracy in price prediction
and develop effective trading strategies. The project follows a structured
approach involving data collection, preprocessing, and model refinement,
followed by rigorous backtesting for profitability and risk assessment. This
work contributes to both the academic and practical fields by providing a
robust predictive model and optimized trading strategies that address the
challenges of cryptocurrency trading.",stat.ML
Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner,"Estimating causal quantities from observational data is crucial for
understanding the safety and effectiveness of medical treatments. However, to
make reliable inferences, medical practitioners require not only estimating
averaged causal quantities, such as the conditional average treatment effect,
but also understanding the randomness of the treatment effect as a random
variable. This randomness is referred to as aleatoric uncertainty and is
necessary for understanding the probability of benefit from treatment or
quantiles of the treatment effect. Yet, the aleatoric uncertainty of the
treatment effect has received surprisingly little attention in the causal
machine learning community. To fill this gap, we aim to quantify the aleatoric
uncertainty of the treatment effect at the covariate-conditional level, namely,
the conditional distribution of the treatment effect (CDTE). Unlike average
causal quantities, the CDTE is not point identifiable without strong additional
assumptions. As a remedy, we employ partial identification to obtain sharp
bounds on the CDTE and thereby quantify the aleatoric uncertainty of the
treatment effect. We then develop a novel, orthogonal learner for the bounds on
the CDTE, which we call AU-learner. We further show that our AU-learner has
several strengths in that it satisfies Neyman-orthogonality and is doubly
robust. Finally, we propose a fully-parametric deep learning instantiation of
our AU-learner.",stat.ML
Solving stochastic partial differential equations using neural networks in the Wiener chaos expansion,"In this paper, we solve stochastic partial differential equations (SPDEs)
numerically by using (possibly random) neural networks in the truncated Wiener
chaos expansion of their corresponding solution. Moreover, we provide some
approximation rates for learning the solution of SPDEs with additive and/or
multiplicative noise. Finally, we apply our results in numerical examples to
approximate the solution of three SPDEs: the stochastic heat equation, the
Heath-Jarrow-Morton equation, and the Zakai equation.",stat.ML
Near-Optimal and Tractable Estimation under Shift-Invariance,"How hard is it to estimate a discrete-time signal $(x_{1}, ..., x_{n}) \in
\mathbb{C}^n$ satisfying an unknown linear recurrence relation of order $s$ and
observed in i.i.d. complex Gaussian noise? The class of all such signals is
parametric but extremely rich: it contains all exponential polynomials over
$\mathbb{C}$ with total degree $s$, including harmonic oscillations with $s$
arbitrary frequencies. Geometrically, this class corresponds to the projection
onto $\mathbb{C}^{n}$ of the union of all shift-invariant subspaces of
$\mathbb{C}^\mathbb{Z}$ of dimension $s$. We show that the statistical
complexity of this class, as measured by the squared minimax radius of the
$(1-\delta)$-confidence $\ell_2$-ball, is nearly the same as for the class of
$s$-sparse signals, namely $O\left(s\log(en) + \log(\delta^{-1})\right) \cdot
\log^2(es) \cdot \log(en/s).$ Moreover, the corresponding near-minimax
estimator is tractable, and it can be used to build a test statistic with a
near-minimax detection threshold in the associated detection problem. These
statistical results rest upon an approximation-theoretic one: we show that
finite-dimensional shift-invariant subspaces admit compactly supported
reproducing kernels whose Fourier spectra have nearly the smallest possible
$\ell_p$-norms, for all $p \in [1,+\infty]$ at once.",stat.ML
Proxy-informed Bayesian transfer learning with unknown sources,"Generalization outside the scope of one's training data requires leveraging
prior knowledge about the effects that transfer, and the effects that don't,
between different data sources. Bayesian transfer learning is a principled
paradigm for specifying this knowledge, and refining it on the basis of data
from the source (training) and target (prediction) tasks. We address the
challenging transfer learning setting where the learner (i) cannot fine-tune in
the target task, and (ii) does not know which source data points correspond to
the same task (i.e., the data sources are unknown). We propose a proxy-informed
robust method for probabilistic transfer learning (PROMPT), which provides a
posterior predictive estimate tailored to the structure of the target task,
without requiring the learner have access to any outcome information from the
target task. Instead, PROMPT relies on the availability of proxy information.
PROMPT uses the same proxy information for two purposes: (i) estimation of
effects specific to the target task, and (ii) construction of a robust
reweighting of the source data for estimation of effects that transfer between
tasks. We provide theoretical results on the effect of this reweighting on the
risk of negative transfer, and demonstrate application of PROMPT in two
synthetic settings.",stat.ML
Online Data Collection for Efficient Semiparametric Inference,"While many works have studied statistical data fusion, they typically assume
that the various datasets are given in advance. However, in practice,
estimation requires difficult data collection decisions like determining the
available data sources, their costs, and how many samples to collect from each
source. Moreover, this process is often sequential because the data collected
at a given time can improve collection decisions in the future. In our setup,
given access to multiple data sources and budget constraints, the agent must
sequentially decide which data source to query to efficiently estimate a target
parameter. We formalize this task using Online Moment Selection, a
semiparametric framework that applies to any parameter identified by a set of
moment conditions. Interestingly, the optimal budget allocation depends on the
(unknown) true parameters. We present two online data collection policies,
Explore-then-Commit and Explore-then-Greedy, that use the parameter estimates
at a given time to optimally allocate the remaining budget in the future steps.
We prove that both policies achieve zero regret (assessed by asymptotic MSE)
relative to an oracle policy. We empirically validate our methods on both
synthetic and real-world causal effect estimation tasks, demonstrating that the
online data collection policies outperform their fixed counterparts.",stat.ML
Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs,"We study episodic linear mixture MDPs with the unknown transition and
adversarial rewards under full-information feedback, employing dynamic regret
as the performance measure. We start with in-depth analyses of the strengths
and limitations of the two most popular methods: occupancy-measure-based and
policy-based methods. We observe that while the occupancy-measure-based method
is effective in addressing non-stationary environments, it encounters
difficulties with the unknown transition. In contrast, the policy-based method
can deal with the unknown transition effectively but faces challenges in
handling non-stationary environments. Building on this, we propose a novel
algorithm that combines the benefits of both methods. Specifically, it employs
(i) an occupancy-measure-based global optimization with a two-layer structure
to handle non-stationary environments; and (ii) a policy-based variance-aware
value-targeted regression to tackle the unknown transition. We bridge these two
parts by a novel conversion. Our algorithm enjoys an $\widetilde{\mathcal{O}}(d
\sqrt{H^3 K} + \sqrt{HK(H + \bar{P}_K)})$ dynamic regret, where $d$ is the
feature dimension, $H$ is the episode length, $K$ is the number of episodes,
$\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up
to logarithmic factors by establishing a matching lower bound. To the best of
our knowledge, this is the first work that achieves near-optimal dynamic regret
for adversarial linear mixture MDPs with the unknown transition without prior
knowledge of the non-stationarity measure.",stat.ML
Benign landscape for Burer-Monteiro factorizations of MaxCut-type semidefinite programs,"We consider MaxCut-type semidefinite programs (SDP) which admit a low rank
solution. To numerically leverage the low rank hypothesis, a standard
algorithmic approach is the Burer-Monteiro factorization, which allows to
significantly reduce the dimensionality of the problem at the cost of its
convexity. We give a sharp condition on the conditioning of the Laplacian
matrix associated with the SDP under which any second-order critical point of
the non-convex problem is a global minimizer. By applying our theorem, we
improve on recent results about the correctness of the Burer-Monteiro approach
on $\mathbb{Z}_2$-synchronization problems.",stat.ML
Correlating Variational Autoencoders Natively For Multi-View Imputation,"Multi-view data from the same source often exhibit correlation. This is
mirrored in correlation between the latent spaces of separate variational
autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is
proposed that incorporates a joint prior with a non-zero correlation structure
between the latent spaces of the VAEs. By enforcing such correlation structure,
more strongly correlated latent spaces are uncovered. Using conditional
distributions to move between these latent spaces, missing views can be imputed
and used for downstream analysis. Learning this correlation structure involves
maintaining validity of the prior distribution, as well as a successful
parameterization that allows end-to-end learning.",stat.ML
Graph Agnostic Causal Bayesian Optimisation,"We study the problem of globally optimising a target variable of an unknown
causal graph on which a sequence of soft or hard interventions can be
performed. The problem of optimising the target variable associated with a
causal graph is formalised as Causal Bayesian Optimisation (CBO). We study the
CBO problem under the cumulative regret objective with unknown causal graphs
for two settings, namely structural causal models with hard interventions and
function networks with soft interventions. We propose Graph Agnostic Causal
Bayesian Optimisation (GACBO), an algorithm that actively discovers the causal
structure that contributes to achieving optimal rewards. GACBO seeks to balance
exploiting the actions that give the best rewards against exploring the causal
structures and functions. To the best of our knowledge, our work is the first
to study causal Bayesian optimization with cumulative regret objectives in
scenarios where the graph is unknown or partially known. We show our proposed
algorithm outperforms baselines in simulated experiments and real-world
applications.",stat.ML
Testing Generalizability in Causal Inference,"Ensuring robust model performance across diverse real-world scenarios
requires addressing both transportability across domains with covariate shifts
and extrapolation beyond observed data ranges. However, there is no formal
procedure for statistically evaluating generalizability in machine learning
algorithms, particularly in causal inference. Existing methods often rely on
arbitrary metrics like AUC or MSE and focus predominantly on toy datasets,
providing limited insights into real-world applicability. To address this gap,
we propose a systematic and quantitative framework for evaluating model
generalizability under covariate distribution shifts, specifically within
causal inference settings. Our approach leverages the frugal parameterization,
allowing for flexible simulations from fully and semi-synthetic benchmarks,
offering comprehensive evaluations for both mean and distributional regression
methods. By basing simulations on real data, our method ensures more realistic
evaluations, which is often missing in current work relying on simplified
datasets. Furthermore, using simulations and statistical testing, our framework
is robust and avoids over-reliance on conventional metrics. Grounded in
real-world data, it provides realistic insights into model performance,
bridging the gap between synthetic evaluations and practical applications.",stat.ML
Your copula is a classifier in disguise: classification-based copula density estimation,"We propose reinterpreting copula density estimation as a discriminative task.
Under this novel estimation scheme, we train a classifier to distinguish
samples from the joint density from those of the product of independent
marginals, recovering the copula density in the process. We derive equivalences
between well-known copula classes and classification problems naturally arising
in our interpretation. Furthermore, we show our estimator achieves theoretical
guarantees akin to maximum likelihood estimation. By identifying a connection
with density ratio estimation, we benefit from the rich literature and models
available for such problems. Empirically, we demonstrate the applicability of
our approach by estimating copulas of real and high-dimensional datasets,
outperforming competing copula estimators in density evaluation as well as
sampling.",stat.ML
Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression: A Distribution-Free Analysis,"We study nonparametric regression by an over-parameterized two-layer neural
network trained by gradient descent (GD) in this paper. We show that, if the
neural network is trained by GD with early stopping, then the trained network
renders a sharp rate of the nonparametric regression risk of $\cO(\eps_n^2)$,
which is the same rate as that for the classical kernel regression trained by
GD with early stopping, where $\eps_n$ is the critical population rate of the
Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of
the training data. It is remarked that our result does not require
distributional assumptions on the training data, in a strong contrast with many
existing results which rely on specific distributions such as the spherical
uniform data distribution or distributions satisfying certain restrictive
conditions. The rate $\cO(\eps_n^2)$ is known to be minimax optimal for
specific cases, such as the case that the NTK has a polynomial eigenvalue decay
rate which happens under certain distributional assumptions. Our result
formally fills the gap between training a classical kernel regression model and
training an over-parameterized but finite-width neural network by GD for
nonparametric regression without distributional assumptions. We also provide
confirmative answers to certain open questions or address particular concerns
in the literature of training over-parameterized neural networks by GD with
early stopping for nonparametric regression, including the characterization of
the stopping time, the lower bound for the network width, and the constant
learning rate used in GD.",stat.ML
ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate,"Adam is one of the most popular optimization algorithms in deep learning.
However, it is known that Adam does not converge in theory unless choosing a
hyperparameter, i.e., $\beta_2$, in a problem-dependent manner. There have been
many attempts to fix the non-convergence (e.g., AMSGrad), but they require an
impractical assumption that the gradient noise is uniformly bounded. In this
paper, we propose a new adaptive gradient method named ADOPT, which achieves
the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice
of $\beta_2$ without depending on the bounded noise assumption. ADOPT addresses
the non-convergence issue of Adam by removing the current gradient from the
second moment estimate and changing the order of the momentum update and the
normalization by the second moment estimate. We also conduct intensive
numerical experiments, and verify that our ADOPT achieves superior results
compared to Adam and its variants across a wide range of tasks, including image
classification, generative modeling, natural language processing, and deep
reinforcement learning. The implementation is available at
https://github.com/iShohei220/adopt.",stat.ML
Community detection with the Bethe-Hessian,"The Bethe-Hessian matrix, introduced by Saade, Krzakala, and Zdeborov\'a
(2014), is a Hermitian matrix designed for applying spectral clustering
algorithms to sparse networks. Rather than employing a non-symmetric and
high-dimensional non-backtracking operator, a spectral method based on the
Bethe-Hessian matrix is conjectured to also reach the Kesten-Stigum detection
threshold in the sparse stochastic block model (SBM). We provide the first
rigorous analysis of the Bethe-Hessian spectral method in the SBM under both
the bounded expected degree and the growing degree regimes. Specifically, we
demonstrate that: (i) When the expected degree $d\geq 2$, the number of
negative outliers of the Bethe-Hessian matrix can consistently estimate the
number of blocks above the Kesten-Stigum threshold, thus confirming a
conjecture from Saade, Krzakala, and Zdeborov\'a (2014) for $d\geq 2$. (ii) For
sufficiently large $d$, its eigenvectors can be used to achieve weak recovery.
(iii) As $d\to\infty$, we establish the concentration of the locations of its
negative outlier eigenvalues, and weak consistency can be achieved via a
spectral method based on the Bethe-Hessian matrix.",stat.ML
Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.",stat.ML
Generalization and Risk Bounds for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) have achieved great success in the
prediction of sequential data. However, their theoretical studies are still
lagging behind because of their complex interconnected structures. In this
paper, we establish a new generalization error bound for vanilla RNNs, and
provide a unified framework to calculate the Rademacher complexity that can be
applied to a variety of loss functions. When the ramp loss is used, we show
that our bound is tighter than the existing bounds based on the same
assumptions on the Frobenius and spectral norms of the weight matrices and a
few mild conditions. Our numerical results show that our new generalization
bound is the tightest among all existing bounds in three public datasets. Our
bound improves the second tightest one by an average percentage of 13.80% and
3.01% when the $\tanh$ and ReLU activation functions are used, respectively.
Moreover, we derive a sharp estimation error bound for RNN-based estimators
obtained through empirical risk minimization (ERM) in multi-class
classification problems when the loss function satisfies a Bernstein condition.",stat.ML
Automatic doubly robust inference for linear functionals via calibrated debiased machine learning,"In causal inference, many estimands of interest can be expressed as a linear
functional of the outcome regression function; this includes, for example,
average causal effects of static, dynamic and stochastic interventions. For
learning such estimands, in this work, we propose novel debiased machine
learning estimators that are doubly robust asymptotically linear, thus
providing not only doubly robust consistency but also facilitating doubly
robust inference (e.g., confidence intervals and hypothesis tests). To do so,
we first establish a key link between calibration, a machine learning technique
typically used in prediction and classification tasks, and the conditions
needed to achieve doubly robust asymptotic linearity. We then introduce
calibrated debiased machine learning (C-DML), a unified framework for doubly
robust inference, and propose a specific C-DML estimator that integrates
cross-fitting, isotonic calibration, and debiased machine learning estimation.
A C-DML estimator maintains asymptotic linearity when either the outcome
regression or the Riesz representer of the linear functional is estimated
sufficiently well, allowing the other to be estimated at arbitrarily slow rates
or even inconsistently. We propose a simple bootstrap-assisted approach for
constructing doubly robust confidence intervals. Our theoretical and empirical
results support the use of C-DML to mitigate bias arising from the inconsistent
or slow estimation of nuisance functions.",stat.ML
New random projections for isotropic kernels using stable spectral distributions,"Rahimi and Recht [31] introduced the idea of decomposing shift-invariant
kernels by randomly sampling from their spectral distribution. This famous
technique, known as Random Fourier Features (RFF), is in principle applicable
to any shift-invariant kernel whose spectral distribution can be identified and
simulated. In practice, however, it is usually applied to the Gaussian kernel
because of its simplicity, since its spectral distribution is also Gaussian.
Clearly, simple spectral sampling formulas would be desirable for broader
classes of kernel functions. In this paper, we propose to decompose spectral
kernel distributions as a scale mixture of $\alpha$-stable random vectors. This
provides a simple and ready-to-use spectral sampling formula for a very large
class of multivariate shift-invariant kernels, including exponential power
kernels, generalized Mat\'ern kernels, generalized Cauchy kernels, as well as
newly introduced kernels such as the Beta, Kummer, and Tricomi kernels. In
particular, we show that the spectral densities of all these kernels are scale
mixtures of the multivariate Gaussian distribution. This provides a very simple
way to modify existing Random Fourier Features software based on Gaussian
kernels to cover a much richer class of multivariate kernels. This result has
broad applications for support vector machines, kernel ridge regression,
Gaussian processes, and other kernel-based machine learning techniques for
which the random Fourier features technique is applicable.",stat.ML
A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks,"We propose a general framework for deriving generalization bounds for
parallel positively homogeneous neural networks--a class of neural networks
whose input-output map decomposes as the sum of positively homogeneous maps.
Examples of such networks include matrix factorization and sensing,
single-layer multi-head attention mechanisms, tensor factorization, deep linear
and ReLU networks, and more. Our general framework is based on linking the
non-convex empirical risk minimization (ERM) problem to a closely related
convex optimization problem over prediction functions, which provides a global,
achievable lower-bound to the ERM problem. We exploit this convex lower-bound
to perform generalization analysis in the convex space while controlling the
discrepancy between the convex model and its non-convex counterpart. We apply
our general framework to a wide variety of models ranging from low-rank matrix
sensing, to structured matrix sensing, two-layer linear networks, two-layer
ReLU networks, and single-layer multi-head attention mechanisms, achieving
generalization bounds with a sample complexity that scales almost linearly with
the network width.",stat.ML
"Fast, robust approximate message passing","We give a fast, spectral procedure for implementing approximate-message
passing (AMP) algorithms robustly. For any quadratic optimization problem over
symmetric matrices $X$ with independent subgaussian entries, and any separable
AMP algorithm $\mathcal A$, our algorithm performs a spectral pre-processing
step and then mildly modifies the iterates of $\mathcal A$. If given the
perturbed input $X + E \in \mathbb R^{n \times n}$ for any $E$ supported on a
$\varepsilon n \times \varepsilon n$ principal minor, our algorithm outputs a
solution $\hat v$ which is guaranteed to be close to the output of $\mathcal A$
on the uncorrupted $X$, with $\|\mathcal A(X) - \hat v\|_2 \le f(\varepsilon)
\|\mathcal A(X)\|_2$ where $f(\varepsilon) \to 0$ as $\varepsilon \to 0$
depending only on $\varepsilon$.",stat.ML
"Elliptical Wishart distributions: information geometry, maximum likelihood estimator, performance analysis and statistical learning","This paper deals with Elliptical Wishart distributions - which generalize the
Wishart distribution - in the context of signal processing and machine
learning. Two algorithms to compute the maximum likelihood estimator (MLE) are
proposed: a fixed point algorithm and a Riemannian optimization method based on
the derived information geometry of Elliptical Wishart distributions. The
existence and uniqueness of the MLE are characterized as well as the
convergence of both estimation algorithms. Statistical properties of the MLE
are also investigated such as consistency, asymptotic normality and an
intrinsic version of Fisher efficiency. On the statistical learning side, novel
classification and clustering methods are designed. For the $t$-Wishart
distribution, the performance of the MLE and statistical learning algorithms
are evaluated on both simulated and real EEG and hyperspectral data, showcasing
the interest of our proposed methods.",stat.ML
Differentiability and Approximation of Probability Functions under Gaussian Mixture Models: A Bayesian Approach,"In this work, we study probability functions associated with Gaussian mixture
models. Our primary focus is on extending the use of spherical radial
decomposition for multivariate Gaussian random vectors to the context of
Gaussian mixture models, which are not inherently spherical but only
conditionally so. Specifically, the conditional probability distribution, given
a random parameter of the random vector, follows a Gaussian distribution,
allowing us to apply Bayesian analysis tools to the probability function. This
assumption, together with spherical radial decomposition for Gaussian random
vectors, enables us to represent the probability function as an integral over
the Euclidean sphere. Using this representation, we establish sufficient
conditions to ensure the differentiability of the probability function and
provide and integral representation of its gradient. Furthermore, leveraging
the Bayesian decomposition, we approximate the probability function using
random sampling over the parameter space and the Euclidean sphere. Finally, we
present numerical examples that illustrate the advantages of this approach over
classical approximations based on random vector sampling.",stat.ML
Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach,"In this study, the novel hybrid machine learning approach is proposed in
carbon price fluctuation prediction. Specifically, a research framework
integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term
Memory (LSTM) neural network algorithm is proposed. The advantage of the
combined framework is that it can make feature extraction more efficient. Then,
based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty
as regularization method is adopted to predict. Referring to the
characteristics of high correlation between energy indicator price and
blockchain information in previous literature, and we primarily includes
indicators related to blockchain information through regularization process.
Based on the above methods, this paper uses a dataset containing an amount of
data to carry out the carbon price prediction. The experimental results show
that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM
architecture. Blockchain information can effectively predict the price. Since
parameter norm penalty as regularization, Ridge Regression (RR) as L2
regularization is better than Smoothly Clipped Absolute Deviation Penalty
(SCAD) as L1 regularization in price forecasting. Thus, the proposed RR-DILATED
CNN-LSTM approach can effectively and accurately predict the fluctuation trend
of the carbon price. Therefore, the new forecasting methods and theoretical
ecology proposed in this study provide a new basis for trend prediction and
evaluating digital assets policy represented by the carbon price for both the
academia and practitioners.",stat.ML
Point processes with event time uncertainty,"Point processes are widely used statistical models for uncovering the
temporal patterns in dependent event data. In many applications, the event time
cannot be observed exactly, calling for the incorporation of time uncertainty
into the modeling of point process data. In this work, we introduce a framework
to model time-uncertain point processes possibly on a network. We start by
deriving the formulation in the continuous-time setting under a few assumptions
motivated by application scenarios. After imposing a time grid, we obtain a
discrete-time model that facilitates inference and can be computed by
first-order optimization methods such as Gradient Descent or Variation
inequality (VI) using batch-based Stochastic Gradient Descent (SGD). The
parameter recovery guarantee is proved for VI inference at an $O(1/k)$
convergence rate using $k$ SGD steps. Our framework handles non-stationary
processes by modeling the inference kernel as a matrix (or tensor on a network)
and it covers the stationary process, such as the classical Hawkes process, as
a special case. We experimentally show that the proposed approach outperforms
previous General Linear model (GLM) baselines on simulated and real data and
reveals meaningful causal relations on a Sepsis-associated Derangements
dataset.",stat.ML
Explanations that reveal all through the definition of encoding,"Feature attributions attempt to highlight what inputs drive predictive power.
Good attributions or explanations are thus those that produce inputs that
retain this predictive power; accordingly, evaluations of explanations score
their quality of prediction. However, evaluations produce scores better than
what appears possible from the values in the explanation for a class of
explanations, called encoding explanations. Probing for encoding remains a
challenge because there is no general characterization of what gives the extra
predictive power. We develop a definition of encoding that identifies this
extra predictive power via conditional dependence and show that the definition
fits existing examples of encoding. This definition implies, in contrast to
encoding explanations, that non-encoding explanations contain all the
informative inputs used to produce the explanation, giving them a ""what you see
is what you get"" property, which makes them transparent and simple to use.
Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank
non-encoding explanations above encoding ones, and develop STRIPE-X which ranks
them correctly. After empirically demonstrating the theoretical insights, we
use STRIPE-X to uncover encoding in LLM-generated explanations for predicting
the sentiment in movie reviews.",stat.ML
Classifier Chain Networks for Multi-Label Classification,"The classifier chain is a widely used method for analyzing multi-labeled data
sets. In this study, we introduce a generalization of the classifier chain: the
classifier chain network. The classifier chain network enables joint estimation
of model parameters, and allows to account for the influence of earlier label
predictions on subsequent classifiers in the chain. Through simulations, we
evaluate the classifier chain network's performance against multiple benchmark
methods, demonstrating competitive results even in scenarios that deviate from
its modeling assumptions. Furthermore, we propose a new measure for detecting
conditional dependencies between labels and illustrate the classifier chain
network's effectiveness using an empirical data set.",stat.ML
FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.",stat.ML
A Directional Rockafellar-Uryasev Regression,"Most ost Big Data datasets suffer from selection bias. For example, X
(Twitter) training observations differ largely from the testing offline
observations as individuals on Twitter are generally more educated, democratic
or left-leaning. Therefore, one major obstacle to reliable estimation is the
differences between training and testing data. How can researchers make use of
such data even in the presence of non-ignorable selection mechanisms? A number
of methods have been developed for this issue, such as distributionally robust
optimization (DRO) or learning fairness. A possible avenue to reducing the
effect of bias is meta-information. Researchers, being field exerts, might have
prior information on the form and extent of selection bias affecting their
dataset, and in which direction the selection might cause the estimate to
change, e.g. over or under estimation. At the same time, there is no direct way
to leverage these types of information in learning. I propose a loss function
which takes into account two types of meta data information given by the
researcher: quantity and direction (under or over sampling) of bias in the
training set. Estimation with the proposed loss function is then implemented
through a neural network, the directional Rockafellar-Uryasev (dRU) regression
model. I test the dRU model on a biased training dataset, a Big Data online
drawn electoral poll. I apply the proposed model using meta data information
coherent with the political and sampling information obtained from previous
studies. The results show that including meta information improves the
electoral results predictions compared to a model that does not include them.",stat.ML
Distributionally Robust Optimization,"Distributionally robust optimization (DRO) studies decision problems under
uncertainty where the probability distribution governing the uncertain problem
parameters is itself uncertain. A key component of any DRO model is its
ambiguity set, that is, a family of probability distributions consistent with
any available structural or statistical information. DRO seeks decisions that
perform best under the worst distribution in the ambiguity set. This worst case
criterion is supported by findings in psychology and neuroscience, which
indicate that many decision-makers have a low tolerance for distributional
ambiguity. DRO is rooted in statistics, operations research and control theory,
and recent research has uncovered its deep connections to regularization
techniques and adversarial training in machine learning. This survey presents
the key findings of the field in a unified and self-contained manner.",stat.ML
Pretrained transformer efficiently learns low-dimensional target functions in-context,"Transformers can efficiently learn in-context from example demonstrations.
Most existing theoretical analyses studied the in-context learning (ICL)
ability of transformers for linear function classes, where it is typically
shown that the minimizer of the pretraining loss implements one gradient
descent step on the least squares objective. However, this simplified linear
setting arguably does not demonstrate the statistical efficiency of ICL, since
the pretrained transformer does not outperform directly solving linear
regression on the test prompt. In this paper, we study ICL of a nonlinear
function class via transformer with nonlinear MLP layer: given a class of
\textit{single-index} target functions $f_*(\boldsymbol{x}) =
\sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index
features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional
subspace, we show that a nonlinear transformer optimized by gradient descent
(with a pretraining sample complexity that depends on the \textit{information
exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a
prompt length that only depends on the dimension of the distribution of target
functions $r$; in contrast, any algorithm that directly learns $f_*$ on test
prompt yields a statistical complexity that scales with the ambient dimension
$d$. Our result highlights the adaptivity of the pretrained transformer to
low-dimensional structures of the function class, which enables
sample-efficient ICL that outperforms estimators that only have access to the
in-context data.",stat.ML
Linear Causal Bandits: Unknown Graph and Soft Interventions,"Designing causal bandit algorithms depends on two central categories of
assumptions: (i) the extent of information about the underlying causal graphs
and (ii) the extent of information about interventional statistical models.
There have been extensive recent advances in dispensing with assumptions on
either category. These include assuming known graphs but unknown interventional
distributions, and the converse setting of assuming unknown graphs but access
to restrictive hard/$\operatorname{do}$ interventions, which removes the
stochasticity and ancestral dependencies. Nevertheless, the problem in its
general form, i.e., unknown graph and unknown stochastic intervention models,
remains open. This paper addresses this problem and establishes that in a graph
with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after
$T$ interaction rounds the regret upper bound scales as
$\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c>1$ is a
constant and $R$ is a measure of intervention power. A universal minimax lower
bound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.
Importantly, the graph size $N$ has a diminishing effect on the regret as $T$
grows. These bounds have matching behavior in $T$, exponential dependence on
$L$, and polynomial dependence on $d$ (with the gap $d\ $). On the algorithmic
aspect, the paper presents a novel way of designing a computationally efficient
CB algorithm, addressing a challenge that the existing CB algorithms using soft
interventions face.",stat.ML
Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,"Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.",stat.ML
Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate
differential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$
samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians
up to low total variation distance, with differential privacy. Our work
improves over the previous best result [AAL24b] (which required roughly $k^2
d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.
Moreover, we give the first optimal bound for privately learning mixtures of
$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the
sample complexity for privately learning mixtures of univariate Gaussians is
linear in the number of components $k$, whereas the previous best sample
complexity [AAL21] was quadratic in $k$. Our algorithms utilize various
techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],
sample compression for distributions [ABDH+20], and methods for bounding
volumes of sumsets.",stat.ML
On the Utilization of Unique Node Identifiers in Graph Neural Networks,"Graph Neural Networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.",stat.ML
Towards safe Bayesian optimization with Wiener kernel regression,"Bayesian Optimization (BO) is a data-driven strategy for
minimizing/maximizing black-box functions based on probabilistic surrogate
models. In the presence of safety constraints, the performance of BO crucially
relies on tight probabilistic error bounds related to the uncertainty
surrounding the surrogate model. For the case of Gaussian Process surrogates
and Gaussian measurement noise, we present a novel error bound based on the
recently proposed Wiener kernel regression. We prove that under rather mild
assumptions, the proposed error bound is tighter than bounds previously
documented in the literature which leads to enlarged safety regions. We draw
upon a numerical example to demonstrate the efficacy of the proposed error
bound in safe BO.",stat.ML
Powerful batch conformal prediction for classification,"In a supervised classification split conformal/inductive framework with $K$
classes, a calibration sample of $n$ labeled examples is observed for inference
on the label of a new unlabeled example. In this work, we explore the case
where a ""batch"" of $m$ independent such unlabeled examples is given, and a
multivariate prediction set with $1-\alpha$ coverage should be provided for
this batch. Hence, the batch prediction set takes the form of a collection of
label vectors of size $m$, while the calibration sample only contains
univariate labels. Using the Bonferroni correction consists in concatenating
the individual prediction sets at level $1-\alpha/m$ (Vovk 2013). We propose a
uniformly more powerful solution, based on specific combinations of conformal
$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the
pooled evidence of fairly ""easy"" examples of the batch can help provide
narrower batch prediction sets. We also introduced adaptive versions of the
novel procedure that are particularly effective when the batch prediction set
is expected to be large. The theoretical guarantees are provided when all
examples are iid, as well as more generally when iid is assumed only
conditionally within each class. In particular, our results are also valid
under a label distribution shift since the distribution of the labels need not
be the same in the calibration sample and in the new `batch'. The usefulness of
the method is illustrated on synthetic and real data examples.",stat.ML
Variable Selection in Convex Piecewise Linear Regression,"This paper presents Sparse Gradient Descent as a solution for variable
selection in convex piecewise linear regression where the model is given as
$\mathrm{max}\langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$
where $x \in \mathbb R^d$ is the covariate vector. Here,
$\{a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth
weight vectors and intercepts. A non-asymptotic local convergence analysis is
provided for Sp-GD under sub-Gaussian noise when the covariate distribution
satisfies sub-Gaussianity and anti-concentration property. When the model order
and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given
$\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where
$\sigma_z^2$ denotes the noise variance. This also implies the exact parameter
recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. Since
optimizing the squared loss for sparse max-affine is non-convex, an
initialization scheme is proposed to provide a suitable initial estimate within
the basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke the
convergence guarantees. The initialization scheme uses sparse principal
component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$
then applies an $r$-covering search to estimate the model parameters. A
non-asymptotic analysis is presented for this initialization scheme when the
covariates and noise samples follow Gaussian distributions. When the model
order and parameters are fixed, this initialization scheme provides an
$\epsilon$-accurate estimate given
$\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$
observations. Numerical Monte Carlo results corroborate theoretical findings
for Sp-GD and the initialization scheme.",stat.ML
Targeted Learning for Variable Importance,"Variable importance is one of the most widely used measures for interpreting
machine learning with significant interest from both statistics and machine
learning communities. Recently, increasing attention has been directed toward
uncertainty quantification in these metrics. Current approaches largely rely on
one-step procedures, which, while asymptotically efficient, can present higher
sensitivity and instability in finite sample settings. To address these
limitations, we propose a novel method by employing the targeted learning (TL)
framework, designed to enhance robustness in inference for variable importance
metrics. Our approach is particularly suited for conditional permutation
variable importance. We show that it (i) retains the asymptotic efficiency of
traditional methods, (ii) maintains comparable computational complexity, and
(iii) delivers improved accuracy, especially in finite sample contexts. We
further support these findings with numerical experiments that illustrate the
practical advantages of our method and validate the theoretical results.",stat.ML
Recursive Learning of Asymptotic Variational Objectives,"General state-space models (SSMs) are widely used in statistical machine
learning and are among the most classical generative models for sequential
time-series data. SSMs, comprising latent Markovian states, can be subjected to
variational inference (VI), but standard VI methods like the
importance-weighted autoencoder (IWAE) lack functionality for streaming data.
To enable online VI in SSMs when the observations are received in real time, we
propose maximising an IWAE-type variational lower bound on the asymptotic
contrast function, rather than the standard IWAE ELBO, using stochastic
approximation. Unlike the recursive maximum likelihood method, which directly
maximises the asymptotic contrast, our approach, called online sequential IWAE
(OSIWAE), allows for online learning of both model parameters and a Markovian
recognition model for inferring latent states. By approximating filter state
posteriors and their derivatives using sequential Monte Carlo (SMC) methods, we
create a particle-based framework for online VI in SSMs. This approach is more
theoretically well-founded than recently proposed online variational SMC
methods. We provide rigorous theoretical results on the learning objective and
a numerical study demonstrating the method's efficiency in learning model
parameters and particle proposal kernels.",stat.ML
Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning,"Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.",stat.ML
Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity,"While overparameterization is known to benefit generalization, its impact on
Out-Of-Distribution (OOD) detection is less understood. This paper investigates
the influence of model complexity in OOD detection. We propose an expected OOD
risk metric to evaluate classifiers confidence on both training and OOD
samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD
risk of binary least-squares classifiers applied to Gaussian data. We show that
the OOD risk depicts an infinite peak, when the number of parameters is equal
to the number of samples, which we associate with the double descent
phenomenon. Our experimental study on different OOD detection methods across
multiple neural architectures extends our theoretical insights and highlights a
double descent curve. Our observations suggest that overparameterization does
not necessarily lead to better OOD detection. Using the Neural Collapse
framework, we provide insights to better understand this behavior. To
facilitate reproducibility, our code will be made publicly available upon
publication.",stat.ML
Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks,"The Gauss-Newton (GN) matrix plays an important role in machine learning,
most evident in its use as a preconditioning matrix for a wide family of
popular adaptive methods to speed up optimization. Besides, it can also provide
key insights into the optimization landscape of neural networks. In the context
of deep neural networks, understanding the GN matrix involves studying the
interaction between different weight matrices as well as the dependencies
introduced by the data, thus rendering its analysis challenging. In this work,
we take a first step towards theoretically characterizing the conditioning of
the GN matrix in neural networks. We establish tight bounds on the condition
number of the GN in deep linear networks of arbitrary depth and width, which we
also extend to two-layer ReLU networks. We expand the analysis to further
architectural components, such as residual connections and convolutional
layers. Finally, we empirically validate the bounds and uncover valuable
insights into the influence of the analyzed architectural components.",stat.ML
SpecRaGE: Robust and Generalizable Multi-view Spectral Representation Learning,"Multi-view representation learning (MvRL) has garnered substantial attention
in recent years, driven by the increasing demand for applications that can
effectively process and analyze data from multiple sources. In this context,
graph Laplacian-based MvRL methods have demonstrated remarkable success in
representing multi-view data. However, these methods often struggle with
generalization to new data and face challenges with scalability. Moreover, in
many practical scenarios, multi-view data is contaminated by noise or outliers.
In such cases, modern deep-learning-based MvRL approaches that rely on
alignment or contrastive objectives can lead to misleading results, as they may
impose incorrect consistency between clear and corrupted data sources. We
introduce $\textit{SpecRaGE}$, a novel fusion-based framework that integrates
the strengths of graph Laplacian methods with the power of deep learning to
overcome these challenges. SpecRage uses neural networks to learn parametric
mapping that approximates a joint diagonalization of graph Laplacians. This
solution bypasses the need for alignment while enabling generalizable and
scalable learning of informative and meaningful representations. Moreover, it
incorporates a meta-learning fusion module that dynamically adapts to data
quality, ensuring robustness against outliers and noisy views. Our extensive
experiments demonstrate that SpecRaGE outperforms state-of-the-art methods,
particularly in scenarios with data contamination, paving the way for more
reliable and efficient multi-view learning. Our code will be made publicly
available upon acceptance.",stat.ML
Finite-sample performance of the maximum likelihood estimator in logistic regression,"Logistic regression is a classical model for describing the probabilistic
dependence of binary responses to multivariate covariates. We consider the
predictive performance of the maximum likelihood estimator (MLE) for logistic
regression, assessed in terms of logistic risk. We consider two questions:
first, that of the existence of the MLE (which occurs when the dataset is not
linearly separated), and second that of its accuracy when it exists. These
properties depend on both the dimension of covariates and on the signal
strength. In the case of Gaussian covariates and a well-specified logistic
model, we obtain sharp non-asymptotic guarantees for the existence and excess
logistic risk of the MLE. We then generalize these results in two ways: first,
to non-Gaussian covariates satisfying a certain two-dimensional margin
condition, and second to the general case of statistical learning with a
possibly misspecified logistic model. Finally, we consider the case of a
Bernoulli design, where the behavior of the MLE is highly sensitive to the
parameter direction.",stat.ML
Encoding Multi-level Dynamics in Effect Heterogeneity Estimation,"Earth Observation (EO) data are increasingly used in policy analysis by
enabling granular estimation of treatment effects. However, a challenge in
EO-based causal inference lies in balancing the trade-off between capturing
fine-grained individual heterogeneity and broader contextual information. This
paper introduces Multi-scale Concatenation, a family of composable procedures
that transform arbitrary single-scale CATE estimation algorithms into
multi-scale algorithms. We benchmark the performance of Multi-scale
Concatenation on a CATE estimation pipeline combining Vision Transformer (ViT)
models fine-tuned on satellite images to encode images of different scales with
Causal Forests to obtain the final CATE estimate. We first perform simulation
studies, showing how a multi-scale approach captures multi-level dynamics that
single-scale ViT models fail to capture. We then apply the multi-scale method
to two randomized controlled trials (RCTs) conducted in Peru and Uganda using
Landsat satellite imagery. In the RCT analysis, the Rank Average Treatment
Effect Ratio (RATE Ratio) measure is employed to assess performance without
ground truth individual treatment effects. Results indicate that Multi-scale
Concatenation improves the performance of deep learning models in EO-based CATE
estimation without the complexity of designing new multi-scale architectures
for a specific use case.",stat.ML
Semiparametric conformal prediction,"Many risk-sensitive applications require well-calibrated prediction sets over
multiple, potentially correlated target variables, for which the prediction
algorithm may report correlated non-conformity scores. In this work, we treat
the scores as random vectors and aim to construct the prediction set accounting
for their joint correlation structure. Drawing from the rich literature on
multivariate quantiles and semiparametric statistics, we propose an algorithm
to estimate the $1-\alpha$ quantile of the scores, where $\alpha$ is the
user-specified miscoverage rate. In particular, we flexibly estimate the joint
cumulative distribution function (CDF) of the scores using nonparametric vine
copulas and improve the asymptotic efficiency of the quantile estimate using
its influence function. The vine decomposition allows our method to scale well
to a large number of targets. We report desired coverage and competitive
efficiency on a range of real-world regression problems, including those with
missing-at-random labels in the calibration set.",stat.ML
Low-Rank Tensors for Multi-Dimensional Markov Models,"This work presents a low-rank tensor model for multi-dimensional Markov
chains. A common approach to simplify the dynamical behavior of a Markov chain
is to impose low-rankness on the transition probability matrix. Inspired by the
success of these matrix techniques, we present low-rank tensors for
representing transition probabilities on multi-dimensional state spaces.
Through tensor decomposition, we provide a connection between our method and
classical probabilistic models. Moreover, our proposed model yields a
parsimonious representation with fewer parameters than matrix-based approaches.
Unlike these methods, which impose low-rankness uniformly across all states,
our tensor method accounts for the multi-dimensionality of the state space. We
also propose an optimization-based approach to estimate a Markov model as a
low-rank tensor. Our optimization problem can be solved by the alternating
direction method of multipliers (ADMM), which enjoys convergence to a
stationary solution. We empirically demonstrate that our tensor model estimates
Markov chains more efficiently than conventional techniques, requiring both
fewer samples and parameters. We perform numerical simulations for both a
synthetic low-rank Markov chain and a real-world example with New York City
taxi data, showcasing the advantages of multi-dimensionality for modeling state
spaces.",stat.ML
Amortized Bayesian Experimental Design for Decision-Making,"Many critical decisions, such as personalized medical diagnoses and product
pricing, are made based on insights gained from designing, observing, and
analyzing a series of experiments. This highlights the crucial role of
experimental design, which goes beyond merely collecting information on system
parameters as in traditional Bayesian experimental design (BED), but also plays
a key part in facilitating downstream decision-making. Most recent BED methods
use an amortized policy network to rapidly design experiments. However, the
information gathered through these methods is suboptimal for down-the-line
decision-making, as the experiments are not inherently designed with downstream
objectives in mind. In this paper, we present an amortized decision-aware BED
framework that prioritizes maximizing downstream decision utility. We introduce
a novel architecture, the Transformer Neural Decision Process (TNDP), capable
of instantly proposing the next experimental design, whilst inferring the
downstream decision, thus effectively amortizing both tasks within a unified
workflow. We demonstrate the performance of our method across several tasks,
showing that it can deliver informative designs and facilitate accurate
decision-making.",stat.ML
Theory-inspired Label Shift Adaptation via Aligned Distribution Mixture,"As a prominent challenge in addressing real-world issues within a dynamic
environment, label shift, which refers to the learning setting where the source
(training) and target (testing) label distributions do not match, has recently
received increasing attention. Existing label shift methods solely use
unlabeled target samples to estimate the target label distribution, and do not
involve them during the classifier training, resulting in suboptimal
utilization of available information. One common solution is to directly blend
the source and target distributions during the training of the target
classifier. However, we illustrate the theoretical deviation and limitations of
the direct distribution mixture in the label shift setting. To tackle this
crucial yet unexplored issue, we introduce the concept of aligned distribution
mixture, showcasing its theoretical optimality and generalization error bounds.
By incorporating insights from generalization theory, we propose an innovative
label shift framework named as Aligned Distribution Mixture (ADM). Within this
framework, we enhance four typical label shift methods by introducing
modifications to the classifier training process. Furthermore, we also propose
a one-step approach that incorporates a pioneering coupling weight estimation
strategy. Considering the distinctiveness of the proposed one-step approach, we
develop an efficient bi-level optimization strategy. Experimental results
demonstrate the effectiveness of our approaches, together with their
effectiveness in COVID-19 diagnosis applications.",stat.ML
Towards Harmless Rawlsian Fairness Regardless of Demographic Prior,"Due to privacy and security concerns, recent advancements in group fairness
advocate for model training regardless of demographic information. However,
most methods still require prior knowledge of demographics. In this study, we
explore the potential for achieving fairness without compromising its utility
when no prior demographics are provided to the training set, namely
\emph{harmless Rawlsian fairness}. We ascertain that such a fairness
requirement with no prior demographic information essential promotes training
losses to exhibit a Dirac delta distribution. To this end, we propose a simple
but effective method named VFair to minimize the variance of training losses
inside the optimal set of empirical losses. This problem is then optimized by a
tailored dynamic update approach that operates in both loss and gradient
dimensions, directing the model towards relatively fairer solutions while
preserving its intact utility. Our experimental findings indicate that
regression tasks, which are relatively unexplored from literature, can achieve
significant fairness improvement through VFair regardless of any prior, whereas
classification tasks usually do not because of their quantized utility
measurements. The implementation of our method is publicly available at
\url{https://github.com/wxqpxw/VFair}.",stat.ML
Optimal Classification under Performative Distribution Shift,"Performative learning addresses the increasingly pervasive situations in
which algorithmic decisions may induce changes in the data distribution as a
consequence of their public deployment. We propose a novel view in which these
performative effects are modelled as push-forward measures. This general
framework encompasses existing models and enables novel performative gradient
estimation methods, leading to more efficient and scalable learning strategies.
For distribution shifts, unlike previous models which require full
specification of the data distribution, we only assume knowledge of the shift
operator that represents the performative changes. This approach can also be
integrated into various change-of-variablebased models, such as VAEs or
normalizing flows. Focusing on classification with a linear-in-parameters
performative effect, we prove the convexity of the performative risk under a
new set of assumptions. Notably, we do not limit the strength of performative
effects but rather their direction, requiring only that classification becomes
harder when deploying more accurate models. In this case, we also establish a
connection with adversarially robust classification by reformulating the
minimization of the performative risk as a min-max variational problem.
Finally, we illustrate our approach on synthetic and real datasets.",stat.ML
Learning Controlled Stochastic Differential Equations,"Identification of nonlinear dynamical systems is crucial across various
fields, facilitating tasks such as control, prediction, optimization, and fault
detection. Many applications require methods capable of handling complex
systems while providing strong learning guarantees for safe and reliable
performance. However, existing approaches often focus on simplified scenarios,
such as deterministic models, known diffusion, discrete systems,
one-dimensional dynamics, or systems constrained by strong structural
assumptions such as linearity. This work proposes a novel method for estimating
both drift and diffusion coefficients of continuous, multidimensional,
nonlinear controlled stochastic differential equations with non-uniform
diffusion. We assume regularity of the coefficients within a Sobolev space,
allowing for broad applicability to various dynamical systems in robotics,
finance, climate modeling, and biology. Leveraging the Fokker-Planck equation,
we split the estimation into two tasks: (a) estimating system dynamics for a
finite set of controls, and (b) estimating coefficients that govern those
dynamics. We provide strong theoretical guarantees, including finite-sample
bounds for \(L^2\), \(L^\infty\), and risk metrics, with learning rates
adaptive to coefficients' regularity, similar to those in nonparametric
least-squares regression literature. The practical effectiveness of our
approach is demonstrated through extensive numerical experiments. Our method is
available as an open-source Python library.",stat.ML
Understanding Variational Autoencoders with Intrinsic Dimension and Information Imbalance,"This work presents an analysis of the hidden representations of Variational
Autoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information
Imbalance (II). We show that VAEs undergo a transition in behaviour once the
bottleneck size is larger than the ID of the data, manifesting in a double
hunchback ID profile and a qualitative shift in information processing as
captured by the II. Our results also highlight two distinct training phases for
architectures with sufficiently large bottleneck sizes, consisting of a rapid
fit and a slower generalisation, as assessed by a differentiated behaviour of
ID, II, and KL loss. These insights demonstrate that II and ID could be
valuable tools for aiding architecture search, for diagnosing underfitting in
VAEs, and, more broadly, they contribute to advancing a unified understanding
of deep generative models through geometric analysis.",stat.ML
"See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers","Time series anomaly detection (TSAD) is becoming increasingly vital due to
the rapid growth of time series data across various sectors. Anomalies in web
service data, for example, can signal critical incidents such as system
failures or server malfunctions, necessitating timely detection and response.
However, most existing TSAD methodologies rely heavily on manual feature
engineering or require extensive labeled training data, while also offering
limited interpretability. To address these challenges, we introduce a
pioneering framework called the Time Series Anomaly Multimodal Analyzer (TAMA),
which leverages the power of Large Multimodal Models (LMMs) to enhance both the
detection and interpretation of anomalies in time series data. By converting
time series into visual formats that LMMs can efficiently process, TAMA
leverages few-shot in-context learning capabilities to reduce dependence on
extensive labeled datasets. Our methodology is validated through rigorous
experimentation on multiple real-world datasets, where TAMA consistently
outperforms state-of-the-art methods in TSAD tasks. Additionally, TAMA provides
rich, natural language-based semantic analysis, offering deeper insights into
the nature of detected anomalies. Furthermore, we contribute one of the first
open-source datasets that includes anomaly detection labels, anomaly type
labels, and contextual description, facilitating broader exploration and
advancement within this critical field. Ultimately, TAMA not only excels in
anomaly detection but also provides a comprehensive approach for understanding
the underlying causes of anomalies, pushing TSAD forward through innovative
methodologies and insights.",stat.ML
EXAGREE: Towards Explanation Agreement in Explainable Machine Learning,"Explanations in machine learning are critical for trust, transparency, and
fairness. Yet, complex disagreements among these explanations limit the
reliability and applicability of machine learning models, especially in
high-stakes environments. We formalize four fundamental ranking-based
explanation disagreement problems and introduce a novel framework, EXplanation
AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine
learning, particularly from stakeholder-centered perspectives. Our approach
leverages a Rashomon set for attribution predictions and then optimizes within
this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that
minimize disagreement with diverse stakeholder needs while maintaining
predictive performance. Rigorous empirical analysis on synthetic and real-world
datasets demonstrates that EXAGREE reduces explanation disagreement and
improves fairness across subgroups in various domains. EXAGREE not only
provides researchers with a new direction for studying explanation disagreement
problems but also offers data scientists a tool for making better-informed
decisions in practical applications.",stat.ML
RobPy: a Python Package for Robust Statistical Methods,"Robust estimation provides essential tools for analyzing data that contain
outliers, ensuring that statistical models remain reliable even in the presence
of some anomalous data. While robust methods have long been available in R,
users of Python have lacked a comprehensive package that offers these methods
in a cohesive framework. RobPy addresses this gap by offering a wide range of
robust methods in Python, built upon established libraries including NumPy,
SciPy, and scikit-learn. This package includes tools for robust preprocessing,
univariate estimation, covariance matrices, regression, and principal component
analysis, which are able to detect outliers and to mitigate their effect. In
addition, RobPy provides specialized diagnostic plots for visualizing casewise
and cellwise outliers. This paper presents the structure of the RobPy package,
demonstrates its functionality through examples, and compares its features to
existing implementations in other statistical software. By bringing robust
methods to Python, RobPy enables more users to perform robust data analysis in
a modern and versatile programming language.",stat.ML
You are out of context!,"This research proposes a novel drift detection methodology for machine
learning (ML) models based on the concept of ''deformation'' in the vector
space representation of data. Recognizing that new data can act as forces
stretching, compressing, or twisting the geometric relationships learned by a
model, we explore various mathematical frameworks to quantify this deformation.
We investigate measures such as eigenvalue analysis of covariance matrices to
capture global shape changes, local density estimation using kernel density
estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in
data concentration. Additionally, we draw inspiration from continuum mechanics
by proposing a ''strain tensor'' analogy to capture multi-faceted deformations
across different data types. This requires careful estimation of the
displacement field, and we delve into strategies ranging from density-based
approaches to manifold learning and neural network methods. By continuously
monitoring these deformation metrics and correlating them with model
performance, we aim to provide a sensitive, interpretable, and adaptable drift
detection system capable of distinguishing benign data evolution from true
drift, enabling timely interventions and ensuring the reliability of machine
learning systems in dynamic environments. Addressing the computational
challenges of this methodology, we discuss mitigation strategies like
dimensionality reduction, approximate algorithms, and parallelization for
real-time and large-scale applications. The method's effectiveness is
demonstrated through experiments on real-world text data, focusing on detecting
context shifts in Generative AI. Our results, supported by publicly available
code, highlight the benefits of this deformation-based approach in capturing
subtle drifts that traditional statistical methods often miss. Furthermore, we
present a detailed application example within the healthcare domain, showcasing
the methodology's potential in diverse fields. Future work will focus on
further improving computational efficiency and exploring additional
applications across different ML domains.",stat.ML
Differentially private and decentralized randomized power method,"The randomized power method has gained significant interest due to its
simplicity and efficient handling of large-scale spectral analysis and
recommendation tasks. As modern datasets contain sensitive private information,
we need to give formal guarantees on the possible privacy leaks caused by this
method. This paper focuses on enhancing privacy preserving variants of the
method. We propose a strategy to reduce the variance of the noise introduced to
achieve Differential Privacy (DP). We also adapt the method to a decentralized
framework with a low computational and communication overhead, while preserving
the accuracy. We leverage Secure Aggregation (a form of Multi-Party
Computation) to allow the algorithm to perform computations using data
distributed among multiple users or devices, without revealing individual data.
We show that it is possible to use a noise scale in the decentralized setting
that is similar to the one in the centralized setting. We improve upon existing
convergence bounds for both the centralized and decentralized versions. The
proposed method is especially relevant for decentralized applications such as
distributed recommender systems, where privacy concerns are paramount.",stat.ML
Stein Variational Newton Neural Network Ensembles,"Deep neural network ensembles are powerful tools for uncertainty
quantification, which have recently been re-interpreted from a Bayesian
perspective. However, current methods inadequately leverage second-order
information of the loss landscape, despite the recent availability of efficient
Hessian approximations. We propose a novel approximate Bayesian inference
method that modifies deep ensembles to incorporate Stein Variational Newton
updates. Our approach uniquely integrates scalable modern Hessian
approximations, achieving faster convergence and more accurate posterior
distribution approximations. We validate the effectiveness of our method on
diverse regression and classification tasks, demonstrating superior performance
with a significantly reduced number of training epochs compared to existing
ensemble-based methods, while enhancing uncertainty quantification and
robustness against overfitting.",stat.ML
ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer,"Numerous industrial sectors necessitate models capable of providing robust
forecasts across various horizons. Despite the recent strides in crafting
specific architectures for time-series forecasting and developing pre-trained
universal models, a comprehensive examination of their capability in
accommodating varied-horizon forecasting during inference is still lacking.
This paper bridges this gap through the design and evaluation of the Elastic
Time-Series Transformer (ElasTST). The ElasTST model incorporates a
non-autoregressive design with placeholders and structured self-attention
masks, warranting future outputs that are invariant to adjustments in inference
horizons. A tunable version of rotary position embedding is also integrated
into ElasTST to capture time-series-specific periods and enhance adaptability
to different horizons. Additionally, ElasTST employs a multi-scale patch
design, effectively integrating both fine-grained and coarse-grained
information. During the training phase, ElasTST uses a horizon reweighting
strategy that approximates the effect of random sampling across multiple
horizons with a single fixed horizon setting. Through comprehensive experiments
and comparisons with state-of-the-art time-series architectures and
contemporary foundation models, we demonstrate the efficacy of ElasTST's unique
design elements. Our findings position ElasTST as a robust solution for the
practical necessity of varied-horizon forecasting.",stat.ML
OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning,"Semi-supervised learning (SSL) offers a robust framework for harnessing the
potential of unannotated data. Traditionally, SSL mandates that all classes
possess labeled instances. However, the emergence of open-world SSL (OwSSL)
introduces a more practical challenge, wherein unlabeled data may encompass
samples from unseen classes. This scenario leads to misclassification of unseen
classes as known ones, consequently undermining classification accuracy. To
overcome this challenge, this study revisits two methodologies from
self-supervised and semi-supervised learning, self-labeling and consistency,
tailoring them to address the OwSSL problem. Specifically, we propose an
effective framework called OwMatch, combining conditional self-labeling and
open-world hierarchical thresholding. Theoretically, we analyze the estimation
of class distribution on unlabeled data through rigorous statistical analysis,
thus demonstrating that OwMatch can ensure the unbiasedness of the self-label
assignment estimator with reliability. Comprehensive empirical analyses
demonstrate that our method yields substantial performance enhancements across
both known and unknown classes in comparison to previous studies. Code is
available at https://github.com/niusj03/OwMatch.",stat.ML
Risk-sensitive control as inference with Rényi divergence,"This paper introduces the risk-sensitive control as inference (RCaI) that
extends CaI by using R\'{e}nyi divergence variational inference. RCaI is shown
to be equivalent to log-probability regularized risk-sensitive control, which
is an extension of the maximum entropy (MaxEnt) control. We also prove that the
risk-sensitive optimal policy can be obtained by solving a soft Bellman
equation, which reveals several equivalences between RCaI, MaxEnt control, the
optimal posterior for CaI, and linearly-solvable control. Moreover, based on
RCaI, we derive the risk-sensitive reinforcement learning (RL) methods: the
policy gradient and the soft actor-critic. As the risk-sensitivity parameter
vanishes, we recover the risk-neutral CaI and RL, which means that RCaI is a
unifying framework. Furthermore, we give another risk-sensitive generalization
of the MaxEnt control using R\'{e}nyi entropy regularization. We show that in
both of our extensions, the optimal policies have the same structure even
though the derivations are very different.",stat.ML
Fixing the Loose Brake: Exponential-Tailed Stopping Time in Best Arm Identification,"The best arm identification problem requires identifying the best alternative
(i.e., arm) in active experimentation using the smallest number of experiments
(i.e., arm pulls), which is crucial for cost-efficient and timely
decision-making processes. In the fixed confidence setting, an algorithm must
stop data-dependently and return the estimated best arm with a correctness
guarantee. Since this stopping time is random, we desire its distribution to
have light tails. Unfortunately, many existing studies focus on high
probability or in expectation bounds on the stopping time, which allow heavy
tails and, for high probability bounds, even not stopping at all. We first
prove that this never-stopping event can indeed happen for some popular
algorithms. Motivated by this, we propose algorithms that provably enjoy an
exponential-tailed stopping time, which improves upon the polynomial tail bound
reported by Kalyanakrishnan et al. (2012). The first algorithm is based on a
fixed budget algorithm called Sequential Halving along with a doubling trick.
The second algorithm is a meta algorithm that takes in any fixed confidence
algorithm with a high probability stopping guarantee and turns it into one that
enjoys an exponential-tailed stopping time. Our results imply that there is
much more to be desired for contemporary fixed confidence algorithms.",stat.ML
Clustering Based on Density Propagation and Subcluster Merging,"We propose the DPSM method, a density-based node clustering approach that
automatically determines the number of clusters and can be applied in both data
space and graph space. Unlike traditional density-based clustering methods,
which necessitate calculating the distance between any two nodes, our proposed
technique determines density through a propagation process, thereby making it
suitable for a graph space. In DPSM, nodes are partitioned into small clusters
based on propagated density. The partitioning technique has been proved to be
sound and complete. We then extend the concept of spectral clustering from
individual nodes to these small clusters, while introducing the CluCut measure
to guide cluster merging. This measure is modified in various ways to account
for cluster properties, thus provides guidance on when to terminate the merging
process. Various experiments have validated the effectiveness of DOSM and the
accuracy of these conclusions.",stat.ML
Mitigating Spurious Correlations via Disagreement Probability,"Models trained with empirical risk minimization (ERM) are prone to be biased
towards spurious correlations between target labels and bias attributes, which
leads to poor performance on data groups lacking spurious correlations. It is
particularly challenging to address this problem when access to bias labels is
not permitted. To mitigate the effect of spurious correlations without bias
labels, we first introduce a novel training objective designed to robustly
enhance model performance across all data samples, irrespective of the presence
of spurious correlations. From this objective, we then derive a debiasing
method, Disagreement Probability based Resampling for debiasing (DPR), which
does not require bias labels. DPR leverages the disagreement between the target
label and the prediction of a biased model to identify bias-conflicting
samples-those without spurious correlations-and upsamples them according to the
disagreement probability. Empirical evaluations on multiple benchmarks
demonstrate that DPR achieves state-of-the-art performance over existing
baselines that do not use bias labels. Furthermore, we provide a theoretical
analysis that details how DPR reduces dependency on spurious correlations.",stat.ML
A General Recipe for Contractive Graph Neural Networks -- Technical Report,"Graph Neural Networks (GNNs) have gained significant popularity for learning
representations of graph-structured data due to their expressive power and
scalability. However, despite their success in domains such as social network
analysis, recommendation systems, and bioinformatics, GNNs often face
challenges related to stability, generalization, and robustness to noise and
adversarial attacks. Regularization techniques have shown promise in addressing
these challenges by controlling model complexity and improving robustness.
Building on recent advancements in contractive GNN architectures, this paper
presents a novel method for inducing contractive behavior in any GNN through
SVD regularization. By deriving a sufficient condition for contractiveness in
the update step and applying constraints on network parameters, we demonstrate
the impact of SVD regularization on the Lipschitz constant of GNNs. Our
findings highlight the role of SVD regularization in enhancing the stability
and generalization of GNNs, contributing to the development of more robust
graph-based learning algorithms dynamics.",stat.ML
Conformal Risk Minimization with Variance Reduction,"Conformal prediction (CP) is a distribution-free framework for achieving
probabilistic guarantees on black-box models. CP is generally applied to a
model post-training. Recent research efforts, on the other hand, have focused
on optimizing CP efficiency during training. We formalize this concept as the
problem of conformal risk minimization (CRM). In this direction, conformal
training (ConfTr) by Stutz et al.(2022) is a technique that seeks to minimize
the expected prediction set size of a model by simulating CP in-between
training updates. Despite its potential, we identify a strong source of sample
inefficiency in ConfTr that leads to overly noisy estimated gradients,
introducing training instability and limiting practical use. To address this
challenge, we propose variance-reduced conformal training (VR-ConfTr), a CRM
method that incorporates a variance reduction technique in the gradient
estimation of the ConfTr objective function. Through extensive experiments on
various benchmark datasets, we demonstrate that VR-ConfTr consistently achieves
faster convergence and smaller prediction sets compared to baselines.",stat.ML
Guiding Genetic Programming with Graph Neural Networks,"In evolutionary computation, it is commonly assumed that a search algorithm
acquires knowledge about a problem instance by sampling solutions from the
search space and evaluating them with a fitness function. This is necessarily
inefficient because fitness reveals very little about solutions -- yet they
contain more information that can be potentially exploited. To address this
observation in genetic programming, we propose EvoNUDGE, which uses a graph
neural network to elicit additional knowledge from symbolic regression
problems. The network is queried on the problem before an evolutionary run to
produce a library of subprograms, which is subsequently used to seed the
initial population and bias the actions of search operators. In an extensive
experiment on a large number of problem instances, EvoNUDGE is shown to
significantly outperform multiple baselines, including the conventional
tree-based genetic programming and the purely neural variant of the method.",stat.ML
Multiclass Transductive Online Learning,"We consider the problem of multiclass transductive online learning when the
number of labels can be unbounded. Previous works by Ben-David et al. [1997]
and Hanneke et al. [2023b] only consider the case of binary and finite label
spaces, respectively. The latter work determined that their techniques fail to
extend to the case of unbounded label spaces, and they pose the question of
characterizing the optimal mistake bound for unbounded label spaces. We answer
this question by showing that a new dimension, termed the Level-constrained
Littlestone dimension, characterizes online learnability in this setting. Along
the way, we show that the trichotomy of possible minimax rates of the expected
number of mistakes established by Hanneke et al. [2023b] for finite label
spaces in the realizable setting continues to hold even when the label space is
unbounded. In particular, if the learner plays for $T \in \mathbb{N}$ rounds,
its minimax expected number of mistakes can only grow like $\Theta(T)$,
$\Theta(\log T)$, or $\Theta(1)$. To prove this result, we give another
combinatorial dimension, termed the Level-constrained Branching dimension, and
show that its finiteness characterizes constant minimax expected
mistake-bounds. The trichotomy is then determined by a combination of the
Level-constrained Littlestone and Branching dimensions. Quantitatively, our
upper bounds improve upon existing multiclass upper bounds in Hanneke et al.
[2023b] by removing the dependence on the label set size. In doing so, we
explicitly construct learning algorithms that can handle extremely large or
unbounded label spaces. A key and novel component of our algorithm is a new
notion of shattering that exploits the sequential nature of transductive online
learning. Finally, we complete our results by proving expected regret bounds in
the agnostic setting, extending the result of Hanneke et al. [2023b].",stat.ML
"Denoising Diffusions with Optimal Transport: Localization, Curvature, and Multi-Scale Complexity","Adding noise is easy; what about denoising? Diffusion is easy; what about
reverting a diffusion? Diffusion-based generative models aim to denoise a
Langevin diffusion chain, moving from a log-concave equilibrium measure $\nu$,
say isotropic Gaussian, back to a complex, possibly non-log-concave initial
measure $\mu$. The score function performs denoising, going backward in time,
predicting the conditional mean of the past location given the current. We show
that score denoising is the optimal backward map in transportation cost. What
is its localization uncertainty? We show that the curvature function determines
this localization uncertainty, measured as the conditional variance of the past
location given the current. We study in this paper the effectiveness of the
diffuse-then-denoise process: the contraction of the forward diffusion chain,
offset by the possible expansion of the backward denoising chain, governs the
denoising difficulty. For any initial measure $\mu$, we prove that this offset
net contraction at time $t$ is characterized by the curvature complexity of a
smoothed $\mu$ at a specific signal-to-noise ratio (SNR) scale $r(t)$. We
discover that the multi-scale curvature complexity collectively determines the
difficulty of the denoising chain. Our multi-scale complexity quantifies a
fine-grained notion of average-case curvature instead of the worst-case.
Curiously, it depends on an integrated tail function, measuring the relative
mass of locations with positive curvature versus those with negative curvature;
denoising at a specific SNR scale is easy if such an integrated tail is light.
We conclude with several non-log-concave examples to demonstrate how the
multi-scale complexity probes the bottleneck SNR for the diffuse-then-denoise
process.",stat.ML
Counterfactual explainability of black-box prediction models,"It is crucial to be able to explain black-box prediction models to use them
effectively and safely in practice. Most existing tools for model explanations
are associational rather than causal, and we use two paradoxical examples to
show that such explanations are generally inadequate. Motivated by the concept
of genetic heritability in twin studies, we propose a new notion called
counterfactual explainability for black-box prediction models. Counterfactual
explainability has three key advantages: (1) it leverages counterfactual
outcomes and extends methods for global sensitivity analysis (such as
functional analysis of variance and Sobol's indices) to a causal setting; (2)
it is defined not only for the totality of a set of input factors but also for
their interactions (indeed, it is a probability measure on a whole
``explanation algebra''); (3) it also applies to dependent input factors whose
causal relationship can be modeled by a directed acyclic graph, thus
incorporating causal mechanisms into the explanation.",stat.ML
Strategic Conformal Prediction,"When a machine learning model is deployed, its predictions can alter its
environment, as better informed agents strategize to suit their own interests.
With such alterations in mind, existing approaches to uncertainty
quantification break. In this work we propose a new framework, Strategic
Conformal Prediction, which is capable of robust uncertainty quantification in
such a setting. Strategic Conformal Prediction is backed by a series of
theoretical guarantees spanning marginal coverage, training-conditional
coverage, tightness and robustness to misspecification that hold in a
distribution-free manner. Experimental analysis further validates our method,
showing its remarkable effectiveness in face of arbitrary strategic
alterations, whereas other methods break.",stat.ML
Online Graph Learning via Time-Vertex Adaptive Filters: From Theory to Cardiac Fibrillation,"Graph Signal Processing (GSP) provides a powerful framework for analysing
complex, interconnected systems by modelling data as signals on graphs. Recent
advances in GSP have enabled the learning of graph structures from observed
signals, but these methods often struggle with time-varying systems and
real-time applications. Adaptive filtering techniques, while effective for
online learning, have seen limited application in graph topology estimation
from a GSP perspective. To this end, we introduce AdaCGP, an online algorithm
for adaptive estimation of the Graph Shift Operator (GSO) from multivariate
time series. The GSO is estimated from an adaptive time-vertex autoregressive
model through recursive update formulae designed to address sparsity,
shift-invariance and bias. Through simulations, we show that AdaCGP performs
consistently well across various graph topologies, and achieves improvements in
excess of 82% for GSO estimation compared to baseline adaptive vector
autoregressive models. In addition, our online variable splitting approach for
enforcing sparsity enables near-perfect precision in identifying causal
connections while maintaining low false positive rates upon optimisation of the
forecast error. Finally, AdaCGP's ability to track changes in graph structure
is demonstrated on recordings of ventricular fibrillation dynamics in response
to an anti-arrhythmic drug. AdaCGP is shown to be able to identify the
stability of critical conduction patterns that may be maintaining the
arrhythmia in an intuitive way, together with its potential to support
diagnosis and treatment strategies.",stat.ML
Statistical guarantees for denoising reflected diffusion models,"In recent years, denoising diffusion models have become a crucial area of
research due to their abundance in the rapidly expanding field of generative
AI. While recent statistical advances have delivered explanations for the
generation ability of idealised denoising diffusion models for high-dimensional
target data, implementations introduce thresholding procedures for the
generating process to overcome issues arising from the unbounded state space of
such models. This mismatch between theoretical design and implementation of
diffusion models has been addressed empirically by using a \emph{reflected}
diffusion process as the driver of noise instead. In this paper, we study
statistical guarantees of these denoising reflected diffusion models. In
particular, we establish minimax optimal rates of convergence in total
variation, up to a polylogarithmic factor, under Sobolev smoothness
assumptions. Our main contributions include the statistical analysis of this
novel class of denoising reflected diffusion models and a refined score
approximation method in both time and space, leveraging spectral decomposition
and rigorous neural network analysis.",stat.ML
Adaptive Conformal Inference by Particle Filtering under Hidden Markov Models,"Conformal inference is a statistical method used to construct prediction sets
for point predictors, providing reliable uncertainty quantification with
probability guarantees. This method utilizes historical labeled data to
estimate the conformity or nonconformity between predictions and true labels.
However, conducting conformal inference for hidden states under hidden Markov
models (HMMs) presents a significant challenge, as the hidden state data is
unavailable, resulting in the absence of a true label set to serve as a
conformal calibration set. This paper proposes an adaptive conformal inference
framework that leverages a particle filtering approach to address this issue.
Rather than directly focusing on the unobservable hidden state, we innovatively
use weighted particles as an approximation of the actual posterior distribution
of the hidden state. Our goal is to produce prediction sets that encompass
these particles to achieve a specific aggregate weight sum, referred to as the
aggregated coverage level. The proposed framework can adapt online to the
time-varying distribution of data and achieve the defined marginal aggregated
coverage level in both one-step and multi-step inference over the long term. We
verify the effectiveness of this approach through a real-time target
localization simulation study.",stat.ML
G-SPARC: SPectral ARchitectures tackling the Cold-start problem in Graph learning,"Graphs play a central role in modeling complex relationships across various
domains. Most graph learning methods rely heavily on neighborhood information,
raising the question of how to handle cold-start nodes - nodes with no known
connections within the graph. These models often overlook the cold-start nodes,
making them ineffective for real-world scenarios. To tackle this, we propose
G-SPARC, a novel framework addressing cold-start nodes, that leverages
generalizable spectral embedding. This framework enables extension to
state-of-the-art methods making them suitable for practical applications. By
utilizing a key idea of transitioning from graph representation to spectral
representation, our approach is generalizable to cold-start nodes, capturing
the global structure of the graph without relying on adjacency data.
Experimental results demonstrate that our method outperforms existing models on
cold-start nodes across various tasks like node classification, node
clustering, and link prediction. G-SPARC provides a breakthrough built-in
solution to the cold-start problem in graph learning. Our code will be publicly
available upon acceptance.",stat.ML
Educational Effects in Mathematics: Conditional Average Treatment Effect depending on the Number of Treatments,"This study examines the educational effect of the Academic Support Center at
Kogakuin University. Following the initial assessment, it was suggested that
group bias had led to an underestimation of the Center's true impact. To
address this issue, the authors applied the theory of causal inference. By
using T-learner, the conditional average treatment effect (CATE) of the
Center's face-to-face (F2F) personal assistance program was evaluated.
Extending T-learner, the authors produced a new CATE function that depends on
the number of treatments (F2F sessions) and used the estimated function to
predict the CATE performance of F2F assistance.",stat.ML
DSDE: Using Proportion Estimation to Improve Model Selection for Out-of-Distribution Detection,"Model library is an effective tool for improving the performance of
single-model Out-of-Distribution (OoD) detector, mainly through model selection
and detector fusion. However, existing methods in the literature do not provide
uncertainty quantification for model selection results. Additionally, the model
ensemble process primarily focuses on controlling the True Positive Rate (TPR)
while neglecting the False Positive Rate (FPR). In this paper, we emphasize the
significance of the proportion of models in the library that identify the test
sample as an OoD sample. This proportion holds crucial information and directly
influences the error rate of OoD detection.To address this, we propose
inverting the commonly-used sequential p-value strategies. We define the
rejection region initially and then estimate the error rate. Furthermore, we
introduce a novel perspective from change-point detection and propose an
approach for proportion estimation with automatic hyperparameter selection. We
name the proposed approach as DOS-Storey-based Detector Ensemble (DSDE).
Experimental results on CIFAR10 and CIFAR100 demonstrate the effectiveness of
our approach in tackling OoD detection challenges. Specifically, the CIFAR10
experiments show that DSDE reduces the FPR from 11.07% to 3.31% compared to the
top-performing single-model detector.",stat.ML
Scaling Laws with Hidden Structure,"Statistical learning in high-dimensional spaces is challenging without a
strong underlying data structure. Recent advances with foundational models
suggest that text and image data contain such hidden structures, which help
mitigate the curse of dimensionality. Inspired by results from nonparametric
statistics, we hypothesize that this phenomenon can be partially explained in
terms of decomposition of complex tasks into simpler subtasks. In this paper,
we present a controlled experimental framework to test whether neural networks
can indeed exploit such ``hidden factorial structures.'' We find that they do
leverage these latent patterns to learn discrete distributions more
efficiently, and derive scaling laws linking model sizes, hidden
factorizations, and accuracy. We also study the interplay between our
structural assumptions and the models' capacity for generalization.",stat.ML
Network Causal Effect Estimation In Graphical Models Of Contagion And Latent Confounding,"A key question in many network studies is whether the observed correlations
between units are primarily due to contagion or latent confounding. Here, we
study this question using a segregated graph (Shpitser, 2015) representation of
these mechanisms, and examine how uncertainty about the true underlying
mechanism impacts downstream computation of network causal effects,
particularly under full interference -- settings where we only have a single
realization of a network and each unit may depend on any other unit in the
network. Under certain assumptions about asymptotic growth of the network, we
derive likelihood ratio tests that can be used to identify whether different
sets of variables -- confounders, treatments, and outcomes -- across units
exhibit dependence due to contagion or latent confounding. We then propose
network causal effect estimation strategies that provide unbiased and
consistent estimates if the dependence mechanisms are either known or correctly
inferred using our proposed tests. Together, the proposed methods allow network
effect estimation in a wider range of full interference scenarios that have not
been considered in prior work. We evaluate the effectiveness of our methods
with synthetic data and the validity of our assumptions using real-world
networks.",stat.ML
The Implicit Bias of Gradient Descent on Separable Multiclass Data,"Implicit bias describes the phenomenon where optimization-based training
algorithms, without explicit regularization, show a preference for simple
estimators even when more complex estimators have equal objective values.
Multiple works have developed the theory of implicit bias for binary
classification under the assumption that the loss satisfies an exponential tail
property. However, there is a noticeable gap in analysis for multiclass
classification, with only a handful of results which themselves are restricted
to the cross-entropy loss. In this work, we employ the framework of Permutation
Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to
introduce a multiclass extension of the exponential tail property. This class
of losses includes not only cross-entropy but also other losses. Using this
framework, we extend the implicit bias result of Soudry et al. [2018] to
multiclass classification. Furthermore, our proof techniques closely mirror
those of the binary case, thus illustrating the power of the PERM framework for
bridging the binary-multiclass gap.",stat.ML
Generalized Eigenvalue Problems with Generative Priors,"Generalized eigenvalue problems (GEPs) find applications in various fields of
science and engineering. For example, principal component analysis, Fisher's
discriminant analysis, and canonical correlation analysis are specific
instances of GEPs and are widely used in statistical data processing. In this
work, we study GEPs under generative priors, assuming that the underlying
leading generalized eigenvector lies within the range of a Lipschitz continuous
generative model. Under appropriate conditions, we show that any optimal
solution to the corresponding optimization problems attains the optimal
statistical rate. Moreover, from a computational perspective, we propose an
iterative algorithm called the Projected Rayleigh Flow Method (PRFM) to
approximate the optimal solution. We theoretically demonstrate that under
suitable assumptions, PRFM converges linearly to an estimated vector that
achieves the optimal statistical rate. Numerical results are provided to
demonstrate the effectiveness of the proposed method.",stat.ML
FEET: A Framework for Evaluating Embedding Techniques,"In this study, we introduce FEET, a standardized protocol designed to guide
the development and benchmarking of foundation models. While numerous benchmark
datasets exist for evaluating these models, we propose a structured evaluation
protocol across three distinct scenarios to gain a comprehensive understanding
of their practical performance. We define three primary use cases: frozen
embeddings, few-shot embeddings, and fully fine-tuned embeddings. Each scenario
is detailed and illustrated through two case studies: one in sentiment analysis
and another in the medical domain, demonstrating how these evaluations provide
a thorough assessment of foundation models' effectiveness in research
applications. We recommend this protocol as a standard for future research
aimed at advancing representation learning models.",stat.ML
MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning,"Real-world machine learning applications often face simultaneous covariate
and semantic shifts, challenging traditional domain generalization and
out-of-distribution (OOD) detection methods. We introduce Meta-learned Across
Domain Out-of-distribution Detection (MADOD), a novel framework designed to
address both shifts concurrently. MADOD leverages meta-learning and
G-invariance to enhance model generalizability and OOD detection in unseen
domains. Our key innovation lies in task construction: we randomly designate
in-distribution classes as pseudo-OODs within each meta-learning task,
simulating OOD scenarios using existing data. This approach, combined with
energy-based regularization, enables the learning of robust, domain-invariant
features while calibrating decision boundaries for effective OOD detection.
Operating in a test domain-agnostic setting, MADOD eliminates the need for
adaptation during inference, making it suitable for scenarios where test data
is unavailable. Extensive experiments on real-world and synthetic datasets
demonstrate MADOD's superior performance in semantic OOD detection across
unseen domains, achieving an AUPR improvement of 8.48% to 20.81%, while
maintaining competitive in-distribution classification accuracy, representing a
significant advancement in handling both covariate and semantic shifts.",stat.ML
Marginal Causal Flows for Validation and Inference,"Investigating the marginal causal effect of an intervention on an outcome
from complex data remains challenging due to the inflexibility of employed
models and the lack of complexity in causal benchmark datasets, which often
fail to reproduce intricate real-world data patterns. In this paper we
introduce Frugal Flows, a novel likelihood-based machine learning model that
uses normalising flows to flexibly learn the data-generating process, while
also directly inferring the marginal causal quantities from observational data.
We propose that these models are exceptionally well suited for generating
synthetic data to validate causal methods. They can create synthetic datasets
that closely resemble the empirical dataset, while automatically and exactly
satisfying a user-defined average treatment effect. To our knowledge, Frugal
Flows are the first generative model to both learn flexible data
representations and also exactly parameterise quantities such as the average
treatment effect and the degree of unobserved confounding. We demonstrate the
above with experiments on both simulated and real-world datasets.",stat.ML
The impact of MRI image quality on statistical and predictive analysis on voxel based morphology,"Image Quality of MRI brain scans is strongly influenced by within scanner
head movements and the resulting image artifacts alter derived measures like
brain volume and cortical thickness. Automated image quality assessment is key
to controlling for confounding effects of poor image quality. In this study, we
systematically test for the influence of image quality on univariate statistics
and machine learning classification. We analyzed group effects of sex/gender on
local brain volume and made predictions of sex/gender using logistic
regression, while correcting for brain size. From three large publicly
available datasets, two age and sex-balanced samples were derived to test the
generalizability of the effect for pooled sample sizes of n=760 and n=1094.
Results of the Bonferroni corrected t-tests over 3747 gray matter features
showed a strong influence of low-quality data on the ability to find
significant sex/gender differences for the smaller sample. Increasing sample
size and more so image quality showed a stark increase in detecting significant
effects in univariate group comparisons. For the classification of sex/gender
using logistic regression, both increasing sample size and image quality had a
marginal effect on the Area under the Receiver Operating Characteristic Curve
for most datasets and subsamples. Our results suggest a more stringent quality
control for univariate approaches than for multivariate classification with a
leaning towards higher quality for classical group statistics and bigger sample
sizes for machine learning applications in neuroimaging.",stat.ML
ProGen: Revisiting Probabilistic Spatial-Temporal Time Series Forecasting from a Continuous Generative Perspective Using Stochastic Differential Equations,"Accurate forecasting of spatiotemporal data remains challenging due to
complex spatial dependencies and temporal dynamics. The inherent uncertainty
and variability in such data often render deterministic models insufficient,
prompting a shift towards probabilistic approaches, where diffusion-based
generative models have emerged as effective solutions. In this paper, we
present ProGen, a novel framework for probabilistic spatiotemporal time series
forecasting that leverages Stochastic Differential Equations (SDEs) and
diffusion-based generative modeling techniques in the continuous domain. By
integrating a novel denoising score model, graph neural networks, and a
tailored SDE, ProGen provides a robust solution that effectively captures
spatiotemporal dependencies while managing uncertainty. Our extensive
experiments on four benchmark traffic datasets demonstrate that ProGen
outperforms state-of-the-art deterministic and probabilistic models. This work
contributes a continuous, diffusion-based generative approach to spatiotemporal
forecasting, paving the way for future research in probabilistic modeling and
stochastic processes.",stat.ML
Conformalized High-Density Quantile Regression via Dynamic Prototypes-based Probability Density Estimation,"Recent methods in quantile regression have adopted a classification
perspective to handle challenges posed by heteroscedastic, multimodal, or
skewed data by quantizing outputs into fixed bins. Although these
regression-as-classification frameworks can capture high-density prediction
regions and bypass convex quantile constraints, they are restricted by
quantization errors and the curse of dimensionality due to a constant number of
bins per dimension. To address these limitations, we introduce a conformalized
high-density quantile regression approach with a dynamically adaptive set of
prototypes. Our method optimizes the set of prototypes by adaptively adding,
deleting, and relocating quantization bins throughout the training process.
Moreover, our conformal scheme provides valid coverage guarantees, focusing on
regions with the highest probability density. Experiments across diverse
datasets and dimensionalities confirm that our method consistently achieves
high-quality prediction regions with enhanced coverage and robustness, all
while utilizing fewer prototypes and memory, ensuring scalability to higher
dimensions. The code is available at https://github.com/batuceng/max_quantile .",stat.ML
Hierarchical and Density-based Causal Clustering,"Understanding treatment effect heterogeneity is vital for scientific and
policy research. However, identifying and evaluating heterogeneous treatment
effects pose significant challenges due to the typically unknown subgroup
structure. Recently, a novel approach, causal k-means clustering, has emerged
to assess heterogeneity of treatment effect by applying the k-means algorithm
to unknown counterfactual regression functions. In this paper, we expand upon
this framework by integrating hierarchical and density-based clustering
algorithms. We propose plug-in estimators that are simple and readily
implementable using off-the-shelf algorithms. Unlike k-means clustering, which
requires the margin condition, our proposed estimators do not rely on strong
structural assumptions on the outcome process. We go on to study their rate of
convergence, and show that under the minimal regularity conditions, the
additional cost of causal clustering is essentially the estimation error of the
outcome regression functions. Our findings significantly extend the
capabilities of the causal clustering framework, thereby contributing to the
progression of methodologies for identifying homogeneous subgroups in treatment
response, consequently facilitating more nuanced and targeted interventions.
The proposed methods also open up new avenues for clustering with generic
pseudo-outcomes. We explore finite sample properties via simulation, and
illustrate the proposed methods in voting and employment projection datasets.",stat.ML
XNB: Explainable Class-Specific NaIve-Bayes Classifier,"In today's data-intensive landscape, where high-dimensional datasets are
increasingly common, reducing the number of input features is essential to
prevent overfitting and improve model accuracy. Despite numerous efforts to
tackle dimensionality reduction, most approaches apply a universal set of
features across all classes, potentially missing the unique characteristics of
individual classes. This paper presents the Explainable Class-Specific Naive
Bayes (XNB) classifier, which introduces two critical innovations: 1) the use
of Kernel Density Estimation to calculate posterior probabilities, allowing for
a more accurate and flexible estimation process, and 2) the selection of
class-specific feature subsets, ensuring that only the most relevant variables
for each class are utilized. Extensive empirical analysis on high-dimensional
genomic datasets shows that XNB matches the classification performance of
traditional Naive Bayes while drastically improving model interpretability. By
isolating the most relevant features for each class, XNB not only reduces the
feature set to a minimal, distinct subset for each class but also provides
deeper insights into how the model makes predictions. This approach offers
significant advantages in fields where both precision and explainability are
critical.",stat.ML
Federated Learning with Relative Fairness,"This paper proposes a federated learning framework designed to achieve
\textit{relative fairness} for clients. Traditional federated learning
frameworks typically ensure absolute fairness by guaranteeing minimum
performance across all client subgroups. However, this approach overlooks
disparities in model performance between subgroups. The proposed framework uses
a minimax problem approach to minimize relative unfairness, extending previous
methods in distributionally robust optimization (DRO). A novel fairness index,
based on the ratio between large and small losses among clients, is introduced,
allowing the framework to assess and improve the relative fairness of trained
models. Theoretical guarantees demonstrate that the framework consistently
reduces unfairness. We also develop an algorithm, named \textsc{Scaff-PD-IA},
which balances communication and computational efficiency while maintaining
minimax-optimal convergence rates. Empirical evaluations on real-world datasets
confirm its effectiveness in maintaining model performance while reducing
disparity.",stat.ML
Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing,"Federated Learning (FL) is essential for efficient data exchange in Internet
of Things (IoT) environments, as it trains Machine Learning (ML) models locally
and shares only model updates. However, FL is vulnerable to privacy threats
like model inversion and membership inference attacks, which can expose
sensitive training data. To address these privacy concerns, Differential
Privacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML
models degrades performance, especially in dynamic IoT systems where
continuous, lifelong FL learning accumulates excessive noise over time. To
mitigate this issue, we introduce Federated HyperDimensional computing with
Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)
framework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy
carefully manages the balance between privacy and performance by theoretically
tracking cumulative noise from previous rounds and adding only the necessary
incremental noise to meet privacy requirements. In a real-world case study
involving in-process monitoring of manufacturing machining operations,
FedHDPrivacy demonstrates robust performance, outperforming standard FL
frameworks-including Federated Averaging (FedAvg), Federated Stochastic
Gradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized
Averaging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy
also shows potential for future enhancements, such as multimodal data fusion.",stat.ML
Axiomatic Explainer Globalness via Optimal Transport,"Explainability methods are often challenging to evaluate and compare. With a
multitude of explainers available, practitioners must often compare and select
explainers based on quantitative evaluation metrics. One particular
differentiator between explainers is the diversity of explanations for a given
dataset; i.e. whether all explanations are identical, unique and uniformly
distributed, or somewhere between these two extremes. In this work, we define a
complexity measure for explainers, globalness, which enables deeper
understanding of the distribution of explanations produced by feature
attribution and feature selection methods for a given dataset. We establish the
axiomatic properties that any such measure should possess and prove that our
proposed measure, Wasserstein Globalness, meets these criteria. We validate the
utility of Wasserstein Globalness using image, tabular, and synthetic datasets,
empirically showing that it both facilitates meaningful comparison between
explainers and improves the selection process for explainability methods.",stat.ML
Relax and Merge: A Simple Yet Effective Framework for Solving Fair $k$-Means and $k$-sparse Wasserstein Barycenter Problems,"The fairness of clustering algorithms has gained widespread attention across
various areas, including machine learning, In this paper, we study fair
$k$-means clustering in Euclidean space. Given a dataset comprising several
groups, the fairness constraint requires that each cluster should contain a
proportion of points from each group within specified lower and upper bounds.
Due to these fairness constraints, determining the optimal locations of $k$
centers is a quite challenging task. We propose a novel ``Relax and Merge''
framework that returns a $(1+4\rho + O(\epsilon))$-approximate solution, where
$\rho$ is the approximate ratio of an off-the-shelf vanilla $k$-means algorithm
and $O(\epsilon)$ can be an arbitrarily small positive number. If equipped with
a PTAS of $k$-means, our solution can achieve an approximation ratio of
$(5+O(\epsilon))$ with only a slight violation of the fairness constraints,
which improves the current state-of-the-art approximation guarantee.
Furthermore, using our framework, we can also obtain a $(1+4\rho
+O(\epsilon))$-approximate solution for the $k$-sparse Wasserstein Barycenter
problem, which is a fundamental optimization problem in the field of optimal
transport, and a $(2+6\rho)$-approximate solution for the strictly fair
$k$-means clustering with no violation, both of which are better than the
current state-of-the-art methods. In addition, the empirical results
demonstrate that our proposed algorithm can significantly outperform baseline
approaches in terms of clustering cost.",stat.ML
Artificial Intelligence for Microbiology and Microbiome Research,"Advancements in artificial intelligence (AI) have transformed many scientific
fields, with microbiology and microbiome research now experiencing significant
breakthroughs through machine learning and deep learning applications. This
review provides a comprehensive overview of AI-driven approaches tailored for
microbiology and microbiome studies, emphasizing both technical advancements
and biological insights. We begin with an introduction to foundational AI
techniques, including primary machine learning paradigms and various deep
learning architectures, and offer guidance on choosing between machine learning
and deep learning methods based on specific research goals. The primary section
on application scenarios spans diverse research areas, from taxonomic
profiling, functional annotation & prediction, microbe-X interactions,
microbial ecology, metabolic modeling, precision nutrition, clinical
microbiology, to prevention & therapeutics. Finally, we discuss challenges
unique to this field, including the balance between interpretability and
complexity, the ""small n, large p"" problem, and the critical need for
standardized benchmarking datasets to validate and compare models. Together,
this review underscores AI's transformative role in microbiology and microbiome
research, paving the way for innovative methodologies and applications that
enhance our understanding of microbial life and its impact on our planet and
our health.",stat.ML
An unified approach to link prediction in collaboration networks,"This article investigates and compares three approaches to link prediction in
colaboration networks, namely, an ERGM (Exponential Random Graph Model; Robins
et al. 2007), a GCN (Graph Convolutional Network; Kipf and Welling 2017), and a
Word2Vec+MLP model (Word2Vec model combined with a multilayer neural network;
Mikolov et al. 2013a and Goodfellow et al. 2016). The ERGM, grounded in
statistical methods, is employed to capture general structural patterns within
the network, while the GCN and Word2Vec+MLP models leverage deep learning
techniques to learn adaptive structural representations of nodes and their
relationships. The predictive performance of the models is assessed through
extensive simulation exercises using cross-validation, with metrics based on
the receiver operating characteristic curve. The results clearly show the
superiority of machine learning approaches in link prediction, particularly in
large networks, where traditional models such as ERGM exhibit limitations in
scalability and the ability to capture inherent complexities. These findings
highlight the potential benefits of integrating statistical modeling techniques
with deep learning methods to analyze complex networks, providing a more robust
and effective framework for future research in this field.",stat.ML
Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired
data-for example, images and their corresponding text captions-to learn general
representations that transfer efficiently to downstream tasks. While such
approaches are generally applied to two modalities, domains such as robotics,
healthcare, and video need to support many types of data at once. We show that
the pairwise application of CLIP fails to capture joint information between
modalities, thereby limiting the quality of the learned representations. To
address this issue, we present Symile, a simple contrastive learning approach
that captures higher-order information between any number of modalities. Symile
provides a flexible, architecture-agnostic objective for learning
modality-specific representations. To develop Symile's objective, we derive a
lower bound on total correlation, and show that Symile representations for any
set of modalities form a sufficient statistic for predicting the remaining
modalities. Symile outperforms pairwise CLIP, even with modalities missing in
the data, on cross-modal classification and retrieval across several
experiments including on an original multilingual dataset of 33M image, text
and audio samples and a clinical dataset of chest X-rays, electrocardiograms,
and laboratory measurements. All datasets and code used in this work are
publicly available at https://github.com/rajesh-lab/symile.",stat.ML
Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference,"Model selection in Gaussian processes scales prohibitively with the size of
the training dataset, both in time and memory. While many approximations exist,
all incur inevitable approximation error. Recent work accounts for this error
in the form of computational uncertainty, which enables -- at the cost of
quadratic complexity -- an explicit tradeoff between computation and precision.
Here we extend this development to model selection, which requires significant
enhancements to the existing approach, including linear-time scaling in the
size of the dataset. We propose a novel training loss for hyperparameter
optimization and demonstrate empirically that the resulting method can
outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model
selection, on medium to large-scale datasets. Our experiments show that model
selection for computation-aware GPs trained on 1.8 million data points can be
done within a few hours on a single GPU. As a result of this work, Gaussian
processes can be trained on large-scale datasets without significantly
compromising their ability to quantify uncertainty -- a fundamental
prerequisite for optimal decision-making.",stat.ML
Abstracted Shapes as Tokens -- A Generalizable and Interpretable Model for Time-series Classification,"In time-series analysis, many recent works seek to provide a unified view and
representation for time-series across multiple domains, leading to the
development of foundation models for time-series data. Despite diverse modeling
techniques, existing models are black boxes and fail to provide insights and
explanations about their representations. In this paper, we present VQShape, a
pre-trained, generalizable, and interpretable model for time-series
representation learning and classification. By introducing a novel
representation for time-series data, we forge a connection between the latent
space of VQShape and shape-level features. Using vector quantization, we show
that time-series from different domains can be described using a unified set of
low-dimensional codes, where each code can be represented as an abstracted
shape in the time domain. On classification tasks, we show that the
representations of VQShape can be utilized to build interpretable classifiers,
achieving comparable performance to specialist models. Additionally, in
zero-shot learning, VQShape and its codebook can generalize to previously
unseen datasets and domains that are not included in the pre-training process.
The code and pre-trained weights are available at
https://github.com/YunshiWen/VQShape.",stat.ML
LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models,"Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.",cs.LG
Leonardo vindicated: Pythagorean trees for minimal reconstruction of the natural branching structures,"Trees continue to fascinate with their natural beauty and as engineering
masterpieces optimal with respect to several independent criteria. Pythagorean
tree is a well-known fractal design that realistically mimics the natural tree
branching structures. We study various types of Pythagorean-like fractal trees
with different shapes of the base, branching angles and relaxed scales in an
attempt to identify and explain which variants are the closest match to the
branching structures commonly observed in the natural world. Pursuing
simultaneously the realism and minimalism of the fractal tree model, we have
developed a flexibly parameterised and fast algorithm to grow and visually
examine deep Pythagorean-inspired fractal trees with the capability to orderly
over- or underestimate the Leonardo da Vinci's tree branching rule as well as
control various imbalances and branching angles. We tested the realism of the
generated fractal tree images by means of the classification accuracy of
detecting natural tree with the transfer-trained deep Convolutional Neural
Networks (CNNs). Having empirically established the parameters of the fractal
trees that maximize the CNN's natural tree class classification accuracy we
have translated them back to the scales and angles of branches and came to the
interesting conclusions that support the da Vinci branching rule and golden
ratio based scaling for both the shape of the branch and imbalance between the
child branches, and claim the flexibly parameterized fractal trees can be used
to generate artificial examples to train robust detectors of different species
of trees.",cs.LG
Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.",cs.LG
Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings,"Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.",cs.LG
Investigating the Effectiveness of Explainability Methods in Parkinson's Detection from Speech,"Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.",cs.LG
Derivational Morphology Reveals Analogical Generalization in Large Language Models,"What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.",cs.LG
"Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization","Second-order optimization has been shown to accelerate the training of deep
neural networks in many applications, often yielding faster progress per
iteration on the training loss compared to first-order optimizers. However, the
generalization properties of second-order methods are still being debated.
Theoretical investigations have proved difficult to carry out outside the
tractable settings of heavily simplified model classes -- thus, the relevance
of existing theories to practical deep learning applications remains unclear.
Similarly, empirical studies in large-scale models and real datasets are
significantly confounded by the necessity to approximate second-order updates
in practice. It is often unclear whether the observed generalization behaviour
arises specifically from the second-order nature of the parameter updates, or
instead reflects the specific structured (e.g.\ Kronecker) approximations used
or any damping-based interpolation towards first-order updates. Here, we show
for the first time that exact Gauss-Newton (GN) updates take on a tractable
form in a class of deep reversible architectures that are sufficiently
expressive to be meaningfully applied to common benchmark datasets. We exploit
this novel setting to study the training and generalization properties of the
GN optimizer. We find that exact GN generalizes poorly. In the mini-batch
training setting, this manifests as rapidly saturating progress even on the
\emph{training} loss, with parameter updates found to overfit each
mini-batchatch without producing the features that would support generalization
to other mini-batches. We show that our experiments run in the ``lazy'' regime,
in which the neural tangent kernel (NTK) changes very little during the course
of training. This behaviour is associated with having no significant changes in
neural representations, explaining the lack of generalization.",cs.LG
Doubly Robust Regression Discontinuity Designs,"This study introduces a doubly robust (DR) estimator for regression
discontinuity (RD) designs. In RD designs, treatment effects are estimated in a
quasi-experimental setting where treatment assignment depends on whether a
running variable surpasses a predefined cutoff. A common approach in RD
estimation is to apply nonparametric regression methods, such as local linear
regression. In such an approach, the validity relies heavily on the consistency
of nonparametric estimators and is limited by the nonparametric convergence
rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we
propose the DR-RD estimator, which combines two distinct estimators for the
conditional expected outcomes. If either of these estimators is consistent, the
treatment effect estimator remains consistent. Furthermore, due to the
debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if
both regression estimators satisfy certain mild conditions, which also
simplifies statistical inference.",cs.LG
Optimal Control of Mechanical Ventilators with Learned Respiratory Dynamics,"Deciding on appropriate mechanical ventilator management strategies
significantly impacts the health outcomes for patients with respiratory
diseases. Acute Respiratory Distress Syndrome (ARDS) is one such disease that
requires careful ventilator operation to be effectively treated. In this work,
we frame the management of ventilators for patients with ARDS as a sequential
decision making problem using the Markov decision process framework. We
implement and compare controllers based on clinical guidelines contained in the
ARDSnet protocol, optimal control theory, and learned latent dynamics
represented as neural networks. The Pulse Physiology Engine's respiratory
dynamics simulator is used to establish a repeatable benchmark, gather
simulated data, and quantitatively compare these controllers. We score
performance in terms of measured improvement in established ARDS health markers
(pertaining to improved respiratory rate, oxygenation, and vital signs). Our
results demonstrate that techniques leveraging neural networks and optimal
control can automatically discover effective ventilation management strategies
without access to explicit ventilator management procedures or guidelines (such
as those defined in the ARDSnet protocol).",cs.LG
Sleep Staging from Airflow Signals Using Fourier Approximations of Persistence Curves,"Sleep staging is a challenging task, typically manually performed by sleep
technologists based on electroencephalogram and other biosignals of patients
taken during overnight sleep studies. Recent work aims to leverage automated
algorithms to perform sleep staging not based on electroencephalogram signals,
but rather based on the airflow signals of subjects. Prior work uses ideas from
topological data analysis (TDA), specifically Hermite function expansions of
persistence curves (HEPC) to featurize airflow signals. However, finite order
HEPC captures only partial information. In this work, we propose Fourier
approximations of persistence curves (FAPC), and use this technique to perform
sleep staging based on airflow signals. We analyze performance using an XGBoost
model on 1155 pediatric sleep studies taken from the Nationwide Children's
Hospital Sleep DataBank (NCHSDB), and find that FAPC methods provide
complimentary information to HEPC methods alone, leading to a 4.9% increase in
performance over baseline methods.",cs.LG
On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients,"The holy grail of machine learning is to enable Continual Federated Learning
(CFL) to enhance the efficiency, privacy, and scalability of AI systems while
learning from streaming data. The primary challenge of a CFL system is to
overcome global catastrophic forgetting, wherein the accuracy of the global
model trained on new tasks declines on the old tasks. In this work, we propose
Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel
replay-memory based federated strategy consisting of edge-based gradient
updates on memory and aggregated gradients on the current data. We provide
convergence analysis of the C-FLAG approach which addresses forgetting and bias
while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We
formulate an optimization sub-problem that minimizes catastrophic forgetting,
translating CFL into an iterative algorithm with adaptive learning rates that
ensure seamless learning across tasks. We empirically show that C-FLAG
outperforms several state-of-the-art baselines on both task and
class-incremental settings with respect to metrics such as accuracy and
forgetting.",cs.LG
Tukey g-and-h neural network regression for non-Gaussian data,"This paper addresses non-Gaussian regression with neural networks via the use
of the Tukey g-and-h distribution.The Tukey g-and-h transform is a flexible
parametric transform with two parameters $g$ and $h$ which, when applied to a
standard normal random variable, introduces both skewness and kurtosis,
resulting in a distribution commonly called the Tukey g-and-h distribution.
Specific values of $g$ and $h$ produce good approximations to other families of
distributions, such as the Cauchy and student-t distributions. The flexibility
of the Tukey g-and-h distribution has driven its popularity in the statistical
community, in applied sciences and finance. In this work we consider the
training of a neural network to predict the parameters of a Tukey g-and-h
distribution in a regression framework via the minimization of the
corresponding negative log-likelihood, despite the latter having no closed-form
expression. We demonstrate the efficiency of our procedure in simulated
examples and apply our method to a real-world dataset of global crop yield for
several types of crops. Finally, we show how we can carry out a goodness-of-fit
analysis between the predicted distributions and the test data. A Pytorch
implementation is made available on Github and as a Pypi package.",cs.LG
Learning Memory Mechanisms for Decision Making through Demonstrations,"In Partially Observable Markov Decision Processes, integrating an agent's
history into memory poses a significant challenge for decision-making.
Traditional imitation learning, relying on observation-action pairs for expert
demonstrations, fails to capture the expert's memory mechanisms used in
decision-making. To capture memory processes as demonstrations, we introduce
the concept of memory dependency pairs $(p, q)$ indicating that events at time
$p$ are recalled for decision-making at time $q$. We introduce AttentionTuner
to leverage memory dependency pairs in Transformers and find significant
improvements across several tasks compared to standard Transformers when
evaluated on Memory Gym and the Long-term Memory Benchmark. Code is available
at https://github.com/WilliamYue37/AttentionTuner.",cs.LG
Towards Low-bit Communication for Tensor Parallel LLM Inference,"Tensor parallelism provides an effective way to increase server large
language model (LLM) inference efficiency despite adding an additional
communication cost. However, as server LLMs continue to scale in size, they
will need to be distributed across more devices, magnifying the communication
cost. One way to approach this problem is with quantization, but current
methods for LLMs tend to avoid quantizing the features that tensor parallelism
needs to communicate. Taking advantage of consistent outliers in communicated
features, we introduce a quantization method that reduces communicated values
on average from 16 bits to 4.2 bits while preserving nearly all of the original
performance. For instance, our method maintains around 98.0% and 99.5% of Gemma
2 27B's and Llama 2 13B's original performance, respectively, averaged across
all tasks we evaluated on.",cs.LG
Doubly Mild Generalization for Offline Reinforcement Learning,"Offline Reinforcement Learning (RL) suffers from the extrapolation error and
value overestimation. From a generalization perspective, this issue can be
attributed to the over-generalization of value functions or policies towards
out-of-distribution (OOD) actions. Significant efforts have been devoted to
mitigating such generalization, and recent in-sample learning approaches have
further succeeded in entirely eschewing it. Nevertheless, we show that mild
generalization beyond the dataset can be trusted and leveraged to improve
performance under certain conditions. To appropriately exploit generalization
in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild
action generalization and (ii) mild generalization propagation. The former
refers to selecting actions in a close neighborhood of the dataset to maximize
the Q values. Even so, the potential erroneous generalization can still be
propagated, accumulated, and exacerbated by bootstrapping. In light of this,
the latter concept is introduced to mitigate the generalization propagation
without impeding the propagation of RL learning signals. Theoretically, DMG
guarantees better performance than the in-sample optimal policy in the oracle
generalization scenario. Even under worst-case generalization, DMG can still
control value overestimation at a certain level and lower bound the
performance. Empirically, DMG achieves state-of-the-art performance across
Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting
from its flexibility in both generalization aspects, DMG enjoys a seamless
transition from offline to online learning and attains strong online
fine-tuning performance.",cs.LG
Prediction of Acoustic Communication Performance for AUVs using Gaussian Process Classification,"Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic
communication to coordinate their actions effectively. However, the reliability
of underwater acoustic communication decreases as the communication range
between vehicles increases. Consequently, teams of cooperating AUVs typically
make conservative assumptions about the maximum range at which they can
communicate reliably. To address this limitation, we propose a novel approach
that involves learning a map representing the probability of successful
communication based on the locations of the transmitting and receiving
vehicles. This probabilistic communication map accounts for factors such as the
range between vehicles, environmental noise, and multi-path effects at a given
location. In pursuit of this goal, we investigate the application of Gaussian
process binary classification to generate the desired communication map. We
specialize existing results to this specific binary classification problem and
explore methods to incorporate uncertainty in vehicle location into the mapping
process. Furthermore, we compare the prediction performance of the probability
communication map generated using binary classification with that of a
signal-to-noise ratio (SNR) communication map generated using Gaussian process
regression. Our approach is experimentally validated using communication and
navigation data collected during trials with a pair of Virginia Tech 690 AUVs.",cs.LG
A Stochastic Optimization Framework for Private and Fair Learning From Decentralized Data,"Machine learning models are often trained on sensitive data (e.g., medical
records and race/gender) that is distributed across different ""silos"" (e.g.,
hospitals). These federated learning models may then be used to make
consequential decisions, such as allocating healthcare resources. Two key
challenges emerge in this setting: (i) maintaining the privacy of each person's
data, even if other silos or an adversary with access to the central server
tries to infer this data; (ii) ensuring that decisions are fair to different
demographic groups (e.g., race/gender). In this paper, we develop a novel
algorithm for private and fair federated learning (FL). Our algorithm satisfies
inter-silo record-level differential privacy (ISRL-DP), a strong notion of
private FL requiring that silo i's sent messages satisfy record-level
differential privacy for all i. Our framework can be used to promote different
fairness notions, including demographic parity and equalized odds. We prove
that our algorithm converges under mild smoothness assumptions on the loss
function, whereas prior work required strong convexity for convergence. As a
byproduct of our analysis, we obtain the first convergence guarantee for
ISRL-DP nonconvex-strongly concave min-max FL. Experiments demonstrate the
state-of-the-art fairness-accuracy tradeoffs of our algorithm across different
privacy levels.",cs.LG
INTRABENCH: Interactive Radiological Benchmark,"Current interactive segmentation approaches, inspired by the success of
META's Segment Anything model, have achieved notable advancements, however,
they come with substantial limitations that hinder their practical application
in real clinical scenarios. These include unrealistic human interaction
requirements, such as slice-by-slice operations for 2D models on 3D data, a
lack of iterative refinement, and insufficient evaluation experiments. These
shortcomings prevent accurate assessment of model performance and lead to
inconsistent outcomes across studies. IntRaBench overcomes these challenges by
offering a comprehensive and reproducible framework for evaluating interactive
segmentation methods in realistic, clinically relevant scenarios. It includes
diverse datasets, target structures, and segmentation models, and provides a
flexible codebase that allows seamless integration of new models and prompting
strategies. Additionally, we introduce advanced techniques to minimize
clinician interaction, ensuring fair comparisons between 2D and 3D models. By
open-sourcing IntRaBench, we invite the research community to integrate their
models and prompting techniques, ensuring continuous and transparent evaluation
of interactive segmentation models in 3D medical imaging.",cs.LG
Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules,"Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.",cs.LG
CDXFormer: Boosting Remote Sensing Change Detection with Extended Long Short-Term Memory,"In complex scenes and varied conditions, effectively integrating
spatial-temporal context is crucial for accurately identifying changes.
However, current RS-CD methods lack a balanced consideration of performance and
efficiency. CNNs lack global context, Transformers have quadratic computational
complexity, and Mambas are restricted by CUDA acceleration. In this paper, we
propose CDXFormer, with a core component that is a powerful XLSTM-based feature
enhancement layer, integrating the advantages of linear computational
complexity, global context perception, and strong interpret-ability.
Specifically, we introduce a scale-specific Feature Enhancer layer,
incorporating a Cross-Temporal Global Perceptron customized for
semantic-accurate deep features, and a Cross-Temporal Spatial Refiner
customized for detail-rich shallow features. Additionally, we propose a
Cross-Scale Interactive Fusion module to progressively interact global change
representations with spatial responses. Extensive experimental results
demonstrate that CDXFormer achieves state-of-the-art performance across three
benchmark datasets, offering a compelling balance between efficiency and
accuracy. Code is available at https://github.com/xwmaxwma/rschange.",cs.LG
Tucano: Advancing Neural Text Generation for Portuguese,"Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/",cs.LG
Evidential time-to-event prediction model with well-calibrated uncertainty estimation,"Time-to-event analysis, or Survival analysis, provides valuable insights into
clinical prognosis and treatment recommendations. However, this task is
typically more challenging than other regression tasks due to the censored
observations. Moreover, concerns regarding the reliability of predictions
persist among clinicians, mainly attributed to the absence of confidence
assessment, robustness, and calibration of prediction. To address those
challenges, we introduce an evidential regression model designed especially for
time-to-event prediction tasks, with which the most plausible event time, is
directly quantified by aggregated Gaussian random fuzzy numbers (GRFNs). The
GRFNs are a newly introduced family of random fuzzy subsets of the real line
that generalizes both Gaussian random variables and Gaussian possibility
distributions. Different from conventional methods that construct models based
on strict data distribution, e.g., proportional hazard function, our model only
assumes the event time is encoded in a real line GFRN without any strict
distribution assumption, therefore offering more flexibility in complex data
scenarios. Furthermore, the epistemic and aleatory uncertainty regarding the
event time is quantified within the aggregated GRFN as well. Our model can,
therefore, provide more detailed clinical decision-making guidance with two
more degrees of information. The model is fit by minimizing a generalized
negative log-likelihood function that accounts for data censoring based on
uncertainty evidence reasoning. Experimental results on simulated datasets with
varying data distributions and censoring scenarios, as well as on real-world
datasets across diverse clinical settings and tasks, demonstrate that our model
achieves both accurate and reliable performance, outperforming state-of-the-art
methods.",cs.LG
FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training,"With the increase in the number of parameters in large language models, the
process of pre-training and fine-tuning increasingly demands larger volumes of
GPU memory. A significant portion of this memory is typically consumed by the
optimizer state. To overcome this challenge, recent approaches such as low-rank
adaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao
et al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been
proposed. However, in all these algorithms, the $\textit{effective rank of the
weight updates remains low-rank}$, which can lead to a substantial loss of
information from the gradient. This loss can be critically important,
especially during the pre-training stage. In this paper, we introduce
$\texttt{FRUGAL}$ ($\textbf{F}$ull-$\textbf{R}$ank $\textbf{U}$pdates with
$\textbf{G}$r$\textbf{A}$dient sp$\textbf{L}$itting), a new memory-efficient
optimization framework. $\texttt{FRUGAL}$ leverages gradient splitting to
perform low-dimensional updates using advanced algorithms (such as Adam), while
updates along the remaining directions are executed via state-free methods like
SGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with
various low-rank update selection techniques, including GaLore and BAdam. We
provide theoretical convergence guarantees for our framework when using SGDM
for low-dimensional updates and SGD for state-free updates. Additionally, our
method consistently outperforms concurrent approaches across various fixed
memory budgets, achieving state-of-the-art results in pre-training and
fine-tuning tasks while balancing memory efficiency and performance metrics.",cs.LG
Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of Factored-POMDPs,"Learning representations of underlying environmental dynamics from partial
observations is a critical challenge in machine learning. In the context of
Partially Observable Markov Decision Processes (POMDPs), state representations
are often inferred from the history of past observations and actions. We
demonstrate that incorporating future information is essential to accurately
capture causal dynamics and enhance state representations. To address this, we
introduce a Dynamical Variational Auto-Encoder (DVAE) designed to learn causal
Markovian dynamics from offline trajectories in a POMDP. Our method employs an
extended hindsight framework that integrates past, current, and multi-step
future information within a factored-POMDP setting. Empirical results reveal
that this approach uncovers the causal graph governing hidden state transitions
more effectively than history-based and typical hindsight-based models.",cs.LG
Suite-IN: Aggregating Motion Features from Apple Suite for Robust Inertial Navigation,"With the rapid development of wearable technology, devices like smartphones,
smartwatches, and headphones equipped with IMUs have become essential for
applications such as pedestrian positioning. However, traditional pedestrian
dead reckoning (PDR) methods struggle with diverse motion patterns, while
recent data-driven approaches, though improving accuracy, often lack robustness
due to reliance on a single device.In our work, we attempt to enhance the
positioning performance using the low-cost commodity IMUs embedded in the
wearable devices. We propose a multi-device deep learning framework named
Suite-IN, aggregating motion data from Apple Suite for inertial navigation.
Motion data captured by sensors on different body parts contains both local and
global motion information, making it essential to reduce the negative effects
of localized movements and extract global motion representations from multiple
devices.",cs.LG
Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices,"In recent years, Large Language Models (LLMs) through Transformer structures
have dominated many machine learning tasks, especially text processing.
However, these models require massive amounts of data for training and induce
high resource requirements, particularly in terms of the large number of
Floating Point Operations (FLOPs) and the high amounts of memory needed. To
fine-tune such a model in a parameter-efficient way, techniques like Adapter or
LoRA have been developed. However, we observe that the application of LoRA,
when used in federated learning (FL), while still being parameter-efficient, is
memory and FLOP inefficient. Based on that observation, we develop a novel
layer finetuning scheme that allows devices in cross-device FL to make use of
pretrained neural networks (NNs) while adhering to given resource constraints.
We show that our presented scheme outperforms the current state of the art when
dealing with homogeneous or heterogeneous computation and memory constraints
and is on par with LoRA regarding limited communication, thereby achieving
significantly higher accuracies in FL training.",cs.LG
Dual-Criterion Model Aggregation in Federated Learning: Balancing Data Quantity and Quality,"Federated learning (FL) has become one of the key methods for
privacy-preserving collaborative learning, as it enables the transfer of models
without requiring local data exchange. Within the FL framework, an aggregation
algorithm is recognized as one of the most crucial components for ensuring the
efficacy and security of the system. Existing average aggregation algorithms
typically assume that all client-trained data holds equal value or that weights
are based solely on the quantity of data contributed by each client. In
contrast, alternative approaches involve training the model locally after
aggregation to enhance adaptability. However, these approaches fundamentally
ignore the inherent heterogeneity between different clients' data and the
complexity of variations in data at the aggregation stage, which may lead to a
suboptimal global model.
  To address these issues, this study proposes a novel dual-criterion weighted
aggregation algorithm involving the quantity and quality of data from the
client node. Specifically, we quantify the data used for training and perform
multiple rounds of local model inference accuracy evaluation on a specialized
dataset to assess the data quality of each client. These two factors are
utilized as weights within the aggregation process, applied through a
dynamically weighted summation of these two factors. This approach allows the
algorithm to adaptively adjust the weights, ensuring that every client can
contribute to the global model, regardless of their data's size or initial
quality. Our experiments show that the proposed algorithm outperforms several
existing state-of-the-art aggregation approaches on both a general-purpose
open-source dataset, CIFAR-10, and a dataset specific to visual obstacle
avoidance.",cs.LG
Federated Low-Rank Adaptation with Differential Privacy over Wireless Networks,"Fine-tuning large pre-trained foundation models (FMs) on distributed edge
devices presents considerable computational and privacy challenges. Federated
fine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative
model training without the need to share raw data. To lessen the computational
burden on resource-limited devices, combining low-rank adaptation (LoRA) with
federated learning enables parameter-efficient fine-tuning. Additionally, the
split FedFT architecture partitions an FM between edge devices and a central
server, reducing the necessity for complete model deployment on individual
devices. However, the risk of privacy eavesdropping attacks in FedFT remains a
concern, particularly in sensitive areas such as healthcare and finance. In
this paper, we propose a split FedFT framework with differential privacy (DP)
over wireless networks, where the inherent wireless channel noise in the uplink
transmission is utilized to achieve DP guarantees without adding an extra
artificial noise. We shall investigate the impact of the wireless noise on
convergence performance of the proposed framework. We will also show that by
updating only one of the low-rank matrices in the split FedFT with DP, the
proposed method can mitigate the noise amplification effect. Simulation results
will demonstrate that the proposed framework achieves higher accuracy under
strict privacy budgets compared to baseline methods.",cs.LG
Kernel-based retrieval models for hyperspectral image data optimized with Kernel Flows,"Kernel-based statistical methods are efficient, but their performance depends
heavily on the selection of kernel parameters. In literature, the optimization
studies on kernel-based chemometric methods is limited and often reduced to
grid searching. Previously, the authors introduced Kernel Flows (KF) to learn
kernel parameters for Kernel Partial Least-Squares (K-PLS) regression. KF is
easy to implement and helps minimize overfitting. In cases of high collinearity
between spectra and biogeophysical quantities in spectroscopy, simpler methods
like Principal Component Regression (PCR) may be more suitable. In this study,
we propose a new KF-type approach to optimize Kernel Principal Component
Regression (K-PCR) and test it alongside KF-PLS. Both methods are benchmarked
against non-linear regression techniques using two hyperspectral remote sensing
datasets.",cs.LG
PatchCTG: Patch Cardiotocography Transformer for Antepartum Fetal Health Monitoring,"Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but
traditional methods like the Dawes-Redman system are often limited by high
inter-observer variability, leading to inconsistent interpretations and
potential misdiagnoses. This paper introduces PatchCTG, a transformer-based
model specifically designed for CTG analysis, employing patch-based
tokenisation, instance normalisation and channel-independent processing to
capture essential local and global temporal dependencies within CTG signals.
PatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over
20,000 CTG traces across diverse clinical outcomes after applying the inclusion
and exclusion criteria. With extensive hyperparameter optimisation, PatchCTG
achieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at
Youden's index threshold, demonstrating adaptability to various clinical needs.
Testing across varying temporal thresholds showed robust predictive
performance, particularly with finetuning on data closer to delivery, achieving
a sensitivity of 52% and specificity of 88% for near-delivery cases. These
findings suggest the potential of PatchCTG to enhance clinical decision-making
in antepartum care by providing a reliable, objective tool for fetal health
assessment. The source code is available at
https://github.com/jaleedkhan/PatchCTG.",cs.LG
Interaction Asymmetry: A General Principle for Learning Composable Abstractions,"Learning disentangled representations of concepts and re-composing them in
unseen ways is crucial for generalizing to out-of-domain situations. However,
the underlying properties of concepts that enable such disentanglement and
compositional generalization remain poorly understood. In this work, we propose
the principle of interaction asymmetry which states: ""Parts of the same concept
have more complex interactions than parts of different concepts"". We formalize
this via block diagonality conditions on the $(n+1)$th order derivatives of the
generator mapping concepts to observed data, where different orders of
""complexity"" correspond to different $n$. Using this formalism, we prove that
interaction asymmetry enables both disentanglement and compositional
generalization. Our results unify recent theoretical results for learning
concepts of objects, which we show are recovered as special cases with
$n\!=\!0$ or $1$. We provide results for up to $n\!=\!2$, thus extending these
prior works to more flexible generator functions, and conjecture that the same
proof strategies generalize to larger $n$. Practically, our theory suggests
that, to disentangle concepts, an autoencoder should penalize its latent
capacity and the interactions between concepts during decoding. We propose an
implementation of these criteria using a flexible Transformer-based VAE, with a
novel regularizer on the attention weights of the decoder. On synthetic image
datasets consisting of objects, we provide evidence that this model can achieve
comparable object disentanglement to existing models that use more explicit
object-centric priors.",cs.LG
Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.",cs.LG
Automatic Album Sequencing,"Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing",cs.LG
ASER: Activation Smoothing and Error Reconstruction for Large Language Model Quantization,"Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective low-bit quantization. The limited numerical mapping makes the
quantized model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training quantization. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
quantization error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to low-bit ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art quantization
algorithms, showing potential to activation quantization, with minor overhead.",cs.LG
Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit Q-Learning,"Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In this work, we present a novel
hierarchical transformer-based approach leveraging a learned quantizer of the
space. This quantization enables the training of a simpler zone-conditioned
low-level policy and simplifies planning, which is reduced to discrete
autoregressive prediction. Among other benefits, zone-level reasoning in
planning enables explicit trajectory stitching rather than implicit stitching
based on noisy value function estimates. By combining this transformer-based
planner with recent advancements in offline RL, our proposed approach achieves
state-of-the-art results in complex long-distance navigation environments.",cs.LG
Spatially Regularized Graph Attention Autoencoder Framework for Detecting Rainfall Extremes,"We introduce a novel Graph Attention Autoencoder (GAE) with spatial
regularization to address the challenge of scalable anomaly detection in
spatiotemporal rainfall data across India from 1990 to 2015. Our model
leverages a Graph Attention Network (GAT) to capture spatial dependencies and
temporal dynamics in the data, further enhanced by a spatial regularization
term ensuring geographic coherence. We construct two graph datasets employing
rainfall, pressure, and temperature attributes from the Indian Meteorological
Department and ERA5 Reanalysis on Single Levels, respectively. Our network
operates on graph representations of the data, where nodes represent geographic
locations, and edges, inferred through event synchronization, denote
significant co-occurrences of rainfall events. Through extensive experiments,
we demonstrate that our GAE effectively identifies anomalous rainfall patterns
across the Indian landscape. Our work paves the way for sophisticated
spatiotemporal anomaly detection methodologies in climate science, contributing
to better climate change preparedness and response strategies.",cs.LG
Exploring the loss landscape of regularized neural networks via convex duality,"We discuss several aspects of the loss landscape of regularized neural
networks: the structure of stationary points, connectivity of optimal
solutions, path with nonincreasing loss to arbitrary global optimum, and the
nonuniqueness of optimal solutions, by casting the problem into an equivalent
convex problem and considering its dual. Starting from two-layer neural
networks with scalar output, we first characterize the solution set of the
convex problem using its dual and further characterize all stationary points.
With the characterization, we show that the topology of the global optima goes
through a phase transition as the width of the network changes, and construct
counterexamples where the problem may have a continuum of optimal solutions.
Finally, we show that the solution set characterization and connectivity
results can be extended to different architectures, including two-layer
vector-valued neural networks and parallel three-layer neural networks.",cs.LG
Convergence Rate Analysis of LION,"The LION (evoLved sIgn mOmeNtum) optimizer for deep neural network training
was found by Google via program search, with the simple sign update yet showing
impressive performance in training large scale networks. Although previous
studies have investigated its convergence properties, a comprehensive analysis,
especially the convergence rate, is still desirable. Recognizing that LION can
be regarded as solving a specific constrained problem, this paper focuses on
demonstrating its convergence to the Karush-Kuhn-Tucker (KKT) point at the rate
of $\cal O(\sqrt{d}K^{-1/4})$ measured by gradient $\ell_1$ norm, where $d$ is
the problem dimension and $K$ is the number of iteration steps. Step further,
we remove the constraint and establish that LION converges to the critical
point of the general unconstrained problem at the same rate. This rate not only
delivers the currently optimal dependence on the problem dimension $d$ but also
tightly matches the theoretical lower bound for nonconvex stochastic
optimization algorithms, which is typically measured using the gradient
$\ell_2$ norm, with respect to the number of iterations $K$. Through extensive
experiments, we not only demonstrate that LION achieves lower loss and higher
performance compared to standard SGD, but also empirically confirm that the
gradient $\ell_1/\ell_2$ norm ratio aligns with $\Theta(\sqrt{d})$, thus
proving that our convergence rate matches the theoretical lower bound with
respect to $d$ in the empirical sense.",cs.LG
EMPERROR: A Flexible Generative Perception Error Model for Probing Self-Driving Planners,"To handle the complexities of real-world traffic, learning planners for
self-driving from data is a promising direction. While recent approaches have
shown great progress, they typically assume a setting in which the ground-truth
world state is available as input. However, when deployed, planning needs to be
robust to the long-tail of errors incurred by a noisy perception system, which
is often neglected in evaluation. To address this, previous work has proposed
drawing adversarial samples from a perception error model (PEM) mimicking the
noise characteristics of a target object detector. However, these methods use
simple PEMs that fail to accurately capture all failure modes of detection. In
this paper, we present EMPERROR, a novel transformer-based generative PEM,
apply it to stress-test an imitation learning (IL)-based planner and show that
it imitates modern detectors more faithfully than previous work. Furthermore,
it is able to produce realistic noisy inputs that increase the planner's
collision rate by up to 85%, demonstrating its utility as a valuable tool for a
more complete evaluation of self-driving planners.",cs.LG
OWLed: Outlier-weighed Layerwise Pruning for Efficient Autonomous Driving Framework,"The integration of Large Language Models (LLMs) into autonomous driving
systems offers promising enhancements in environmental understanding and
decision-making. However, the substantial computational demands of deploying
LLMs locally on vehicles render this approach unfeasible for real-world
automotive applications. To address this challenge, we introduce OWLed, the
Outlier-Weighed Layerwise Pruning for Efficient Autonomous Driving Framework
that leverages outlier-weighted layerwise sparsity for model compression. Our
method assigns non-uniform sparsity ratios to different layers based on the
distribution of outlier features, significantly reducing the model size without
the need for fine-tuning. To ensure the compressed model adapts well to
autonomous driving tasks, we incorporate driving environment data into both the
calibration and pruning processes. Our empirical studies reveal that the
encoder component is more sensitive to pruning than the LLM, highlighting its
critical role in the system. Experimental results demonstrate that OWLed
outperforms existing methods in perception, action prediction, and language
understanding while substantially lowering computational requirements. These
findings underscore the potential of combining advanced pruning techniques with
LLMs to develop efficient and robust autonomous driving systems capable of
handling complex scenarios. Code will be made publicly available.",cs.LG
Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning,"In many Deep Reinforcement Learning (RL) problems, decisions in a trained
policy vary in significance for the expected safety and performance of the
policy. Since RL policies are very complex, testing efforts should concentrate
on states in which the agent's decisions have the highest impact on the
expected outcome. In this paper, we propose a novel model-based method to
rigorously compute a ranking of state importance across the entire state space.
We then focus our testing efforts on the highest-ranked states. In this paper,
we focus on testing for safety. However, the proposed methods can be easily
adapted to test for performance. In each iteration, our testing framework
computes optimistic and pessimistic safety estimates. These estimates provide
lower and upper bounds on the expected outcomes of the policy execution across
all modeled states in the state space. Our approach divides the state space
into safe and unsafe regions upon convergence, providing clear insights into
the policy's weaknesses. Two important properties characterize our approach.
(1) Optimal Test-Case Selection: At any time in the testing process, our
approach evaluates the policy in the states that are most critical for safety.
(2) Guaranteed Safety: Our approach can provide formal verification guarantees
over the entire state space by sampling only a fraction of the policy. Any
safety properties assured by the pessimistic estimate are formally proven to
hold for the policy. We provide a detailed evaluation of our framework on
several examples, showing that our method discovers unsafe policy behavior with
low testing effort.",cs.LG
What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?,"Despite the remarkable capabilities of modern large language models (LLMs),
the mechanisms behind their problem-solving abilities remain elusive. In this
work, we aim to better understand how the learning dynamics of LLM finetuning
shapes downstream generalization. Our analysis focuses on reasoning tasks,
whose problem structure allows us to distinguish between memorization (the
exact replication of reasoning steps from the training data) and performance
(the correctness of the final solution). We find that a model's generalization
behavior can be effectively characterized by a training metric we call
pre-memorization train accuracy: the accuracy of model samples on training
queries before they begin to copy the exact reasoning steps from the training
set. On the dataset level, this metric is able to reliably predict test
accuracy, achieving $R^2$ of around or exceeding 0.9 across various models
(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On
a per-example level, this metric is also indicative of whether individual model
predictions are robust to perturbations in the training query. By connecting a
model's learning behavior to its generalization, pre-memorization train
accuracy can guide targeted improvements to training strategies. We focus on
data curation as an example, and show that prioritizing examples with low
pre-memorization accuracy leads to 1.5-2x improvements in data efficiency
compared to i.i.d. data scaling, and outperforms other standard data curation
techniques.",cs.LG
Safe Exploitative Play with Untrusted Type Beliefs,"The combination of the Bayesian game and learning has a rich history, with
the idea of controlling a single agent in a system composed of multiple agents
with unknown behaviors given a set of types, each specifying a possible
behavior for the other agents. The idea is to plan an agent's own actions with
respect to those types which it believes are most likely to maximize the
payoff. However, the type beliefs are often learned from past actions and
likely to be incorrect. With this perspective in mind, we consider an agent in
a game with type predictions of other components, and investigate the impact of
incorrect beliefs to the agent's payoff. In particular, we formally define a
tradeoff between risk and opportunity by comparing the payoff obtained against
the optimal payoff, which is represented by a gap caused by trusting or
distrusting the learned beliefs. Our main results characterize the tradeoff by
establishing upper and lower bounds on the Pareto front for both normal-form
and stochastic Bayesian games, with numerical results provided.",cs.LG
Rethinking Structure Learning For Graph Neural Networks,"To improve the performance of Graph Neural Networks (GNNs), Graph Structure
Learning (GSL) has been extensively applied to reconstruct or refine original
graph structures, effectively addressing issues like heterophily,
over-squashing, and noisy structures. While GSL is generally thought to improve
GNN performance, it often leads to longer training times and more
hyperparameter tuning. Besides, the distinctions among current GSL methods
remain ambiguous from the perspective of GNN training, and there is a lack of
theoretical analysis to quantify their effectiveness. Recent studies further
suggest that, under fair comparisons with the same hyperparameter tuning, GSL
does not consistently outperform baseline GNNs. This motivates us to ask a
critical question: is GSL really useful for GNNs? To address this question,
this paper makes two key contributions. First, we propose a new GSL framework,
which includes three steps: GSL base (the representation used for GSL)
construction, new structure construction, and view fusion, to better understand
the effectiveness of GSL in GNNs. Second, after graph convolution, we analyze
the differences in mutual information (MI) between node representations derived
from the original topology and those from the newly constructed topology.
Surprisingly, our empirical observations and theoretical analysis show that no
matter which type of graph structure construction methods are used, after
feeding the same GSL bases to the newly constructed graph, there is no MI gain
compared to the original GSL bases. To fairly reassess the effectiveness of
GSL, we conduct ablation experiments and find that it is the pretrained GSL
bases that enhance GNN performance, and in most cases, GSL cannot improve GNN
performance. This finding encourages us to rethink the essential components in
GNNs, such as self-training and structural encoding, in GNN design rather than
GSL.",cs.LG
Is Graph Convolution Always Beneficial For Every Feature?,"Graph Neural Networks (GNNs) have demonstrated strong capabilities in
processing structured data. While traditional GNNs typically treat each feature
dimension equally during graph convolution, we raise an important question: Is
the graph convolution operation equally beneficial for each feature? If not,
the convolution operation on certain feature dimensions can possibly lead to
harmful effects, even worse than the convolution-free models. In prior studies,
to assess the impacts of graph convolution on features, people proposed metrics
based on feature homophily to measure feature consistency with the graph
topology. However, these metrics have shown unsatisfactory alignment with GNN
performance and have not been effectively employed to guide feature selection
in GNNs. To address these limitations, we introduce a novel metric, Topological
Feature Informativeness (TFI), to distinguish between GNN-favored and
GNN-disfavored features, where its effectiveness is validated through both
theoretical analysis and empirical observations. Based on TFI, we propose a
simple yet effective Graph Feature Selection (GFS) method, which processes
GNN-favored and GNN-disfavored features separately, using GNNs and non-GNN
models. Compared to original GNNs, GFS significantly improves the extraction of
useful topological information from each feature with comparable computational
costs. Extensive experiments show that after applying GFS to 8 baseline and
state-of-the-art (SOTA) GNN architectures across 10 datasets, 83.75% of the
GFS-augmented cases show significant performance boosts. Furthermore, our
proposed TFI metric outperforms other feature selection methods. These results
validate the effectiveness of both GFS and TFI. Additionally, we demonstrate
that GFS's improvements are robust to hyperparameter tuning, highlighting its
potential as a universal method for enhancing various GNN architectures.",cs.LG
"Understanding Audiovisual Deepfake Detection: Techniques, Challenges, Human Factors and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.",cs.LG
xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell Lung Cancer,"Understanding how deep learning models predict oncology patient risk can
provide critical insights into disease progression, support clinical
decision-making, and pave the way for trustworthy and data-driven precision
medicine. Building on recent advances in the spatial modeling of the tumor
microenvironment using graph neural networks, we present an explainable cell
graph (xCG) approach for survival prediction. We validate our model on a public
cohort of imaging mass cytometry (IMC) data for 416 cases of lung
adenocarcinoma. We explain survival predictions in terms of known phenotypes on
the cell level by computing risk attributions over cell graphs, for which we
propose an efficient grid-based layer-wise relevance propagation (LRP) method.
Our ablation studies highlight the importance of incorporating the cancer stage
and model ensembling to improve the quality of risk estimates. Our xCG method,
together with the IMC data, is made publicly available to support further
research.",cs.LG
Top-$nσ$: Not All Logits Are You Need,"Large language models (LLMs) typically employ greedy decoding or
low-temperature sampling for reasoning tasks, reflecting a perceived trade-off
between diversity and accuracy. We challenge this convention by introducing
top-$n\sigma$, a novel sampling method that operates directly on pre-softmax
logits by leveraging a statistical threshold. Our key insight is that logits
naturally separate into a Gaussian-distributed noisy region and a distinct
informative region, enabling efficient token filtering without complex
probability manipulations. Unlike existing methods (e.g., top-$p$, min-$p$)
that inadvertently include more noise tokens at higher temperatures,
top-$n\sigma$ maintains a stable sampling space regardless of temperature
scaling. We also provide a theoretical analysis of top-$n\sigma$ to better
understand its behavior. The extensive experimental results across four
reasoning-focused datasets demonstrate that our method not only outperforms
existing sampling approaches but also surpasses greedy decoding, while
maintaining consistent performance even at high temperatures.",cs.LG
Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling,"Scheduling problems pose significant challenges in resource, industry, and
operational management. This paper addresses the Unrelated Parallel Machine
Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent
Reinforcement Learning (MARL) approach. The study introduces the Reinforcement
Learning environment and conducts empirical analyses, comparing MARL with
Single-Agent algorithms. The experiments employ various deep neural network
policies for single- and Multi-Agent approaches. Results demonstrate the
efficacy of the Maskable extension of the Proximal Policy Optimization (PPO)
algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in
Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced
scenarios, Multi-Agent approaches reveal challenges in cooperative learning but
a scalable capacity. This research contributes insights into applying MARL
techniques to scheduling optimization, emphasizing the need for algorithmic
sophistication balanced with scalability for intelligent scheduling solutions.",cs.LG
CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only ASR,"CTC compressor can be an effective approach to integrate audio encoders to
decoder-only models, which has gained growing interest for different speech
applications. In this work, we propose a novel CTC compressor based joint
speech and text training (CJST) framework for decoder-only ASR. CJST matches
speech and text modalities from both directions by exploring a simple modality
adaptor and several features of the CTC compressor, including sequence
compression, on-the-fly forced peaky alignment and CTC class embeddings.
Experimental results on the Librispeech and TED-LIUM2 corpora show that the
proposed CJST achieves an effective text injection without the need of duration
handling, leading to the best performance for both in-domain and cross-domain
scenarios. We also provide a comprehensive study on CTC compressor, covering
various compression modes, edge case handling and behavior under both clean and
noisy data conditions, which reveals the most robust setting to use CTC
compressor for decoder-only models.",cs.LG
Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.",cs.LG
SegQC: a segmentation network-based framework for multi-metric segmentation quality control and segmentation error detection in volumetric medical images,"Quality control of structures segmentation in volumetric medical images is
important for identifying segmentation errors in clinical practice and for
facilitating model development. This paper introduces SegQC, a novel framework
for segmentation quality estimation and segmentation error detection. SegQC
computes an estimate measure of the quality of a segmentation in volumetric
scans and in their individual slices and identifies possible segmentation error
regions within a slice. The key components include: 1. SegQC-Net, a deep
network that inputs a scan and its segmentation mask and outputs segmentation
error probabilities for each voxel in the scan; 2. three new segmentation
quality metrics, two overlap metrics and a structure size metric, computed from
the segmentation error probabilities; 3. a new method for detecting possible
segmentation errors in scan slices computed from the segmentation error
probabilities. We introduce a new evaluation scheme to measure segmentation
error discrepancies based on an expert radiologist corrections of automatically
produced segmentations that yields smaller observer variability and is closer
to actual segmentation errors. We demonstrate SegQC on three fetal structures
in 198 fetal MRI scans: fetal brain, fetal body and the placenta. To assess the
benefits of SegQC, we compare it to the unsupervised Test Time Augmentation
(TTA)-based quality estimation. Our studies indicate that SegQC outperforms
TTA-based quality estimation in terms of Pearson correlation and MAE for fetal
body and fetal brain structures segmentation. Our segmentation error detection
method achieved recall and precision rates of 0.77 and 0.48 for fetal body, and
0.74 and 0.55 for fetal brain segmentation error detection respectively. SegQC
enhances segmentation metrics estimation for whole scans and individual slices,
as well as provides error regions detection.",cs.LG
Decision Feedback In-Context Symbol Detection over Block-Fading Channels,"Pre-trained Transformers, through in-context learning (ICL), have
demonstrated exceptional capabilities to adapt to new tasks using example
prompts \textit{without model update}. Transformer-based wireless receivers,
where prompts consist of the pilot data in the form of transmitted and received
signal pairs, have shown high estimation accuracy when pilot data are abundant.
However, pilot information is often costly and limited in practice. In this
work, we propose the \underline{DE}cision \underline{F}eedback
\underline{IN}-Cont\underline{E}xt \underline{D}etection (DEFINED) solution as
a new wireless receiver design, which bypasses channel estimation and directly
performs symbol detection using the (sometimes extremely) limited pilot data.
The key innovation in DEFINED is the proposed decision feedback mechanism in
ICL, where we sequentially incorporate the detected symbols into the prompts to
improve the detections for subsequent symbols. Extensive experiments across a
broad range of wireless communication settings demonstrate that DEFINED
achieves significant performance improvements, in some cases only needing a
single pilot pair.",cs.LG
Entropy Controllable Direct Preference Optimization,"In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.",cs.LG
Overcoming the Curse of Dimensionality in Reinforcement Learning Through Approximate Factorization,"Reinforcement Learning (RL) algorithms are known to suffer from the curse of
dimensionality, which refers to the fact that large-scale problems often lead
to exponentially high sample complexity. A common solution is to use deep
neural networks for function approximation; however, such approaches typically
lack theoretical guarantees. To provably address the curse of dimensionality,
we observe that many real-world problems exhibit task-specific model structures
that, when properly leveraged, can improve the sample efficiency of RL.
Building on this insight, we propose overcoming the curse of dimensionality by
approximately factorizing the original Markov decision processes (MDPs) into
smaller, independently evolving MDPs. This factorization enables the
development of sample-efficient RL algorithms in both model-based and
model-free settings, with the latter involving a variant of variance-reduced
Q-learning. We provide improved sample complexity guarantees for both proposed
algorithms. Notably, by leveraging model structure through the approximate
factorization of the MDP, the dependence of sample complexity on the size of
the state-action space can be exponentially reduced. Numerically, we
demonstrate the practicality of our proposed methods through experiments on
both synthetic MDP tasks and a wind farm-equipped storage control problem.",cs.LG
Disentangling Tabular Data towards Better One-Class Anomaly Detection,"Tabular anomaly detection under the one-class classification setting poses a
significant challenge, as it involves accurately conceptualizing ""normal""
derived exclusively from a single category to discern anomalies from normal
data variations. Capturing the intrinsic correlation among attributes within
normal samples presents one promising method for learning the concept. To do
so, the most recent effort relies on a learnable mask strategy with a
reconstruction task. However, this wisdom may suffer from the risk of producing
uniform masks, i.e., essentially nothing is masked, leading to less effective
correlation learning. To address this issue, we presume that attributes related
to others in normal samples can be divided into two non-overlapping and
correlated subsets, defined as CorrSets, to capture the intrinsic correlation
effectively. Accordingly, we introduce an innovative method that disentangles
CorrSets from normal tabular data. To our knowledge, this is a pioneering
effort to apply the concept of disentanglement for one-class anomaly detection
on tabular data. Extensive experiments on 20 tabular datasets show that our
method substantially outperforms the state-of-the-art methods and leads to an
average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.",cs.LG
Uncertainty-Aware Test-Time Adaptation for Inverse Consistent Diffeomorphic Lung Image Registration,"Diffeomorphic deformable image registration ensures smooth invertible
transformations across inspiratory and expiratory chest CT scans. Yet, in
practice, deep learning-based diffeomorphic methods struggle to capture large
deformations between inspiratory and expiratory volumes, and therefore lack
inverse consistency. Existing methods also fail to account for model
uncertainty, which can be useful for improving performance. We propose an
uncertainty-aware test-time adaptation framework for inverse consistent
diffeomorphic lung registration. Our method uses Monte Carlo (MC) dropout to
estimate spatial uncertainty that is used to improve model performance. We
train and evaluate our method for inspiratory-to-expiratory CT registration on
a large cohort of 675 subjects from the COPDGene study, achieving a higher Dice
similarity coefficient (DSC) between the lung boundaries (0.966) compared to
both VoxelMorph (0.953) and TransMorph (0.953). Our method demonstrates
consistent improvements in the inverse registration direction as well with an
overall DSC of 0.966, higher than VoxelMorph (0.958) and TransMorph (0.956).
Paired t-tests indicate statistically significant improvements.",cs.LG
Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models,"Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs)
to output harmful responses, raise significant safety concerns. Among these
methods, gradient-based approaches, which use gradients to generate malicious
prompts, have been widely studied due to their high success rates in white-box
settings, where full access to the model is available. However, these methods
have notable limitations: they require white-box access, which is not always
feasible, and involve high memory usage. To address scenarios where white-box
access is unavailable, attackers often resort to transfer attacks. In transfer
attacks, malicious inputs generated using white-box models are applied to
black-box models, but this typically results in reduced attack performance. To
overcome these challenges, we propose Zer0-Jack, a method that bypasses the
need for white-box access by leveraging zeroth-order optimization. We propose
patch coordinate descent to efficiently generate malicious image inputs to
directly attack black-box MLLMs, which significantly reduces memory usage
further. Through extensive experiments, Zer0-Jack achieves a high attack
success rate across various models, surpassing previous transfer-based methods
and performing comparably with existing white-box jailbreak techniques.
Notably, Zer0-Jack achieves a 95\% attack success rate on MiniGPT-4 with the
Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its
effectiveness. Additionally, we show that Zer0-Jack can directly attack
commercial MLLMs such as GPT-4o. Codes are provided in the supplement.",cs.LG
Exogenous Randomness Empowering Random Forests,"We offer theoretical and empirical insights into the impact of exogenous
randomness on the effectiveness of random forests with tree-building rules
independent of training data. We formally introduce the concept of exogenous
randomness and identify two types of commonly existing randomness: Type I from
feature subsampling, and Type II from tie-breaking in tree-building processes.
We develop non-asymptotic expansions for the mean squared error (MSE) for both
individual trees and forests and establish sufficient and necessary conditions
for their consistency. In the special example of the linear regression model
with independent features, our MSE expansions are more explicit, providing more
understanding of the random forests' mechanisms. It also allows us to derive an
upper bound on the MSE with explicit consistency rates for trees and forests.
Guided by our theoretical findings, we conduct simulations to further explore
how exogenous randomness enhances random forest performance. Our findings
unveil that feature subsampling reduces both the bias and variance of random
forests compared to individual trees, serving as an adaptive mechanism to
balance bias and variance. Furthermore, our results reveal an intriguing
phenomenon: the presence of noise features can act as a ""blessing"" in enhancing
the performance of random forests thanks to feature subsampling.",cs.LG
Unraveling the Gradient Descent Dynamics of Transformers,"While the Transformer architecture has achieved remarkable success across
various domains, a thorough theoretical foundation explaining its optimization
dynamics is yet to be fully developed. In this study, we aim to bridge this
understanding gap by answering the following two core questions: (1) Which
types of Transformer architectures allow Gradient Descent (GD) to achieve
guaranteed convergence? and (2) Under what initial conditions and architectural
specifics does the Transformer achieve rapid convergence during training? By
analyzing the loss landscape of a single Transformer layer using Softmax and
Gaussian attention kernels, our work provides concrete answers to these
questions. Our findings demonstrate that, with appropriate weight
initialization, GD can train a Transformer model (with either kernel type) to
achieve a global optimal solution, especially when the input embedding
dimension is large. Nonetheless, certain scenarios highlight potential
pitfalls: training a Transformer using the Softmax attention kernel may
sometimes lead to suboptimal local solutions. In contrast, the Gaussian
attention kernel exhibits a much favorable behavior. Our empirical study
further validate the theoretical findings.",cs.LG
Accident Impact Prediction based on a deep convolutional and recurrent neural network model,"Traffic accidents pose a significant threat to public safety, resulting in
numerous fatalities, injuries, and a substantial economic burden each year. The
development of predictive models capable of real-time forecasting of
post-accident impact using readily available data can play a crucial role in
preventing adverse outcomes and enhancing overall safety. However, existing
accident predictive models encounter two main challenges: first, reliance on
either costly or non-real-time data, and second the absence of a comprehensive
metric to measure post-accident impact accurately. To address these
limitations, this study proposes a deep neural network model known as the
cascade model. It leverages readily available real-world data from Los Angeles
County to predict post-accident impacts. The model consists of two components:
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). The LSTM
model captures temporal patterns, while the CNN extracts patterns from the
sparse accident dataset. Furthermore, an external traffic congestion dataset is
incorporated to derive a new feature called the ""accident impact"" factor, which
quantifies the influence of an accident on surrounding traffic flow. Extensive
experiments were conducted to demonstrate the effectiveness of the proposed
hybrid machine learning method in predicting the post-accident impact compared
to state-of-the-art baselines. The results reveal a higher precision in
predicting minimal impacts (i.e., cases with no reported accidents) and a
higher recall in predicting more significant impacts (i.e., cases with reported
accidents).",cs.LG
Model Stealing for Any Low-Rank Language Model,"Model stealing, where a learner tries to recover an unknown model via
carefully chosen queries, is a critical problem in machine learning, as it
threatens the security of proprietary models and the privacy of data they are
trained on. In recent years, there has been particular interest in stealing
large language models (LLMs). In this paper, we aim to build a theoretical
understanding of stealing language models by studying a simple and
mathematically tractable setting. We study model stealing for Hidden Markov
Models (HMMs), and more generally low-rank language models.
  We assume that the learner works in the conditional query model, introduced
by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient
algorithm in the conditional query model, for learning any low-rank
distribution. In other words, our algorithm succeeds at stealing any language
model whose output distribution is low-rank. This improves upon the previous
result by Kakade, Krishnamurthy, Mahajan and Zhang, which also requires the
unknown distribution to have high ""fidelity"", a property that holds only in
restricted cases. There are two key insights behind our algorithm: First, we
represent the conditional distributions at each timestep by constructing
barycentric spanners among a collection of vectors of exponentially large
dimension. Second, for sampling from our representation, we iteratively solve a
sequence of convex optimization problems that involve projection in relative
entropy to prevent compounding of errors over the length of the sequence. This
is an interesting example where, at least theoretically, allowing a machine
learning model to solve more complex problems at inference time can lead to
drastic improvements in its performance.",cs.LG
Effective Virtual Reality Teleoperation of an Upper-body Humanoid with Modified Task Jacobians and Relaxed Barrier Functions for Self-Collision Avoidance,"We present an approach for retartgeting off-the-shelf Virtual Reality (VR)
trackers to effectively teleoperate an upper-body humanoid while ensuring
self-collision-free motions. Key to the effectiveness was the proper assignment
of trackers to joint sets via modified task Jacobians and relaxed barrier
functions for self-collision avoidance. The approach was validated on
Apptronik's Astro hardware by demonstrating manipulation capabilities on a
table-top environment with pick-and-place box packing and a two-handed box pick
up and handover task.",cs.LG
SecEncoder: Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.",cs.LG
Collaborative and Federated Black-box Optimization: A Bayesian Optimization Perspective,"We focus on collaborative and federated black-box optimization (BBOpt), where
agents optimize their heterogeneous black-box functions through collaborative
sequential experimentation. From a Bayesian optimization perspective, we
address the fundamental challenges of distributed experimentation,
heterogeneity, and privacy within BBOpt, and propose three unifying frameworks
to tackle these issues: (i) a global framework where experiments are centrally
coordinated, (ii) a local framework that allows agents to make decisions based
on minimal shared information, and (iii) a predictive framework that enhances
local surrogates through collaboration to improve decision-making. We
categorize existing methods within these frameworks and highlight key open
questions to unlock the full potential of federated BBOpt. Our overarching goal
is to shift federated learning from its predominantly descriptive/predictive
paradigm to a prescriptive one, particularly in the context of BBOpt - an
inherently sequential decision-making problem.",cs.LG
Bayesian Deep Learning Approach for Real-time Lane-based Arrival Curve Reconstruction at Intersection using License Plate Recognition Data,"The acquisition of real-time and accurate traffic arrival information is of
vital importance for proactive traffic control systems, especially in partially
connected vehicle environments. License plate recognition (LPR) data that
record both vehicle departures and identities are proven to be desirable in
reconstructing lane-based arrival curves in previous works. Existing LPR
databased methods are predominantly designed for reconstructing historical
arrival curves. For real-time reconstruction of multi-lane urban roads, it is
pivotal to determine the lane choice of real-time link-based arrivals, which
has not been exploited in previous studies. In this study, we propose a
Bayesian deep learning approach for real-time lane-based arrival curve
reconstruction, in which the lane choice patterns and uncertainties of
link-based arrivals are both characterized. Specifically, the learning process
is designed to effectively capture the relationship between partially observed
link-based arrivals and lane-based arrivals, which can be physically
interpreted as lane choice proportion. Moreover, the lane choice uncertainties
are characterized using Bayesian parameter inference techniques, minimizing
arrival curve reconstruction uncertainties, especially in low LPR data matching
rate conditions. Real-world experiment results conducted in multiple matching
rate scenarios demonstrate the superiority and necessity of lane choice
modeling in reconstructing arrival curves.",cs.LG
Robust Offline Reinforcement Learning for Non-Markovian Decision Processes,"Distributionally robust offline reinforcement learning (RL) aims to find a
policy that performs the best under the worst environment within an uncertainty
set using an offline dataset collected from a nominal model. While recent
advances in robust RL focus on Markov decision processes (MDPs), robust
non-Markovian RL is limited to planning problem where the transitions in the
uncertainty set are known. In this paper, we study the learning problem of
robust offline non-Markovian RL. Specifically, when the nominal model admits a
low-rank structure, we propose a new algorithm, featuring a novel dataset
distillation and a lower confidence bound (LCB) design for robust values under
different types of the uncertainty set. We also derive new dual forms for these
robust values in non-Markovian RL, making our algorithm more amenable to
practical implementation. By further introducing a novel type-I concentrability
coefficient tailored for offline low-rank non-Markovian decision processes, we
prove that our algorithm can find an $\epsilon$-optimal robust policy using
$O(1/\epsilon^2)$ offline samples. Moreover, we extend our algorithm to the
case when the nominal model does not have specific structure. With a new
type-II concentrability coefficient, the extended algorithm also enjoys
polynomial sample efficiency under all different types of the uncertainty set.",cs.LG
FM-TS: Flow Matching for Time Series Generation,"Time series generation has emerged as an essential tool for analyzing
temporal data across numerous fields. While diffusion models have recently
gained significant attention in generating high-quality time series, they tend
to be computationally demanding and reliant on complex stochastic processes. To
address these limitations, we introduce FM-TS, a rectified Flow Matching-based
framework for Time Series generation, which simplifies the time series
generation process by directly optimizing continuous trajectories. This
approach avoids the need for iterative sampling or complex noise schedules
typically required in diffusion-based models. FM-TS is more efficient in terms
of training and inference. Moreover, FM-TS is highly adaptive, supporting both
conditional and unconditional time series generation. Notably, through our
novel inference design, the model trained in an unconditional setting can
seamlessly generalize to conditional tasks without the need for retraining.
Extensive benchmarking across both settings demonstrates that FM-TS
consistently delivers superior performance compared to existing approaches
while being more efficient in terms of training and inference. For instance, in
terms of discriminative score, FM-TS achieves 0.005, 0.019, 0.011, 0.005,
0.053, and 0.106 on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI
unconditional time series datasets, respectively, significantly outperforming
the second-best method which achieves 0.006, 0.067, 0.061, 0.008, 0.122, and
0.167 on the same datasets. We have achieved superior performance in solar
forecasting and MuJoCo imputation tasks, significantly enhanced by our
innovative $t$ power sampling method. The code is available at
https://github.com/UNITES-Lab/FMTS.",cs.LG
AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search in Deep Recommender System,"Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to
represent various categorical features. Traditional DLRMs adopt unified
embedding size for all features, leading to suboptimal performance and
redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works
focus on obtaining mixed embedding sizes with strong model performance.
However, previous AES works can hardly address several challenges together: (1)
The search results of embedding sizes are unstable; (2) Recommendation effect
with AES results is unsatisfactory; (3) Memory cost of embeddings is
uncontrollable. To address these challenges, we propose a novel one-shot AES
framework called AdaS&S, in which a supernet encompassing various candidate
embeddings is built and AES is performed as searching network architectures
within it. Our framework contains two main stages: In the first stage, we
decouple training parameters from searching embedding sizes, and propose the
Adaptive Sampling method to yield a well-trained supernet, which further helps
to produce stable AES results. In the second stage, to obtain embedding sizes
that benefits the model effect, we design a reinforcement learning search
process which utilizes the supernet trained previously. Meanwhile, to adapt
searching to specific resource constraint, we introduce the resource
competition penalty to balance the model effectiveness and memory cost of
embeddings. We conduct extensive experiments on public datasets to show the
superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving
about 20% of model parameters. Empirical analysis also shows that the stability
of searching results in AdaS&S significantly exceeds other methods.",cs.LG
A Novel Automatic Real-time Motion Tracking Method for Magnetic Resonance Imaging-guided Radiotherapy: Leveraging the Enhanced Tracking-Learning-Detection Framework with Automatic Segmentation,"Objective: Ensuring the precision in motion tracking for MRI-guided
Radiotherapy (MRIgRT) is crucial for the delivery of effective treatments. This
study refined the motion tracking accuracy in MRIgRT through the innovation of
an automatic real-time tracking method, leveraging an enhanced
Tracking-Learning-Detection (ETLD) framework coupled with automatic
segmentation. Methods: We developed a novel MRIgRT motion tracking method by
integrating two primary methods: the ETLD framework and an improved Chan-Vese
model (ICV), named ETLD+ICV. The TLD framework was upgraded to suit real-time
cine MRI, including advanced image preprocessing, no-reference image quality
assessment, an enhanced median-flow tracker, and a refined detector with
dynamic search region adjustments. Additionally, ICV was combined for precise
coverage of the target volume, which refined the segmented region frame by
frame using tracking results, with key parameters optimized. Tested on 3.5D MRI
scans from 10 patients with liver metastases, our method ensures precise
tracking and accurate segmentation vital for MRIgRT. Results: An evaluation of
106,000 frames across 77 treatment fractions revealed sub-millimeter tracking
errors of less than 0.8mm, with over 99% precision and 98% recall for all
subjects, underscoring the robustness and efficacy of the ETLD. Moreover, the
ETLD+ICV yielded a dice global score of more than 82% for all subjects,
demonstrating the proposed method's extensibility and precise target volume
coverage. Conclusions: This study successfully developed an automatic real-time
motion tracking method for MRIgRT that markedly surpasses current methods. The
novel method not only delivers exceptional precision in tracking and
segmentation but also demonstrates enhanced adaptability to clinical demands,
positioning it as an indispensable asset in the quest to augment the efficacy
of radiotherapy treatments.",cs.LG
LAuReL: Learned Augmented Residual Layer,"One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.",cs.LG
ADMM for Structured Fractional Minimization,"We consider a class of structured fractional minimization problems, where the
numerator includes a differentiable function, a simple nonconvex nonsmooth
function, a concave nonsmooth function, and a convex nonsmooth function
composed with a linear operator, while the denominator is a continuous function
that is either weakly convex or has a weakly convex square root. These problems
are widespread and span numerous essential applications in machine learning and
data science. Existing methods are mainly based on subgradient methods and
smoothing proximal gradient methods, which may suffer from slow convergence and
numerical stability issues. In this paper, we introduce {\sf FADMM}, the first
Alternating Direction Method of Multipliers tailored for this class of
problems. {\sf FADMM} decouples the original problem into linearized proximal
subproblems, featuring two variants: one using Dinkelbach's parametric method
({\sf FADMM-D}) and the other using the quadratic transform method ({\sf
FADMM-Q}). By introducing a novel Lyapunov function, we establish that {\sf
FADMM} converges to $\epsilon$-approximate critical points of the problem
within an oracle complexity of $\mathcal{O}(1/\epsilon^{3})$. Our experiments
on synthetic and real-world data for sparse Fisher discriminant analysis,
robust Sharpe ratio minimization, and robust sparse recovery demonstrate the
effectiveness of our approach.
  Keywords: Fractional Minimization, Nonconvex Optimization, Proximal
Linearized ADMM, Nonsmooth Optimization, Convergence Analysis",cs.LG
Quantifying Knowledge Distillation Using Partial Information Decomposition,"Knowledge distillation provides an effective method for deploying complex
machine learning models in resource-constrained environments. It typically
involves training a smaller student model to emulate either the probabilistic
outputs or the internal feature representations of a larger teacher model. By
doing so, the student model often achieves substantially better performance on
a downstream task compared to when it is trained independently. Nevertheless,
the teacher's internal representations can also encode noise or additional
information that may not be relevant to the downstream task. This observation
motivates our primary question: What are the information-theoretic limits of
knowledge transfer? To this end, we leverage a body of work in information
theory called Partial Information Decomposition (PID) to quantify the
distillable and distilled knowledge of a teacher's representation corresponding
to a given student and a downstream task. Moreover, we demonstrate that this
metric can be practically used in distillation to address challenges caused by
the complexity gap between the teacher and the student representations.",cs.LG
Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling,"Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.",cs.LG
Privacy-Preserving Verifiable Neural Network Inference Service,"Machine learning has revolutionized data analysis and pattern recognition,
but its resource-intensive training has limited accessibility. Machine Learning
as a Service (MLaaS) simplifies this by enabling users to delegate their data
samples to an MLaaS provider and obtain the inference result using a
pre-trained model. Despite its convenience, leveraging MLaaS poses significant
privacy and reliability concerns to the client. Specifically, sensitive
information from the client inquiry data can be leaked to an adversarial MLaaS
provider. Meanwhile, the lack of a verifiability guarantee can potentially
result in biased inference results or even unfair payment issues. While
existing trustworthy machine learning techniques, such as those relying on
verifiable computation or secure computation, offer solutions to privacy and
reliability concerns, they fall short of simultaneously protecting the privacy
of client data and providing provable inference verifiability.
  In this paper, we propose vPIN, a privacy-preserving and verifiable CNN
inference scheme that preserves privacy for client data samples while ensuring
verifiability for the inference. vPIN makes use of partial homomorphic
encryption and commit-and-prove succinct non-interactive argument of knowledge
techniques to achieve desirable security properties. In vPIN, we develop
various optimization techniques to minimize the proving circuit for homomorphic
inference evaluation thereby, improving the efficiency and performance of our
technique. We fully implemented and evaluated our vPIN scheme on standard
datasets (e.g., MNIST, CIFAR-10). Our experimental results show that vPIN
achieves high efficiency in terms of proving time, verification time, and proof
size, while providing client data privacy guarantees and provable
verifiability.",cs.LG
Machines and Mathematical Mutations: Using GNNs to Characterize Quiver Mutation Classes,"Machine learning is becoming an increasingly valuable tool in mathematics,
enabling one to identify subtle patterns across collections of examples so vast
that they would be impossible for a single researcher to feasibly review and
analyze. In this work, we use graph neural networks to investigate quiver
mutation -- an operation that transforms one quiver (or directed multigraph)
into another -- which is central to the theory of cluster algebras with deep
connections to geometry, topology, and physics. In the study of cluster
algebras, the question of mutation equivalence is of fundamental concern: given
two quivers, can one efficiently determine if one quiver can be transformed
into the other through a sequence of mutations? Currently, this question has
only been resolved in specific cases. In this paper, we use graph neural
networks and AI explainability techniques to discover mutation equivalence
criteria for the previously unknown case of quivers of type $\tilde{D}_n$.
Along the way, we also show that even without explicit training to do so, our
model captures structure within its hidden representation that allows us to
reconstruct known criteria from type $D_n$, adding to the growing evidence that
modern machine learning models are capable of learning abstract and general
rules from mathematical data.",cs.LG
IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark,"Recent evaluations of LLMs on coreference resolution have revealed that
traditional output formats and evaluation metrics do not fully capture the
models' referential understanding. To address this, we introduce IdentifyMe, a
new benchmark for mention resolution presented in a multiple-choice question
(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long
narratives and employs heuristics to exclude easily identifiable mentions,
creating a more challenging task. The benchmark also consists of a curated
mixture of different mention types and corresponding entities, allowing for a
fine-grained analysis of model performance. We evaluate both closed- and open
source LLMs on IdentifyMe and observe a significant performance gap (20-30%)
between the state-of-the-art sub-10B open models vs. closed ones. We observe
that pronominal mentions, which have limited surface information, are typically
much harder for models to resolve than nominal mentions. Additionally, we find
that LLMs often confuse entities when their mentions overlap in nested
structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,
highlighting the strong referential capabilities of state-of-the-art LLMs while
also indicating room for further improvement.",cs.LG
BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including
generation of code snippets, but often struggle with generating code for
complex Machine Learning (ML) tasks. Although existing LLM single-agent based
systems give varying performance depending on the task complexity, they purely
rely on larger and expensive models such as GPT-4. Our investigation reveals
that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama
perform far worse than GPT-4 in a single-agent setting. With the motivation of
developing a cost-efficient LLM based solution for solving ML tasks, we propose
an LLM Multi-Agent based system which leverages combination of experts using
profiling, efficient retrieval of past observations, LLM cascades, and
ask-the-expert calls. Through empirical analysis on ML engineering tasks in the
MLAgentBench benchmark, we demonstrate the effectiveness of our system, using
no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and
expert to serve occasional ask-the-expert calls for planning. With 94.2\%
reduction in the cost (from \$0.931 per run cost averaged over all tasks for
GPT-4 single agent system to \$0.054), our system is able to yield better
average success rate of 32.95\% as compared to GPT-4 single-agent system
yielding 22.72\% success rate averaged over all the tasks of MLAgentBench.",cs.LG
"MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation Models, Convolutional Neural Networks, and Uncertainty Quantification for High-Speed Video Phase Detection Data","Purpose: High-speed video (HSV) phase detection (PD) segmentation is vital in
nuclear reactors, chemical processing, and electronics cooling for detecting
vapor, liquid, and microlayer phases. Traditional segmentation models face
pixel-level accuracy and generalization issues in multimodal data. MSEG-VCUQ
introduces VideoSAM, a hybrid framework leveraging convolutional neural
networks (CNNs) and transformer-based vision models to enhance segmentation
accuracy and generalizability across complex multimodal PD tasks. Methods:
VideoSAM combines U-Net CNN and the Segment Anything Model (SAM) for advanced
feature extraction and segmentation across diverse HSV PD modalities, spanning
fluids like water, FC-72, nitrogen, and argon under varied heat flux
conditions. The framework also incorporates uncertainty quantification (UQ) to
assess pixel-based discretization errors, delivering reliable metrics such as
contact line density and dry area fraction under experimental conditions.
Results: VideoSAM outperforms SAM and modality-specific CNN models in
segmentation accuracy, excelling in environments with complex phase boundaries,
overlapping bubbles, and dynamic liquid-vapor interactions. Its hybrid
architecture supports cross-dataset generalization, adapting effectively to
varying modalities. The UQ module provides accurate error estimates, enhancing
the reliability of segmentation outputs for advanced HSV PD research.
Conclusion: MSEG-VCUQ, via VideoSAM, offers a robust solution for HSV PD
segmentation, addressing previous limitations with advanced deep learning and
UQ techniques. The open-source datasets and tools introduced enable scalable,
precise, and adaptable segmentation for multimodal PD datasets, supporting
advancements in HSV analysis and autonomous experimentation. The codes and data
used for this paper are publicly available at:
\url{https://github.com/chikap421/mseg_vcuq}",cs.LG
"Optimizing Data Delivery: Insights from User Preferences on Visuals, Tables, and Text","In this work, we research user preferences to see a chart, table, or text
given a question asked by the user. This enables us to understand when it is
best to show a chart, table, or text to the user for the specific question. For
this, we conduct a user study where users are shown a question and asked what
they would prefer to see and used the data to establish that a user's personal
traits does influence the data outputs that they prefer. Understanding how user
characteristics impact a user's preferences is critical to creating data tools
with a better user experience. Additionally, we investigate to what degree an
LLM can be used to replicate a user's preference with and without user
preference data. Overall, these findings have significant implications
pertaining to the development of data tools and the replication of human
preferences using LLMs. Furthermore, this work demonstrates the potential use
of LLMs to replicate user preference data which has major implications for
future user modeling and personalization research.",cs.LG
Fast unsupervised ground metric learning with tree-Wasserstein distance,"The performance of unsupervised methods such as clustering depends on the
choice of distance metric between features, or ground metric. Commonly, ground
metrics are decided with heuristics or learned via supervised algorithms.
However, since many datasets are unlabelled, unsupervised ground metric
learning approaches have been introduced. One recent, promising option uses
Wasserstein singular vectors (WSV), which emerge when computing optimal
transport distances between features and samples simultaneously. While WSV is
effective, it has complexity $\mathcal{O}(n^5)$, which is prohibitively
expensive in some applications. In this work, we propose to augment the WSV
method by embedding samples and features on trees, on which we compute the
tree-Wasserstein distance (TWD). We demonstrate theoretically and empirically
that the algorithm converges to a better approximation of the full WSV approach
than the best known alternatives, and does so with $\mathcal{O}(n^3)$
complexity. In addition, we prove that the initial tree structure can be chosen
flexibly, since tree geometry does not constrain the richness of the
approximation up to the number of edge weights. This proof suggests a fast,
recursive algorithm for computing the tree parameter basis set, which we find
crucial to realising the efficiency gains at scale. Finally, we employ the
tree-WSV algorithm to several single-cell RNA sequencing genomics datasets,
demonstrating its scalability and utility for unsupervised cell-type clustering
problems. These results poise unsupervised ground metric learning with TWD as a
low-rank approximation of WSV with the potential for widespread low-compute
application.",cs.LG
Just Label the Repeats for In-The-Wild Audio-to-Score Alignment,"We propose an efficient workflow for high-quality offline alignment of
in-the-wild performance audio and corresponding sheet music scans (images).
Recent work on audio-to-score alignment extends dynamic time warping (DTW) to
be theoretically able to handle jumps in sheet music induced by repeat
signs-this method requires no human annotations, but we show that it often
yields low-quality alignments. As an alternative, we propose a workflow and
interface that allows users to quickly annotate jumps (by clicking on repeat
signs), requiring a small amount of human supervision but yielding much higher
quality alignments on average. Additionally, we refine audio and score feature
representations to improve alignment quality by: (1) integrating measure
detection into the score feature representation, and (2) using raw onset
prediction probabilities from a music transcription model instead of piano
roll. We propose an evaluation protocol for audio-to-score alignment that
computes the distance between the estimated and ground truth alignment in units
of measures. Under this evaluation, we find that our proposed jump annotation
workflow and improved feature representations together improve alignment
accuracy by 150% relative to prior work (33% to 82%).",cs.LG
Predicting BWR Criticality with Data-Driven Machine Learning Model,"One of the challenges in operating nuclear power plants is to decide the
amount of fuel needed in a cycle. Large-scale nuclear power plants are designed
to operate at base load, meaning that they are expected to always operate at
full power. Economically, a nuclear power plant should burn enough fuel to
maintain criticality until the end of a cycle (EOC). If the reactor goes
subcritical before the end of a cycle, it may result in early coastdown as the
fuel in the core is already depleted. On contrary, if the reactor still has
significant excess reactivity by the end of a cycle, the remaining fuels will
remain unused. In both cases, the plant may lose a significant amount of money.
This work proposes an innovative method based on a data-driven deep learning
model to estimate the excess criticality of a boiling water reactor.",cs.LG
Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources,"Machine learning is increasingly used to select which individuals receive
limited-resource interventions in domains such as human services, education,
development, and more. However, it is often not apparent what the right
quantity is for models to predict. In particular, policymakers rarely have
access to data from a randomized controlled trial (RCT) that would enable
accurate estimates of treatment effects -- which individuals would benefit more
from the intervention. Observational data is more likely to be available,
creating a substantial risk of bias in treatment effect estimates.
Practitioners instead commonly use a technique termed ""risk-based targeting""
where the model is just used to predict each individual's status quo outcome
(an easier, non-causal task). Those with higher predicted risk are offered
treatment. There is currently almost no empirical evidence to inform which
choices lead to the most effect machine learning-informed targeting strategies
in social domains. In this work, we use data from 5 real-world RCTs in a
variety of domains to empirically assess such choices. We find that risk-based
targeting is almost always inferior to targeting based on even biased estimates
of treatment effects. Moreover, these results hold even when the policymaker
has strong normative preferences for assisting higher-risk individuals. Our
results imply that, despite the widespread use of risk prediction models in
applied settings, practitioners may be better off incorporating even weak
evidence about heterogeneous causal effects to inform targeting.",cs.LG
ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting,"Addressing the challenges of irregularity and concept drift in streaming time
series is crucial in real-world predictive modelling. Previous studies in time
series continual learning often propose models that require buffering of long
sequences, potentially restricting the responsiveness of the inference system.
Moreover, these models are typically designed for regularly sampled data, an
unrealistic assumption in real-world scenarios. This paper introduces
ODEStream, a novel buffer-free continual learning framework that incorporates a
temporal isolation layer that integrates temporal dependencies within the data.
Simultaneously, it leverages the capability of neural ordinary differential
equations to process irregular sequences and generate a continuous data
representation, enabling seamless adaptation to changing dynamics in a data
streaming scenario. Our approach focuses on learning how the dynamics and
distribution of historical data change with time, facilitating the direct
processing of streaming sequences. Evaluations on benchmark real-world datasets
demonstrate that ODEStream outperforms the state-of-the-art online learning and
streaming analysis baselines, providing accurate predictions over extended
periods while minimising performance degradation over time by learning how the
sequence dynamics change.",cs.LG
Federated Learning Client Pruning for Noisy Labels,"Federated Learning (FL) enables collaborative model training across
decentralized edge devices while preserving data privacy. However, existing FL
methods often assume clean annotated datasets, impractical for
resource-constrained edge devices. In reality, noisy labels are prevalent,
posing significant challenges to FL performance. Prior approaches attempt label
correction and robust training techniques but exhibit limited efficacy,
particularly under high noise levels. This paper introduces ClipFL (Federated
Learning Client Pruning), a novel framework addressing noisy labels from a
fresh perspective. ClipFL identifies and excludes noisy clients based on their
performance on a clean validation dataset, tracked using a Noise Candidacy
Score (NCS). The framework comprises three phases: pre-client pruning to
identify potential noisy clients and calculate their NCS, client pruning to
exclude a percentage of clients with the highest NCS, and post-client pruning
for fine-tuning the global model with standard FL on clean clients. Empirical
evaluation demonstrates ClipFL's efficacy across diverse datasets and noise
levels, achieving accurate noisy client identification, superior performance,
faster convergence, and reduced communication costs compared to
state-of-the-art FL methods. Our code is available at
https://github.com/MMorafah/ClipFL.",cs.LG
Identifying Differential Patient Care Through Inverse Intent Inference,"Sepsis is a life-threatening condition defined by end-organ dysfunction due
to a dysregulated host response to infection. Although the Surviving Sepsis
Campaign has launched and has been releasing sepsis treatment guidelines to
unify and normalize the care for sepsis patients, it has been reported in
numerous studies that disparities in care exist across the trajectory of
patient stay in the emergency department and intensive care unit. Here, we
apply a number of reinforcement learning techniques including behavioral
cloning, imitation learning, and inverse reinforcement learning, to learn the
optimal policy in the management of septic patient subgroups using expert
demonstrations. Then we estimate the counterfactual optimal policies by
applying the model to another subset of unseen medical populations and identify
the difference in cure by comparing it to the real policy. Our data comes from
the sepsis cohort of MIMIC-IV and the clinical data warehouses of the Mass
General Brigham healthcare system. The ultimate objective of this work is to
use the optimal learned policy function to estimate the counterfactual
treatment policy and identify deviations across sub-populations of interest. We
hope this approach would help us identify any disparities in care and also
changes in cure in response to the publication of national sepsis treatment
guidelines.",cs.LG
Factorised Active Inference for Strategic Multi-Agent Interactions,"Understanding how individual agents make strategic decisions within
collectives is important for advancing fields as diverse as economics,
neuroscience, and multi-agent systems. Two complementary approaches can be
integrated to this end. The Active Inference framework (AIF) describes how
agents employ a generative model to adapt their beliefs about and behaviour
within their environment. Game theory formalises strategic interactions between
agents with potentially competing objectives. To bridge the gap between the
two, we propose a factorisation of the generative model whereby each agent
maintains explicit, individual-level beliefs about the internal states of other
agents, and uses them for strategic planning in a joint context. We apply our
model to iterated general-sum games with 2 and 3 players, and study the
ensemble effects of game transitions, where the agents' preferences (game
payoffs) change over time. This non-stationarity, beyond that caused by
reciprocal adaptation, reflects a more naturalistic environment in which agents
need to adapt to changing social contexts. Finally, we present a dynamical
analysis of key AIF quantities: the variational free energy (VFE) and the
expected free energy (EFE) from numerical simulation data. The ensemble-level
EFE allows us to characterise the basins of attraction of games with multiple
Nash Equilibria under different conditions, and we find that it is not
necessarily minimised at the aggregate level. By integrating AIF and game
theory, we can gain deeper insights into how intelligent collectives emerge,
learn, and optimise their actions in dynamic environments, both cooperative and
non-cooperative.",cs.LG
Exploring Variational Autoencoders for Medical Image Generation: A Comprehensive Study,"Variational autoencoder (VAE) is one of the most common techniques in the
field of medical image generation, where this architecture has shown advanced
researchers in recent years and has developed into various architectures. VAE
has advantages including improving datasets by adding samples in smaller
datasets and in datasets with imbalanced classes, and this is how data
augmentation works. This paper provides a comprehensive review of studies on
VAE in medical imaging, with a special focus on their ability to create
synthetic images close to real data so that they can be used for data
augmentation. This study reviews important architectures and methods used to
develop VAEs for medical images and provides a comparison with other generative
models such as GANs on issues such as image quality, and low diversity of
generated samples. We discuss recent developments and applications in several
medical fields highlighting the ability of VAEs to improve segmentation and
classification accuracy.",cs.LG
Warmstarting for Scaling Language Models,"Scaling model sizes to scale performance has worked remarkably well for the
current large language models paradigm. The research and empirical findings of
various scaling studies led to novel scaling results and laws that guides
subsequent research. High training costs for contemporary scales of data and
models result in a lack of thorough understanding of how to tune and arrive at
such training setups. One direction to ameliorate the cost of pretraining large
models is to warmstart the large-scale training from smaller models that are
cheaper to tune. In this work, we attempt to understand if the behavior of
optimal hyperparameters can be retained under warmstarting for scaling. We
explore simple operations that allow the application of theoretically motivated
methods of zero-shot transfer of optimal hyperparameters using {\mu}Transfer.
We investigate the aspects that contribute to the speedup in convergence and
the preservation of stable training dynamics under warmstarting with
{\mu}Transfer. We find that shrinking smaller model weights, zero-padding, and
perturbing the resulting larger model with scaled initialization from {\mu}P
enables effective warmstarting of $\mut{}$.",cs.LG
Multimodal Fusion Balancing Through Game-Theoretic Regularization,"Multimodal learning can complete the picture of information extraction by
uncovering key dependencies between data sources. However, current systems fail
to fully leverage multiple modalities for optimal performance. This has been
attributed to modality competition, where modalities strive for training
resources, leaving some underoptimized. We show that current balancing methods
struggle to train multimodal models that surpass even simple baselines, such as
ensembles. This raises the question: how can we ensure that all modalities in
multimodal training are sufficiently trained, and that learning from new
modalities consistently improves performance? This paper proposes the
Multimodal Competition Regularizer (MCR), a new loss component inspired by
mutual information (MI) decomposition designed to prevent the adverse effects
of competition in multimodal training. Our key contributions are: 1)
Introducing game-theoretic principles in multimodal learning, where each
modality acts as a player competing to maximize its influence on the final
outcome, enabling automatic balancing of the MI terms. 2) Refining lower and
upper bounds for each MI term to enhance the extraction of task-relevant unique
and shared information across modalities. 3) Suggesting latent space
permutations for conditional MI estimation, significantly improving
computational efficiency. MCR outperforms all previously suggested training
strategies and is the first to consistently improve multimodal learning beyond
the ensemble baseline, clearly demonstrating that combining modalities leads to
significant performance gains on both synthetic and large real-world datasets.",cs.LG
Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations,"While a large body of work inspects language models for biases concerning
gender, race, occupation and religion, biases of geographical nature are
relatively less explored. Some recent studies benchmark the degree to which
large language models encode geospatial knowledge. However, the impact of the
encoded geographical knowledge (or lack thereof) on real-world applications has
not been documented. In this work, we examine large language models for two
common scenarios that require geographical knowledge: (a) travel
recommendations and (b) geo-anchored story generation. Specifically, we study
four popular language models, and across about $100$K travel requests, and
$200$K story generations, we observe that travel recommendations corresponding
to poorer countries are less unique with fewer location references, and stories
from these regions more often convey emotions of hardship and sadness compared
to those from wealthier nations.",cs.LG
SynRL: Aligning Synthetic Clinical Trial Data with Human-preferred Clinical Endpoints Using Reinforcement Learning,"Each year, hundreds of clinical trials are conducted to evaluate new medical
interventions, but sharing patient records from these trials with other
institutions can be challenging due to privacy concerns and federal
regulations. To help mitigate privacy concerns, researchers have proposed
methods for generating synthetic patient data. However, existing approaches for
generating synthetic clinical trial data disregard the usage requirements of
these data, including maintaining specific properties of clinical outcomes, and
only use post hoc assessments that are not coupled with the data generation
process. In this paper, we propose SynRL which leverages reinforcement learning
to improve the performance of patient data generators by customizing the
generated data to meet the user-specified requirements for synthetic data
outcomes and endpoints. Our method includes a data value critic function to
evaluate the quality of the generated data and uses reinforcement learning to
align the data generator with the users' needs based on the critic's feedback.
We performed experiments on four clinical trial datasets and demonstrated the
advantages of SynRL in improving the quality of the generated synthetic data
while keeping the privacy risks low. We also show that SynRL can be utilized as
a general framework that can customize data generation of multiple types of
synthetic data generators. Our code is available at
https://anonymous.4open.science/r/SynRL-DB0F/.",cs.LG
Anomaly Detection in OKTA Logs using Autoencoders,"Okta logs are used today to detect cybersecurity events using various
rule-based models with restricted look back periods. These functions have
limitations, such as a limited retrospective analysis, a predefined rule set,
and susceptibility to generating false positives. To address this, we adopt
unsupervised techniques, specifically employing autoencoders. To properly use
an autoencoder, we need to transform and simplify the complexity of the log
data we receive from our users. This transformed and filtered data is then fed
into the autoencoder, and the output is evaluated.",cs.LG
Merit-Based Sortition in Decentralized Systems,"In decentralized systems, it is often necessary to select an 'active' subset
of participants from the total participant pool, with the goal of satisfying
computational limitations or optimizing resource efficiency. This selection can
sometimes be made at random, mirroring the sortition practice invented in
classical antiquity aimed at achieving a high degree of statistical
representativeness. However, the recent emergence of specialized decentralized
networks that solve concrete coordination problems and are characterized by
measurable success metrics often requires prioritizing performance optimization
over representativeness. We introduce a simple algorithm for 'merit-based
sortition', in which the quality of each participant influences its probability
of being drafted into the active set, while simultaneously retaining
representativeness by allowing inactive participants an infinite number of
chances to be drafted into the active set with non-zero probability. Using a
suite of numerical experiments, we demonstrate that our algorithm boosts the
quality metric describing the performance of the active set by $>2$ times the
intrinsic stochasticity. This implies that merit-based sortition ensures a
statistically significant performance boost to the drafted, 'active' set, while
retaining the property of classical, random sortition that it enables upward
mobility from a much larger 'inactive' set. This way, merit-based sortition
fulfils a key requirement for decentralized systems in need of performance
optimization.",cs.LG
Artificial Intelligence Ecosystem for Automating Self-Directed Teaching,"This research introduces an innovative artificial intelligence-driven
educational concept designed to optimize self-directed learning through
personalized course delivery and automated teaching assistance. The system
leverages fine-tuned AI models to create an adaptive learning environment that
encompasses customized roadmaps, automated presentation generation, and
three-dimensional modeling for complex concept visualization. By integrating
real-time virtual assistance for doubt resolution, the platform addresses the
immediate educational needs of learners while promoting autonomous learning
practices. This study explores the psychological advantages of self-directed
learning and demonstrates how AI automation can enhance educational outcomes
through personalized content delivery and interactive support mechanisms. The
research contributes to the growing field of educational technology by
presenting a comprehensive framework that combines automated content
generation, visual learning aids, and intelligent tutoring to create an
efficient, scalable solution for modern educational needs. Preliminary findings
suggest that this approach not only accommodates diverse learning styles but
also strengthens student engagement and knowledge retention through its
emphasis on self-paced, independent learning methodologies.",cs.LG
The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,"Language models have shown impressive performance on tasks within their
training distribution, but often struggle with novel problems requiring complex
reasoning. We investigate the effectiveness of test-time training (TTT) --
updating model parameters temporarily during inference using a loss derived
from input data -- as a mechanism for improving models' reasoning capabilities,
using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through
systematic experimentation, we identify three crucial components for successful
TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and
augmentations (3) per-instance training. TTT significantly improves performance
on ARC tasks, achieving up to 6x improvement in accuracy compared to base
fine-tuned models; applying TTT to an 8B-parameter language model, we achieve
53% accuracy on the ARC's public validation set, improving the state-of-the-art
by nearly 25% for public and purely neural approaches. By ensembling our method
with recent program generation approaches, we get SoTA public validation
accuracy of 61.9%, matching the average human score. Our findings suggest that
explicit symbolic search is not the only path to improved abstract reasoning in
neural language models; additional test-time applied to continued training on
few-shot examples can also be extremely effective.",cs.LG
DeepONet as a Multi-Operator Extrapolation Model: Distributed Pretraining with Physics-Informed Fine-Tuning,"We propose a novel fine-tuning method to achieve multi-operator learning
through training a distributed neural operator with diverse function data and
then zero-shot fine-tuning the neural network using physics-informed losses for
downstream tasks. Operator learning effectively approximates solution operators
for PDEs and various PDE-related problems, yet it often struggles to generalize
to new tasks. To address this, we investigate fine-tuning a pretrained model,
while carefully selecting an initialization that enables rapid adaptation to
new tasks with minimal data. Our approach combines distributed learning to
integrate data from various operators in pre-training, while physics-informed
methods enable zero-shot fine-tuning, minimizing the reliance on downstream
data. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,
applying both to train complex nonlinear target operators that are difficult to
learn only using random initialization. Through comprehensive numerical
examples, we demonstrate the advantages of our approach, showcasing significant
improvements in accuracy. Our findings provide a robust framework for advancing
multi-operator learning and highlight the potential of transfer learning
techniques in this domain.",cs.LG
"Score-based generative diffusion with ""active"" correlated noise sources","Diffusion models exhibit robust generative properties by approximating the
underlying distribution of a dataset and synthesizing data by sampling from the
approximated distribution. In this work, we explore how the generative
performance may be be modulated if noise sources with temporal correlations --
akin to those used in the field of active matter -- are used for the
destruction of the data in the forward process. Our numerical and analytical
experiments suggest that the corresponding reverse process may exhibit improved
generative properties.",cs.LG
Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models,"Adding Object into images based on text instructions is a challenging task in
semantic image editing, requiring a balance between preserving the original
scene and seamlessly integrating the new object in a fitting location. Despite
extensive efforts, existing models often struggle with this balance,
particularly with finding a natural location for adding an object in complex
scenes. We introduce Add-it, a training-free approach that extends diffusion
models' attention mechanisms to incorporate information from three key sources:
the scene image, the text prompt, and the generated image itself. Our weighted
extended-attention mechanism maintains structural consistency and fine details
while ensuring natural object placement. Without task-specific fine-tuning,
Add-it achieves state-of-the-art results on both real and generated image
insertion benchmarks, including our newly constructed ""Additing Affordance
Benchmark"" for evaluating object placement plausibility, outperforming
supervised methods. Human evaluations show that Add-it is preferred in over 80%
of cases, and it also demonstrates improvements in various automated metrics.",cs.LG
Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide
a rich source of physical knowledge about the dynamics and motions of objects
and tasks. However, video models are not grounded in the embodiment of an
agent, and do not describe how to actuate the world to reach the visual states
depicted in a video. To tackle this problem, current methods use a separate
vision-based inverse dynamic model trained on embodiment-specific data to map
image states to actions. Gathering data to train such a model is often
expensive and challenging, and this model is limited to visual settings similar
to the ones in which data are available. In this paper, we investigate how to
directly ground video models to continuous actions through self-exploration in
the embodied environment -- using generated video states as visual goals for
exploration. We propose a framework that uses trajectory level action
generation in combination with video guidance to enable an agent to solve
complex tasks without any external supervision, e.g., rewards, action labels,
or segmentation masks. We validate the proposed approach on 8 tasks in Libero,
6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual
Navigation. We show how our approach is on par with or even surpasses multiple
behavior cloning baselines trained on expert demonstrations while without
requiring any action annotations.",cs.LG
Feature Selection Based on Wasserstein Distance,"This paper presents a novel feature selection method leveraging the
Wasserstein distance to improve feature selection in machine learning. Unlike
traditional methods based on correlation or Kullback-Leibler (KL) divergence,
our approach uses the Wasserstein distance to assess feature similarity,
inherently capturing class relationships and making it robust to noisy labels.
We introduce a Markov blanket-based feature selection algorithm and demonstrate
its effectiveness. Our analysis shows that the Wasserstein distance-based
feature selection method effectively reduces the impact of noisy labels without
relying on specific noise models. We provide a lower bound on its
effectiveness, which remains meaningful even in the presence of noise.
Experimental results across multiple datasets demonstrate that our approach
consistently outperforms traditional methods, particularly in noisy settings.",cs.LG
Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks,"A key objective of interpretability research on large language models (LLMs)
is to develop methods for robustly steering models toward desired behaviors. To
this end, two distinct approaches to interpretability -- ``bottom-up"" and
``top-down"" -- have been presented, but there has been little quantitative
comparison between them. We present a case study comparing the effectiveness of
representative vector steering methods from each branch: function vectors (FV;
arXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;
arXiv:2311.06668) as a top-down method. While both aim to capture compact
representations of broad in-context learning tasks, we find they are effective
only on specific types of tasks: ICVs outperform FVs in behavioral shifting,
whereas FVs excel in tasks requiring more precision. We discuss the
implications for future evaluations of steering methods and for further
research into top-down and bottom-up steering given these findings.",cs.LG
General Geospatial Inference with a Population Dynamics Foundation Model,"Supporting the health and well-being of dynamic populations around the world
requires governmental agencies, organizations and researchers to understand and
reason over complex relationships between human behavior and local contexts in
order to identify high-risk groups and strategically allocate limited
resources. Traditional approaches to these classes of problems often entail
developing manually curated, task-specific features and models to represent
human behavior and the natural and built environment, which can be challenging
to adapt to new, or even, related tasks. To address this, we introduce a
Population Dynamics Foundation Model (PDFM) that aims to capture the
relationships between diverse data modalities and is applicable to a broad
range of geospatial tasks. We first construct a geo-indexed dataset for postal
codes and counties across the United States, capturing rich aggregated
information on human behavior from maps, busyness, and aggregated search
trends, and environmental factors such as weather and air quality. We then
model this data and the complex relationships between locations using a graph
neural network, producing embeddings that can be adapted to a wide range of
downstream tasks using relatively simple models. We evaluate the effectiveness
of our approach by benchmarking it on 27 downstream tasks spanning three
distinct domains: health indicators, socioeconomic factors, and environmental
measurements. The approach achieves state-of-the-art performance on all 27
geospatial interpolation tasks, and on 25 out of the 27 extrapolation and
super-resolution tasks. We combined the PDFM with a state-of-the-art
forecasting foundation model, TimesFM, to predict unemployment and poverty,
achieving performance that surpasses fully supervised forecasting. The full set
of embeddings and sample code are publicly available for researchers.",cs.LG
Data-Driven Predictive Control of Nonholonomic Robots Based on a Bilinear Koopman Realization: Data Does Not Replace Geometry,"Advances in machine learning and the growing trend towards effortless data
generation in real-world systems has led to an increasing interest for
data-inferred models and data-based control in robotics. It seems appealing to
govern robots solely based on data, bypassing the traditional, more elaborate
pipeline of system modeling through first-principles and subsequent controller
design. One promising data-driven approach is the Extended Dynamic Mode
Decomposition (EDMD) for control-affine systems, a system class which contains
many vehicles and machines of immense practical importance including, e.g.,
typical wheeled mobile robots. EDMD can be highly data-efficient,
computationally inexpensive, can deal with nonlinear dynamics as prevalent in
robotics and mechanics, and has a sound theoretical foundation rooted in
Koopman theory. On this background, this present paper examines how EDMD models
can be integrated into predictive controllers for nonholonomic mobile robots.
In addition to the conventional kinematic mobile robot, we also cover the
complete data-driven control pipeline - from data acquisition to control design
- when the robot is not treated in terms of first-order kinematics but in a
second-order manner, allowing to account for actuator dynamics. Using only
real-world measurement data, it is shown in both simulations and hardware
experiments that the surrogate models enable high-precision predictive
controllers in the studied cases. However, the findings raise significant
concerns about purely data-centric approaches that overlook the underlying
geometry of nonholonomic systems, showing that, for nonholonomic systems, some
geometric insight seems necessary and cannot be easily compensated for with
large amounts of data.",cs.LG
NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics,"Large language models (LLMs) prompted with text and audio represent the state
of the art in various auditory tasks, including speech, music, and general
audio, showing emergent abilities on unseen tasks. However, these capabilities
have yet to be fully demonstrated in bioacoustics tasks, such as detecting
animal vocalizations in large recordings, classifying rare and endangered
species, and labeling context and behavior - tasks that are crucial for
conservation, biodiversity monitoring, and the study of animal behavior. In
this work, we present NatureLM-audio, the first audio-language foundation model
specifically designed for bioacoustics. Our carefully curated training dataset
comprises text-audio pairs spanning a diverse range of bioacoustics, speech,
and music data, designed to address the challenges posed by limited annotated
datasets in the field. We demonstrate successful transfer of learned
representations from music and speech to bioacoustics, and our model shows
promising generalization to unseen taxa and tasks. Importantly, we test
NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of
the art (SotA) on several bioacoustics tasks, including zero-shot
classification of unseen species. To advance bioacoustics research, we also
open-source the code for generating training and benchmark data, as well as for
training the model.",cs.LG
Constructing Gaussian Processes via Samplets,"Gaussian Processes face two primary challenges: constructing models for large
datasets and selecting the optimal model. This master's thesis tackles these
challenges in the low-dimensional case. We examine recent convergence results
to identify models with optimal convergence rates and pinpoint essential
parameters. Utilizing this model, we propose a Samplet-based approach to
efficiently construct and train the Gaussian Processes, reducing the cubic
computational complexity to a log-linear scale. This method facilitates optimal
regression while maintaining efficient performance.",cs.LG
Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised Domain Adaptation,"Multi-source unsupervised domain adaptation aims to leverage labeled data
from multiple source domains for training a machine learning model to
generalize well on a target domain without labels. Source domain selection
plays a crucial role in determining the model's performance. It relies on the
similarities amongst source and target domains. Nonetheless, existing work for
source domain selection often involves heavyweight computational procedures,
especially when dealing with numerous source domains and the need to identify
the best ones from them. In this paper, we introduce a framework for gradual
fine tuning (GFT) of machine learning models on multiple source domains. We
represent multiple source domains as an undirected weighted graph. We then give
a new generalization error bound for GFT along any path within the graph, which
is used to determine the optimal path corresponding to the optimal training
order. With this formulation, we introduce three lightweight graph-routing
strategies which tend to minimize the error bound. Our best strategy improves
$2.3\%$ of accuracy over the state-of-the-art on Natural Language Inference
(NLI) task and achieves competitive performance on Sentiment Analysis (SA)
task, especially a $3.9\%$ improvement on a more diverse subset of data we use
for SA.",cs.LG
Revisiting Ensembling in One-Shot Federated Learning,"Federated learning (FL) is an appealing approach to training machine learning
models without sharing raw data. However, standard FL algorithms are iterative
and thus induce a significant communication cost. One-shot federated learning
(OFL) trades the iterative exchange of models between clients and the server
with a single round of communication, thereby saving substantially on
communication costs. Not surprisingly, OFL exhibits a performance gap in terms
of accuracy with respect to FL, especially under high data heterogeneity. We
introduce FENS, a novel federated ensembling scheme that approaches the
accuracy of FL with the communication efficiency of OFL. Learning in FENS
proceeds in two phases: first, clients train models locally and send them to
the server, similar to OFL; second, clients collaboratively train a lightweight
prediction aggregator model using FL. We showcase the effectiveness of FENS
through exhaustive experiments spanning several datasets and heterogeneity
levels. In the particular case of heterogeneously distributed CIFAR-10 dataset,
FENS achieves up to a 26.9% higher accuracy over state-of-the-art (SOTA) OFL,
being only 3.1% lower than FL. At the same time, FENS incurs at most 4.3x more
communication than OFL, whereas FL is at least 10.9x more
communication-intensive than FENS.",cs.LG
Counterfactual Generation from Language Models,"Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to intervene on
these models. To understand the impact of interventions precisely, it is useful
to examine counterfactuals -- e.g., how a given sentence would have appeared
had it been generated by the model following a specific intervention. We
highlight that counterfactual reasoning is conceptually distinct from
interventions, as articulated in Pearl's causal hierarchy. Based on this
observation, we propose a framework for generating true string counterfactuals
by reformulating language models as Generalized Structural-equation. Models
using the Gumbel-max trick. This allows us to model the joint distribution over
original strings and their counterfactuals resulting from the same
instantiation of the sampling noise. We develop an algorithm based on hindsight
Gumbel sampling that allows us to infer the latent noise variables and generate
counterfactuals of observed strings. Our experiments demonstrate that the
approach produces meaningful counterfactuals while at the same time showing
that commonly used intervention techniques have considerable undesired side
effects.",cs.LG
Joint Age-State Belief is All You Need: Minimizing AoII via Pull-Based Remote Estimation,"Age of incorrect information (AoII) is a recently proposed freshness and
mismatch metric that penalizes an incorrect estimation along with its duration.
Therefore, keeping track of AoII requires the knowledge of both the source and
estimation processes. In this paper, we consider a time-slotted pull-based
remote estimation system under a sampling rate constraint where the information
source is a general discrete-time Markov chain (DTMC) process. Moreover, packet
transmission times from the source to the monitor are non-zero which disallows
the monitor to have perfect information on the actual AoII process at any time.
Hence, for this pull-based system, we propose the monitor to maintain a
sufficient statistic called {\em belief} which stands for the joint
distribution of the age and source processes to be obtained from the history of
all observations. Using belief, we first propose a maximum a posteriori (MAP)
estimator to be used at the monitor as opposed to existing martingale
estimators in the literature. Second, we obtain the optimality equations from
the belief-MDP (Markov decision process) formulation. Finally, we propose two
belief-dependent policies one of which is based on deep reinforcement learning,
and the other one is a threshold-based policy based on the instantaneous
expected AoII.",cs.LG
More Expressive Attention with Negative Weights,"We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.",cs.LG
Anytime Sequential Halving in Monte-Carlo Tree Search,"Monte-Carlo Tree Search (MCTS) typically uses multi-armed bandit (MAB)
strategies designed to minimize cumulative regret, such as UCB1, as its
selection strategy. However, in the root node of the search tree, it is more
sensible to minimize simple regret. Previous work has proposed using Sequential
Halving as selection strategy in the root node, as, in theory, it performs
better with respect to simple regret. However, Sequential Halving requires a
budget of iterations to be predetermined, which is often impractical. This
paper proposes an anytime version of the algorithm, which can be halted at any
arbitrary time and still return a satisfactory result, while being designed
such that it approximates the behavior of Sequential Halving. Empirical results
in synthetic MAB problems and ten different board games demonstrate that the
algorithm's performance is competitive with Sequential Halving and UCB1 (and
their analogues in MCTS).",cs.LG
Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network,"Mining machinery operating in variable environments faces high wear and
unpredictable stress, challenging Predictive Maintenance (PdM). This paper
introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a
hierarchical inference framework across edge devices, gateways, and cloud
services for real-time condition monitoring. The system dynamically adjusts
inference locations--on-device, on-gateway, or on-cloud--based on trade-offs
among accuracy, latency, and battery life, leveraging Tiny Machine Learning
(TinyML) techniques for model optimization on resource-constrained devices.
Performance evaluations showed that on-sensor and on-gateway inference modes
achieved over 90\% classification accuracy, while cloud-based inference reached
99\%. On-sensor inference reduced power consumption by approximately 44\%,
enabling up to 104 hours of operation. Latency was lowest for on-device
inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or
cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution
for reliable anomaly detection and PdM, crucial for maintaining machinery
uptime in remote environments. By balancing accuracy, latency, and energy
consumption, this approach advances PdM frameworks for industrial applications.",cs.LG
Conditional simulation via entropic optimal transport: Toward non-parametric estimation of conditional Brenier maps,"Conditional simulation is a fundamental task in statistical modeling:
Generate samples from the conditionals given finitely many data points from a
joint distribution. One promising approach is to construct conditional Brenier
maps, where the components of the map pushforward a reference distribution to
conditionals of the target. While many estimators exist, few, if any, come with
statistical or algorithmic guarantees. To this end, we propose a non-parametric
estimator for conditional Brenier maps based on the computational scalability
of \emph{entropic} optimal transport. Our estimator leverages a result of
Carlier et al. (2010), which shows that optimal transport maps under a rescaled
quadratic cost asymptotically converge to conditional Brenier maps; our
estimator is precisely the entropic analogues of these converging maps. We
provide heuristic justifications for choosing the scaling parameter in the cost
as a function of the number of samples by fully characterizing the Gaussian
setting. We conclude by comparing the performance of the estimator to other
machine learning and non-parametric approaches on benchmark datasets and
Bayesian inference problems.",cs.LG
Variational Graph Contrastive Learning,"Graph representation learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-supervised learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of graph
characteristics while controlling the distribution of generated subgraphs. We
employ optimal transport distances, including Wasserstein and
Gromov-Wasserstein distances, to effectively measure the similarity between
subgraphs, enhancing the robustness of the contrastive learning process.
Extensive experiments across multiple benchmarks demonstrate that SGEC
outperforms or presents competitive performance against state-of-the-art
approaches. Our findings provide insights into the design of SSL methods for
GRL, emphasizing the importance of the distribution of the generated
contrastive pairs.",cs.LG
Benchmarking LLMs' Judgments with No Gold Standard,"We introduce the GEM (Generative Estimator for Mutual Information), an
evaluation metric for assessing language generation by Large Language Models
(LLMs), particularly in generating informative judgments, without the need for
a gold standard reference. GEM broadens the scenarios where we can benchmark
LLM generation performance-from traditional ones, like machine translation and
summarization, where gold standard references are readily available, to
subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate
and reference responses, without requiring the reference to be a gold standard.
In experiments on a human-annotated dataset, GEM demonstrates competitive
correlations with human scores compared to the state-of-the-art GPT-4o
Examiner, and outperforms all other baselines. Additionally, GEM is more robust
against strategic manipulations, such as rephrasing or elongation, which can
artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which
evaluates LLMs based on how well they can generate high-quality peer reviews
for academic research papers. Because GRE-bench is based upon GEM, it inherits
its robustness properties. Additionally, GRE-bench circumvents data
contamination problems (or data leakage) by using the continuous influx of new
open-access research papers and peer reviews each year. We show GRE-bench
results of various popular LLMs on their peer review capabilities using the
ICLR2023 dataset.",cs.LG
Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models,"We introduce Edify Image, a family of diffusion models capable of generating
photorealistic image content with pixel-perfect accuracy. Edify Image utilizes
cascaded pixel-space diffusion models trained using a novel Laplacian diffusion
process, in which image signals at different frequency bands are attenuated at
varying rates. Edify Image supports a wide range of applications, including
text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama
generation, and finetuning for image customization.",cs.LG
Fast and Robust Contextual Node Representation Learning over Dynamic Graphs,"Real-world graphs grow rapidly with edge and vertex insertions over time,
motivating the problem of efficiently maintaining robust node representation
over evolving graphs. Recent efficient GNNs are designed to decouple recursive
message passing from the learning process, and favor Personalized PageRank
(PPR) as the underlying feature propagation mechanism. However, most PPR-based
GNNs are designed for static graphs, and efficient PPR maintenance remains as
an open problem. Further, there is surprisingly little theoretical
justification for the choice of PPR, despite its impressive empirical
performance.
  In this paper, we are inspired by the recent PPR formulation as an explicit
$\ell_1$-regularized optimization problem and propose a unified dynamic graph
learning framework based on sparse node-wise attention. We also present a set
of desired properties to justify the choice of PPR in STOA GNNs, and serves as
the guideline for future node attention designs. Meanwhile, we take advantage
of the PPR-equivalent optimization formulation and employ the proximal gradient
method (ISTA) to improve the efficiency of PPR-based GNNs upto 6 times.
Finally, we instantiate a simple-yet-effective model (\textsc{GoPPE}) with
robust positional encodings by maximizing PPR previously used as attention. The
model performs comparably to or better than the STOA baselines and greatly
outperforms when the initial node attributes are noisy during graph evolution,
demonstrating the effectiveness and robustness of \textsc{GoPPE}.",cs.LG
"Efficient Adaptive Optimization via Subset-Norm and Subspace-Momentum: Fast, Memory-Reduced Training with Convergence Guarantees","We introduce two complementary techniques for efficient adaptive optimization
that reduce memory requirements while accelerating training of large-scale
neural networks. The first technique, Subset-Norm adaptive step size,
generalizes AdaGrad-Norm and AdaGrad(-Coordinate) by reducing the second moment
term's memory footprint from $O(d)$ to $O(\sqrt{d})$ through step-size sharing,
where $d$ is the model size. For non-convex smooth objectives under
coordinate-wise sub-gaussian gradient noise, we prove a noise-adapted
high-probability convergence guarantee showing improved dimensional dependence
over existing methods. Our second technique, Subspace-Momentum, reduces the
momentum state's memory footprint by operating in a low-dimensional subspace
while applying standard SGD in the orthogonal complement. We establish
high-probability convergence rates under similar relaxed assumptions. Empirical
evaluation on LLaMA models from 60M to 1B parameters demonstrates the
effectiveness of our methods, where combining subset-norm with
subspace-momentum achieves Adam's validation perplexity in approximately half
the training tokens (6.8B vs 13.1B) while using only 20% of the Adam's
optimizer-states memory footprint and requiring minimal additional
hyperparameter tuning.",cs.LG
ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition,"Transformer models have demonstrated remarkable success in many domains such
as natural language processing (NLP) and computer vision. With the growing
interest in transformer-based architectures, they are now utilized for gesture
recognition. So, we also explore and devise a novel ConvMixFormer architecture
for dynamic hand gestures. The transformers use quadratic scaling of the
attention features with the sequential data, due to which these models are
computationally complex and heavy. We have considered this drawback of the
transformer and designed a resource-efficient model that replaces the
self-attention in the transformer with the simple convolutional layer-based
token mixer. The computational cost and the parameters used for the
convolution-based mixer are comparatively less than the quadratic
self-attention. Convolution-mixer helps the model capture the local spatial
features that self-attention struggles to capture due to their sequential
processing nature. Further, an efficient gate mechanism is employed instead of
a conventional feed-forward network in the transformer to help the model
control the flow of features within different stages of the proposed model.
This design uses fewer learnable parameters which is nearly half the vanilla
transformer that helps in fast and efficient training. The proposed method is
evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has
achieved state-of-the-art results on single and multimodal inputs. We have also
shown the parameter efficiency of the proposed ConvMixFormer model compared to
other methods. The source code is available at
https://github.com/mallikagarg/ConvMixFormer.",cs.LG
TinyML Security: Exploring Vulnerabilities in Resource-Constrained Machine Learning Systems,"Tiny Machine Learning (TinyML) systems, which enable machine learning
inference on highly resource-constrained devices, are transforming edge
computing but encounter unique security challenges. These devices, restricted
by RAM and CPU capabilities two to three orders of magnitude smaller than
conventional systems, make traditional software and hardware security solutions
impractical. The physical accessibility of these devices exacerbates their
susceptibility to side-channel attacks and information leakage. Additionally,
TinyML models pose security risks, with weights potentially encoding sensitive
data and query interfaces that can be exploited. This paper offers the first
thorough survey of TinyML security threats. We present a device taxonomy that
differentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities
unique to TinyML. We list various attack vectors, assess their threat levels
using the Common Vulnerability Scoring System, and evaluate both existing and
possible defenses. Our analysis identifies where traditional security measures
are adequate and where solutions tailored to TinyML are essential. Our results
underscore the pressing need for specialized security solutions in TinyML to
ensure robust and secure edge computing applications. We aim to inform the
research community and inspire innovative approaches to protecting this rapidly
evolving and critical field.",cs.LG
Training Neural Networks as Recognizers of Formal Languages,"Characterizing the computational power of neural network architectures in
terms of formal language theory remains a crucial line of research, as it
describes lower and upper bounds on the reasoning capabilities of modern AI.
However, when empirically testing these bounds, existing work often leaves a
discrepancy between experiments and the formal claims they are meant to
support. The problem is that formal language theory pertains specifically to
recognizers: machines that receive a string as input and classify whether it
belongs to a language. On the other hand, it is common to instead use proxy
tasks that are similar in only an informal sense, such as language modeling or
sequence-to-sequence transduction. We correct this mismatch by training and
evaluating neural networks directly as binary classifiers of strings, using a
general method that can be applied to a wide variety of languages. As part of
this, we extend an algorithm recently proposed by Sn{\ae}bjarnarson et al.
(2024) to do length-controlled sampling of strings from regular languages, with
much better asymptotic time complexity than previous methods. We provide
results on a variety of languages across the Chomsky hierarchy for three neural
architectures: a simple RNN, an LSTM, and a causally-masked transformer. We
find that the RNN and LSTM often outperform the transformer, and that auxiliary
training objectives such as language modeling can help, although no single
objective uniformly improves performance across languages and architectures.
Our contributions will facilitate theoretically sound empirical testing of
language recognition claims in future work. We have released our datasets as a
benchmark called FLaRe (Formal Language Recognition), along with our code.",cs.LG
Learning Multi-Agent Collaborative Manipulation for Long-Horizon Quadrupedal Pushing,"Recently, quadrupedal locomotion has achieved significant success, but their
manipulation capabilities, particularly in handling large objects, remain
limited, restricting their usefulness in demanding real-world applications such
as search and rescue, construction, industrial automation, and room
organization. This paper tackles the task of obstacle-aware, long-horizon
pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent
reinforcement learning framework with three levels of control. The high-level
controller integrates an RRT planner and a centralized adaptive policy to
generate subgoals, while the mid-level controller uses a decentralized
goal-conditioned policy to guide the robots toward these sub-goals. A
pre-trained low-level locomotion policy executes the movement commands. We
evaluate our method against several baselines in simulation, demonstrating
significant improvements over baseline approaches, with 36.0% higher success
rates and 24.5% reduction in completion time than the best baseline. Our
framework successfully enables long-horizon, obstacle-aware manipulation tasks
like Push-Cuboid and Push-T on Go1 robots in the real world.",cs.LG
Effectively Leveraging Momentum Terms in Stochastic Line Search Frameworks for Fast Optimization of Finite-Sum Problems,"In this work, we address unconstrained finite-sum optimization problems, with
particular focus on instances originating in large scale deep learning
scenarios. Our main interest lies in the exploration of the relationship
between recent line search approaches for stochastic optimization in the
overparametrized regime and momentum directions. First, we point out that
combining these two elements with computational benefits is not
straightforward. To this aim, we propose a solution based on mini-batch
persistency. We then introduce an algorithmic framework that exploits a mix of
data persistency, conjugate-gradient type rules for the definition of the
momentum parameter and stochastic line searches. The resulting algorithm is
empirically shown to outperform other popular methods from the literature,
obtaining state-of-the-art results in both convex and nonconvex large scale
training problems.",cs.LG
Bounded Rationality Equilibrium Learning in Mean Field Games,"Mean field games (MFGs) tractably model behavior in large agent populations.
The literature on learning MFG equilibria typically focuses on finding Nash
equilibria (NE), which assume perfectly rational agents and are hence
implausible in many realistic situations. To overcome these limitations, we
incorporate bounded rationality into MFGs by leveraging the well-known concept
of quantal response equilibria (QRE). Two novel types of MFG QRE enable the
modeling of large agent populations where individuals only noisily estimate the
true objective. We also introduce a second source of bounded rationality to
MFGs by restricting agents' planning horizon. The resulting novel receding
horizon (RH) MFGs are combined with QRE and existing approaches to model
different aspects of bounded rationality in MFGs. We formally define MFG QRE
and RH MFGs and compare them to existing equilibrium concepts such as
entropy-regularized NE. Subsequently, we design generalized fixed point
iteration and fictitious play algorithms to learn QRE and RH equilibria. After
a theoretical analysis, we give different examples to evaluate the capabilities
of our learning algorithms and outline practical differences between the
equilibrium concepts.",cs.LG
Differentially-Private Collaborative Online Personalized Mean Estimation,"We consider the problem of collaborative personalized mean estimation under a
privacy constraint in an environment of several agents continuously receiving
data according to arbitrary unknown agent-specific distributions. In
particular, we provide a method based on hypothesis testing coupled with
differential privacy and data variance estimation. Two privacy mechanisms and
two data variance estimation schemes are proposed, and we provide a theoretical
convergence analysis of the proposed algorithm for any bounded unknown
distributions on the agents' data, showing that collaboration provides faster
convergence than a fully local approach where agents do not share data.
Moreover, we provide analytical performance curves for the case with an oracle
class estimator, i.e., the class structure of the agents, where agents
receiving data from distributions with the same mean are considered to be in
the same class, is known. The theoretical faster-than-local convergence
guarantee is backed up by extensive numerical results showing that for a
considered scenario the proposed approach indeed converges much faster than a
fully local approach, and performs comparably to ideal performance where all
data is public. This illustrates the benefit of private collaboration in an
online setting.",cs.LG
Towards Characterizing Cyber Networks with Large Language Models,"Threat hunting analyzes large, noisy, high-dimensional data to find sparse
adversarial behavior. We believe adversarial activities, however they are
disguised, are extremely difficult to completely obscure in high dimensional
space. In this paper, we employ these latent features of cyber data to find
anomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM
was trained on Zeek network traffic logs from both a real-world production
network and an from Internet of Things (IoT) cybersecurity testbed. The model
is deliberately overtrained on a sliding window of data to characterize each
window closely. We use the Adjusted Rand Index (ARI) to comparing the k-means
clustering of CLEM output to expert labeling of the embeddings. Our approach
demonstrates that there is promise in using natural language modeling to
understand cyber data.",cs.LG
OCMDP: Observation-Constrained Markov Decision Process,"In many practical applications, decision-making processes must balance the
costs of acquiring information with the benefits it provides. Traditional
control systems often assume full observability, an unrealistic assumption when
observations are expensive. We tackle the challenge of simultaneously learning
observation and control strategies in such cost-sensitive environments by
introducing the Observation-Constrained Markov Decision Process (OCMDP), where
the policy influences the observability of the true state. To manage the
complexity arising from the combined observation and control actions, we
develop an iterative, model-free deep reinforcement learning algorithm that
separates the sensing and control components of the policy. This decomposition
enables efficient learning in the expanded action space by focusing on when and
what to observe, as well as determining optimal control actions, without
requiring knowledge of the environment's dynamics. We validate our approach on
a simulated diagnostic task and a realistic healthcare environment using
HeartPole. Given both scenarios, the experimental results demonstrate that our
model achieves a substantial reduction in observation costs on average,
significantly outperforming baseline methods by a notable margin in efficiency.",cs.LG
To Train or Not to Train: Balancing Efficiency and Training Cost in Deep Reinforcement Learning for Mobile Edge Computing,"Artificial Intelligence (AI) is a key component of 6G networks, as it enables
communication and computing services to adapt to end users' requirements and
demand patterns. The management of Mobile Edge Computing (MEC) is a meaningful
example of AI application: computational resources available at the network
edge need to be carefully allocated to users, whose jobs may have different
priorities and latency requirements. The research community has developed
several AI algorithms to perform this resource allocation, but it has neglected
a key aspect: learning is itself a computationally demanding task, and
considering free training results in idealized conditions and performance in
simulations. In this work, we consider a more realistic case in which the cost
of learning is specifically accounted for, presenting a new algorithm to
dynamically select when to train a Deep Reinforcement Learning (DRL) agent that
allocates resources. Our method is highly general, as it can be directly
applied to any scenario involving a training overhead, and it can approach the
same performance as an ideal learning agent even under realistic training
conditions.",cs.LG
An Interpretable X-ray Style Transfer via Trainable Local Laplacian Filter,"Radiologists have preferred visual impressions or 'styles' of X-ray images
that are manually adjusted to their needs to support their diagnostic
performance. In this work, we propose an automatic and interpretable X-ray
style transfer by introducing a trainable version of the Local Laplacian Filter
(LLF). From the shape of the LLF's optimized remap function, the
characteristics of the style transfer can be inferred and reliability of the
algorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray
style features by replacing the remap function with a Multi-Layer Perceptron
(MLP) and adding a trainable normalization layer. We demonstrate the
effectiveness of the proposed method by transforming unprocessed mammographic
X-ray images into images that match the style of target mammograms and achieve
a Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline
LLF style transfer method from Aubry et al.",cs.LG
Universal Response and Emergence of Induction in LLMs,"While induction is considered a key mechanism for in-context learning in
LLMs, understanding its precise circuit decomposition beyond toy models remains
elusive. Here, we study the emergence of induction behavior within LLMs by
probing their response to weak single-token perturbations of the residual
stream. We find that LLMs exhibit a robust, universal regime in which their
response remains scale-invariant under changes in perturbation strength,
thereby allowing us to quantify the build-up of token correlations throughout
the model. By applying our method, we observe signatures of induction behavior
within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across
all models, we find that these induction signatures gradually emerge within
intermediate layers and identify the relevant model sections composing this
behavior. Our results provide insights into the collective interplay of
components within LLMs and serve as a benchmark for large-scale circuit
analysis.",cs.LG
Empirical Quantum Advantage Analysis of Quantum Kernel in Gene Expression Data,"The incorporation of quantum ansatz with machine learning classification
models demonstrates the ability to extract patterns from data for
classification tasks. However, taking advantage of the enhanced computational
power of quantum machine learning necessitates dealing with various
constraints. In this paper, we focus on constraints like finding suitable
datasets where quantum advantage is achievable and evaluating the relevance of
features chosen by classical and quantum methods. Additionally, we compare
quantum and classical approaches using benchmarks and estimate the
computational complexity of quantum circuits to assess real-world usability.
For our experimental validation, we selected the gene expression dataset, given
the critical role of genetic variations in regulating physiological behavior
and disease susceptibility. Through this study, we aim to contribute to the
advancement of quantum machine learning methodologies, offering valuable
insights into their potential for addressing complex classification challenges
in various domains.",cs.LG
Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training,"Network pruning is a set of computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has focused on pruning and re-training, which
nowadays is inconvenient due to the vast amount of pre-trained models, which
are in any case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs, that
modifies the block-wise and row-wise sparsity ratios to maximize the
\emph{neuron alignment} among activations. Moreover, differently from existing
methods, our approach adaptively selects the best parameters for the block-wise
and row-wise sparsity ratios w.r.t. to the model and the desired sparsity
(given as input), and requires \emph{no re-training}. We test our method on 4
different LLM families and 3 different sparsity ratios, showing how it
consistently outperforms the latest state-of-the-art techniques. The code is
available at https://github.com/eliacunegatti/NeuroAL.",cs.LG
General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization,"This work investigates the effectiveness of schedule-free methods, developed
by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings,
inspired by their remarkable empirical success in training neural networks.
Specifically, we show that schedule-free SGD achieves optimal iteration
complexity for nonsmooth, nonconvex optimization problems. Our proof begins
with the development of a general framework for online-to-nonconvex conversion,
which converts a given online learning algorithm into an optimization algorithm
for nonconvex losses. Our general framework not only recovers existing
conversions but also leads to two novel conversion schemes. Notably, one of
these new conversions corresponds directly to schedule-free SGD, allowing us to
establish its optimality. Additionally, our analysis provides valuable insights
into the parameter choices for schedule-free SGD, addressing a theoretical gap
that the convex theory cannot explain.",cs.LG
Reconstruction of neuromorphic dynamics from a single scalar time series using variational autoencoder and neural network map,"This paper examines the reconstruction of a family of dynamical systems with
neuromorphic behavior using a single scalar time series. A model of a
physiological neuron based on the Hodgkin-Huxley formalism is considered.
Single time series of one of its variables is shown to be enough to train a
neural network that can operate as a discrete time dynamical system with one
control parameter. The neural network system is created in two steps. First,
the delay-coordinate embedding vectors are constructed form the original time
series and their dimension is reduced with by means of a variational
autoencoder to obtain the recovered state-space vectors. It is shown that an
appropriate reduced dimension can be determined by analyzing the autoencoder
training process. Second, pairs of the recovered state-space vectors at
consecutive time steps supplied with a constant value playing the role of a
control parameter are used to train another neural network to make it operate
as a recurrent map. The regimes of thus created neural network system observed
when its control parameter is varied are in very good accordance with those of
the original system, though they were not explicitly presented during training.",cs.LG
Unified Bayesian representation for high-dimensional multi-modal biomedical data for small-sample classification,"We present BALDUR, a novel Bayesian algorithm designed to deal with
multi-modal datasets and small sample sizes in high-dimensional settings while
providing explainable solutions. To do so, the proposed model combines within a
common latent space the different data views to extract the relevant
information to solve the classification task and prune out the
irrelevant/redundant features/data views. Furthermore, to provide generalizable
solutions in small sample size scenarios, BALDUR efficiently integrates dual
kernels over the views with a small sample-to-feature ratio. Finally, its
linear nature ensures the explainability of the model outcomes, allowing its
use for biomarker identification. This model was tested over two different
neurodegeneration datasets, outperforming the state-of-the-art models and
detecting features aligned with markers already described in the scientific
literature.",cs.LG
HeteroSample: Meta-path Guided Sampling for Heterogeneous Graph Representation Learning,"The rapid expansion of Internet of Things (IoT) has resulted in vast,
heterogeneous graphs that capture complex interactions among devices, sensors,
and systems. Efficient analysis of these graphs is critical for deriving
insights in IoT scenarios such as smart cities, industrial IoT, and intelligent
transportation systems. However, the scale and diversity of IoT-generated data
present significant challenges, and existing methods often struggle with
preserving the structural integrity and semantic richness of these complex
graphs. Many current approaches fail to maintain the balance between
computational efficiency and the quality of the insights generated, leading to
potential loss of critical information necessary for accurate decision-making
in IoT applications. We introduce HeteroSample, a novel sampling method
designed to address these challenges by preserving the structural integrity,
node and edge type distributions, and semantic patterns of IoT-related graphs.
HeteroSample works by incorporating the novel top-leader selection, balanced
neighborhood expansion, and meta-path guided sampling strategies. The key idea
is to leverage the inherent heterogeneous structure and semantic relationships
encoded by meta-paths to guide the sampling process. This approach ensures that
the resulting subgraphs are representative of the original data while
significantly reducing computational overhead. Extensive experiments
demonstrate that HeteroSample outperforms state-of-the-art methods, achieving
up to 15% higher F1 scores in tasks such as link prediction and node
classification, while reducing runtime by 20%.These advantages make
HeteroSample a transformative tool for scalable and accurate IoT applications,
enabling more effective and efficient analysis of complex IoT systems,
ultimately driving advancements in smart cities, industrial IoT, and beyond.",cs.LG
Data-Driven Gradient Optimization for Field Emission Management in a Superconducting Radio-Frequency Linac,"Field emission can cause significant problems in superconducting
radio-frequency linear accelerators (linacs). When cavity gradients are pushed
higher, radiation levels within the linacs may rise exponentially, causing
degradation of many nearby systems. This research aims to utilize machine
learning with uncertainty quantification to predict radiation levels at
multiple locations throughout the linacs and ultimately optimize cavity
gradients to reduce field emission induced radiation while maintaining the
total linac energy gain necessary for the experimental physics program. The
optimized solutions show over 40% reductions for both neutron and gamma
radiation from the standard operational settings.",cs.LG
Leveraging LSTM for Predictive Modeling of Satellite Clock Bias,"Satellite clock bias prediction plays a crucial role in enhancing the
accuracy of satellite navigation systems. In this paper, we propose an approach
utilizing Long Short-Term Memory (LSTM) networks to predict satellite clock
bias. We gather data from the PRN 8 satellite of the Galileo and preprocess it
to obtain a single difference sequence, crucial for normalizing the data.
Normalization allows resampling of the data, ensuring that the predictions are
equidistant and complete. Our methodology involves training the LSTM model on
varying lengths of datasets, ranging from 7 days to 31 days. We employ a
training set consisting of two days' worth of data in each case. Our LSTM model
exhibits exceptional accuracy, with a Root Mean Square Error (RMSE) of 2.11
$\times$ 10$^{-11}$. Notably, our approach outperforms traditional methods used
for similar time-series forecasting projects, being 170 times more accurate
than RNN, 2.3 $\times$ 10$^7$ times more accurate than MLP, and 1.9 $\times$
10$^4$ times more accurate than ARIMA. This study holds significant potential
in enhancing the accuracy and efficiency of low-power receivers used in various
devices, particularly those requiring power conservation. By providing more
accurate predictions of satellite clock bias, the findings of this research can
be integrated into the algorithms of such devices, enabling them to function
with heightened precision while conserving power. Improved accuracy in clock
bias predictions ensures that low-power receivers can maintain optimal
performance levels, thereby enhancing the overall reliability and effectiveness
of satellite navigation systems. Consequently, this advancement holds promise
for a wide range of applications, including remote areas, IoT devices, wearable
technology, and other devices where power efficiency and navigation accuracy
are paramount.",cs.LG
A neural-network based anomaly detection system and a safety protocol to protect vehicular network,"This thesis addresses the use of Cooperative Intelligent Transport Systems
(CITS) to improve road safety and efficiency by enabling vehicle-to-vehicle
communication, highlighting the importance of secure and accurate data
exchange. To ensure safety, the thesis proposes a Machine Learning-based
Misbehavior Detection System (MDS) using Long Short-Term Memory (LSTM) networks
to detect and mitigate incorrect or misleading messages within vehicular
networks. Trained offline on the VeReMi dataset, the detection model is tested
in real-time within a platooning scenario, demonstrating that it can prevent
nearly all accidents caused by misbehavior by triggering a defense protocol
that dissolves the platoon if anomalies are detected. The results show that
while the system can accurately detect general misbehavior, it struggles to
label specific types due to varying traffic conditions, implying the difficulty
of creating a universally adaptive protocol. However, the thesis suggests that
with more data and further refinement, this MDS could be implemented in
real-world CITS, enhancing driving safety by mitigating risks from misbehavior
in cooperative driving networks.",cs.LG
Hierarchical Conditional Tabular GAN for Multi-Tabular Synthetic Data Generation,"The generation of synthetic data is a state-of-the-art approach to leverage
when access to real data is limited or privacy regulations limit the usability
of sensitive data. A fair amount of research has been conducted on synthetic
data generation for single-tabular datasets, but only a limited amount of
research has been conducted on multi-tabular datasets with complex table
relationships. In this paper we propose the algorithm HCTGAN to synthesize
multi-tabular data from complex multi-tabular datasets. We compare our results
to the probabilistic model HMA1. Our findings show that our proposed algorithm
can more efficiently sample large amounts of synthetic data for deep and
complex multi-tabular datasets, whilst achieving adequate data quality and
always guaranteeing referential integrity. We conclude that the HCTGAN
algorithm is suitable for generating large amounts of synthetic data
efficiently for deep multi-tabular datasets with complex relationships. We
additionally suggest that the HMA1 model should be used on smaller datasets
when emphasis is on data quality.",cs.LG
Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching,"In inverse reinforcement learning (IRL), an agent seeks to replicate expert
demonstrations through interactions with the environment. Traditionally, IRL is
treated as an adversarial game, where an adversary searches over reward models,
and a learner optimizes the reward through repeated RL procedures. This
game-solving approach is both computationally expensive and difficult to
stabilize. In this work, we propose a novel approach to IRL by direct policy
optimization: exploiting a linear factorization of the return as the inner
product of successor features and a reward vector, we design an IRL algorithm
by policy gradient descent on the gap between the learner and expert features.
Our non-adversarial method does not require learning a reward function and can
be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our
approach works in state-only settings without expert action labels, a setting
which behavior cloning (BC) cannot solve. Empirical results demonstrate that
our method learns from as few as a single expert demonstration and achieves
improved performance on various control tasks.",cs.LG
Estimating Causal Effects in Partially Directed Parametric Causal Factor Graphs,"Lifting uses a representative of indistinguishable individuals to exploit
symmetries in probabilistic relational models, denoted as parametric factor
graphs, to speed up inference while maintaining exact answers. In this paper,
we show how lifting can be applied to causal inference in partially directed
graphs, i.e., graphs that contain both directed and undirected edges to
represent causal relationships between random variables. We present partially
directed parametric causal factor graphs (PPCFGs) as a generalisation of
previously introduced parametric causal factor graphs, which require a fully
directed graph. We further show how causal inference can be performed on a
lifted level in PPCFGs, thereby extending the applicability of lifted causal
inference to a broader range of models requiring less prior knowledge about
causal relationships.",cs.LG
Which PPML Would a User Choose? A Structured Decision Support Framework for Developers to Rank PPML Techniques Based on User Acceptance Criteria,"Using Privacy-Enhancing Technologies (PETs) for machine learning often
influences the characteristics of a machine learning approach, e.g., the needed
computational power, timing of the answers or how the data can be utilized.
When designing a new service, the developer faces the problem that some
decisions require a trade-off. For example, the use of a PET may cause a delay
in the responses or adding noise to the data to improve the users' privacy
might have a negative impact on the accuracy of the machine learning approach.
As of now, there is no structured way how the users' perception of a machine
learning based service can contribute to the selection of Privacy Preserving
Machine Learning (PPML) methods. This is especially a challenge since one
cannot assume that users have a deep technical understanding of these
technologies. Therefore, they can only be asked about certain attributes that
they can perceive when using the service and not directly which PPML they
prefer.
  This study introduces a decision support framework with the aim of supporting
the selection of PPML technologies based on user preferences. Based on prior
work analysing User Acceptance Criteria (UAC), we translate these criteria into
differentiating characteristics for various PPML techniques. As a final result,
we achieve a technology ranking based on the User Acceptance Criteria while
providing technology insights for the developers. We demonstrate its
application using the use case of classifying privacy-relevant information.
  Our contribution consists of the decision support framework which consists of
a process to connect PPML technologies with UAC, a process for evaluating the
characteristics that separate PPML techniques, and a ranking method to evaluate
the best PPML technique for the use case.",cs.LG
Causal-discovery-based root-cause analysis and its application in time-series prediction error diagnosis,"Recent rapid advancements of machine learning have greatly enhanced the
accuracy of prediction models, but most models remain ""black boxes"", making
prediction error diagnosis challenging, especially with outliers. This lack of
transparency hinders trust and reliability in industrial applications.
Heuristic attribution methods, while helpful, often fail to capture true causal
relationships, leading to inaccurate error attributions. Various root-cause
analysis methods have been developed using Shapley values, yet they typically
require predefined causal graphs, limiting their applicability for prediction
errors in machine learning models. To address these limitations, we introduce
the Causal-Discovery-based Root-Cause Analysis (CD-RCA) method that estimates
causal relationships between the prediction error and the explanatory
variables, without needing a pre-defined causal graph. By simulating synthetic
error data, CD-RCA can identify variable contributions to outliers in
prediction errors by Shapley values. Extensive simulations show CD-RCA
outperforms current heuristic attribution methods, and a sensitivity analysis
reveals new patterns where Shapley values may misattribute errors, paving the
way for more accurate error attribution methods.",cs.LG
Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration,"Learning diverse and high-performance behaviors from a limited set of
demonstrations is a grand challenge. Traditional imitation learning methods
usually fail in this task because most of them are designed to learn one
specific behavior even with multiple demonstrations. Therefore, novel
techniques for quality diversity imitation learning are needed to solve the
above challenge. This work introduces Wasserstein Quality Diversity Imitation
Learning (WQDIL), which 1) improves the stability of imitation learning in the
quality diversity setting with latent adversarial training based on a
Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting issue
using a measure-conditioned reward function with a single-step archive
exploration bonus. Empirically, our method significantly outperforms
state-of-the-art IL methods, achieving near-expert or beyond-expert QD
performance on the challenging continuous control tasks derived from MuJoCo
environments.",cs.LG
Data-driven discovery of mechanical models directly from MRI spectral data,"Finding interpretable biomechanical models can provide insight into the
functionality of organs with regard to physiology and disease. However,
identifying broadly applicable dynamical models for in vivo tissue remains
challenging. In this proof of concept study we propose a reconstruction
framework for data-driven discovery of dynamical models from experimentally
obtained undersampled MRI spectral data. The method makes use of the previously
developed spectro-dynamic framework which allows for reconstruction of
displacement fields at high spatial and temporal resolution required for model
identification. The proposed framework combines this method with data-driven
discovery of interpretable models using Sparse Identification of Non-linear
Dynamics (SINDy). The design of the reconstruction algorithm is such that a
symbiotic relation between the reconstruction of the displacement fields and
the model identification is created. Our method does not rely on periodicity of
the motion. It is successfully validated using spectral data of a dynamic
phantom gathered on a clinical MRI scanner. The dynamic phantom is programmed
to perform motion adhering to 5 different (non-linear) ordinary differential
equations. The proposed framework performed better than a 2-step approach where
the displacement fields were first reconstructed from the undersampled data
without any information on the model, followed by data-driven discovery of the
model using the reconstructed displacement fields. This study serves as a first
step in the direction of data-driven discovery of in vivo models.",cs.LG
Understanding Generalization in Quantum Machine Learning with Margins,"Understanding and improving generalization capabilities is crucial for both
classical and quantum machine learning (QML). Recent studies have revealed
shortcomings in current generalization theories, particularly those relying on
uniform bounds, across both classical and quantum settings. In this work, we
present a margin-based generalization bound for QML models, providing a more
reliable framework for evaluating generalization. Our experimental studies on
the quantum phase recognition (QPR) dataset demonstrate that margin-based
metrics are strong predictors of generalization performance, outperforming
traditional metrics like parameter count. By connecting this margin-based
metric to quantum information theory, we demonstrate how to enhance the
generalization performance of QML through a classical-quantum hybrid approach
when applied to classical data.",cs.LG
Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal Air Quality Sensor Fusion,"The deployment of affordable Internet of Things (IoT) sensors for air
pollution monitoring has increased in recent years due to their scalability and
cost-effectiveness. However, accurately calibrating these sensors in
uncontrolled environments remains a significant challenge. While expensive
reference sensors can provide accurate ground truth data, they are often
deployed on a limited scale due to high costs, leading to a scarcity of labeled
data. In diverse urban environments, data distributions constantly shift due to
varying factors such as traffic patterns, industrial activities, and weather
conditions, which impact sensor readings. Consequently, traditional machine
learning models -- despite their increasing deployment for environmental sensor
calibration -- often struggle to provide reliable pollutant measurements across
different locations due to domain shifts. To address these challenges, we
propose a novel unsupervised domain adaptation (UDA) method specifically
tailored for regression tasks on graph-structured data. Our approach leverages
Graph Neural Networks (GNNs) to model the relationships between sensors. To
effectively capture critical spatial-temporal interactions, we incorporate
spatial-temporal graph neural networks (STGNNs), which extend GNNs by
incorporating temporal dynamics. To handle the resulting larger embeddings, we
propose a domain adaptation method using a closed-form solution inspired by the
Tikhonov-regularized least-squares problem. This method leverages Cholesky
decomposition and power iteration to align the subspaces between source and
target domains. By aligning these subspaces, our approach allows low-cost IoT
sensors to learn calibration parameters from expensive reference sensors. This
facilitates reliable pollutant measurements in new locations without the need
for additional costly equipment.",cs.LG
Slowing Down Forgetting in Continual Learning,"A common challenge in continual learning (CL) is catastrophic forgetting,
where the performance on old tasks drops after new, additional tasks are
learned. In this paper, we propose a novel framework called ReCL to slow down
forgetting in CL. Our framework exploits an implicit bias of gradient-based
neural networks due to which these converge to margin maximization points. Such
convergence points allow us to reconstruct old data from previous tasks, which
we then combine with the current training data. Our framework is flexible and
can be applied on top of existing, state-of-the-art CL methods to slow down
forgetting. We further demonstrate the performance gain from our framework
across a large series of experiments, including different CL scenarios (class
incremental, domain incremental, task incremental learning) different datasets
(MNIST, CIFAR10), and different network architectures. Across all experiments,
we find large performance gains through ReCL. To the best of our knowledge, our
framework is the first to address catastrophic forgetting by leveraging models
in CL as their own memory buffers.",cs.LG
Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI,"Segmentation of cardiac magnetic resonance images (MRI) is crucial for the
analysis and assessment of cardiac function, helping to diagnose and treat
various cardiovascular diseases. Most recent techniques rely on deep learning
and usually require an extensive amount of labeled data. To overcome this
problem, few-shot learning has the capability of reducing data dependency on
labeled data. In this work, we introduce a new method that merges few-shot
learning with a U-Net architecture and Gaussian Process Emulators (GPEs),
enhancing data integration from a support set for improved performance. GPEs
are trained to learn the relation between the support images and the
corresponding masks in latent space, facilitating the segmentation of unseen
query images given only a small labeled support set at inference. We test our
model with the M&Ms-2 public dataset to assess its ability to segment the heart
in cardiac magnetic resonance imaging from different orientations, and compare
it with state-of-the-art unsupervised and few-shot methods. Our architecture
shows higher DICE coefficients compared to these methods, especially in the
more challenging setups where the size of the support set is considerably
small.",cs.LG
LongSafetyBench: Long-Context LLMs Struggle with Safety Issues,"With the development of large language models (LLMs), the sequence length of
these models continues to increase, drawing significant attention to
long-context language models. However, the evaluation of these models has been
primarily limited to their capabilities, with a lack of research focusing on
their safety. Existing work, such as ManyShotJailbreak, has to some extent
demonstrated that long-context language models can exhibit safety concerns.
However, the methods used are limited and lack comprehensiveness. In response,
we introduce \textbf{LongSafetyBench}, the first benchmark designed to
objectively and comprehensively evaluate the safety of long-context models.
LongSafetyBench consists of 10 task categories, with an average length of
41,889 words. After testing eight long-context language models on
LongSafetyBench, we found that existing models generally exhibit insufficient
safety capabilities. The proportion of safe responses from most mainstream
long-context LLMs is below 50\%. Moreover, models' safety performance in
long-context scenarios does not always align with that in short-context
scenarios. Further investigation revealed that long-context models tend to
overlook harmful content within lengthy texts. We also proposed a simple yet
effective solution, allowing open-source models to achieve performance
comparable to that of top-tier closed-source models. We believe that
LongSafetyBench can serve as a valuable benchmark for evaluating the safety
capabilities of long-context language models. We hope that our work will
encourage the broader community to pay attention to the safety of long-context
models and contribute to the development of solutions to improve the safety of
long-context LLMs.",cs.LG
SPARTAN: A Sparse Transformer Learning Local Causation,"Causal structures play a central role in world models that flexibly adapt to
changes in the environment. While recent works motivate the benefits of
discovering local causal graphs for dynamics modelling, in this work we
demonstrate that accurately capturing these relationships in complex settings
remains challenging for the current state-of-the-art. To remedy this
shortcoming, we postulate that sparsity is a critical ingredient for the
discovery of such local causal structures. To this end we present the SPARse
TrANsformer World model (SPARTAN), a Transformer-based world model that learns
local causal structures between entities in a scene. By applying sparsity
regularisation on the attention pattern between object-factored tokens, SPARTAN
identifies sparse local causal models that accurately predict future object
states. Furthermore, we extend our model to capture sparse interventions with
unknown targets on the dynamics of the environment. This results in a highly
interpretable world model that can efficiently adapt to changes. Empirically,
we evaluate SPARTAN against the current state-of-the-art in object-centric
world models on observation-based environments and demonstrate that our model
can learn accurate local causal graphs and achieve significantly improved
few-shot adaptation to changes in the dynamics of the environment as well as
robustness against removing irrelevant distractors.",cs.LG
WassFFed: Wasserstein Fair Federated Learning,"Federated Learning (FL) employs a training approach to address scenarios
where users' data cannot be shared across clients. Achieving fairness in FL is
imperative since training data in FL is inherently geographically distributed
among diverse user groups. Existing research on fairness predominantly assumes
access to the entire training data, making direct transfer to FL challenging.
However, the limited existing research on fairness in FL does not effectively
address two key challenges, i.e., (CH1) Current methods fail to deal with the
inconsistency between fair optimization results obtained with surrogate
functions and fair classification results. (CH2) Directly aggregating local
fair models does not always yield a globally fair model due to non Identical
and Independent data Distributions (non-IID) among clients. To address these
challenges, we propose a Wasserstein Fair Federated Learning framework, namely
WassFFed. To tackle CH1, we ensure that the outputs of local models, rather
than the loss calculated with surrogate functions or classification results
with a threshold, remain independent of various user groups. To resolve CH2, we
employ a Wasserstein barycenter calculation of all local models' outputs for
each user group, bringing local model outputs closer to the global output
distribution to ensure consistency between the global model and local models.
We conduct extensive experiments on three real-world datasets, demonstrating
that WassFFed outperforms existing approaches in striking a balance between
accuracy and fairness.",cs.LG
GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs,"Graph-based patterns are extensively employed and favored by practitioners
within industrial companies due to their capacity to represent the behavioral
attributes and topological relationships among users, thereby offering enhanced
interpretability in comparison to black-box models commonly utilized for
classification and recognition tasks. For instance, within the scenario of
transaction risk management, a graph pattern that is characteristic of a
particular risk category can be readily employed to discern transactions
fraught with risk, delineate networks of criminal activity, or investigate the
methodologies employed by fraudsters. Nonetheless, graph data in industrial
settings is often characterized by its massive scale, encompassing data sets
with millions or even billions of nodes, making the manual extraction of graph
patterns not only labor-intensive but also necessitating specialized knowledge
in particular domains of risk. Moreover, existing methodologies for mining
graph patterns encounter significant obstacles when tasked with analyzing
large-scale attributed graphs. In this work, we introduce GraphRPM, an
industry-purpose parallel and distributed risk pattern mining framework on
large attributed graphs. The framework incorporates a novel edge-involved graph
isomorphism network alongside optimized operations for parallel graph
computation, which collectively contribute to a considerable reduction in
computational complexity and resource expenditure. Moreover, the intelligent
filtration of efficacious risky graph patterns is facilitated by the proposed
evaluation metrics. Comprehensive experimental evaluations conducted on
real-world datasets of varying sizes substantiate the capability of GraphRPM to
adeptly address the challenges inherent in mining patterns from large-scale
industrial attributed graphs, thereby underscoring its substantial value for
industrial deployment.",cs.LG
CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal Large Language Models,"Category-agnostic pose estimation (CAPE) has traditionally relied on support
images with annotated keypoints, a process that is often cumbersome and may
fail to fully capture the necessary correspondences across diverse object
categories. Recent efforts have begun exploring the use of text-based queries,
where the need for support keypoints is eliminated. However, the optimal use of
textual descriptions for keypoints remains an underexplored area. In this work,
we introduce CapeLLM, a novel approach that leverages a text-based multimodal
large language model (MLLM) for CAPE. Our method only employs query image and
detailed text descriptions as an input to estimate category-agnostic keypoints.
We conduct extensive experiments to systematically explore the design space of
LLM-based CAPE, investigating factors such as choosing the optimal description
for keypoints, neural network architectures, and training strategies. Thanks to
the advanced reasoning capabilities of the pre-trained MLLM, CapeLLM
demonstrates superior generalization and robust performance. Our approach sets
a new state-of-the-art on the MP-100 benchmark in the challenging 1-shot
setting, marking a significant advancement in the field of category-agnostic
pose estimation.",cs.LG
Effect sizes as a statistical feature-selector-based learning to detect breast cancer,"Breast cancer detection is still an open research field, despite a tremendous
effort devoted to work in this area. Effect size is a statistical concept that
measures the strength of the relationship between two variables on a numeric
scale. Feature selection is widely used to reduce the dimensionality of data by
selecting only a subset of predictor variables to improve a learning model. In
this work, an algorithm and experimental results demonstrate the feasibility of
developing a statistical feature-selector-based learning tool capable of
reducing the data dimensionality using parametric effect size measures from
features extracted from cell nuclei images. The SVM classifier with a linear
kernel as a learning tool achieved an accuracy of over 90%. These excellent
results suggest that the effect size is within the standards of the
feature-selector methods",cs.LG
Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to
employ reasoning according to commonsense. Previous studies predominantly
employ an extracting-and-modeling paradigm to harness the information in KG,
which first extracts relevant subgraphs based on pre-defined rules and then
proceeds to design various strategies aiming to improve the representations and
fusion of the extracted structural knowledge. Despite their effectiveness,
there are still two challenges. On one hand, subgraphs extracted by rule-based
methods may have the potential to overlook critical nodes and result in
uncontrollable subgraph size. On the other hand, the misalignment between graph
and text modalities undermines the effectiveness of knowledge fusion,
ultimately impacting the task performance. To deal with the problems above, we
propose a novel framework: \textbf{S}ubgraph R\textbf{E}trieval Enhanced by
Gra\textbf{P}h-\textbf{T}ext \textbf{A}lignment, named \textbf{SEPTA}. Firstly,
we transform the knowledge graph into a database of subgraph vectors and
propose a BFS-style subgraph sampling strategy to avoid information loss,
leveraging the analogy between BFS and the message-passing mechanism. In
addition, we propose a bidirectional contrastive learning approach for
graph-text alignment, which effectively enhances both subgraph retrieval and
knowledge fusion. Finally, all the retrieved information is combined for
reasoning in the prediction module. Extensive experiments on five datasets
demonstrate the effectiveness and robustness of our framework.",cs.LG
Computable Model-Independent Bounds for Adversarial Quantum Machine Learning,"By leveraging the principles of quantum mechanics, QML opens doors to novel
approaches in machine learning and offers potential speedup. However, machine
learning models are well-documented to be vulnerable to malicious
manipulations, and this susceptibility extends to the models of QML. This
situation necessitates a thorough understanding of QML's resilience against
adversarial attacks, particularly in an era where quantum computing
capabilities are expanding. In this regard, this paper examines
model-independent bounds on adversarial performance for QML. To the best of our
knowledge, we introduce the first computation of an approximate lower bound for
adversarial error when evaluating model resilience against sophisticated
quantum-based adversarial attacks. Experimental results are compared to the
computed bound, demonstrating the potential of QML models to achieve high
robustness. In the best case, the experimental error is only 10% above the
estimated bound, offering evidence of the inherent robustness of quantum
models. This work not only advances our theoretical understanding of quantum
model resilience but also provides a precise reference bound for the future
development of robust QML algorithms.",cs.LG
Scientific machine learning in ecological systems: A study on the predator-prey dynamics,"In this study, we apply two pillars of Scientific Machine Learning: Neural
Ordinary Differential Equations (Neural ODEs) and Universal Differential
Equations (UDEs) to the Lotka Volterra Predator Prey Model, a fundamental
ecological model describing the dynamic interactions between predator and prey
populations. The Lotka-Volterra model is critical for understanding ecological
dynamics, population control, and species interactions, as it is represented by
a system of differential equations. In this work, we aim to uncover the
underlying differential equations without prior knowledge of the system,
relying solely on training data and neural networks. Using robust modeling in
the Julia programming language, we demonstrate that both Neural ODEs and UDEs
can be effectively utilized for prediction and forecasting of the
Lotka-Volterra system. More importantly, we introduce the forecasting breakdown
point: the time at which forecasting fails for both Neural ODEs and UDEs. We
observe how UDEs outperform Neural ODEs by effectively recovering the
underlying dynamics and achieving accurate forecasting with significantly less
training data. Additionally, we introduce Gaussian noise of varying magnitudes
(from mild to high) to simulate real-world data perturbations and show that
UDEs exhibit superior robustness, effectively recovering the underlying
dynamics even in the presence of noisy data, while Neural ODEs struggle with
high levels of noise. Through extensive hyperparameter optimization, we offer
insights into neural network architectures, activation functions, and
optimizers that yield the best results. This study opens the door to applying
Scientific Machine Learning frameworks for forecasting tasks across a wide
range of ecological and scientific domains.",cs.LG
Fast and Efficient Transformer-based Method for Bird's Eye View Instance Prediction,"Accurate object detection and prediction are critical to ensure the safety
and efficiency of self-driving architectures. Predicting object trajectories
and occupancy enables autonomous vehicles to anticipate movements and make
decisions with future information, increasing their adaptability and reducing
the risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate
the detection, tracking, and prediction stages, which can lead to significant
prediction errors due to accumulated inaccuracies between stages. Recent
advances have improved the feature representation of multi-camera perception
systems through Bird's-Eye View (BEV) transformations, boosting the development
of end-to-end systems capable of predicting environmental elements directly
from vehicle sensor data. These systems, however, often suffer from high
processing times and number of parameters, creating challenges for real-world
deployment. To address these issues, this paper introduces a novel BEV instance
prediction architecture based on a simplified paradigm that relies only on
instance segmentation and flow prediction. The proposed system prioritizes
speed, aiming at reduced parameter counts and inference times compared to
existing SOTA architectures, thanks to the incorporation of an efficient
transformer-based architecture. Furthermore, the implementation of the proposed
architecture is optimized for performance improvements in PyTorch version 2.1.
Code and trained models are available at
https://github.com/miguelag99/Efficient-Instance-Prediction",cs.LG
"1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs","This paper presents a detailed system description of our entry for the
CHiPSAL 2025 shared task, focusing on language detection, hate speech
identification, and target detection in Devanagari script languages. We
experimented with a combination of large language models and their ensembles,
including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like
focal loss to address challenges in the natural understanding of Devanagari
languages, such as multilingual processing and class imbalance. Our approach
achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804
for Sub-tasks A, B, and C respectively. This work provides insights into the
effectiveness of transformer models in tasks with domain-specific and
linguistic challenges, as well as areas for potential improvement in future
iterations.",cs.LG
Generative Feature Training of Thin 2-Layer Networks,"We consider the approximation of functions by 2-layer neural networks with a
small number of hidden weights based on the squared loss and small datasets.
Due to the highly non-convex energy landscape, gradient-based training often
suffers from local minima. As a remedy, we initialize the hidden weights with
samples from a learned proposal distribution, which we parameterize as a deep
generative model. To train this model, we exploit the fact that with fixed
hidden weights, the optimal output weights solve a linear equation. After
learning the generative model, we refine the sampled weights with a
gradient-based post-processing in the latent space. Here, we also include a
regularization scheme to counteract potential noise. Finally, we demonstrate
the effectiveness of our approach by numerical examples.",cs.LG
LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models,"In this paper, we propose a novel LLM-Neo framework that efficiently
transfers knowledge from a large language model (LLM) teacher to a compact
student. Initially, we revisit the knowledge distillation (KD) and low-rank
adaption (LoRA), and argue that they share the same paradigm. Inspired by this
observation, we explore the strategy that combines LoRA and KD to enhance the
efficiency of knowledge transfer. We first summarize some guidelines for this
design and further develop the LLM-Neo. Experimental results on compressing
Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further
analysis demonstrates the robustness of the proposed LLM-Neo on variants of
LoRA. The trained models have been available at
\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this
repository}.",cs.LG
Spatially Constrained Transformer with Efficient Global Relation Modelling for Spatio-Temporal Prediction,"Accurate spatio-temporal prediction is crucial for the sustainable
development of smart cities. However, current approaches often struggle to
capture important spatio-temporal relationships, particularly overlooking
global relations among distant city regions. Most existing techniques
predominantly rely on Convolutional Neural Networks (CNNs) to capture global
relations. However, CNNs exhibit neighbourhood bias, making them insufficient
for capturing distant relations. To address this limitation, we propose
ST-SampleNet, a novel transformer-based architecture that combines CNNs with
self-attention mechanisms to capture both local and global relations
effectively. Moreover, as the number of regions increases, the quadratic
complexity of self-attention becomes a challenge. To tackle this issue, we
introduce a lightweight region sampling strategy that prunes non-essential
regions and enhances the efficiency of our approach. Furthermore, we introduce
a spatially constrained position embedding that incorporates spatial
neighbourhood information into the self-attention mechanism, aiding in semantic
interpretation and improving the performance of ST-SampleNet. Our experimental
evaluation on three real-world datasets demonstrates the effectiveness of
ST-SampleNet. Additionally, our efficient variant achieves a 40% reduction in
computational costs with only a marginal compromise in performance,
approximately 1%.",cs.LG
Learning Interpretable Network Dynamics via Universal Neural Symbolic Regression,"Discovering governing equations of complex network dynamics is a fundamental
challenge in contemporary science with rich data, which can uncover the
mysterious patterns and mechanisms of the formation and evolution of complex
phenomena in various fields and assist in decision-making. In this work, we
develop a universal computational tool that can automatically, efficiently, and
accurately learn the symbolic changing patterns of complex system states by
combining the excellent fitting ability from deep learning and the equation
inference ability from pre-trained symbolic regression. We conduct intensive
experimental verifications on more than ten representative scenarios from
physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the
outstanding effectiveness and efficiency of our tool by comparing with the
state-of-the-art symbolic regression techniques for network dynamics. The
application to real-world systems including global epidemic transmission and
pedestrian movements has verified its practical applicability. We believe that
our tool can serve as a universal solution to dispel the fog of hidden
mechanisms of changes in complex phenomena, advance toward interpretability,
and inspire more scientific discoveries.",cs.LG
Optimized Quality of Service prediction in FSO Links over South Africa using Ensemble Learning,"Fibre optic communication system is expected to increase exponentially in
terms of application due to the numerous advantages over copper wires. The
optical network evolution presents several advantages such as over
long-distance, low-power requirement, higher carrying capacity and high
bandwidth among others Such network bandwidth surpasses methods of transmission
that include copper cables and microwaves. Despite these benefits, free-space
optical communications are severely impacted by harsh weather situations like
mist, precipitation, blizzard, fume, soil, and drizzle debris in the
atmosphere, all of which have an impact on the Quality of Service (QoS)
rendered by the systems. The primary goal of this article is to optimize the
QoS using the ensemble learning models Random Forest, ADaBoost Regression,
Stacking Regression, Gradient Boost Regression, and Multilayer Neural Network.
To accomplish the stated goal, meteorological data, visibility, wind speed, and
altitude were obtained from the South Africa Weather Services archive during a
ten-year period (2010 to 2019) at four different locations: Polokwane,
Kimberley, Bloemfontein, and George. We estimated the data rate, power
received, fog-induced attenuation, bit error rate and power penalty using the
collected and processed data. The RMSE and R-squared values of the model across
all the study locations, Polokwane, Kimberley, Bloemfontein, and George, are
0.0073 and 0.9951, 0.0065 and 0.9998, 0.0060 and 0.9941, and 0.0032 and 0.9906,
respectively. The result showed that using ensemble learning techniques in
transmission modeling can significantly enhance service quality and meet
customer service level agreements and ensemble method was successful in
efficiently optimizing the signal to noise ratio, which in turn enhanced the
QoS at the point of reception.",cs.LG
Adaptive Conditional Expert Selection Network for Multi-domain Recommendation,"Mixture-of-Experts (MOE) has recently become the de facto standard in
Multi-domain recommendation (MDR) due to its powerful expressive ability.
However, such MOE-based method typically employs all experts for each instance,
leading to scalability issue and low-discriminability between domains and
experts. Furthermore, the design of commonly used domain-specific networks
exacerbates the scalability issues. To tackle the problems, We propose a novel
method named CESAA consists of Conditional Expert Selection (CES) Module and
Adaptive Expert Aggregation (AEA) Module to tackle these challenges.
Specifically, CES first combines a sparse gating strategy with domain-shared
experts. Then AEA utilizes mutual information loss to strengthen the
correlations between experts and specific domains, and significantly improve
the distinction between experts. As a result, only domain-shared experts and
selected domain-specific experts are activated for each instance, striking a
balance between computational efficiency and model performance. Experimental
results on both public ranking and industrial retrieval datasets verify the
effectiveness of our method in MDR tasks.",cs.LG
Large Language Model in Medical Informatics: Direct Classification and Enhanced Text Representations for Automatic ICD Coding,"Addressing the complexity of accurately classifying International
Classification of Diseases (ICD) codes from medical discharge summaries is
challenging due to the intricate nature of medical documentation. This paper
explores the use of Large Language Models (LLM), specifically the LLAMA
architecture, to enhance ICD code classification through two methodologies:
direct application as a classifier and as a generator of enriched text
representations within a Multi-Filter Residual Convolutional Neural Network
(MultiResCNN) framework. We evaluate these methods by comparing them against
state-of-the-art approaches, revealing LLAMA's potential to significantly
improve classification outcomes by providing deep contextual insights into
medical texts.",cs.LG
Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous Stochastic Disturbances in RTC,"The difficulty of exploring and training online on real production systems
limits the scope of real-time online data/feedback-driven decision making. The
most feasible approach is to adopt offline reinforcement learning from limited
trajectory samples. However, after deployment, such policies fail due to
exogenous factors that temporarily or permanently disturb/alter the transition
distribution of the assumed decision process structure induced by offline
samples. This results in critical policy failures and generalization errors in
sensitive domains like Real-Time Communication (RTC). We solve this crucial
problem of identifying robust actions in presence of domain shifts due to
unseen exogenous stochastic factors in the wild. As it is impossible to learn
generalized offline policies within the support of offline data that are robust
to these unseen exogenous disturbances, we propose a novel post-deployment
shaping of policies (Streetwise), conditioned on real-time characterization of
out-of-distribution sub-spaces. This leads to robust actions in bandwidth
estimation (BWE) of network bottlenecks in RTC and in standard benchmarks. Our
extensive experimental results on BWE and other standard offline RL benchmark
environments demonstrate a significant improvement ($\approx$ 18% on some
scenarios) in final returns wrt. end-user metrics over state-of-the-art
baselines.",cs.LG
Generative midtended cognition and Artificial Intelligence. Thinging with thinging things,"This paper introduces the concept of ``generative midtended cognition'',
exploring the integration of generative AI with human cognition. The term
""generative"" reflects AI's ability to iteratively produce structured outputs,
while ""midtended"" captures the potential hybrid (human-AI) nature of the
process. It stands between traditional conceptions of intended creation,
understood directed from within, and extended processes that bring
exo-biological processes into the creative process. We examine current
generative technologies (based on multimodal transformer architectures typical
of large language models like ChatGPT), to explain how they can transform human
cognitive agency beyond what standard theories of extended cognition can
capture. We suggest that the type of cognitive activity typical of the coupling
between a human and generative technologies is closer (but not equivalent) to
social cognition than to classical extended cognitive paradigms. Yet, it
deserves a specific treatment. We provide an explicit definition of generative
midtended cognition in which we treat interventions by AI systems as
constitutive of the agent's intentional creative processes. Furthermore, we
distinguish two dimensions of generative hybrid creativity: 1. Width: captures
the sensitivity of the context of the generative process (from the single
letter to the whole historical and surrounding data), 2. Depth: captures the
granularity of iteration loops involved in the process. Generative midtended
cognition stands in the middle depth between conversational forms of cognition
in which complete utterances or creative units are exchanged, and
micro-cognitive (e.g. neural) subpersonal processes. Finally, the paper
discusses the potential risks and benefits of widespread generative AI
adoption, including the challenges of authenticity, generative power asymmetry,
and creative boost or atrophy.",cs.LG
Predicting ionic conductivity in solids from the machine-learned potential energy landscape,"Discovering new superionic materials is essential for advancing solid-state
batteries, which offer improved energy density and safety compared to the
traditional lithium-ion batteries with liquid electrolytes. Conventional
computational methods for identifying such materials are resource-intensive and
not easily scalable. Recently, universal interatomic potential models have been
developed using equivariant graph neural networks. These models are trained on
extensive datasets of first-principles force and energy calculations. One can
achieve significant computational advantages by leveraging them as the
foundation for traditional methods of assessing the ionic conductivity, such as
molecular dynamics or nudged elastic band techniques. However, the
generalization error from model inference on diverse atomic structures arising
in such calculations can compromise the reliability of the results. In this
work, we propose an approach for the quick and reliable evaluation of ionic
conductivity through the analysis of a universal interatomic potential. Our
method incorporates a set of heuristic structure descriptors that effectively
employ the rich knowledge of the underlying model while requiring minimal
generalization capabilities. Using our descriptors, we rank lithium-containing
materials in the Materials Project database according to their expected ionic
conductivity. Eight out of the ten highest-ranked materials are confirmed to be
superionic at room temperature in first-principles calculations. Notably, our
method achieves a speed-up factor of approximately 50 compared to molecular
dynamics driven by a machine-learning potential, and is at least 3,000 times
faster compared to first-principles molecular dynamics.",cs.LG
Structuring the Processing Frameworks for Data Stream Evaluation and Application,"The following work addresses the problem of frameworks for data stream
processing that can be used to evaluate the solutions in an environment that
resembles real-world applications. The definition of structured frameworks
stems from a need to reliably evaluate the data stream classification methods,
considering the constraints of delayed and limited label access. The current
experimental evaluation often boundlessly exploits the assumption of their
complete and immediate access to monitor the recognition quality and to adapt
the methods to the changing concepts. The problem is leveraged by reviewing
currently described methods and techniques for data stream processing and
verifying their outcomes in simulated environment. The effect of the work is a
proposed taxonomy of data stream processing frameworks, showing the linkage
between drift detection and classification methods considering a natural
phenomenon of label delay.",cs.LG
ScaleKD: Strong Vision Transformers Could Be Excellent Teachers,"In this paper, we question if well pre-trained vision transformer (ViT)
models could be used as teachers that exhibit scalable properties to advance
cross architecture knowledge distillation (KD) research, in the context of
using large-scale datasets for evaluation. To make this possible, our analysis
underlines the importance of seeking effective strategies to align (1) feature
computing paradigm differences, (2) model scale differences, and (3) knowledge
density differences. By combining three coupled components namely cross
attention projector, dual-view feature mimicking and teacher parameter
perception tailored to address the above problems, we present a simple and
effective KD method, called ScaleKD. Our method can train student backbones
that span across a variety of convolutional neural network (CNN), multi-layer
perceptron (MLP), and ViT architectures on image classification datasets,
achieving state-of-the-art distillation performance. For instance, taking a
well pre-trained Swin-L as the teacher model, our method gets
75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% top-1 accuracies for
MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16
models trained on ImageNet-1K dataset from scratch, showing
3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% absolute gains to the
individually trained counterparts. Intriguingly, when scaling up the size of
teacher models or their pre-training datasets, our method showcases the desired
scalable properties, bringing increasingly larger gains to student models. The
student backbones trained by our method transfer well on downstream MS-COCO and
ADE20K datasets. More importantly, our method could be used as a more efficient
alternative to the time-intensive pre-training paradigm for any target student
model if a strong pre-trained ViT is available, reducing the amount of viewed
training samples up to 195x.",cs.LG
White-Box Diffusion Transformer for single-cell RNA-seq generation,"As a powerful tool for characterizing cellular subpopulations and cellular
heterogeneity, single cell RNA sequencing (scRNA-seq) technology offers
advantages of high throughput and multidimensional analysis. However, the
process of data acquisition is often constrained by high cost and limited
sample availability. To overcome these limitations, we propose a hybrid model
based on Diffusion model and White-Box transformer that aims to generate
synthetic and biologically plausible scRNA-seq data. Diffusion model
progressively introduce noise into the data and then recover the original data
through a denoising process, a forward and reverse process that is particularly
suitable for generating complex data distributions. White-Box transformer is a
deep learning architecture that emphasizes mathematical interpretability. By
minimizing the encoding rate of the data and maximizing the sparsity of the
representation, it not only reduces the computational burden, but also provides
clear insight into underlying structure. Our White-Box Diffusion Transformer
combines the generative capabilities of Diffusion model with the mathematical
interpretability of White-Box transformer. Through experiments using six
different single-cell RNA-Seq datasets, we visualize both generated and real
data using t-SNE dimensionality reduction technique, as well as quantify
similarity between generated and real data using various metrics to demonstrate
comparable performance of White-Box Diffusion Transformer and Diffusion
Transformer in generating scRNA-seq data alongside significant improvements in
training efficiency and resource utilization. Our code is available at
https://github.com/lingximamo/White-Box-Diffusion-Transformer",cs.LG
QuadWBG: Generalizable Quadrupedal Whole-Body Grasping,"Legged robots with advanced manipulation capabilities have the potential to
significantly improve household duties and urban maintenance. Despite
considerable progress in developing robust locomotion and precise manipulation
methods, seamlessly integrating these into cohesive whole-body control for
real-world applications remains challenging. In this paper, we present a
modular framework for robust and generalizable whole-body loco-manipulation
controller based on a single arm-mounted camera. By using reinforcement
learning (RL), we enable a robust low-level policy for command execution over 5
dimensions (5D) and a grasp-aware high-level policy guided by a novel metric,
Generalized Oriented Reachability Map (GORM). The proposed system achieves
state-of-the-art one-time grasping accuracy of 89% in the real world, including
challenging tasks such as grasping transparent objects. Through extensive
simulations and real-world experiments, we demonstrate that our system can
effectively manage a large workspace, from floor level to above body height,
and perform diverse whole-body loco-manipulation tasks.",cs.LG
MP-PINN: A Multi-Phase Physics-Informed Neural Network for Epidemic Forecasting,"Forecasting temporal processes such as virus spreading in epidemics often
requires more than just observed time-series data, especially at the beginning
of a wave when data is limited. Traditional methods employ mechanistic models
like the SIR family, which make strong assumptions about the underlying
spreading process, often represented as a small set of compact differential
equations. Data-driven methods such as deep neural networks make no such
assumptions and can capture the generative process in more detail, but fail in
long-term forecasting due to data limitations. We propose a new hybrid method
called MP-PINN (Multi-Phase Physics-Informed Neural Network) to overcome the
limitations of these two major approaches. MP-PINN instils the spreading
mechanism into a neural network, enabling the mechanism to update in phases
over time, reflecting the dynamics of the epidemics due to policy
interventions. Experiments on COVID-19 waves demonstrate that MP-PINN achieves
superior performance over pure data-driven or model-driven approaches for both
short-term and long-term forecasting.",cs.LG
Model Partition and Resource Allocation for Split Learning in Vehicular Edge Networks,"The integration of autonomous driving technologies with vehicular networks
presents significant challenges in privacy preservation, communication
efficiency, and resource allocation. This paper proposes a novel U-shaped split
federated learning (U-SFL) framework to address these challenges on the way of
realizing in vehicular edge networks. U-SFL is able to enhance privacy
protection by keeping both raw data and labels on the vehicular user (VU) side
while enabling parallel processing across multiple vehicles. To optimize
communication efficiency, we introduce a semantic-aware auto-encoder (SAE) that
significantly reduces the dimensionality of transmitted data while preserving
essential semantic information. Furthermore, we develop a deep reinforcement
learning (DRL) based algorithm to solve the NP-hard problem of dynamic resource
allocation and split point selection. Our comprehensive evaluation demonstrates
that U-SFL achieves comparable classification performance to traditional split
learning (SL) while substantially reducing data transmission volume and
communication latency. The proposed DRL-based optimization algorithm shows good
convergence in balancing latency, energy consumption, and learning performance.",cs.LG
Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis,"Combining gradient compression methods (e.g., CountSketch, quantization) and
adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated
learning (FL), with potential benefits on both fewer communication rounds and
less per-round communication. In spite of the preliminary empirical success of
sketched adaptive methods, existing convergence analyses show the communication
cost to have a linear dependence on the ambient dimension, i.e., number of
parameters, which is prohibitively high for modern deep learning models. In
this work, we introduce specific sketched adaptive federated learning (SAFL)
algorithms and, as our main contribution, provide theoretical convergence
analyses in different FL settings with guarantees on communication cost
depending only logarithmically (instead of linearly) on the ambient dimension.
Unlike existing analyses, we show that the entry-wise sketching noise existent
in the preconditioners and the first moments of SAFL can be implicitly
addressed by leveraging the recently-popularized anisotropic curvatures in deep
learning losses, e.g., fast decaying loss Hessian eigen-values. In the i.i.d.
client setting of FL, we show that SAFL achieves asymptotic $O(1/\sqrt{T})$
convergence, and converges faster in the initial epochs. In the non-i.i.d.
client setting, where non-adaptive methods lack convergence guarantees, we show
that SACFL (SAFL with clipping) algorithms can provably converge in spite of
the additional heavy-tailed noise. Our theoretical claims are supported by
empirical studies on vision and language tasks, and in both fine-tuning and
training-from-scratch regimes. Surprisingly, as a by-product of our analysis,
the proposed SAFL methods are competitive with the state-of-the-art
communication-efficient federated learning algorithms based on error feedback.",cs.LG
PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing,"Code Large Language Models (Code LLMs), such as Code llama and
DeepSeek-Coder, have demonstrated exceptional performance in the code
generation tasks. However, most existing models focus on the abilities of
generating correct code, but often struggle with bug repair. We introduce a
suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are
mainly consisted of two parts: A Progressive Dataset Construction (PDC) from
scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data
expansion methods from the perspectives of breadth first and depth first
respectively. DM-SFT introduces an efficient bug-fixing supervised learning
approach, which effectively reduce the total training steps and mitigate the
""disorientation"" in SQL code bug-fixing training. In our evaluation, the code
LLM models trained with two methods have exceeds all current best performing
model which size is much larger.",cs.LG
Research on an intelligent fault diagnosis method for nuclear power plants based on ETCN-SSA combined algorithm,"Utilizing fault diagnosis methods is crucial for nuclear power professionals
to achieve efficient and accurate fault diagnosis for nuclear power plants
(NPPs). The performance of traditional methods is limited by their dependence
on complex feature extraction and skilled expert knowledge, which can be
time-consuming and subjective. This paper proposes a novel intelligent fault
diagnosis method for NPPs that combines enhanced temporal convolutional network
(ETCN) with sparrow search algorithm (SSA). ETCN utilizes temporal
convolutional network (TCN), self-attention (SA) mechanism and residual block
for enhancing performance. ETCN excels at extracting local features and
capturing time series information, while SSA adaptively optimizes its
hyperparameters for superior performance. The proposed method's performance is
experimentally verified on a CPR1000 simulation dataset. Compared to other
advanced intelligent fault diagnosis methods, the proposed one demonstrates
superior performance across all evaluation metrics. This makes it a promising
tool for NPP intelligent fault diagnosis, ultimately enhancing operational
reliability.",cs.LG
Multi-Stage Knowledge Integration of Vision-Language Models for Continual Learning,"Vision Language Models (VLMs), pre-trained on large-scale image-text
datasets, enable zero-shot predictions for unseen data but may underperform on
specific unseen tasks. Continual learning (CL) can help VLMs effectively adapt
to new data distributions without joint training, but faces challenges of
catastrophic forgetting and generalization forgetting. Although significant
progress has been achieved by distillation-based methods, they exhibit two
severe limitations. One is the popularly adopted single-teacher paradigm fails
to impart comprehensive knowledge, The other is the existing methods
inadequately leverage the multimodal information in the original training
dataset, instead they rely on additional data for distillation, which increases
computational and storage overhead. To mitigate both limitations, by drawing on
Knowledge Integration Theory (KIT), we propose a Multi-Stage Knowledge
Integration network (MulKI) to emulate the human learning process in
distillation methods. MulKI achieves this through four stages, including
Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making
Connections. During the four stages, we first leverage prototypes to align
across modalities, eliciting cross-modal knowledge, then adding new knowledge
by constructing fine-grained intra- and inter-modality relationships with
prototypes. After that, knowledge from two teacher models is adaptively
distinguished and re-weighted. Finally, we connect between models from intra-
and inter-task, integrating preceding and new knowledge. Our method
demonstrates significant improvements in maintaining zero-shot capabilities
while supporting continual learning across diverse downstream tasks, showcasing
its potential in adapting VLMs to evolving data distributions.",cs.LG
Precision Glass Thermoforming Assisted by Neural Networks,"Glass with good processability, chemical inertness, and optical transparency
has been widely used in optical and aesthetic products, many of which require
curve pro-files with high precision. To meet the increasingly tightened
geometrical tolerances and fast product updating rates, the traditional
approach of developing a thermoform-ing process through trials and errors can
cause a large waste of time and resources and often end up with failure. Hence,
there is a need to develop an efficient predictive model, replacing the costly
simulations or experiments, to assist the design of preci-sion glass
thermoforming. In this work, we report a dimensionless back-propagation neural
network (BPNN) that can adequately predict the form errors and thus compen-sate
for these errors in mold design to achieve precision glass molding. Based on
the precision molds, also discussed is the issue of error magnification
considering that cover glass for AR/VR glasses or smartphones, with extremely
large scale of produc-tion, may require a lower level of mold machining
accuracy. It is expected that this BPNN will also be implementable in the
glass-manufacturing industry, i.e., trained using industrial data for precision
mold designs.",cs.LG
Neuromodulated Meta-Learning,"Humans excel at adapting perceptions and actions to diverse environments,
enabling efficient interaction with the external world. This adaptive
capability relies on the biological nervous system (BNS), which activates
different brain regions for distinct tasks. Meta-learning similarly trains
machines to handle multiple tasks but relies on a fixed network structure, not
as flexible as BNS. To investigate the role of flexible network structure (FNS)
in meta-learning, we conduct extensive empirical and theoretical analyses,
finding that model performance is tied to structure, with no universally
optimal pattern across tasks. This reveals the crucial role of FNS in
meta-learning, ensuring meta-learning to generate the optimal structure for
each task, thereby maximizing the performance and learning efficiency of
meta-learning. Motivated by this insight, we propose to define, measure, and
model FNS in meta-learning. First, we define that an effective FNS should
possess frugality, plasticity, and sensitivity. Then, to quantify FNS in
practice, we present three measurements for these properties, collectively
forming the \emph{structure constraint} with theoretical supports. Building on
this, we finally propose Neuromodulated Meta-Learning (NeuronML) to model FNS
in meta-learning. It utilizes bi-level optimization to update both weights and
structure with the structure constraint. Extensive theoretical and empirical
evaluations demonstrate the effectiveness of NeuronML on various tasks. Code is
publicly available at
\href{https://github.com/WangJingyao07/NeuronML}{https://github.com/WangJingyao07/NeuronML}.",cs.LG
Methane projections from Canada's oil sands tailings using scientific deep learning reveal significant underestimation,"Bitumen extraction for the production of synthetic crude oil in Canada's
Athabasca Oil Sands industry has recently come under spotlight for being a
significant source of greenhouse gas emission. A major cause of concern is
methane, a greenhouse gas produced by the anaerobic biodegradation of
hydrocarbons in oil sands residues, or tailings, stored in settle basins
commonly known as oil sands tailing ponds. In order to determine the methane
emitting potential of these tailing ponds and have future methane projections,
we use real-time weather data, mechanistic models developed from laboratory
controlled experiments, and industrial reports to train a physics constrained
machine learning model. Our trained model can successfully identify the
directions of active ponds and estimate their emission levels, which are
generally hard to obtain due to data sampling restrictions. We found that each
active oil sands tailing pond could emit between 950 to 1500 tonnes of methane
per year, whose environmental impact is equivalent to carbon dioxide emissions
from at least 6000 gasoline powered vehicles. Although abandoned ponds are
often presumed to have insignificant emissions, our findings indicate that
these ponds could become active over time and potentially emit up to 1000
tonnes of methane each year. Taking an average over all datasets that was used
in model training, we estimate that emissions around major oil sands regions
would need to be reduced by approximately 12% over a year, to reduce the
average methane concentrations to 2005 levels.",cs.LG
Dockformer: A transformer-based molecular docking paradigm for large-scale virtual screening,"Molecular docking enables virtual screening of compound libraries to identify
potential ligands that target proteins of interest, a crucial step in drug
development; however, as the size of the compound library increases, the
computational complexity of traditional docking models increases. Deep learning
algorithms can provide data-driven research and development models to increase
the speed of the docking process. Unfortunately, few models can achieve
superior screening performance compared to that of traditional models.
Therefore, a novel deep learning-based docking approach named Dockformer is
introduced in this study. Dockformer leverages multimodal information to
capture the geometric topology and structural knowledge of molecules and can
directly generate binding conformations with the corresponding confidence
measures in an end-to-end manner. The experimental results show that Dockformer
achieves success rates of 90.53\% and 82.71\% on the PDBbind core set and
PoseBusters benchmarks, respectively, and more than a 100-fold increase in the
inference process speed, outperforming almost all state-of-the-art docking
methods. In addition, the ability of Dockformer to identify the main protease
inhibitors of coronaviruses is demonstrated in a real-world virtual screening
scenario. Considering its high docking accuracy and screening efficiency,
Dockformer can be regarded as a powerful and robust tool in the field of drug
design.",cs.LG
Beating Adversarial Low-Rank MDPs with Unknown Transition and Bandit Feedback,"We consider regret minimization in low-rank MDPs with fixed transition and
adversarial losses. Previous work has investigated this problem under either
full-information loss feedback with unknown transitions (Zhao et al., 2024), or
bandit loss feedback with known transition (Foster et al., 2022). First, we
improve the $poly(d, A, H)T^{5/6}$ regret bound of Zhao et al. (2024) to
$poly(d, A, H)T^{2/3}$ for the full-information unknown transition setting,
where d is the rank of the transitions, A is the number of actions, H is the
horizon length, and T is the number of episodes. Next, we initiate the study on
the setting with bandit loss feedback and unknown transitions. Assuming that
the loss has a linear structure, we propose both model based and model free
algorithms achieving $poly(d, A, H)T^{2/3}$ regret, though they are
computationally inefficient. We also propose oracle-efficient model-free
algorithms with $poly(d, A, H)T^{4/5}$ regret. We show that the linear
structure is necessary for the bandit case without structure on the reward
function, the regret has to scale polynomially with the number of states. This
is contrary to the full-information case (Zhao et al., 2024), where the regret
can be independent of the number of states even for unstructured reward
function.",cs.LG
Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When Memory,"Significant advances have been made in developing general-purpose embodied AI
in environments like Minecraft through the adoption of LLM-augmented
hierarchical approaches. While these approaches, which combine high-level
planners with low-level controllers, show promise, low-level controllers
frequently become performance bottlenecks due to repeated failures. In this
paper, we argue that the primary cause of failure in many low-level controllers
is the absence of an episodic memory system. To address this, we introduce Mr.
Steve (Memory Recall Steve-1), a novel low-level controller equipped with Place
Event Memory (PEM), a form of episodic memory that captures what, where, and
when information from episodes. This directly addresses the main limitation of
the popular low-level controller, Steve-1. Unlike previous models that rely on
short-term memory, PEM organizes spatial and event-based data, enabling
efficient recall and navigation in long-horizon tasks. Additionally, we propose
an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing
agents to alternate between exploration and task-solving based on recalled
events. Our approach significantly improves task-solving and exploration
efficiency compared to existing methods. We will release our code and demos on
the project page: https://sites.google.com/view/mr-steve.",cs.LG
GSL-PCD: Improving Generalist-Specialist Learning with Point Cloud Feature-based Task Partitioning,"Generalization in Deep Reinforcement Learning (DRL) across unseen environment
variations often requires training over a diverse set of scenarios. Many
existing DRL algorithms struggle with efficiency when handling numerous
variations. The Generalist-Specialist Learning (GSL) framework addresses this
by first training a generalist model on all variations, then creating
specialists from the generalist's weights, each focusing on a subset of
variations. The generalist then refines its learning with assistance from the
specialists. However, random task partitioning in GSL can impede performance by
assigning vastly different variations to the same specialist, often resulting
in each specialist focusing on only one variation, which raises computational
costs. To improve this, we propose Generalist-Specialist Learning with Point
Cloud Feature-based Task Partitioning (GSL-PCD). Our approach clusters
environment variations based on features extracted from object point clouds and
uses balanced clustering with a greedy algorithm to assign similar variations
to the same specialist. Evaluations on robotic manipulation tasks from the
ManiSkill benchmark demonstrate that point cloud feature-based partitioning
outperforms vanilla partitioning by 9.4%, with a fixed number of specialists,
and reduces computational and sample requirements by 50% to achieve comparable
performance.",cs.LG
On the Principles of ReLU Networks with One Hidden Layer,"A neural network with one hidden layer or a two-layer network (regardless of
the input layer) is the simplest feedforward neural network, whose mechanism
may be the basis of more general network architectures. However, even to this
type of simple architecture, it is also a ``black box''; that is, it remains
unclear how to interpret the mechanism of its solutions obtained by the
back-propagation algorithm and how to control the training process through a
deterministic way. This paper systematically studies the first problem by
constructing universal function-approximation solutions. It is shown that, both
theoretically and experimentally, the training solution for the one-dimensional
input could be completely understood, and that for a higher-dimensional input
can also be well interpreted to some extent. Those results pave the way for
thoroughly revealing the black box of two-layer ReLU networks and advance the
understanding of deep ReLU networks.",cs.LG
"Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models","Presenting users with diverse responses from foundation models is crucial for
enhancing user experience and accommodating varying preferences. However,
generating multiple high-quality and diverse responses without sacrificing
accuracy remains a challenge, especially when using greedy sampling. In this
work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that
leverages the abundant synthetic data available in many domains to elicit
diverse responses from foundation models. By leveraging signal provided by data
attribution methods such as influence functions, SPA partitions data into
subsets, each targeting unique aspects of the data, and trains multiple model
adaptations optimized for these subsets. Experimental results demonstrate the
effectiveness of our approach in diversifying foundation model responses while
maintaining high quality, showcased through the HumanEval and MBPP tasks in the
code generation domain and several tasks in the natural language understanding
domain, highlighting its potential to enrich user experience across various
applications.",cs.LG
Real-time Monitoring and Analysis of Track and Field Athletes Based on Edge Computing and Deep Reinforcement Learning Algorithm,"This research focuses on real-time monitoring and analysis of track and field
athletes, addressing the limitations of traditional monitoring systems in terms
of real-time performance and accuracy. We propose an IoT-optimized system that
integrates edge computing and deep learning algorithms. Traditional systems
often experience delays and reduced accuracy when handling complex motion data,
whereas our method, by incorporating a SAC-optimized deep learning model within
the IoT architecture, achieves efficient motion recognition and real-time
feedback. Experimental results show that this system significantly outperforms
traditional methods in response time, data processing accuracy, and energy
efficiency, particularly excelling in complex track and field events. This
research not only enhances the precision and efficiency of athlete monitoring
but also provides new technical support and application prospects for sports
science research.",cs.LG
Shallow Signed Distance Functions for Kinematic Collision Bodies,"We present learning-based implicit shape representations designed for
real-time avatar collision queries arising in the simulation of clothing.
Signed distance functions (SDFs) have been used for such queries for many years
due to their computational efficiency. Recently deep neural networks have been
used for implicit shape representations (DeepSDFs) due to their ability to
represent multiple shapes with modest memory requirements compared to
traditional representations over dense grids. However, the computational
expense of DeepSDFs prevents their use in real-time clothing simulation
applications. We design a learning-based representation of SDFs for human
avatars whoes bodies change shape kinematically due to joint-based skinning.
Rather than using a single DeepSDF for the entire avatar, we use a collection
of extremely computationally efficient (shallow) neural networks that represent
localized deformations arising from changes in body shape induced by the
variation of a single joint. This requires a stitching process to combine each
shallow SDF in the collection together into one SDF representing the signed
closest distance to the boundary of the entire body. To achieve this we augment
each shallow SDF with an additional output that resolves whether or not the
individual shallow SDF value is referring to a closest point on the boundary of
the body, or to a point on the interior of the body (but on the boundary of the
individual shallow SDF). Our model is extremely fast and accurate and we
demonstrate its applicability with real-time simulation of garments driven by
animated characters.",cs.LG
"Truth, beauty, and goodness in grand unification: a machine learning approach","We investigate the flavour sector of the supersymmetric $SU(5)$ Grand Unified
Theory (GUT) model using machine learning techniques. The minimal $SU(5)$ model
is known to predict fermion masses that disagree with observed values in
nature. There are two well-known approaches to address this issue: one involves
introducing a 45-representation Higgs field, while the other employs a
higher-dimensional operator involving the 24-representation GUT Higgs field. We
compare these two approaches by numerically optimising a loss function, defined
as the ratio of determinants of mass matrices. Our findings indicate that the
24-Higgs approach achieves the observed fermion masses with smaller
modifications to the original minimal $SU(5)$ model.",cs.LG
DiffSR: Learning Radar Reflectivity Synthesis via Diffusion Model from Satellite Observations,"Weather radar data synthesis can fill in data for areas where ground
observations are missing. Existing methods often employ reconstruction-based
approaches with MSE loss to reconstruct radar data from satellite observation.
However, such methods lead to over-smoothing, which hinders the generation of
high-frequency details or high-value observation areas associated with
convective weather. To address this issue, we propose a two-stage
diffusion-based method called DiffSR. We first pre-train a reconstruction model
on global-scale data to obtain radar estimation and then synthesize radar
reflectivity by combining radar estimation results with satellite data as
conditions for the diffusion model. Extensive experiments show that our method
achieves state-of-the-art (SOTA) results, demonstrating the ability to generate
high-frequency details and high-value areas.",cs.LG
Learning a Single Neuron Robustly to Distributional Shifts and Adversarial Label Noise,"We study the problem of learning a single neuron with respect to the
$L_2^2$-loss in the presence of adversarial distribution shifts, where the
labels can be arbitrary, and the goal is to find a ``best-fit'' function. More
precisely, given training samples from a reference distribution
$\mathcal{p}_0$, the goal is to approximate the vector $\mathbf{w}^*$ which
minimizes the squared loss with respect to the worst-case distribution that is
close in $\chi^2$-divergence to $\mathcal{p}_{0}$. We design a computationally
efficient algorithm that recovers a vector $ \hat{\mathbf{w}}$ satisfying
$\mathbb{E}_{\mathcal{p}^*} (\sigma(\hat{\mathbf{w}} \cdot \mathbf{x}) - y)^2
\leq C \, \mathbb{E}_{\mathcal{p}^*} (\sigma(\mathbf{w}^* \cdot \mathbf{x}) -
y)^2 + \epsilon$, where $C>1$ is a dimension-independent constant and
$(\mathbf{w}^*, \mathcal{p}^*)$ is the witness attaining the min-max risk
$\min_{\mathbf{w}~:~\|\mathbf{w}\| \leq W} \max_{\mathcal{p}}
\mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{p}} (\sigma(\mathbf{w} \cdot
\mathbf{x}) - y)^2 - \nu \chi^2(\mathcal{p}, \mathcal{p}_0)$. Our algorithm
follows a primal-dual framework and is designed by directly bounding the risk
with respect to the original, nonconvex $L_2^2$ loss. From an optimization
standpoint, our work opens new avenues for the design of primal-dual algorithms
under structured nonconvexity.",cs.LG
Shedding Light on Problems with Hyperbolic Graph Learning,"Recent papers in the graph machine learning literature have introduced a
number of approaches for hyperbolic representation learning. The asserted
benefits are improved performance on a variety of graph tasks, node
classification and link prediction included. Claims have also been made about
the geometric suitability of particular hierarchical graph datasets to
representation in hyperbolic space. Despite these claims, our work makes a
surprising discovery: when simple Euclidean models with comparable numbers of
parameters are properly trained in the same environment, in most cases, they
perform as well, if not better, than all introduced hyperbolic graph
representation learning models, even on graph datasets previously claimed to be
the most hyperbolic as measured by Gromov $\delta$-hyperbolicity (i.e., perfect
trees). This observation gives rise to a simple question: how can this be? We
answer this question by taking a careful look at the field of hyperbolic graph
representation learning as it stands today, and find that a number of papers
fail to diligently present baselines, make faulty modelling assumptions when
constructing algorithms, and use misleading metrics to quantify geometry of
graph datasets. We take a closer look at each of these three problems,
elucidate the issues, perform an analysis of methods, and introduce a
parametric family of benchmark datasets to ascertain the applicability of
(hyperbolic) graph neural networks.",cs.LG
WDMoE: Wireless Distributed Mixture of Experts for Large Language Models,"Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but the role of wireless networks in
supporting LLMs has not been thoroughly explored. In this paper, we propose a
wireless distributed Mixture of Experts (WDMoE) architecture to enable
collaborative deployment of LLMs across edge servers at the base station (BS)
and mobile devices in wireless networks. Specifically, we decompose the MoE
layer in LLMs by placing the gating network and the preceding neural network
layer at BS, while distributing the expert networks among the devices. This
deployment leverages the parallel inference capabilities of expert networks on
mobile devices, effectively utilizing the limited computing and caching
resources of these devices. Accordingly, we develop a performance metric for
WDMoE-based LLMs, which accounts for both model capability and latency. To
minimize the latency while maintaining accuracy, we jointly optimize expert
selection and bandwidth allocation based on the performance metric. Moreover,
we build a hardware testbed using NVIDIA Jetson kits to validate the
effectiveness of WDMoE. Both theoretical simulations and practical hardware
experiments demonstrate that the proposed method can significantly reduce the
latency without compromising LLM performance.",cs.LG
Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation,"Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.",cs.LG
A Zero-Knowledge PCP Theorem,"We show that for every polynomial q* there exist polynomial-size,
constant-query, non-adaptive PCPs for NP which are perfect zero knowledge
against (adaptive) adversaries making at most q* queries to the proof. In
addition, we construct exponential-size constant-query PCPs for NEXP with
perfect zero knowledge against any polynomial-time adversary. This improves
upon both a recent construction of perfect zero-knowledge PCPs for #P (STOC
2024) and the seminal work of Kilian, Petrank and Tardos (STOC 1997).",cs.CR
Federated Low-Rank Adaptation with Differential Privacy over Wireless Networks,"Fine-tuning large pre-trained foundation models (FMs) on distributed edge
devices presents considerable computational and privacy challenges. Federated
fine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative
model training without the need to share raw data. To lessen the computational
burden on resource-limited devices, combining low-rank adaptation (LoRA) with
federated learning enables parameter-efficient fine-tuning. Additionally, the
split FedFT architecture partitions an FM between edge devices and a central
server, reducing the necessity for complete model deployment on individual
devices. However, the risk of privacy eavesdropping attacks in FedFT remains a
concern, particularly in sensitive areas such as healthcare and finance. In
this paper, we propose a split FedFT framework with differential privacy (DP)
over wireless networks, where the inherent wireless channel noise in the uplink
transmission is utilized to achieve DP guarantees without adding an extra
artificial noise. We shall investigate the impact of the wireless noise on
convergence performance of the proposed framework. We will also show that by
updating only one of the low-rank matrices in the split FedFT with DP, the
proposed method can mitigate the noise amplification effect. Simulation results
will demonstrate that the proposed framework achieves higher accuracy under
strict privacy budgets compared to baseline methods.",cs.CR
A Call to Reconsider Certification Authority Authorization (CAA),"Certification Authority Authentication (CAA) is a safeguard against
illegitimate certificate issuance. We show how shortcomings in CAA concepts and
operational aspects undermine its effectiveness in preventing certificate
misissuance. Our discussion reveals pitfalls and highlights best practices when
designing security protocols based on DNS.",cs.CR
"A Survey on Adversarial Machine Learning for Code Data: Realistic Threats, Countermeasures, and Interpretations","Code Language Models (CLMs) have achieved tremendous progress in source code
understanding and generation, leading to a significant increase in research
interests focused on applying CLMs to real-world software engineering tasks in
recent years. However, in realistic scenarios, CLMs are exposed to potential
malicious adversaries, bringing risks to the confidentiality, integrity, and
availability of CLM systems. Despite these risks, a comprehensive analysis of
the security vulnerabilities of CLMs in the extremely adversarial environment
has been lacking. To close this research gap, we categorize existing attack
techniques into three types based on the CIA triad: poisoning attacks
(integrity \& availability infringement), evasion attacks (integrity
infringement), and privacy attacks (confidentiality infringement). We have
collected so far the most comprehensive (79) papers related to adversarial
machine learning for CLM from the research fields of artificial intelligence,
computer security, and software engineering. Our analysis covers each type of
risk, examining threat model categorization, attack techniques, and
countermeasures, while also introducing novel perspectives on eXplainable AI
(XAI) and exploring the interconnections between different risks. Finally, we
identify current challenges and future research opportunities. This study aims
to provide a comprehensive roadmap for both researchers and practitioners and
pave the way towards more reliable CLMs for practical applications.",cs.CR
Double-Signed Fragmented DNSSEC for Countering Quantum Threat,"DNSSEC, a DNS security extension, is essential to accurately translating
domain names to IP addresses. Digital signatures provide the foundation for
this reliable translation, however, the evolution of 'Quantum Computers' has
made traditional digital signatures vulnerable. In light of this, NIST has
recently selected potential post-quantum digital signatures that can operate on
conventional computers and resist attacks made with Quantum Computers. Since
these post-quantum digital signatures are still in their early stages of
development, replacing pre-quantum digital signature schemes in DNSSEC with
post-quantum candidates is risky until the post-quantum candidates have
undergone a thorough security analysis. Given this, herein, we investigate the
viability of employing 'Double-Signatures' in DNSSEC, combining a post-quantum
digital signature and a classic one. The rationale is that double-signatures
will offer protection against quantum threats on conventional signature schemes
as well as unknown non-quantum attacks on post-quantum signature schemes, hence
even if one fails the other provides security guarantees. However, the
inclusion of two signatures in the DNSSEC response message doesn't bode well
with the maximum allowed size of DNSSEC responses (i.e., 1232B, a limitation
enforced by MTU of physical links). To counter this issue, we leverage a way to
do application-layer fragmentation of DNSSEC responses with two signatures. We
implement our solution on top of OQS-BIND and through experiments show that the
addition of two signatures in DNSSEC and application-layer fragmentation of all
relevant resource records and their reassembly does not have any substantial
impact on the efficiency of the resolution process and thus is suitable for the
interim period at least until the quantum computers are fully realized.",cs.CR
SecEncoder: Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.",cs.CR
TIPS: Threat Actor Informed Prioritization of Applications using SecEncoder,"This paper introduces TIPS: Threat Actor Informed Prioritization using
SecEncoder, a specialized language model for security. TIPS combines the
strengths of both encoder and decoder language models to detect and prioritize
compromised applications. By integrating threat actor intelligence, TIPS
enhances the accuracy and relevance of its detections. Extensive experiments
with a real-world benchmark dataset of applications demonstrate TIPS's high
efficacy, achieving an F-1 score of 0.90 in identifying malicious applications.
Additionally, in real-world scenarios, TIPS significantly reduces the backlog
of investigations for security analysts by 87%, thereby streamlining the threat
response process and improving overall security posture.",cs.CR
LLM App Squatting and Cloning,"Impersonation tactics, such as app squatting and app cloning, have posed
longstanding challenges in mobile app stores, where malicious actors exploit
the names and reputations of popular apps to deceive users. With the rapid
growth of Large Language Model (LLM) stores like GPT Store and FlowGPT, these
issues have similarly surfaced, threatening the integrity of the LLM app
ecosystem. In this study, we present the first large-scale analysis of LLM app
squatting and cloning using our custom-built tool, LLMappCrazy. LLMappCrazy
covers 14 squatting generation techniques and integrates Levenshtein distance
and BERT-based semantic analysis to detect cloning by analyzing app functional
similarities. Using this tool, we generated variations of the top 1000 app
names and found over 5,000 squatting apps in the dataset. Additionally, we
observed 3,509 squatting apps and 9,575 cloning cases across six major
platforms. After sampling, we find that 18.7% of the squatting apps and 4.9% of
the cloning apps exhibited malicious behavior, including phishing, malware
distribution, fake content dissemination, and aggressive ad injection.",cs.CR
An Attack Traffic Identification Method Based on Temporal Spectrum,"To address the issues of insufficient robustness, unstable features, and data
noise interference in existing network attack detection and identification
models, this paper proposes an attack traffic detection and identification
method based on temporal spectrum. First, traffic data is segmented by a
sliding window to construct a feature sequence and a corresponding label
sequence for network traffic. Next, the proposed spectral label generation
methods, SSPE and COAP, are applied to transform the label sequence into
spectral labels and the feature sequence into temporal features. Spectral
labels and temporal features are used to capture and represent behavioral
patterns of attacks. Finally, the constructed temporal features and spectral
labels are used to train models, which subsequently detects and identifies
network attack behaviors. Experimental results demonstrate that compared to
traditional methods, models trained with the SSPE or COAP method improve
identification accuracy by 10%, and exhibit strong robustness, particularly in
noisy environments.",cs.CR
Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models,"Smart contracts, self-executing agreements directly encoded in code, are
fundamental to blockchain technology, especially in decentralized finance
(DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses
significant risks, leading to substantial financial losses and eroding trust in
blockchain systems. Existing detection methods, such as PonziGuard, depend on
large amounts of labeled data and struggle to identify unseen Ponzi schemes,
limiting their reliability and generalizability. In contrast, we introduce
PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts,
which requires no labeled training data. PonziSleuth utilizes advanced language
understanding capabilities of LLMs to analyze smart contract source code
through a novel two-step zero-shot chain-of-thought prompting technique. Our
extensive evaluation on benchmark datasets and real-world contracts
demonstrates that PonziSleuth delivers comparable, and often superior,
performance without the extensive data requirements, achieving a balanced
detection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27%
with Mistral. In real-world detection, PonziSleuth successfully identified 15
new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024,
with a false negative rate of 0% and a false positive rate of 0.29%. These
results highlight PonziSleuth's capability to detect diverse and novel Ponzi
schemes, marking a significant advancement in leveraging LLMs for enhancing
blockchain security and mitigating financial scams.",cs.CR
Developers Are Victims Too : A Comprehensive Analysis of The VS Code Extension Ecosystem,"With the wave of high-profile supply chain attacks targeting development and
client organizations, supply chain security has recently become a focal point.
As a result, there is an elevated discussion on securing the development
environment and increasing the transparency of the third-party code that runs
in software products to minimize any negative impact from third-party code in a
software product. However, the literature on secure software development lacks
insight into how the third-party development tools used by every developer
affect the security posture of the developer, the development organization,
and, eventually, the end product. To that end, we have analyzed 52,880
third-party VS Code extensions to understand their threat to the developer, the
code, and the development organizations. We found that ~5.6\% of the analyzed
extensions have suspicious behavior, jeopardizing the integrity of the
development environment and potentially leaking sensitive information on the
developer's product. We also found that the VS Code hosting the third-party
extensions lacks practical security controls and lets untrusted third-party
code run unchecked and with questionable capabilities. We offer recommendations
on possible avenues for fixing some of the issues uncovered during the
analysis.",cs.CR
Nearly-Linear Time Seeded Extractors with Short Seeds,"(abstract shortened due to space constraints)
  Existing constructions of seeded extractors with short seed length and large
output length run in time $\Omega(n \log(1/\varepsilon))$ and often slower,
where $n$ is the input source length and $\varepsilon$ is the error of the
extractor. Since cryptographic applications of extractors require $\varepsilon$
to be small, the resulting runtime makes these extractors unusable in practice.
  Motivated by this, we explore constructions of strong seeded extractors with
short seeds computable in nearly-linear time $O(n \log^c n)$, for any error
$\varepsilon$. We show that an appropriate combination of modern condensers and
classical approaches for constructing seeded extractors for high min-entropy
sources yields strong extractors for $n$-bit sources with any min-entropy $k$
and any target error $\varepsilon$ with seed length $d=O(\log(n/\varepsilon))$
and output length $m=(1-\eta)k$ for an arbitrarily small constant $\eta>0$,
running in nearly-linear time, after a reasonable one-time preprocessing step
(finding a primitive element of $\mathbb{F}_q$ with $q=poly(n/\varepsilon)$ a
power of $2$) that is only required when $k<2^{C\log^*
n}\cdot\log^2(n/\varepsilon)$, for a constant $C>0$ and $\log^*$ the iterated
logarithm, and which can be implemented in time $polylog(n/\varepsilon)$ under
mild conditions on $q$. As a second contribution, we give an instantiation of
Trevisan's extractor that can be evaluated in truly linear time in the RAM
model, as long as the number of output bits is at most
$\frac{n}{\log(1/\varepsilon)polylog(n)}$. Previous fast implementations of
Trevisan's extractor ran in $\widetilde{O}(n)$ time in this setting. In
particular, these extractors directly yield privacy amplification protocols
with the same time complexity and output length, and communication complexity
equal to their seed length.",cs.CR
Privacy-Preserving Verifiable Neural Network Inference Service,"Machine learning has revolutionized data analysis and pattern recognition,
but its resource-intensive training has limited accessibility. Machine Learning
as a Service (MLaaS) simplifies this by enabling users to delegate their data
samples to an MLaaS provider and obtain the inference result using a
pre-trained model. Despite its convenience, leveraging MLaaS poses significant
privacy and reliability concerns to the client. Specifically, sensitive
information from the client inquiry data can be leaked to an adversarial MLaaS
provider. Meanwhile, the lack of a verifiability guarantee can potentially
result in biased inference results or even unfair payment issues. While
existing trustworthy machine learning techniques, such as those relying on
verifiable computation or secure computation, offer solutions to privacy and
reliability concerns, they fall short of simultaneously protecting the privacy
of client data and providing provable inference verifiability.
  In this paper, we propose vPIN, a privacy-preserving and verifiable CNN
inference scheme that preserves privacy for client data samples while ensuring
verifiability for the inference. vPIN makes use of partial homomorphic
encryption and commit-and-prove succinct non-interactive argument of knowledge
techniques to achieve desirable security properties. In vPIN, we develop
various optimization techniques to minimize the proving circuit for homomorphic
inference evaluation thereby, improving the efficiency and performance of our
technique. We fully implemented and evaluated our vPIN scheme on standard
datasets (e.g., MNIST, CIFAR-10). Our experimental results show that vPIN
achieves high efficiency in terms of proving time, verification time, and proof
size, while providing client data privacy guarantees and provable
verifiability.",cs.CR
SDN-Based Smart Cyber Switching (SCS) for Cyber Restoration of a Digital Substation,"In recent years, critical infrastructure and power grids have increasingly
been targets of cyber-attacks, causing widespread and extended blackouts.
Digital substations are particularly vulnerable to such cyber incursions,
jeopardizing grid stability. This paper addresses these risks by proposing a
cybersecurity framework that leverages software-defined networking (SDN) to
bolster the resilience of substations based on the IEC-61850 standard. The
research introduces a strategy involving smart cyber switching (SCS) for
mitigation and concurrent intelligent electronic device (CIED) for restoration,
ensuring ongoing operational integrity and cybersecurity within a substation.
The SCS framework improves the physical network's behavior (i.e., leveraging
commercial SDN capabilities) by incorporating an adaptive port controller (APC)
module for dynamic port management and an intrusion detection system (IDS) to
detect and counteract malicious IEC-61850-based sampled value (SV) and generic
object-oriented system event (GOOSE) messages within the substation's
communication network. The framework's effectiveness is validated through
comprehensive simulations and a hardware-in-the-loop (HIL) testbed,
demonstrating its ability to sustain substation operations during cyber-attacks
and significantly improve the overall resilience of the power grid.",cs.CR
Anomaly Detection in OKTA Logs using Autoencoders,"Okta logs are used today to detect cybersecurity events using various
rule-based models with restricted look back periods. These functions have
limitations, such as a limited retrospective analysis, a predefined rule set,
and susceptibility to generating false positives. To address this, we adopt
unsupervised techniques, specifically employing autoencoders. To properly use
an autoencoder, we need to transform and simplify the complexity of the log
data we receive from our users. This transformed and filtered data is then fed
into the autoencoder, and the output is evaluated.",cs.CR
X-DFS: Explainable Artificial Intelligence Guided Design-for-Security Solution Space Exploration,"Design and manufacturing of integrated circuits predominantly use a globally
distributed semiconductor supply chain involving diverse entities. The modern
semiconductor supply chain has been designed to boost production efficiency,
but is filled with major security concerns such as malicious modifications
(hardware Trojans), reverse engineering (RE), and cloning. While being
deployed, digital systems are also subject to a plethora of threats such as
power, timing, and electromagnetic (EM) side channel attacks. Many
Design-for-Security (DFS) solutions have been proposed to deal with these
vulnerabilities, and such solutions (DFS) relays on strategic modifications
(e.g., logic locking, side channel resilient masking, and dummy logic
insertion) of the digital designs for ensuring a higher level of security.
However, most of these DFS strategies lack robust formalism, are often not
human-understandable, and require an extensive amount of human expert effort
during their development/use. All of these factors make it difficult to keep up
with the ever growing number of microelectronic vulnerabilities. In this work,
we propose X-DFS, an explainable Artificial Intelligence (AI) guided DFS
solution-space exploration approach that can dramatically cut down the
mitigation strategy development/use time while enriching our understanding of
the vulnerability by providing human-understandable decision rationale. We
implement X-DFS and comprehensively evaluate it for reverse engineering threats
(SAIL, SWEEP, and OMLA) and formalize a generalized mechanism for applying
X-DFS to defend against other threats such as hardware Trojans, fault attacks,
and side channel attacks for seamless future extensions.",cs.CR
Watermark Anything with Localized Messages,"Image watermarking methods are not tailored to handle small watermarked
areas. This restricts applications in real-world scenarios where parts of the
image may come from different sources or have been edited. We introduce a
deep-learning model for localized image watermarking, dubbed the Watermark
Anything Model (WAM). The WAM embedder imperceptibly modifies the input image,
while the extractor segments the received image into watermarked and
non-watermarked areas and recovers one or several hidden messages from the
areas found to be watermarked. The models are jointly trained at low resolution
and without perceptual constraints, then post-trained for imperceptibility and
multiple watermarks. Experiments show that WAM is competitive with state-of-the
art methods in terms of imperceptibility and robustness, especially against
inpainting and splicing, even on high-resolution images. Moreover, it offers
new capabilities: WAM can locate watermarked areas in spliced images and
extract distinct 32-bit messages with less than 1 bit error from multiple small
regions - no larger than 10% of the image surface - even for small $256\times
256$ images.",cs.CR
TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models,"With the widespread of digital environments, reliable authentication and
continuous access control has become crucial. It can minimize cyber attacks and
prevent frauds, specially those associated with identity theft. A particular
interest lies on keystroke dynamics (KD), which refers to the task of
recognizing individuals' identity based on their unique typing style. In this
work, we propose the use of pre-trained language models (PLMs) to recognize
such patterns. Although PLMs have shown high performance on multiple NLP
benchmarks, the use of these models on specific tasks requires customization.
BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot
be directly applied to KD, which requires temporal-character information to
recognize users. Recent character-aware PLMs are able to process both subwords
and character-level information and can be an alternative solution.
Notwithstanding, they are still not suitable to be directly fine-tuned for KD
as they are not optimized to account for user's temporal typing information
(e.g., hold time and flight time). To overcome this limitation, we propose
TempCharBERT, an architecture that incorporates temporal-character information
in the embedding layer of CharBERT. This allows modeling keystroke dynamics for
the purpose of user identification and authentication. Our results show a
significant improvement with this customization. We also showed the feasibility
of training TempCharBERT on a federated learning settings in order to foster
data privacy.",cs.CR
ZT-RIC:A Zero Trust RIC Framework for ensuring data Privacy and Confidentiality in Open RAN,"The advancement of 5G and NextG networks through Open Radio Access Network
(O-RAN) architecture enables a shift toward virtualized, modular, and
disaggregated configurations. A core component of O-RAN is the RAN Intelligent
Controller (RIC), which manages RAN using machine learning-driven xApps that
access sensitive data from RAN and User Equipment (UE), stored in the near
Real-Time RIC (Near-RT RIC) database. This shared, open environment increases
the risk of unauthorized data exposure. To address these concerns, this paper
proposes a zero-trust RIC (ZT-RIC) framework that preserves data privacy across
the RIC platform, including the RIC database, xApps, and E2 interface. ZT-RIC
employs Inner Product Functional Encryption (IPFE) to encrypt RAN/UE data at
the base station, preventing leaks through the E2 interface and shared
database. Additionally, ZT-RIC enables xApps to perform inference on encrypted
data without exposing sensitive information. For evaluation, a state-of-the-art
InterClass xApp, which detects jamming signals using RAN key performance
metrics (KPMs), is implemented. Testing on an LTE/5G O-RAN testbed shows that
ZT-RIC preserves data confidentiality while achieving 97.9% accuracy in jamming
detection and meeting sub-second latency requirements, with a round-trip time
(RTT) of 0.527 seconds.",cs.CR
TinyML Security: Exploring Vulnerabilities in Resource-Constrained Machine Learning Systems,"Tiny Machine Learning (TinyML) systems, which enable machine learning
inference on highly resource-constrained devices, are transforming edge
computing but encounter unique security challenges. These devices, restricted
by RAM and CPU capabilities two to three orders of magnitude smaller than
conventional systems, make traditional software and hardware security solutions
impractical. The physical accessibility of these devices exacerbates their
susceptibility to side-channel attacks and information leakage. Additionally,
TinyML models pose security risks, with weights potentially encoding sensitive
data and query interfaces that can be exploited. This paper offers the first
thorough survey of TinyML security threats. We present a device taxonomy that
differentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities
unique to TinyML. We list various attack vectors, assess their threat levels
using the Common Vulnerability Scoring System, and evaluate both existing and
possible defenses. Our analysis identifies where traditional security measures
are adequate and where solutions tailored to TinyML are essential. Our results
underscore the pressing need for specialized security solutions in TinyML to
ensure robust and secure edge computing applications. We aim to inform the
research community and inspire innovative approaches to protecting this rapidly
evolving and critical field.",cs.CR
Towards Characterizing Cyber Networks with Large Language Models,"Threat hunting analyzes large, noisy, high-dimensional data to find sparse
adversarial behavior. We believe adversarial activities, however they are
disguised, are extremely difficult to completely obscure in high dimensional
space. In this paper, we employ these latent features of cyber data to find
anomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM
was trained on Zeek network traffic logs from both a real-world production
network and an from Internet of Things (IoT) cybersecurity testbed. The model
is deliberately overtrained on a sliding window of data to characterize each
window closely. We use the Adjusted Rand Index (ARI) to comparing the k-means
clustering of CLEM output to expert labeling of the embeddings. Our approach
demonstrates that there is promise in using natural language modeling to
understand cyber data.",cs.CR
Eavesdropping on Semantic Communication: Timing Attacks and Countermeasures,"Semantic communication is a new paradigm that considers the meaning of
transmitted information to optimize communication. One possible application is
the remote monitoring of a process under communication costs: scheduling
updates based on semantic considerations can significantly reduce transmission
frequency while maintaining high-quality tracking performance. However,
semantic scheduling also opens a timing-based side-channel that an eavesdropper
may exploit to obtain information about the state of the remote process, even
if the content of updates is perfectly secure. In this work, we study an
eavesdropping attack against pull-based semantic scheduling for the tracking of
remote Markov processes. We provide a theoretical framework for defining the
effectiveness of the attack and of possible countermeasures, as well as a
practical heuristic that can provide a balance between the performance gains
offered by semantic communication and the information leakage.",cs.CR
ProP: Efficient Backdoor Detection via Propagation Perturbation for Overparametrized Models,"Backdoor attacks pose significant challenges to the security of machine
learning models, particularly for overparameterized models like deep neural
networks. In this paper, we propose ProP (Propagation Perturbation), a novel
and scalable backdoor detection method that leverages statistical output
distributions to identify backdoored models and their target classes without
relying on exhausive optimization strategies. ProP introduces a new metric, the
benign score, to quantify output distributions and effectively distinguish
between benign and backdoored models. Unlike existing approaches, ProP operates
with minimal assumptions, requiring no prior knowledge of triggers or malicious
samples, making it highly applicable to real-world scenarios. Extensive
experimental validation across multiple popular backdoor attacks demonstrates
that ProP achieves high detection accuracy and computational efficiency,
outperforming existing methods. These results highlight ProP's potential as a
robust and practical solution for backdoor detection.",cs.CR
The Inherent Adversarial Robustness of Analog In-Memory Computing,"A key challenge for Deep Neural Network (DNN) algorithms is their
vulnerability to adversarial attacks. Inherently non-deterministic compute
substrates, such as those based on Analog In-Memory Computing (AIMC), have been
speculated to provide significant adversarial robustness when performing DNN
inference. In this paper, we experimentally validate this conjecture for the
first time on an AIMC chip based on Phase Change Memory (PCM) devices. We
demonstrate higher adversarial robustness against different types of
adversarial attacks when implementing an image classification network.
Additional robustness is also observed when performing hardware-in-the-loop
attacks, for which the attacker is assumed to have full access to the hardware.
A careful study of the various noise sources indicate that a combination of
stochastic noise sources (both recurrent and non-recurrent) are responsible for
the adversarial robustness and that their type and magnitude disproportionately
effects this property. Finally, it is demonstrated, via simulations, that when
a much larger transformer network is used to implement a Natural Language
Processing (NLP) task, additional robustness is still observed.",cs.CR
Which PPML Would a User Choose? A Structured Decision Support Framework for Developers to Rank PPML Techniques Based on User Acceptance Criteria,"Using Privacy-Enhancing Technologies (PETs) for machine learning often
influences the characteristics of a machine learning approach, e.g., the needed
computational power, timing of the answers or how the data can be utilized.
When designing a new service, the developer faces the problem that some
decisions require a trade-off. For example, the use of a PET may cause a delay
in the responses or adding noise to the data to improve the users' privacy
might have a negative impact on the accuracy of the machine learning approach.
As of now, there is no structured way how the users' perception of a machine
learning based service can contribute to the selection of Privacy Preserving
Machine Learning (PPML) methods. This is especially a challenge since one
cannot assume that users have a deep technical understanding of these
technologies. Therefore, they can only be asked about certain attributes that
they can perceive when using the service and not directly which PPML they
prefer.
  This study introduces a decision support framework with the aim of supporting
the selection of PPML technologies based on user preferences. Based on prior
work analysing User Acceptance Criteria (UAC), we translate these criteria into
differentiating characteristics for various PPML techniques. As a final result,
we achieve a technology ranking based on the User Acceptance Criteria while
providing technology insights for the developers. We demonstrate its
application using the use case of classifying privacy-relevant information.
  Our contribution consists of the decision support framework which consists of
a process to connect PPML technologies with UAC, a process for evaluating the
characteristics that separate PPML techniques, and a ranking method to evaluate
the best PPML technique for the use case.",cs.CR
"Enhancing Phishing Detection through Feature Importance Analysis and Explainable AI: A Comparative Study of CatBoost, XGBoost, and EBM Models","Phishing attacks remain a persistent threat to online security, demanding
robust detection methods. This study investigates the use of machine learning
to identify phishing URLs, emphasizing the crucial role of feature selection
and model interpretability for improved performance. Employing Recursive
Feature Elimination, the research pinpointed key features like ""length_url,""
""time_domain_activation"" and ""Page_rank"" as strong indicators of phishing
attempts. The study evaluated various algorithms, including CatBoost, XGBoost,
and Explainable Boosting Machine, assessing their robustness and scalability.
XGBoost emerged as highly efficient in terms of runtime, making it well-suited
for large datasets. CatBoost, on the other hand, demonstrated resilience by
maintaining high accuracy even with reduced features. To enhance transparency
and trustworthiness, Explainable AI techniques, such as SHAP, were employed to
provide insights into feature importance. The study's findings highlight that
effective feature selection and model interpretability can significantly
bolster phishing detection systems, paving the way for more efficient and
adaptable defenses against evolving cyber threats",cs.CR
HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment,"With the introduction of the transformers architecture, LLMs have
revolutionized the NLP field with ever more powerful models. Nevertheless,
their development came up with several challenges. The exponential growth in
computational power and reasoning capabilities of language models has
heightened concerns about their security. As models become more powerful,
ensuring their safety has become a crucial focus in research. This paper aims
to address gaps in the current literature on jailbreaking techniques and the
evaluation of LLM vulnerabilities. Our contributions include the creation of a
novel dataset designed to assess the harmfulness of model outputs across
multiple harm levels, as well as a focus on fine-grained harm-level analysis.
Using this framework, we provide a comprehensive benchmark of state-of-the-art
jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.
Additionally, we examine how quantization techniques, such as AWQ and GPTQ,
influence the alignment and robustness of models, revealing trade-offs between
enhanced robustness with regards to transfer attacks and potential increases in
vulnerability on direct ones. This study aims to demonstrate the influence of
harmful input queries on the complexity of jailbreaking techniques, as well as
to deepen our understanding of LLM vulnerabilities and improve methods for
assessing model robustness when confronted with harmful content, particularly
in the context of compression strategies.",cs.CR
Are Neuromorphic Architectures Inherently Privacy-preserving? An Exploratory Study,"While machine learning (ML) models are becoming mainstream, especially in
sensitive application areas, the risk of data leakage has become a growing
concern. Attacks like membership inference (MIA) have shown that trained models
can reveal sensitive data, jeopardizing confidentiality. While traditional
Artificial Neural Networks (ANNs) dominate ML applications, neuromorphic
architectures, specifically Spiking Neural Networks (SNNs), are emerging as
promising alternatives due to their low power consumption and event-driven
processing, akin to biological neurons. Privacy in ANNs is well-studied;
however, little work has explored the privacy-preserving properties of SNNs.
This paper examines whether SNNs inherently offer better privacy. Using MIAs,
we assess the privacy resilience of SNNs versus ANNs across diverse datasets.
We analyze the impact of learning algorithms (surrogate gradient and
evolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN
privacy. Our findings show that SNNs consistently outperform ANNs in privacy
preservation, with evolutionary algorithms offering additional resilience. For
instance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than
ANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'
0.88. Additionally, we explore the privacy-utility trade-off with
Differentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs
sustain less accuracy loss than ANNs under similar privacy constraints.",cs.CR
Gen-AI for User Safety: A Survey,"Machine Learning and data mining techniques (i.e. supervised and unsupervised
techniques) are used across domains to detect user safety violations. Examples
include classifiers used to detect whether an email is spam or a web-page is
requesting bank login information. However, existing ML/DM classifiers are
limited in their ability to understand natural languages w.r.t the context and
nuances. The aforementioned challenges are overcome with the arrival of Gen-AI
techniques, along with their inherent ability w.r.t translation between
languages, fine-tuning between various tasks and domains.
  In this manuscript, we provide a comprehensive overview of the various work
done while using Gen-AI techniques w.r.t user safety. In particular, we first
provide the various domains (e.g. phishing, malware, content moderation,
counterfeit, physical safety) across which Gen-AI techniques have been applied.
Next, we provide how Gen-AI techniques can be used in conjunction with various
data modalities i.e. text, images, videos, audio, executable binaries to detect
violations of user-safety. Further, also provide an overview of how Gen-AI
techniques can be used in an adversarial setting. We believe that this work
represents the first summarization of Gen-AI techniques for user-safety.",cs.CR
Combining Entangled and Non-Entangled Based Quantum Key Distribution Protocol With GHZ State,"This paper presents a novel hybrid Quantum Key Distribution ,QKD, protocol
that combines entanglement based and non entanglement based approaches to
optimize security and the number of generated keys. We introduce a dynamic
system that integrates a three particle GHZ state method with the two state B92
protocol, using a quantum superposition state to probabilistically switch
between them. The GHZ state component leverages strong three particle
entanglement correlations for enhanced security, while the B92 component offers
simplicity and potentially higher key generation rates. Implemented and
simulated using Qiskit, our approach demonstrates higher number of generated
keys compared to standalone protocols while maintaining robust security. We
present a comprehensive analysis of the security properties and performance
characteristics of the proposed protocol. The results show that this combined
method effectively balances the trade offs inherent in QKD systems, offering a
flexible framework adaptable to varying channel conditions and security
requirements.This research contributes to ongoing efforts to make QKD more
practical and efficient, potentially advancing the development of large scale,
secured quantum networks.",cs.CR
InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance,"The proliferation of AI-generated images has intensified the need for robust
content authentication methods. We present InvisMark, a novel watermarking
technique designed for high-resolution AI-generated images. Our approach
leverages advanced neural network architectures and training strategies to
embed imperceptible yet highly robust watermarks. InvisMark achieves
state-of-the-art performance in imperceptibility (PSNR$\sim$51, SSIM $\sim$
0.998) while maintaining over 97\% bit accuracy across various image
manipulations. Notably, we demonstrate the successful encoding of 256-bit
watermarks, significantly expanding payload capacity while preserving image
quality. This enables the embedding of UUIDs with error correction codes,
achieving near-perfect decoding success rates even under challenging image
distortions. We also address potential vulnerabilities against advanced attacks
and propose mitigation strategies. By combining high imperceptibility, extended
payload capacity, and resilience to manipulations, InvisMark provides a robust
foundation for ensuring media provenance in an era of increasingly
sophisticated AI-generated content. Source code of this paper is available at:
https://github.com/microsoft/InvisMark.",cs.CR
LProtector: An LLM-driven Vulnerability Detection System,"This paper presents LProtector, an automated vulnerability detection system
for C/C++ codebases driven by the large language model (LLM) GPT-4o and
Retrieval-Augmented Generation (RAG). As software complexity grows, traditional
methods face challenges in detecting vulnerabilities effectively. LProtector
leverages GPT-4o's powerful code comprehension and generation capabilities to
perform binary classification and identify vulnerabilities within target
codebases. We conducted experiments on the Big-Vul dataset, showing that
LProtector outperforms two state-of-the-art baselines in terms of F1 score,
demonstrating the potential of integrating LLMs with vulnerability detection.",cs.CR
DDIM-Driven Coverless Steganography Scheme with Real Key,"Typical steganography embeds secret information into images by exploiting
their redundancy. Since the visual imperceptibility of secret information is a
key factor in scheme evaluation, conventional methods aim to balance this
requirement with embedding capacity. Consequently, integrating emerging image
generation models and secret transmission has been extensively explored to
achieve a higher embedding capacity. Previous works mostly focus on generating
stego-images with Generative Adversarial Networks (GANs) and usually rely on
pseudo-keys, namely conditions or parameters involved in the generation
process, which are related to secret images. However, studies on
diffusion-based coverless steganography remain insufficient. In this work, we
leverage the Denoising Diffusion Implicit Model (DDIM) to generate high-quality
stego-images without introducing pseudo-keys, instead employing real keys to
enhance security. Furthermore, our method offers low-image-correlation real-key
protection by incorporating chaotic encryption. Another core innovation is that
our method requires only one-time negotiation for multiple communications,
unlike prior methods that necessitate negotiation for each interaction.",cs.CR
Protection against Source Inference Attacks in Federated Learning using Unary Encoding and Shuffling,"Federated Learning (FL) enables clients to train a joint model without
disclosing their local data. Instead, they share their local model updates with
a central server that moderates the process and creates a joint model. However,
FL is susceptible to a series of privacy attacks. Recently, the source
inference attack (SIA) has been proposed where an honest-but-curious central
server tries to identify exactly which client owns a specific data record. n
this work, we propose a defense against SIAs by using a trusted shuffler,
without compromising the accuracy of the joint model. We employ a combination
of unary encoding with shuffling, which can effectively blend all clients'
model updates, preventing the central server from inferring information about
each client's model update separately. In order to address the increased
communication cost of unary encoding we employ quantization. Our preliminary
experiments show promising results; the proposed mechanism notably decreases
the accuracy of SIAs without compromising the accuracy of the joint model.",cs.CR
Detecting AutoEncoder is Enough to Catch LDM Generated Images,"In recent years, diffusion models have become one of the main methods for
generating images. However, detecting images generated by these models remains
a challenging task. This paper proposes a novel method for detecting images
generated by Latent Diffusion Models (LDM) by identifying artifacts introduced
by their autoencoders. By training a detector to distinguish between real
images and those reconstructed by the LDM autoencoder, the method enables
detection of generated images without directly training on them. The novelty of
this research lies in the fact that, unlike similar approaches, this method
does not require training on synthesized data, significantly reducing
computational costs and enhancing generalization ability. Experimental results
show high detection accuracy with minimal false positives, making this approach
a promising tool for combating fake images.",cs.CR
SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains,"As the integration of the Large Language Models (LLMs) into various
applications increases, so does their susceptibility to misuse, raising
significant security concerns. Numerous jailbreak attacks have been proposed to
assess the security defense of LLMs. Current jailbreak attacks mainly rely on
scenario camouflage, prompt obfuscation, prompt optimization, and prompt
iterative optimization to conceal malicious prompts. In particular, sequential
prompt chains in a single query can lead LLMs to focus on certain prompts while
ignoring others, facilitating context manipulation. This paper introduces
SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We
discuss several scenarios, not limited to examples like Question Bank, Dialog
Completion, and Game Environment, where the harmful prompt is embedded within
benign ones that can fool LLMs into generating harmful responses. The distinct
narrative structures of these scenarios show that SequentialBreak is flexible
enough to adapt to various prompt formats beyond those discussed. Extensive
experiments demonstrate that SequentialBreak uses only a single query to
achieve a substantial gain of attack success rate over existing baselines
against both open-source and closed-source models. Through our research, we
highlight the urgent need for more robust and resilient safeguards to enhance
LLM security and prevent potential misuse. All the result files and website
associated with this research are available in this GitHub repository:
https://anonymous.4open.science/r/JailBreakAttack-4F3B/.",cs.CR
HidePrint: Hiding the Radio Fingerprint via Random Noise,"Radio Frequency Fingerprinting (RFF) techniques allow a receiver to
authenticate a transmitter by analyzing the physical layer of the radio
spectrum. Although the vast majority of scientific contributions focus on
improving the performance of RFF considering different parameters and
scenarios, in this work, we consider RFF as an attack vector to identify and
track a target device.
  We propose, implement, and evaluate HidePrint, a solution to prevent tracking
through RFF without affecting the quality of the communication link between the
transmitter and the receiver. HidePrint hides the transmitter's fingerprint
against an illegitimate eavesdropper by injecting controlled noise in the
transmitted signal. We evaluate our solution against state-of-the-art
image-based RFF techniques considering different adversarial models, different
communication links (wired and wireless), and different configurations. Our
results show that the injection of a Gaussian noise pattern with a standard
deviation of (at least) 0.02 prevents device fingerprinting in all the
considered scenarios, thus making the performance of the identification process
indistinguishable from the random guess while affecting the Signal-to-Noise
Ratio (SNR) of the received signal by only 0.1 dB. Moreover, we introduce
selective radio fingerprint disclosure, a new technique that allows the
transmitter to disclose the radio fingerprint to only a subset of intended
receivers. This technique allows the transmitter to regain anonymity, thus
preventing identification and tracking while allowing authorized receivers to
authenticate the transmitter without affecting the quality of the transmitted
signal.",cs.CR
Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?,"This paper explores the coexistence possibilities of Central Bank Digital
Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum
computing landscape. It examines the implications of emerging quantum
algorithms and cryptographic techniques such as Multi-Party Computation (MPC)
and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies
might integrate defenses like post-quantum cryptography, it highlights the
substantial hurdles in transitioning legacy systems and fostering widespread
adoption of new standards. The paper includes comprehensive evaluations of
CBDCs in a quantum context. It also features comparisons to alternative
cryptocurrency models. Additionally, the paper provides insightful analyses of
pertinent quantum methodologies. Examinations of interfaces between these
methods and blockchain architectures are also included. The paper carries out
considered appraisals of quantum threats and their relevance for cryptocurrency
schemes. Furthermore, it features discussions of the influence of anticipated
advances in quantum computing on algorithms and their applications. The paper
renders the judicious conclusion that long-term coexistence is viable provided
challenges are constructively addressed through ongoing collaborative efforts
to validate solutions and guide evolving policies.",cs.CR
AMAZE: Accelerated MiMC Hardware Architecture for Zero-Knowledge Applications on the Edge,"Collision-resistant, cryptographic hash (CRH) functions have long been an
integral part of providing security and privacy in modern systems. Certain
constructions of zero-knowledge proof (ZKP) protocols aim to utilize CRH
functions to perform cryptographic hashing. Standard CRH functions, such as
SHA2, are inefficient when employed in the ZKP domain, thus calling for
ZK-friendly hashes, which are CRH functions built with ZKP efficiency in mind.
The most mature ZK-friendly hash, MiMC, presents a block cipher and hash
function with a simple algebraic structure that is well-suited, due to its
achieved security and low complexity, for ZKP applications. Although
ZK-friendly hashes have improved the performance of ZKP generation in software,
the underlying computation of ZKPs, including CRH functions, must be optimized
on hardware to enable practical applications. The challenge we address in this
work is determining how to efficiently incorporate ZK-friendly hash functions,
such as MiMC, into hardware accelerators, thus enabling more practical
applications. In this work, we introduce AMAZE, a highly hardware-optimized
open-source framework for computing the MiMC block cipher and hash function.
Our solution has been primarily directed at resource-constrained edge devices;
consequently, we provide several implementations of MiMC with varying power,
resource, and latency profiles. Our extensive evaluations show that the
AMAZE-powered implementation of MiMC outperforms standard CPU implementations
by more than 13$\times$. In all settings, AMAZE enables efficient ZK-friendly
hashing on resource-constrained devices. Finally, we highlight AMAZE's
underlying open-source arithmetic backend as part of our end-to-end design,
thus allowing developers to utilize the AMAZE framework for custom ZKP
applications.",cs.CR
Harpocrates: Oblivious Privacy in a Statically Typed World,"In this paper, we introduce Harpocrates, a compiler plugin and a framework
pair for Scala that binds the privacy policies to the data during data creation
in form of oblivious membranes. Harpocrates eliminates raw data for a policy
protected type from the application, ensuring it can only exist in protected
form and centralizes the policy checking to the policy declaration site, making
the privacy logic easy to maintain and verify. Instead of approaching privacy
from an information flow verification perspective, Harpocrates allow the data
to flow freely throughout the application, inside the policy membranes but
enforces the policies when the data is tried to be accessed, mutated,
declassified or passed through the application boundary. The centralization of
the policies allow the maintainers to change the enforced logic simply by
updating a single function while keeping the rest of the application oblivious
to the change. Especially in a setting where the data definition is shared by
multiple applications, the publisher can update the policies without requiring
the dependent applications to make any changes beyond updating the dependency
version.",cs.CR
TinyML NLP Approach for Semantic Wireless Sentiment Classification,"Natural Language Processing (NLP) operations, such as semantic sentiment
analysis and text synthesis, may often impair users' privacy and demand
significant on device computational resources. Centralized learning (CL) on the
edge offers an alternative energy-efficient approach, yet requires the
collection of raw information, which affects the user's privacy. While
Federated learning (FL) preserves privacy, it requires high computational
energy on board tiny user devices. We introduce split learning (SL) as an
energy-efficient alternative, privacy-preserving tiny machine learning (TinyML)
scheme and compare it to FL and CL in the presence of Rayleigh fading and
additive noise. Our results show that SL reduces processing power and CO2
emissions while maintaining high accuracy, whereas FL offers a balanced
compromise between efficiency and privacy. Hence, this study provides insights
into deploying energy-efficient, privacy-preserving NLP models on edge devices.",cs.CR
Federated Split Learning for Human Activity Recognition with Differential Privacy,"This paper proposes a novel intelligent human activity recognition (HAR)
framework based on a new design of Federated Split Learning (FSL) with
Differential Privacy (DP) over edge networks. Our FSL-DP framework leverages
both accelerometer and gyroscope data, achieving significant improvements in
HAR accuracy. The evaluation includes a detailed comparison between traditional
Federated Learning (FL) and our FSL framework, showing that the FSL framework
outperforms FL models in both accuracy and loss metrics. Additionally, we
examine the privacy-performance trade-off under different data settings in the
DP mechanism, highlighting the balance between privacy guarantees and model
accuracy. The results also indicate that our FSL framework achieves faster
communication times per training round compared to traditional FL, further
emphasizing its efficiency and effectiveness. This work provides valuable
insight and a novel framework which was tested on a real-life dataset.",cs.CR
Web Scale Graph Mining for Cyber Threat Intelligence,"Defending against today's increasingly sophisticated and large-scale
cyberattacks demands accurate, real-time threat intelligence. Traditional
approaches struggle to scale, integrate diverse telemetry, and adapt to a
constantly evolving security landscape. We introduce Threat Intelligence
Tracking via Adaptive Networks (TITAN), an industry-scale graph mining
framework that generates cyber threat intelligence at unprecedented speed and
scale. TITAN introduces a suite of innovations specifically designed to address
the complexities of the modern security landscape, including: (1) a dynamic
threat intelligence graph that maps the intricate relationships between
millions of entities, incidents, and organizations; (2) real-time update
mechanisms that automatically decay and prune outdated intel; (3) integration
of security domain knowledge to bootstrap initial reputation scores; and (4)
reputation propagation algorithms that uncover hidden threat actor
infrastructure. Integrated into Microsoft Unified Security Operations Platform
(USOP), which is deployed across hundreds of thousands of organizations
worldwide, TITAN's threat intelligence powers key detection and disruption
capabilities. With an impressive average macro-F1 score of 0.89 and a
precision-recall AUC of 0.94, TITAN identifies millions of high-risk entities
each week, enabling a 6x increase in non-file threat intelligence. Since its
deployment, TITAN has increased the product's incident disruption rate by a
remarkable 21%, while reducing the time to disrupt by a factor of 1.9x, and
maintaining 99% precision, as confirmed by customer feedback and thorough
manual evaluation by security experts--ultimately saving customers from costly
security breaches.",cs.CR
Smart-LLaMA: Two-Stage Post-Training of Large Language Models for Smart Contract Vulnerability Detection and Explanation,"With the rapid development of blockchain technology, smart contract security
has become a critical challenge. Existing smart contract vulnerability
detection methods face three main issues: (1) Insufficient quality of datasets,
lacking detailed explanations and precise vulnerability locations. (2) Limited
adaptability of large language models (LLMs) to the smart contract domain, as
most LLMs are pre-trained on general text data but minimal smart
contract-specific data. (3) Lack of high-quality explanations for detected
vulnerabilities, as existing methods focus solely on detection without clear
explanations. These limitations hinder detection performance and make it harder
for developers to understand and fix vulnerabilities quickly, potentially
leading to severe financial losses. To address these problems, we propose
Smart-LLaMA, an advanced detection method based on the LLaMA language model.
First, we construct a comprehensive dataset covering four vulnerability types
with labels, detailed explanations, and precise vulnerability locations.
Second, we introduce Smart Contract-Specific Continual Pre-Training, using raw
smart contract data to enable the LLM to learn smart contract syntax and
semantics, enhancing their domain adaptability. Furthermore, we propose
Explanation-Guided Fine-Tuning, which fine-tunes the LLM using paired
vulnerable code and explanations, enabling both vulnerability detection and
reasoned explanations. We evaluate explanation quality through LLM and human
evaluation, focusing on Correctness, Completeness, and Conciseness.
Experimental results show that Smart-LLaMA outperforms state-of-the-art
baselines, with average improvements of 6.49% in F1 score and 3.78% in
accuracy, while providing reliable explanations.",cs.CR
BM-PAW: A Profitable Mining Attack in the PoW-based Blockchain System,"Mining attacks enable an adversary to procure a disproportionately large
portion of mining rewards by deviating from honest mining practices within the
PoW-based blockchain system. In this paper, we demonstrate that the security
vulnerabilities of PoW-based blockchain extend beyond what these mining attacks
initially reveal. We introduce a novel mining strategy, named BM-PAW, which
yields superior rewards for both the attacker and the targeted pool compared to
the state-of-the-art mining attack: PAW. Our analysis reveals that BM-PAW
attackers are incentivized to offer appropriate bribe money to other targets,
as they comply with the attacker's directives upon receiving payment. We find
the BM-PAW attacker can circumvent the ""miner's dilemma"" through equilibrium
analysis in a two-pool BM-PAW game scenario, wherein the outcome is determined
by the attacker's mining power. We finally propose practical countermeasures to
mitigate these novel pool attacks.",cs.CR
IDU-Detector: A Synergistic Framework for Robust Masquerader Attack Detection,"In the digital age, users store personal data in corporate databases, making
data security central to enterprise management. Given the extensive attack
surface, assets face challenges like weak authentication, vulnerabilities, and
malware. Attackers may exploit vulnerabilities to gain unauthorized access,
masquerading as legitimate users. Such attacks can lead to privacy breaches,
business disruption, financial losses, and reputational damage. Complex attack
vectors blur lines between insider and external threats. To address this, we
introduce the IDU-Detector, integrating Intrusion Detection Systems (IDS) with
User and Entity Behavior Analytics (UEBA). This integration monitors
unauthorized access, bridges system gaps, ensures continuous monitoring, and
enhances threat identification. Existing insider threat datasets lack depth and
coverage of diverse attack vectors. This hinders detection technologies from
addressing complex attack surfaces. We propose new, diverse datasets covering
more attack scenarios, enhancing detection technologies. Testing our framework,
the IDU-Detector achieved average accuracies of 98.96% and 99.12%. These
results show effectiveness in detecting attacks, improving security and
response speed, and providing higher asset safety assurance.",cs.CR
"A Critical Analysis of Foundations, Challenges and Directions for Zero Trust Security in Cloud Environments","This review discusses the theoretical frameworks and application prospects of
Zero Trust Security (ZTS) in cloud computing context. This is because, as
organisations move more of their applications and data to the cloud, the old
borders-based security model that many implemented are inadequate, therefore a
model that has a trust no one, verify everything approach is required. This
paper analyzes the core principles of ZTS, including micro-segmentation, least
privileged access, and continuous monitoring, while critically examining four
major controversies: scalability issues, Economics, Integration issues with
existing systems, and Compliance to legal requirements. In this paper, having
reviewed the existing literature in the field and various implementation cases,
the main barriers to implementing zero trust security were outlined, including
the dimensions of decreased performance in large-scale production and the need
for major upfront investments that can be difficult for small companies to meet
effectively. This research shows that there is no clear correlation between
security effectiveness and operational efficiency: while organisations
experience up to 40% decrease of security incidents after implementation, they
note first negative impacts on performance. This study also shows that to
support ZTS there is a need to address the context as the economics and
operations of ZTS differ in strengths depending on the size of the
organizations and the infrastructures. Some of these are: performance
enhancement and optimizations, economic optimization, architectural blend, and
privacy-preserving technologies. This review enriches the existing literature
on cloud security by presenting both the theoretical framework of ZTS and the
observed issues, and provides suggestions useful for future research and
practice in the construction of the cloud security architecture.",cs.CR
A Sharded Blockchain-Based Secure Federated Learning Framework for LEO Satellite Networks,"Low Earth Orbit (LEO) satellite networks are increasingly essential for
space-based artificial intelligence (AI) applications. However, as commercial
use expands, LEO satellite networks face heightened cyberattack risks,
especially through satellite-to-satellite communication links, which are more
vulnerable than ground-based connections. As the number of operational
satellites continues to grow, addressing these security challenges becomes
increasingly critical. Traditional approaches, which focus on sending models to
ground stations for validation, often overlook the limited communication
windows available to LEO satellites, leaving critical security risks
unaddressed. To tackle these challenges, we propose a sharded blockchain-based
federated learning framework for LEO networks, called SBFL-LEO. This framework
improves the reliability of inter-satellite communications using blockchain
technology and assigns specific roles to each satellite. Miner satellites
leverage cosine similarity (CS) and Density-Based Spatial Clustering of
Applications with Noise (DBSCAN) to identify malicious models and monitor each
other to detect inaccurate aggregated models. Security analysis and
experimental results demonstrate that our approach outperforms baseline methods
in both model accuracy and energy efficiency, significantly enhancing system
robustness against attacks.",cs.CR
Multiuser Commitment over Noisy Channels,"We consider multi-user commitment models that capture the problem of enabling
multiple bidders to simultaneously submit auctions to verifiers while ensuring
that i) verifiers do not obtain information on the auctions until bidders
reveal them at a later stage; and, ii) bidders cannot change their auction once
committed. Specifically, we assume that bidders and verifiers have access to a
noiseless channel as well as a noisy multiple-access channel or broadcast
channel, where inputs are controlled by the bidders and outputs are observed by
verifiers. In the case of multiple bidders and a single verifier connected by a
non-redundant multiple-access channel, we characterize the commitment capacity
region when bidders are not colluding. When the bidders are colluding, we
derive an achievable region and a tight converse for the sum rate. In both
cases our proposed achievable commitment schemes are constructive. In the case
of a single bidder and multiple verifiers connected by a non-redundant
broadcast channel, in which verifiers could drop out of the network after
auctions are committed, we also characterize the commitment capacity. Our
results demonstrate how commitment schemes can benefit from multi-user
protocols, and develop resilience when some verifiers may become unavailable.",cs.CR
Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM,"Sandboxes and other dynamic analysis processes are prevalent in malware
detection systems nowadays to enhance the capability of detecting 0-day
malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in
modern malware samples, and sandboxes can suffer from false negatives and
analysis failures when analyzing the samples with TADAs. In such cases, human
reverse engineers will get involved in conducting dynamic analysis manually
(i.e., debugging, patching), which in turn also gets obstructed by TADAs. In
this work, we propose a Large Language Model (LLM) based workflow that can
pinpoint the location of the TADA implementation in the code, to help reverse
engineers place breakpoints used in debugging. Our evaluation shows that we
successfully identified the locations of 87.80% known TADA implementations
adopted from public repositories. In addition, we successfully pinpoint the
locations of TADAs in 4 well-known malware samples that are documented in
online malware analysis blogs.",cs.CR
Ideal Pseudorandom Codes,"Pseudorandom codes are error-correcting codes with the property that no
efficient adversary can distinguish encodings from uniformly random strings.
They were recently introduced by Christ and Gunn [CRYPTO 2024] for the purpose
of watermarking the outputs of randomized algorithms, such as generative AI
models. Several constructions of pseudorandom codes have since been proposed,
but none of them are robust to error channels that depend on previously seen
codewords. This stronger kind of robustness is referred to as adaptive
robustness, and it is important for meaningful applications to watermarking.
  In this work, we show the following.
  - Adaptive robustness: We show that the pseudorandom codes of Christ and Gunn
are adaptively robust, resolving a conjecture posed by Cohen, Hoover, and
Schoenbach [S&P 2025].
  - Ideal security: We define an ideal pseudorandom code as one which is
indistinguishable from the ideal functionality, capturing both the
pseudorandomness and robustness properties in one simple definition. We show
that any adaptively robust pseudorandom code for single-bit messages can be
bootstrapped to build an ideal pseudorandom code with linear information rate,
under no additional assumptions.
  - CCA security: In the setting where the encoding key is made public, we
define a CCA-secure pseudorandom code in analogy with CCA-secure encryption. We
show that any adaptively robust public-key pseudorandom code for single-bit
messages can be used to build a CCA-secure pseudorandom code with linear
information rate, in the random oracle model.
  These results immediately imply stronger robustness guarantees for generative
AI watermarking schemes, such as the practical quality-preserving image
watermarks of Gunn, Zhao, and Song (2024).",cs.CR
On Differentially Private String Distances,"Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.",cs.CR
Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods,"Membership inference attacks (MIAs) are widely used to empirically assess the
privacy risks of samples used to train a target machine learning model.
State-of-the-art methods however require training hundreds of shadow models,
with the same size and architecture of the target model, solely to evaluate the
privacy risk. While one might be able to afford this for small models, the cost
often becomes prohibitive for medium and large models.
  We here instead propose a novel approach to identify the at-risk samples
using only artifacts available during training, with little to no additional
computational overhead. Our method analyzes individual per-sample loss traces
and uses them to identify the vulnerable data samples. We demonstrate the
effectiveness of our artifact-based approach through experiments on the CIFAR10
dataset, showing high precision in identifying vulnerable samples as determined
by a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the
same precision as another SOTA MIA when measured against LiRA, despite it being
orders of magnitude cheaper. We then show LT-IQR to outperform alternative loss
aggregation methods, perform ablation studies on hyperparameters, and validate
the robustness of our method to the target metric. Finally, we study the
evolution of the vulnerability score distribution throughout training as a
metric for model-level risk assessment.",cs.CR
Differential Privacy Under Class Imbalance: Methods and Empirical Insights,"Imbalanced learning occurs in classification settings where the distribution
of class-labels is highly skewed in the training data, such as when predicting
rare diseases or in fraud detection. This class imbalance presents a
significant algorithmic challenge, which can be further exacerbated when
privacy-preserving techniques such as differential privacy are applied to
protect sensitive training data. Our work formalizes these challenges and
provides a number of algorithmic solutions. We consider DP variants of
pre-processing methods that privately augment the original dataset to reduce
the class imbalance; these include oversampling, SMOTE, and private synthetic
data generation. We also consider DP variants of in-processing techniques,
which adjust the learning algorithm to account for the imbalance; these include
model bagging, class-weighted empirical risk minimization and class-weighted
deep learning. For each method, we either adapt an existing imbalanced learning
technique to the private setting or demonstrate its incompatibility with
differential privacy. Finally, we empirically evaluate these privacy-preserving
imbalanced learning methods under various data and distributional settings. We
find that private synthetic data methods perform well as a data pre-processing
step, while class-weighted ERMs are an alternative in higher-dimensional
settings where private synthetic data suffers from the curse of dimensionality.",cs.CR
ViT Enhanced Privacy-Preserving Secure Medical Data Sharing and Classification,"Privacy-preserving and secure data sharing are critical for medical image
analysis while maintaining accuracy and minimizing computational overhead are
also crucial. Applying existing deep neural networks (DNNs) to encrypted
medical data is not always easy and often compromises performance and security.
To address these limitations, this research introduces a secure framework
consisting of a learnable encryption method based on the block-pixel operation
to encrypt the data and subsequently integrate it with the Vision Transformer
(ViT). The proposed framework ensures data privacy and security by creating
unique scrambling patterns per key, providing robust performance against
leading bit attacks and minimum difference attacks.",cs.CR
A Survey of AI-Related Cyber Security Risks and Countermeasures in Mobility-as-a-Service,"Mobility-as-a-Service (MaaS) integrates different transport modalities and
can support more personalisation of travellers' journey planning based on their
individual preferences, behaviours and wishes. To fully achieve the potential
of MaaS, a range of AI (including machine learning and data mining) algorithms
are needed to learn personal requirements and needs, to optimise journey
planning of each traveller and all travellers as a whole, to help transport
service operators and relevant governmental bodies to operate and plan their
services, and to detect and prevent cyber attacks from various threat actors
including dishonest and malicious travellers and transport operators. The
increasing use of different AI and data processing algorithms in both
centralised and distributed settings opens the MaaS ecosystem up to diverse
cyber and privacy attacks at both the AI algorithm level and the connectivity
surfaces. In this paper, we present the first comprehensive review on the
coupling between AI-driven MaaS design and the diverse cyber security
challenges related to cyber attacks and countermeasures. In particular, we
focus on how current and emerging AI-facilitated privacy risks (profiling,
inference, and third-party threats) and adversarial AI attacks (evasion,
extraction, and gamification) may impact the MaaS ecosystem. These risks often
combine novel attacks (e.g., inverse learning) with traditional attack vectors
(e.g., man-in-the-middle attacks), exacerbating the risks for the wider
participation actors and the emergence of new business models.",cs.CR
Towards a Re-evaluation of Data Forging Attacks in Practice,"Data forging attacks provide counterfactual proof that a model was trained on
a given dataset, when in fact, it was trained on another. These attacks work by
forging (replacing) mini-batches with ones containing distinct training
examples that produce nearly identical gradients. Data forging appears to break
any potential avenues for data governance, as adversarial model owners may
forge their training set from a dataset that is not compliant to one that is.
Given these serious implications on data auditing and compliance, we critically
analyse data forging from both a practical and theoretical point of view,
finding that a key practical limitation of current attack methods makes them
easily detectable by a verifier; namely that they cannot produce sufficiently
identical gradients. Theoretically, we analyse the question of whether two
distinct mini-batches can produce the same gradient. Generally, we find that
while there may exist an infinite number of distinct mini-batches with
real-valued training examples and labels that produce the same gradient,
finding those that are within the allowed domain e.g. pixel values between
0-255 and one hot labels is a non trivial task. Our results call for the
reevaluation of the strength of existing attacks, and for additional research
into successful data forging, given the serious consequences it may have on
machine learning and privacy.",cs.CR
From Resource Control to Digital Trust with User-Managed Access,"The User-Managed Access (UMA) extension to OAuth 2.0 is a promising candidate
for increasing Digital Trust in personal data ecosystems like Solid. With minor
modifications, it can achieve many requirements regarding usage control and
transaction contextualization, even though additional specification is needed
to address delegation of control and retraction of usage policies.",cs.CR
Obfuscation as Instruction Decorrelation,"Obfuscation of computer programs has historically been approached either as a
practical but \textit{ad hoc} craft to make reverse engineering subjectively
difficult, or as a sound theoretical investigation unfortunately detached from
the numerous existing constraints of engineering practical systems.
  In this paper, we propose \textit{instruction decorrelation} as a new
approach that makes the instructions of a set of real-world programs appear
independent from one another. We contribute: a formal definition of
\textit{instruction independence} with multiple instantiations for various
aspects of programs; a combination of program transformations that meet the
corresponding instances of instruction independence against an
honest-but-curious adversary, specifically random interleaving and memory
access obfuscation; and an implementation of an interpreter that uses a trusted
execution environment (TEE) only to perform memory address translation and
memory shuffling, leaving instructions execution outside the TEE.
  These first steps highlight the practicality of our approach. Combined with
additional techniques to protect the content of memory and to hopefully lower
the requirements on TEEs, this work could potentially lead to more secure
obfuscation techniques that could execute on commonly available hardware.",cs.CR
A Comparative Analysis of Machine Learning Models for DDoS Detection in IoT Networks,"This paper presents the detection of DDoS attacks in IoT networks using
machine learning models. Their rapid growth has made them highly susceptible to
various forms of cyberattacks, many of whose security procedures are
implemented in an irregular manner. It evaluates the efficacy of different
machine learning models, such as XGBoost, K-Nearest Neighbours, Stochastic
Gradient Descent, and Na\""ive Bayes, in detecting DDoS attacks from normal
network traffic. Each model has been explained on several performance metrics,
such as accuracy, precision, recall, and F1-score to understand the suitability
of each model in real-time detection and response against DDoS threats. This
comparative analysis will, therefore, enumerate the unique strengths and
weaknesses of each model with respect to the IoT environments that are dynamic
and hence moving in nature. The effectiveness of these models is analyzed,
showing how machine learning can greatly enhance IoT security frameworks,
offering adaptive, efficient, and reliable DDoS detection capabilities. These
findings have shown the potential of machine learning in addressing the
pressing need for robust IoT security solutions that can mitigate modern cyber
threats and assure network integrity.",cs.CR
Sdn Intrusion Detection Using Machine Learning Method,"Software-defined network (SDN) is a new approach that allows network control
to become directly programmable, and the underlying infrastructure can be
abstracted from applications and network services. Control plane). When it
comes to security, the centralization that this demands is ripe for a variety
of cyber threats that are not typically seen in other network architectures.
The authors in this research developed a novel machine-learning method to
capture infections in networks. We applied the classifier to the UNSW-NB 15
intrusion detection benchmark and trained a model with this data. Random Forest
and Decision Tree are classifiers used to assess with Gradient Boosting and
AdaBoost. Out of these best-performing models was Gradient Boosting with an
accuracy, recall, and F1 score of 99.87%,100%, and 99.85%, respectively, which
makes it reliable in the detection of intrusions for SDN networks. The second
best-performing classifier was also a Random Forest with 99.38% of accuracy,
followed by Ada Boost and Decision Tree. The research shows that the reason
that Gradient Boosting is so effective in this task is that it combines weak
learners and creates a strong ensemble model that can predict if traffic
belongs to a normal or malicious one with high accuracy. This paper indicates
that the GBDT-IDS model is able to improve network security significantly and
has better features in terms of both real-time detection accuracy and low false
positive rates. In future work, we will integrate this model into live SDN
space to observe its application and scalability. This research serves as an
initial base on which one can make further strides forward to enhance security
in SDN using ML techniques and have more secure, resilient networks.",cs.CR
EUREKHA: Enhancing User Representation for Key Hackers Identification in Underground Forums,"Underground forums serve as hubs for cybercriminal activities, offering a
space for anonymity and evasion of conventional online oversight. In these
hidden communities, malicious actors collaborate to exchange illicit knowledge,
tools, and tactics, driving a range of cyber threats from hacking techniques to
the sale of stolen data, malware, and zero-day exploits. Identifying the key
instigators (i.e., key hackers), behind these operations is essential but
remains a complex challenge. This paper presents a novel method called EUREKHA
(Enhancing User Representation for Key Hacker Identification in Underground
Forums), designed to identify these key hackers by modeling each user as a
textual sequence. This sequence is processed through a large language model
(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.
These extracted features are then fed into a Graph Neural Network (GNN) to
model user structural relationships, significantly improving identification
accuracy. Furthermore, we employ BERTopic (Bidirectional Encoder
Representations from Transformers Topic Modeling) to extract personalized
topics from user-generated content, enabling multiple textual representations
per user and optimizing the selection of the most representative sequence. Our
study demonstrates that fine-tuned LLMs outperform state-of-the-art methods in
identifying key hackers. Additionally, when combined with GNNs, our model
achieves significant improvements, resulting in approximately 6% and 10%
increases in accuracy and F1-score, respectively, over existing methods.
EUREKHA was tested on the Hack-Forums dataset, and we provide open-source
access to our code.",cs.CR
"Dave: a decentralized, secure, and lively fraud-proof algorithm","In this paper, we introduce a new fraud-proof algorithm that offers an
unprecedented combination of decentralization, security, and liveness. The
resources that must be mobilized by an honest participant to defeat an
adversary grow only logarithmically with what the adversary ultimately loses.
As a consequence, there is no need to introduce high bonds that prevent an
adversary from creating too many Sybils. This makes the system very inclusive
and frees participants from having to pool resources among themselves to engage
the protocol. Finally, the maximum delay to finalization also grows only
logarithmically with total adversarial expenditure, with the smallest
multiplicative factor to date. In summary: the entire dispute completes in 2--5
challenge periods, the only way to break consensus is to censor the honest
party for more than one challenge period, and the costs of engaging in the
dispute are minimal.",cs.CR
Palermo: Improving the Performance of Oblivious Memory using Protocol-Hardware Co-Design,"Oblivious RAM (ORAM) hides the memory access patterns, enhancing data privacy
by preventing attackers from discovering sensitive information based on the
sequence of memory accesses. The performance of ORAM is often limited by its
inherent trade-off between security and efficiency, as concealing memory access
patterns imposes significant computational and memory overhead. While prior
works focus on improving the ORAM performance by prefetching and eliminating
ORAM requests, we find that their performance is very sensitive to workload
locality behavior and incurs additional management overhead caused by the ORAM
stash pressure.
  This paper presents Palermo: a protocol-hardware co-design to improve ORAM
performance. The key observation in Palermo is that classical ORAM protocols
enforce restrictive dependencies between memory operations that result in low
memory bandwidth utilization. Palermo introduces a new protocol that overlaps
large portions of memory operations, within a single and between multiple ORAM
requests, without breaking correctness and security guarantees. Subsequently,
we propose an ORAM controller architecture that executes the proposed protocol
to service ORAM requests. The hardware is responsible for concurrently issuing
memory requests as well as imposing the necessary dependencies to ensure a
consistent view of the ORAM tree across requests. Using a rich workload mix, we
demonstrate that Palermo outperforms the RingORAM baseline by 2.8x, on average,
incurring a negligible area overhead of 5.78mm^2 (less than 2% in 12th
generation Intel CPU after technology scaling) and 2.14W without sacrificing
security. We further show that Palermo also outperforms the state-of-the-art
works PageORAM, PrORAM, and IR-ORAM.",cs.CR
Quantum Rewinding for IOP-Based Succinct Arguments,"We analyze the post-quantum security of succinct interactive arguments
constructed from interactive oracle proofs (IOPs) and vector commitment
schemes. We prove that an interactive variant of the BCS transformation is
secure in the standard model against quantum adversaries when the vector
commitment scheme is collapsing. Our proof builds on and extends prior work on
the post-quantum security of Kilians succinct interactive argument, which is
instead based on probabilistically checkable proofs (PCPs). We introduce a new
quantum rewinding strategy that works across any number of rounds. As a
consequence of our results, we obtain standard-model post-quantum secure
succinct arguments with the best asymptotic complexity known.",cs.CR
A Quality-Centric Framework for Generic Deepfake Detection,"This paper addresses the generalization issue in deepfake detection by
harnessing forgery quality in training data. Generally, the forgery quality of
different deepfakes varies: some have easily recognizable forgery clues, while
others are highly realistic. Existing works often train detectors on a mix of
deepfakes with varying forgery qualities, potentially leading detectors to
short-cut the easy-to-spot artifacts from low-quality forgery samples, thereby
hurting generalization performance. To tackle this issue, we propose a novel
quality-centric framework for generic deepfake detection, which is composed of
a Quality Evaluator, a low-quality data enhancement module, and a learning
pacing strategy that explicitly incorporates forgery quality into the training
process. The framework is inspired by curriculum learning, which is designed to
gradually enable the detector to learn more challenging deepfake samples,
starting with easier samples and progressing to more realistic ones. We employ
both static and dynamic assessments to assess the forgery quality, combining
their scores to produce a final rating for each training sample. The rating
score guides the selection of deepfake samples for training, with higher-rated
samples having a higher probability of being chosen. Furthermore, we propose a
novel frequency data augmentation method specifically designed for low-quality
forgery samples, which helps to reduce obvious forgery traces and improve their
overall realism. Extensive experiments show that our method can be applied in a
plug-and-play manner and significantly enhance the generalization performance.",cs.CR
Revisiting the Robustness of Watermarking to Paraphrasing Attacks,"Amidst rising concerns about the internet being proliferated with content
generated from language models (LMs), watermarking is seen as a principled way
to certify whether text was generated from a model. Many recent watermarking
techniques slightly modify the output probabilities of LMs to embed a signal in
the generated output that can later be detected. Since early proposals for text
watermarking, questions about their robustness to paraphrasing have been
prominently discussed. Lately, some techniques are deliberately designed and
claimed to be robust to paraphrasing. However, such watermarking schemes do not
adequately account for the ease with which they can be reverse-engineered. We
show that with access to only a limited number of generations from a black-box
watermarked model, we can drastically increase the effectiveness of
paraphrasing attacks to evade watermark detection, thereby rendering the
watermark ineffective.",cs.CR
QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning,"Federated Learning has emerged as a leading approach for decentralized
machine learning, enabling multiple clients to collaboratively train a shared
model without exchanging private data. While FL enhances data privacy, it
remains vulnerable to inference attacks, such as gradient inversion and
membership inference, during both training and inference phases. Homomorphic
Encryption provides a promising solution by encrypting model updates to protect
against such attacks, but it introduces substantial communication overhead,
slowing down training and increasing computational costs. To address these
challenges, we propose QuanCrypt-FL, a novel algorithm that combines low-bit
quantization and pruning techniques to enhance protection against attacks while
significantly reducing computational costs during training. Further, we propose
and implement mean-based clipping to mitigate quantization overflow or errors.
By integrating these methods, QuanCrypt-FL creates a communication-efficient FL
framework that ensures privacy protection with minimal impact on model
accuracy, thereby improving both computational efficiency and attack
resilience. We validate our approach on MNIST, CIFAR-10, and CIFAR-100
datasets, demonstrating superior performance compared to state-of-the-art
methods. QuanCrypt-FL consistently outperforms existing method and matches
Vanilla-FL in terms of accuracy across varying client. Further, QuanCrypt-FL
achieves up to 9x faster encryption, 16x faster decryption, and 1.5x faster
inference compared to BatchCrypt, with training time reduced by up to 3x.",cs.CR
Traceable random numbers from a nonlocal quantum advantage,"The unpredictability of random numbers is fundamental to both digital
security and applications that fairly distribute resources. However, existing
random number generators have limitations-the generation processes cannot be
fully traced, audited, and certified to be unpredictable. The algorithmic steps
used in pseudorandom number generators are auditable, but they cannot guarantee
that their outputs were a priori unpredictable given knowledge of the initial
seed. Device-independent quantum random number generators can ensure that the
source of randomness was unknown beforehand, but the steps used to extract the
randomness are vulnerable to tampering. Here, for the first time, we
demonstrate a fully traceable random number generation protocol based on
device-independent techniques. Our protocol extracts randomness from
unpredictable non-local quantum correlations, and uses distributed intertwined
hash chains to cryptographically trace and verify the extraction process. This
protocol is at the heart of a public traceable and certifiable quantum
randomness beacon that we have launched. Over the first 40 days of operation,
we completed the protocol 7434 out of 7454 attempts -- a success rate of 99.7%.
Each time the protocol succeeded, the beacon emitted a pulse of 512 bits of
traceable randomness. The bits are certified to be uniform with error times
actual success probability bounded by $2^{-64}$. The generation of certifiable
and traceable randomness represents one of the first public services that
operates with an entanglement-derived advantage over comparable classical
approaches.",cs.CR
Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry,"In this work, we conduct a systematic study of stochastic saddle point
problems (SSP) and stochastic variational inequalities (SVI) under the
constraint of $(\epsilon,\delta)$-differential privacy (DP) in both Euclidean
and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in
the $\ell_p/\ell_q$ setup, $p,q\in[1,2]$. Here, we obtain a bound of
$\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the
strong SP-gap, where $n$ is the number of samples and $d$ is the dimension.
This rate is nearly optimal for any $p,q\in[1,2]$. Without additional
assumptions, such as smoothness or linearity requirements, prior work under DP
has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup).
Further, existing algorithms have each only been shown to work for specific
settings of $p$ and $q$ and under certain assumptions on the loss and the
feasible set, whereas we provide a general algorithm for DP SSPs whenever
$p,q\in[1,2]$. Our result is obtained via a novel analysis of the recursive
regularization algorithm. In particular, we develop new tools for analyzing
generalization, which may be of independent interest. Next, we turn our
attention towards SVIs with a monotone, bounded and Lipschitz operator and
consider $\ell_p$-setups, $p\in[1,2]$. Here, we provide the first analysis
which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}}
+ \frac{\sqrt{d}}{n\epsilon}\big)$. For $p-1=\Omega(1)$, this rate is near
optimal due to existing lower bounds. To obtain this result, we develop a
modified version of recursive regularization. Our analysis builds on the
techniques we develop for SSPs as well as employing additional novel components
which handle difficulties arising from adapting the recursive regularization
framework to SVIs.",cs.CR
Adversarial Robustness of In-Context Learning in Transformers for Linear Regression,"Transformers have demonstrated remarkable in-context learning capabilities
across various domains, including statistical learning tasks. While previous
work has shown that transformers can implement common learning algorithms, the
adversarial robustness of these learned algorithms remains unexplored. This
work investigates the vulnerability of in-context learning in transformers to
\textit{hijacking attacks} focusing on the setting of linear regression tasks.
Hijacking attacks are prompt-manipulation attacks in which the adversary's goal
is to manipulate the prompt to force the transformer to generate a specific
output. We first prove that single-layer linear transformers, known to
implement gradient descent in-context, are non-robust and can be manipulated to
output arbitrary predictions by perturbing a single example in the in-context
training set. While our experiments show these attacks succeed on linear
transformers, we find they do not transfer to more complex transformers with
GPT-2 architectures. Nonetheless, we show that these transformers can be
hijacked using gradient-based adversarial attacks. We then demonstrate that
adversarial training enhances transformers' robustness against hijacking
attacks, even when just applied during finetuning. Additionally, we find that
in some settings, adversarial training against a weaker attack model can lead
to robustness to a stronger attack model. Lastly, we investigate the
transferability of hijacking attacks across transformers of varying scales and
initialization seeds, as well as between transformers and ordinary least
squares (OLS). We find that while attacks transfer effectively between
small-scale transformers, they show poor transferability in other scenarios
(small-to-large scale, large-to-large scale, and between transformers and OLS).",cs.CR
PentestAgent: Incorporating LLM Agents to Automated Penetration Testing,"Penetration testing is a critical technique for identifying security
vulnerabilities, traditionally performed manually by skilled security
specialists. This complex process involves gathering information about the
target system, identifying entry points, exploiting the system, and reporting
findings. Despite its effectiveness, manual penetration testing is
time-consuming and expensive, often requiring significant expertise and
resources that many organizations cannot afford. While automated penetration
testing methods have been proposed, they often fall short in real-world
applications due to limitations in flexibility, adaptability, and
implementation.
  Recent advancements in large language models (LLMs) offer new opportunities
for enhancing penetration testing through increased intelligence and
automation. However, current LLM-based approaches still face significant
challenges, including limited penetration testing knowledge and a lack of
comprehensive automation capabilities. To address these gaps, we propose
PentestAgent, a novel LLM-based automated penetration testing framework that
leverages the power of LLMs and various LLM-based techniques like Retrieval
Augmented Generation (RAG) to enhance penetration testing knowledge and
automate various tasks. Our framework leverages multi-agent collaboration to
automate intelligence gathering, vulnerability analysis, and exploitation
stages, reducing manual intervention. We evaluate PentestAgent using a
comprehensive benchmark, demonstrating superior performance in task completion
and overall efficiency. This work significantly advances the practical
applicability of automated penetration testing systems.",cs.CR
How to Delete Without a Trace: Certified Deniability in a Quantum World,"Is it possible to comprehensively destroy a piece of quantum information, so
that nothing is left behind except the memory of whether one had it at one
point? For example, various works, most recently Morimae, Poremba, and Yamakawa
(TQC 2024), show how to construct a signature scheme with certified deletion
where a user who deletes a signature on m cannot later produce a signature for
m. However, in all of the existing schemes, even after deletion the user is
still able keep irrefutable evidence that m was signed, and thus they do not
fully capture the spirit of deletion.
  In this work, we initiate the study of certified deniability in order to
obtain a more comprehensive notion of deletion. Certified deniability uses a
simulation-based security definition, ensuring that any information the user
has kept after deletion could have been learned without being given the
deleteable object to begin with; meaning that deletion leaves no trace behind!
We define and construct two non-interactive primitives that satisfy certified
deniability in the quantum random oracle model: signatures and non-interactive
zero-knowledge arguments (NIZKs). As a consequence, for example, it is not
possible to delete a signature/NIZK and later provide convincing evidence that
it used to exist. Notably, our results utilize uniquely quantum phenomena to
bypass the celebrated result of Pass (CRYPTO, 2003) showing that deniable NIZKs
are impossible even in the random oracle model.",cs.CR
EPIC: Enhancing Privacy through Iterative Collaboration,"Advancements in genomics technology lead to a rising volume of viral (e.g.,
SARS-CoV-2) sequence data, resulting in increased usage of machine learning
(ML) in bioinformatics. Traditional ML techniques require centralized data
collection and processing, posing challenges in realistic healthcare scenarios.
Additionally, privacy, ownership, and stringent regulation issues exist when
pooling medical data into centralized storage to train a powerful deep learning
(DL) model. The Federated learning (FL) approach overcomes such issues by
setting up a central aggregator server and a shared global model. It also
facilitates data privacy by extracting knowledge while keeping the actual data
private. This work proposes a cutting-edge Privacy enhancement through
Iterative Collaboration (EPIC) architecture. The network is divided and
distributed between local and centralized servers. We demonstrate the EPIC
approach to resolve a supervised classification problem to estimate SARS-CoV-2
genomic sequence data lineage without explicitly transferring raw sequence
data. We aim to create a universal decentralized optimization framework that
allows various data holders to work together and converge to a single
predictive model. The findings demonstrate that privacy-preserving strategies
can be successfully used with aggregation approaches without materially
altering the degree of learning convergence. Finally, we highlight a few
potential issues and prospects for study in FL-based approaches to healthcare
applications.",cs.CR
"The impact of mobility, beam sweeping and smart jammers on security vulnerabilities of 5G cells","The vulnerability of 5G networks to jamming attacks has emerged as a
significant concern. This paper contributes in two primary aspects. Firstly, it
investigates the effect of a multi-jammer on 5G cell metrics, specifically
throughput and goodput. The investigation is conducted within the context of a
mobility model for user equipment (UE), with a focus on scenarios involving
connected vehicles (CVs) engaged in a mission. Secondly, the vulnerability of
synchronization signal block (SSB) components is examined concerning jamming
power and beam sweeping. Notably, the study reveals that increasing jamming
power beyond 40 dBm in our specific scenario configuration no longer decreases
network throughput due to the re-transmission of packets through the hybrid
automatic repeat request (HARQ) process. Furthermore, it is observed that under
the same jamming power, the physical downlink shared channel (PDSCH) is more
vulnerable than the primary synchronization signal (PSS) and secondary
synchronization signal (SSS). However, a smart jammer can disrupt the cell
search process by injecting less power and targeting PSS-SSS or physical
broadcast channel (PBCH) data compared to a barrage jammer. On the other hand,
beam sweeping proves effective in mitigating the impact of a smart jammer,
reducing the error vector magnitude root mean square from 51.59% to 23.36%
under the same jamming power.",cs.CR
Watermarking Language Models through Language Models,"This paper presents a novel framework for watermarking language models
through prompts generated by language models. The proposed approach utilizes a
multi-model setup, incorporating a Prompting language model to generate
watermarking instructions, a Marking language model to embed watermarks within
generated content, and a Detecting language model to verify the presence of
these watermarks. Experiments are conducted using ChatGPT and Mistral as the
Prompting and Marking language models, with detection accuracy evaluated using
a pretrained classifier model. Results demonstrate that the proposed framework
achieves high classification accuracy across various configurations, with 95%
accuracy for ChatGPT, 88.79% for Mistral. These findings validate the and
adaptability of the proposed watermarking strategy across different language
model architectures. Hence the proposed framework holds promise for
applications in content attribution, copyright protection, and model
authentication.",cs.CR
Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries,"Security experts reverse engineer (decompile) binary code to identify
critical security vulnerabilities. The limited access to source code in vital
systems - such as firmware, drivers, and proprietary software used in Critical
Infrastructures (CI) - makes this analysis even more crucial on the binary
level. Even with available source code, a semantic gap persists after
compilation between the source and the binary code executed by the processor.
This gap may hinder the detection of vulnerabilities in source code. That being
said, current research on Large Language Models (LLMs) overlooks the
significance of decompiled binaries in this area by focusing solely on source
code. In this work, we are the first to empirically uncover the substantial
semantic limitations of state-of-the-art LLMs when it comes to analyzing
vulnerabilities in decompiled binaries, largely due to the absence of relevant
datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary
code vulnerability dataset. Our dataset is multi-architecture and
multi-optimization, focusing on C/C++ due to their wide usage in CI and
association with numerous vulnerabilities. Specifically, we curate 150,872
samples of vulnerable and non-vulnerable decompiled binary code for the task of
(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)
recovering function names in the domain of decompiled binaries. Subsequently,
we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance
increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and
CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,
using DeBinVul, we report a high performance of 80-90% on the vulnerability
classification task. Furthermore, we report improved performance in function
name recovery and vulnerability description tasks.",cs.CR
Seeing is Deceiving: Exploitation of Visual Pathways in Multi-Modal Language Models,"Multi-Modal Language Models (MLLMs) have transformed artificial intelligence
by combining visual and text data, making applications like image captioning,
visual question answering, and multi-modal content creation possible. This
ability to understand and work with complex information has made MLLMs useful
in areas such as healthcare, autonomous systems, and digital content. However,
integrating multiple types of data also creates security risks. Attackers can
manipulate either the visual or text inputs, or both, to make the model produce
unintended or even harmful responses. This paper reviews how visual inputs in
MLLMs can be exploited by various attack strategies. We break down these
attacks into categories: simple visual tweaks and cross-modal manipulations, as
well as advanced strategies like VLATTACK, HADES, and Collaborative Multimodal
Adversarial Attack (Co-Attack). These attacks can mislead even the most robust
models while looking nearly identical to the original visuals, making them hard
to detect. We also discuss the broader security risks, including threats to
privacy and safety in important applications. To counter these risks, we review
current defense methods like the SmoothVLM framework, pixel-wise randomization,
and MirrorCheck, looking at their strengths and limitations. We also discuss
new methods to make MLLMs more secure, including adaptive defenses, better
evaluation tools, and security approaches that protect both visual and text
data. By bringing together recent developments and identifying key areas for
improvement, this review aims to support the creation of more secure and
reliable multi-modal AI systems for real-world use.",cs.CR
Attention Masks Help Adversarial Attacks to Bypass Safety Detectors,"Despite recent research advancements in adversarial attack methods, current
approaches against XAI monitors are still discoverable and slower. In this
paper, we present an adaptive framework for attention mask generation to enable
stealthy, explainable and efficient PGD image classification adversarial attack
under XAI monitors. Specifically, we utilize mutation XAI mixture and multitask
self-supervised X-UNet for attention mask generation to guide PGD attack.
Experiments on MNIST (MLP), CIFAR-10 (AlexNet) have shown that our system can
outperform benchmark PGD, Sparsefool and SOTA SINIFGSM in balancing among
stealth, efficiency and explainability which is crucial for effectively fooling
SOTA defense protected classifiers.",cs.CR
Why quantum state verification cannot be both efficient and secure: a categorical approach,"The advantage of quantum protocols lies in the inherent properties of the
shared quantum states. These states are sometimes provided by sources that are
not trusted, and therefore need to be verified. Finding secure and efficient
quantum state verification protocols remains a big challenge, and recent works
illustrate trade-offs between efficiency and security for different groups of
states in restricted settings. However, whether a universal trade-off exists
for all quantum states and all verification strategies remains unknown. In this
work, we instantiate the categorical composable cryptography framework to show
a fundamental limit for quantum state verification for all cut-and-choose
approaches used to verify arbitrary quantum states. Our findings show that the
prevailing cut-and-choose techniques cannot lead to quantum state verification
protocols that are both efficient and secure.",cs.CR
"Cloning Games, Black Holes and Cryptography","The no-cloning principle has played a foundational role in quantum
information and cryptography. Following a long-standing tradition of studying
quantum mechanical phenomena through the lens of interactive games, Broadbent
and Lord (TQC 2020) formalized cloning games in order to quantitatively capture
no-cloning in the context of unclonable encryption schemes.
  The conceptual contribution of this paper is the new, natural, notion of Haar
cloning games together with two applications. In the area of black-hole
physics, our game reveals that, in an idealized model of a black hole which
features Haar random (or pseudorandom) scrambling dynamics, the information
from infalling entangled qubits can only be recovered from either the interior
or the exterior of the black hole -- but never from both places at the same
time. In the area of quantum cryptography, our game helps us construct succinct
unclonable encryption schemes from the existence of pseudorandom unitaries,
thereby, for the first time, bridging the gap between ""MicroCrypt"" and
unclonable cryptography. The technical contribution of this work is a tight
analysis of Haar cloning games which requires us to overcome many long-standing
barriers in our understanding of cloning games. Answering these questions
provably requires us to go beyond existing methods (Tomamichel, Fehr, Kaniewski
and Wehner, New Journal of Physics 2013). In particular, we show a new
technique for analyzing cloning games with respect to binary phase states
through the lens of binary subtypes, and combine it with novel bounds on the
operator norms of block-wise tensor products of matrices.",cs.CR
Differential Privacy Overview and Fundamental Techniques,"This chapter is meant to be part of the book ""Differential Privacy in
Artificial Intelligence: From Theory to Practice"" and provides an introduction
to Differential Privacy. It starts by illustrating various attempts to protect
data privacy, emphasizing where and why they failed, and providing the key
desiderata of a robust privacy definition. It then defines the key actors,
tasks, and scopes that make up the domain of privacy-preserving data analysis.
Following that, it formalizes the definition of Differential Privacy and its
inherent properties, including composition, post-processing immunity, and group
privacy. The chapter also reviews the basic techniques and mechanisms commonly
used to implement Differential Privacy in its pure and approximate forms.",cs.CR
Differentially Private Continual Learning using Pre-Trained Models,"This work explores the intersection of continual learning (CL) and
differential privacy (DP). Crucially, continual learning models must retain
knowledge across tasks, but this conflicts with the differential privacy
requirement of restricting individual samples to be memorised in the model. We
propose using pre-trained models to address the trade-offs between privacy and
performance in a continual learning setting. More specifically, we present
necessary assumptions to enable privacy-preservation and propose combining
pre-trained models with parameter-free classifiers and parameter-efficient
adapters that are learned under differential privacy. Our experiments
demonstrate their effectiveness and provide insights into balancing the
competing demands of continual learning and privacy.",cs.CR
EarCapAuth: Biometric Method for Earables Using Capacitive Sensing Eartips,"Earphones can give access to sensitive information via voice assistants which
demands security methods that prevent unauthorized use. Therefore, we developed
EarCapAuth, an authentication mechanism using 48 capacitive electrodes embedded
into the soft silicone eartips of two earables. For evaluation, we gathered
capactive ear canal measurements from 20 participants in 20 wearing sessions
(12 at rest, 8 while walking). A per user classifier trained for authentication
achieves an EER of 7.62% and can be tuned to a FAR (False Acceptance Rate) of
1% at FRR (False Rejection Rate) of 16.14%. For identification, EarCapAuth
achieves 89.95%. This outperforms some earable biometric principles from
related work. Performance under motion slightly decreased to 9.76% EER for
authentication and 86.40% accuracy for identification. Enrollment can be
performed rapidly with multiple short earpiece insertions and a biometric
decision is made every 0.33s. In the future, EarCapAuth could be integrated
into high-resolution brain sensing electrode tips.",cs.CR
Verification of Neural Networks against Convolutional Perturbations via Parameterised Kernels,"We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.",cs.CR
Experimental Secure Multiparty Computation from Quantum Oblivious Transfer with Bit Commitment,"Secure multiparty computation enables collaborative computations across
multiple users while preserving individual privacy, which has a wide range of
applications in finance, machine learning and healthcare. Secure multiparty
computation can be realized using oblivious transfer as a primitive function.
In this paper, we present an experimental implementation of a quantum-secure
quantum oblivious transfer (QOT) protocol using an adapted quantum key
distribution system combined with a bit commitment scheme, surpassing previous
approaches only secure in the noisy storage model. We demonstrate the first
practical application of the QOT protocol by solving the private set
intersection, a prime example of secure multiparty computation, where two
parties aim to find common elements in their datasets without revealing any
other information. In our experiments, two banks can identify common suspicious
accounts without disclosing any other data. This not only proves the
experimental functionality of QOT, but also showcases its real-world commercial
applications.",cs.CR
Intellectual Property Protection for Deep Learning Model and Dataset Intelligence,"With the growing applications of Deep Learning (DL), especially recent
spectacular achievements of Large Language Models (LLMs) such as ChatGPT and
LLaMA, the commercial significance of these remarkable models has soared.
However, acquiring well-trained models is costly and resource-intensive. It
requires a considerable high-quality dataset, substantial investment in
dedicated architecture design, expensive computational resources, and efforts
to develop technical expertise. Consequently, safeguarding the Intellectual
Property (IP) of well-trained models is attracting increasing attention. In
contrast to existing surveys overwhelmingly focusing on model IPP mainly, this
survey not only encompasses the protection on model level intelligence but also
valuable dataset intelligence. Firstly, according to the requirements for
effective IPP design, this work systematically summarizes the general and
scheme-specific performance evaluation metrics. Secondly, from proactive IP
infringement prevention and reactive IP ownership verification perspectives, it
comprehensively investigates and analyzes the existing IPP methods for both
dataset and model intelligence. Additionally, from the standpoint of training
settings, it delves into the unique challenges that distributed settings pose
to IPP compared to centralized settings. Furthermore, this work examines
various attacks faced by deep IPP techniques. Finally, we outline prospects for
promising future directions that may act as a guide for innovative research.",cs.CR
Anonymous Public-Key Quantum Money and Quantum Voting,"Quantum information allows us to build quantum money schemes, where a bank
can issue banknotes in the form of authenticatable quantum states that cannot
be cloned or counterfeited. Similar to paper banknotes, in existing quantum
money schemes, a banknote consists of an unclonable quantum state and a
classical serial number, signed by bank. Thus, they lack one of the most
fundamental properties cryptographers look for in a currency scheme: privacy.
In this work, we first further develop the formal definitions of privacy for
quantum money schemes. Then, we construct the first public-key quantum money
schemes that satisfy these security notions. Namely, - Assuming existence of
indistinguishability obfuscation (iO) and hardness of Learning with Errors
(LWE), we construct a public-key quantum money scheme with anonymity against
users and traceability by authorities.
  Since it is a policy choice whether authorities should be able to track
banknotes or not, we also construct an untraceable money scheme from the same
cryptographic assumptions, where no one (not even the authorities) can track
banknotes.
  Further, we show that the no-cloning principle, a result of quantum
mechanics, allows us to construct schemes, with security guarantees that are
classically impossible, for a seemingly unrelated application: voting! -
Assuming iO and LWE, we construct a universally verifiable quantum voting
scheme with classical votes.
  Finally, as a technical tool, we introduce the notion of publicly
rerandomizable encryption with strong correctness, where no adversary is able
to produce a malicious ciphertext and a malicious randomness such that the
ciphertext before and after rerandomization decrypts to different values! We
believe this might be of independent interest. - Assuming LWE, we construct a
(post-quantum) classical publicly rerandomizable encryption scheme with strong
correctness.",cs.CR
Financial Fraud Detection using Jump-Attentive Graph Neural Networks,"As the availability of financial services online continues to grow, the
incidence of fraud has surged correspondingly. Fraudsters continually seek new
and innovative ways to circumvent the detection algorithms in place.
Traditionally, fraud detection relied on rule-based methods, where rules were
manually created based on transaction data features. However, these techniques
soon became ineffective due to their reliance on manual rule creation and their
inability to detect complex data patterns. Today, a significant portion of the
financial services sector employs various machine learning algorithms, such as
XGBoost, Random Forest, and neural networks, to model transaction data. While
these techniques have proven more efficient than rule-based methods, they still
fail to capture interactions between different transactions and their
interrelationships. Recently, graph-based techniques have been adopted for
financial fraud detection, leveraging graph topology to aggregate neighborhood
information of transaction data using Graph Neural Networks (GNNs). Despite
showing improvements over previous methods, these techniques still struggle to
keep pace with the evolving camouflaging tactics of fraudsters and suffer from
information loss due to over-smoothing. In this paper, we propose a novel
algorithm that employs an efficient neighborhood sampling method, effective for
camouflage detection and preserving crucial feature information from
non-similar nodes. Additionally, we introduce a novel GNN architecture that
utilizes attention mechanisms and preserves holistic neighborhood information
to prevent information loss. We test our algorithm on financial data to show
that our method outperforms other state-of-the-art graph algorithms.",cs.CR
Game-Theoretic Defenses for Robust Conformal Prediction Against Adversarial Attacks in Medical Imaging,"Adversarial attacks pose significant threats to the reliability and safety of
deep learning models, especially in critical domains such as medical imaging.
This paper introduces a novel framework that integrates conformal prediction
with game-theoretic defensive strategies to enhance model robustness against
both known and unknown adversarial perturbations. We address three primary
research questions: constructing valid and efficient conformal prediction sets
under known attacks (RQ1), ensuring coverage under unknown attacks through
conservative thresholding (RQ2), and determining optimal defensive strategies
within a zero-sum game framework (RQ3). Our methodology involves training
specialized defensive models against specific attack types and employing
maximum and minimum classifiers to aggregate defenses effectively. Extensive
experiments conducted on the MedMNIST datasets, including PathMNIST,
OrganAMNIST, and TissueMNIST, demonstrate that our approach maintains high
coverage guarantees while minimizing prediction set sizes. The game-theoretic
analysis reveals that the optimal defensive strategy often converges to a
singular robust model, outperforming uniform and simple strategies across all
evaluated datasets. This work advances the state-of-the-art in uncertainty
quantification and adversarial robustness, providing a reliable mechanism for
deploying deep learning models in adversarial environments.",cs.CR
"Towards Secured Smart Grid 2.0: Exploring Security Threats, Protection Models, and Challenges","Many nations are promoting the green transition in the energy sector to
attain neutral carbon emissions by 2050. Smart Grid 2.0 (SG2) is expected to
explore data-driven analytics and enhance communication technologies to improve
the efficiency and sustainability of distributed renewable energy systems.
These features are beyond smart metering and electric surplus distribution in
conventional smart grids. Given the high dependence on communication networks
to connect distributed microgrids in SG2, potential cascading failures of
connectivity can cause disruption to data synchronization to the remote control
systems. This paper reviews security threats and defense tactics for three
stakeholders: power grid operators, communication network providers, and
consumers. Through the survey, we found that SG2's stakeholders are
particularly vulnerable to substation attacks/vandalism, malware/ransomware
threats, blockchain vulnerabilities and supply chain breakdowns. Furthermore,
incorporating artificial intelligence (AI) into autonomous energy management in
distributed energy resources of SG2 creates new challenges. Accordingly,
adversarial samples and false data injection on electricity reading and
measurement sensors at power plants can fool AI-powered control functions and
cause messy error-checking operations in energy storage, wrong energy
estimation in electric vehicle charging, and even fraudulent transactions in
peer-to-peer energy trading models. Scalable blockchain-based models, physical
unclonable function, interoperable security protocols, and trustworthy AI
models designed for managing distributed microgrids in SG2 are typical
promising protection models for future research.",cs.CR
Enhancing Security Control Production With Generative AI,"Security controls are mechanisms or policies designed for cloud based
services to reduce risk, protect information, and ensure compliance with
security regulations. The development of security controls is traditionally a
labor-intensive and time-consuming process. This paper explores the use of
Generative AI to accelerate the generation of security controls. We
specifically focus on generating Gherkin codes which are the domain-specific
language used to define the behavior of security controls in a structured and
understandable format. By leveraging large language models and in-context
learning, we propose a structured framework that reduces the time required for
developing security controls from 2-3 days to less than one minute. Our
approach integrates detailed task descriptions, step-by-step instructions, and
retrieval-augmented generation to enhance the accuracy and efficiency of the
generated Gherkin code. Initial evaluations on AWS cloud services demonstrate
promising results, indicating that GenAI can effectively streamline the
security control development process, thus providing a robust and dynamic
safeguard for cloud-based infrastructures.",cs.CR
Differentially Private Finite Population Estimation via Survey Weight Regularization,"In general, it is challenging to release differentially private versions of
survey-weighted statistics with low error for acceptable privacy loss. This is
because weighted statistics from complex sample survey data can be more
sensitive to individual survey response and weight values than unweighted
statistics, resulting in differentially private mechanisms that can add
substantial noise to the unbiased estimate of the finite population quantity.
On the other hand, simply disregarding the survey weights adds noise to a
biased estimator, which also can result in an inaccurate estimate. Thus, the
problem of releasing an accurate survey-weighted estimate essentially involves
a trade-off among bias, precision, and privacy. We leverage this trade-off to
develop a differentially private method for estimating finite population
quantities. The key step is to privately estimate a hyperparameter that
determines how much to regularize or shrink survey weights as a function of
privacy loss. We illustrate the differentially private finite population
estimation using the Panel Study of Income Dynamics. We show that optimal
strategies for releasing DP survey-weighted mean income estimates require
orders-of-magnitude less noise than naively using the original survey weights
without modification.",cs.CR
On the Power of Oblivious State Preparation,"We put forth Oblivious State Preparation (OSP) as a cryptographic primitive
that unifies techniques developed in the context of a quantum server
interacting with a classical client. OSP allows a classical polynomial-time
sender to input a choice of one out of two public observables, and a quantum
polynomial-time receiver to recover an eigenstate of the corresponding
observable -- while keeping the sender's choice hidden from any malicious
receiver. We obtain the following results:
  - The existence of (plain) trapdoor claw-free functions implies OSP, and the
existence of dual-mode trapdoor claw-free functions implies round-optimal
(two-round) OSP.
  - OSP implies the existence of proofs of quantumness, test of a qubit, blind
classical delegation of quantum computation, and classical verification of
quantum computation.
  - Two-round OSP implies quantum money with classical communication,
classically-verifiable position verification, and (additionally assuming
classical FHE with log-depth decryption) quantum FHE.
  Several of these applications were previously only known via tailored
LWE-based constructions, whereas our OSP-based constructions yield new results
from a wider variety of assumptions, including hard problems on cryptographic
group actions. Finally, towards understanding the minimal hardness assumptions
required to realize OSP, we prove the following:
  - OSP implies oblivious transfer between one classical and one quantum party.
  - Two-round OSP implies public-key encryption with classical keys and
ciphertexts.
  In particular, these results help to ''explain'' the use of public-key
cryptography in the known approaches to establishing a ''classical leash'' on a
quantum server. For example, combined with a result of Austrin et al. (CRYPTO
22), we conclude that perfectly-correct OSP cannot exist unconditionally in the
(quantum) random oracle model.",cs.CR
Scalable DP-SGD: Shuffling vs. Poisson Subsampling,"We provide new lower bounds on the privacy guarantee of the multi-epoch
Adaptive Batch Linear Queries (ABLQ) mechanism with shuffled batch sampling,
demonstrating substantial gaps when compared to Poisson subsampling; prior
analysis was limited to a single epoch. Since the privacy analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by
analyzing the ABLQ mechanism, this brings into serious question the common
practice of implementing shuffling-based DP-SGD, but reporting privacy
parameters as if Poisson subsampling was used. To understand the impact of this
gap on the utility of trained machine learning models, we introduce a practical
approach to implement Poisson subsampling at scale using massively parallel
computation, and efficiently train models with the same. We compare the utility
of models trained with Poisson-subsampling-based DP-SGD, and the optimistic
estimates of utility when using shuffling, via our new lower bounds on the
privacy guarantee of ABLQ with shuffling.",cs.CR
Security Assessment of Mobile Banking Apps in West African Economic and Monetary Union,"The West African Economic and Monetary Union (WAEMU) states, characterized by
widespread smartphone usage, have witnessed banks and financial institutions
introducing mobile banking applications (MBAs). These apps empower users to
perform transactions such as money transfers, bill payments, and account
inquiries anytime, anywhere. However, this proliferation of MBAs also raises
significant security concerns. Poorly implemented security measures during app
development can expose users and financial institutions to substantial
financial risks through increased vulnerability to cyberattacks. Our study
evaluated fifty-nine WAEMU MBAs using static analysis techniques. These MBAs
were collected from the 160 banks and financial institutions of the eight WAEMU
countries listed on the Central Bank of West African States (BCEAO) website. We
identified security-related code issues that could be exploited by malicious
actors. We investigated the issues found in the older versions to track their
evolution across updates. Additionally, we identified some banks from regions
such as Europe, the United States, and other developing countries and analyzed
their mobile apps for a security comparison with WAEMU MBAs. Key findings
include: (1) WAEMU apps exhibit security issues introduced during development,
posing significant risks of exploitation; (2) Despite frequent updates,
underlying security issues often persist; (3) Compared to MBAs from developed
and developing countries, WAEMU apps exhibit fewer critical security issues;
and (4) Apps from banks that are branches of other non-WAEMU banks often
inherit security concerns from their parent apps while also introducing
additional issues unique to their context. Our research underscores the need
for robust security practices in WAEMU MBAs development to enhance user safety
and trust in financial services.",cs.CR
Quantum Cryptography: an overview of Quantum Key Distribution,"This chapter highlights the transformation of secure communications through
the incorporation of quantum mechanics. Over the past four decades, this
groundbreaking theory has quietly revolutionized private communication. The
chapter provides a concise historical overview of this field's inception,
tracking the development of its pioneering protocol, BB84. It delves deeply
into the protocol's evolution, spotlighting its milestones and challenges.
Furthermore, it offers a panoramic view of the entire quantum key distribution
landscape, encompassing continuous variable protocols designed to harness
existing telecom technologies and device-independent quantum key distribution
protocols aimed at achieving secure key exchange with minimal reliance on the
experimental setup.",cs.CR
Quantum-Safe Hybrid Key Exchanges with KEM-Based Authentication,"Authenticated Key Exchange (AKE) between any two entities is one of the most
important security protocols available for securing our digital networks and
infrastructures. In PQCrypto 2023, Bruckner, Ramacher and Striecks proposed a
novel hybrid AKE (HAKE) protocol, dubbed Muckle+, that is particularly useful
in large quantum-safe networks consisting of a large number of nodes. Their
protocol is hybrid in the sense that it allows key material from conventional
and post-quantum primitives, as well as from quantum key distribution, to be
incorporated into a single end-to-end shared key.
  To achieve the desired authentication properties, Muckle+ utilizes
post-quantum digital signatures. However, available instantiations of such
signatures schemes are not yet efficient enough compared to their post-quantum
key-encapsulation mechanism (KEM) counterparts, particularly in large networks
with potentially several connections in a short period of time.
  To mitigate this gap, we propose Muckle# that pushes the efficiency
boundaries of currently known HAKE constructions. Muckle# uses post-quantum
key-encapsulating mechanisms for implicit authentication inspired by recent
works done in the area of Transport Layer Security (TLS) protocols,
particularly, in KEMTLS (CCS'20).
  We port those ideas to the HAKE framework and develop novel proof techniques
on the way. Due to our novel KEM-based approach, the resulting protocol has a
slightly different message flow compared to prior work that we carefully align
with the HAKE framework and which makes our changes to the Muckle+ non-trivial.",cs.CR
Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion,"Embeddings have become a cornerstone in the functionality of large language
models (LLMs) due to their ability to transform text data into rich, dense
numerical representations that capture semantic and syntactic properties. These
embedding vector databases serve as the long-term memory of LLMs, enabling
efficient handling of a wide range of natural language processing tasks.
However, the surge in popularity of embedding vector databases in LLMs has been
accompanied by significant concerns about privacy leakage. Embedding vector
databases are particularly vulnerable to embedding inversion attacks, where
adversaries can exploit the embeddings to reverse-engineer and extract
sensitive information from the original text data. Existing defense mechanisms
have shown limitations, often struggling to balance security with the
performance of downstream tasks. To address these challenges, we introduce
Eguard, a novel defense mechanism designed to mitigate embedding inversion
attacks. Eguard employs a transformer-based projection network and text mutual
information optimization to safeguard embeddings while preserving the utility
of LLMs. Our approach significantly reduces privacy risks, protecting over 95%
of tokens from inversion while maintaining high performance across downstream
tasks consistent with original embeddings.",cs.CR
WiP: Towards a Secure SECP256K1 for Crypto Wallets: Hardware Architecture and Implementation,"The SECP256K1 elliptic curve algorithm is fundamental in cryptocurrency
wallets for generating secure public keys from private keys, thereby ensuring
the protection and ownership of blockchain-based digital assets. However, the
literature highlights several successful side-channel attacks on hardware
wallets that exploit SECP256K1 to extract private keys. This work proposes a
novel hardware architecture for SECP256K1, optimized for side-channel attack
resistance and efficient resource utilization. The architecture incorporates
complete addition formulas, temporary registers, and parallel processing
techniques, making elliptic curve point addition and doubling operations
indistinguishable. Implementation results demonstrate an average reduction of
45% in LUT usage compared to similar works, emphasizing the design's resource
efficiency.",cs.CR
Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844,"Web3 is reshaping decentralized ecosystems through innovations like Ethereum.
Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling
solutions, which introduces a new 128 KB data structure called blob. This
upgrade incorporates type-3 transactions with blobs to verify data availability
and reduce gas costs for rollups, significantly affecting the strategies of
both builders and rollups. In this paper, we present an in-depth study of
emerging strategies in builder and rollup markets after EIP-4844, containing
hundred million transactions. We find that the efficiency of builder and rollup
strategies is interdependent, akin to two sides of the same coin -- both cannot
be optimized simultaneously. That is, when builders operate efficiently,
rollups tend to overpay in fees, conversely, when rollups optimize their costs,
builders may incur losses in inefficient transaction selection. From the side
of builders, our results show that 29.48% of these blocks have been constructed
inefficiently, which does not produce sufficient profits for builders. Through
our evaluation from the side of rollups, we find that over 72.53% of type-3
transactions pay unnecessary fees, leading to notable economic costs of
rollups. Our work provides critical insights into optimizing block construction
and transaction strategies, advancing the economic efficiency and data
scalability of Web3 infrastructures, yet, much like balancing a seesaw, the
efficiency of builders and rollups cannot be optimized concurrently.",cs.CR
ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization,"Watermarking generative content serves as a vital tool for authentication,
ownership protection, and mitigation of potential misuse. Existing watermarking
methods face the challenge of balancing robustness and concealment. They
empirically inject a watermark that is both invisible and robust and passively
achieve concealment by limiting the strength of the watermark, thus reducing
the robustness. In this paper, we propose to explicitly introduce a watermark
hiding process to actively achieve concealment, thus allowing the embedding of
stronger watermarks. To be specific, we implant a robust watermark in an
intermediate diffusion state and then guide the model to hide the watermark in
the final generated image. We employ an adversarial optimization algorithm to
produce the optimal hiding prompt guiding signal for each watermark. The prompt
embedding is optimized to minimize artifacts in the generated image, while the
watermark is optimized to achieve maximum strength. The watermark can be
verified by reversing the generation process. Experiments on various diffusion
models demonstrate the watermark remains verifiable even under significant
image tampering and shows superior invisibility compared to other
state-of-the-art robust watermarking methods.",cs.CR
FedRISE: Rating Induced Sign Election of Gradients for Byzantine Tolerant Federated Aggregation,"One of the most common defense strategies against model poisoning in
federated learning is to employ a robust aggregator mechanism that makes the
training more resilient. Many of the existing Byzantine robust aggregators
provide theoretical guarantees and are empirically effective against certain
categories of attacks. However, we observe that certain high-strength attacks
can subvert the aggregator and collapse the training. In addition, most
aggregators require identifying tolerant settings to converge. Impact of
attacks becomes more pronounced when the number of Byzantines is near-majority,
and becomes harder to evade if the attacker is omniscient with access to data,
honest updates and aggregation methods. Motivated by these observations, we
develop a robust aggregator called FedRISE for cross-silo FL that is consistent
and less susceptible to poisoning updates by an omniscient attacker. The
proposed method explicitly determines the optimal direction of each gradient
through a sign-voting strategy that uses variance-reduced sparse gradients. We
argue that vote weighting based on the cosine similarity of raw gradients is
misleading, and we introduce a sign-based gradient valuation function that
ignores the gradient magnitude. We compare our method against 8 robust
aggregators under 6 poisoning attacks on 3 datasets and architectures. Our
results show that existing robust aggregators collapse for at least some
attacks under severe settings, while FedRISE demonstrates better robustness
because of a stringent gradient inclusion formulation.",cs.CR
A Novel Access Control and Privacy-Enhancing Approach for Models in Edge Computing,"With the widespread adoption of edge computing technologies and the
increasing prevalence of deep learning models in these environments, the
security risks and privacy threats to models and data have grown more acute.
Attackers can exploit various techniques to illegally obtain models or misuse
data, leading to serious issues such as intellectual property infringement and
privacy breaches. Existing model access control technologies primarily rely on
traditional encryption and authentication methods; however, these approaches
exhibit significant limitations in terms of flexibility and adaptability in
dynamic environments. Although there have been advancements in model
watermarking techniques for marking model ownership, they remain limited in
their ability to proactively protect intellectual property and prevent
unauthorized access. To address these challenges, we propose a novel model
access control method tailored for edge computing environments. This method
leverages image style as a licensing mechanism, embedding style recognition
into the model's operational framework to enable intrinsic access control.
Consequently, models deployed on edge platforms are designed to correctly infer
only on license data with specific style, rendering them ineffective on any
other data. By restricting the input data to the edge model, this approach not
only prevents attackers from gaining unauthorized access to the model but also
enhances the privacy of data on terminal devices. We conducted extensive
experiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,
and the results demonstrate that our method effectively prevents unauthorized
access to the model while maintaining accuracy. Additionally, the model shows
strong resistance against attacks such as forged licenses and fine-tuning.
These results underscore the method's usability, security, and robustness.",cs.CR
Attribute-Based Encryption With Payable Outsourced Decryption Using Blockchain and Responsive Zero Knowledge Proof,"Attribute-Based Encryption (ABE) is a promising solution for access control
in cloud services. However, the heavy decryption overhead hinders its
widespread adoption. A general approach to address this issue is to outsource
decryption to decryption cloud service(DCS). Existing schemes have utilized
various methods to enable users to verify outsourced results; however, they
lack an effective mechanism to achieve exemptibility which enables the honest
DCS to escape from wrong claims. And it is impractical to assume that the DCS
will provide free services. In this paper, we propose a blockchain-based
payable outsourced decryption ABE scheme that achieves both verifiability and
exemptibility without adding redundant information to ABE ciphertext. We use
zero-knowledge proof to verify outsourced results on blockchain and introduce
an optional single-round challenge game under optimistic assumption to address
the high cost of proof generation. Moreover, our system achieves fairness and
decentralized outsourcing to protect the interests of all parties. Finally, we
implement and evaluate our scheme on Ethereum to demonstrate its feasibility
and efficiency, the gas usage in attribute numbers from 5 to 60 is 11$\times$
to 140$\times$ in the happy case and 4$\times$ to 55$\times$ in the challenge
case lower than the scheme of Ge et al. (TDSC'23).",cs.CR
MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.",cs.CR
Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization,"Recent studies have shown that deep learning models are very vulnerable to
poisoning attacks. Many defense methods have been proposed to address this
issue. However, traditional poisoning attacks are not as threatening as
commonly believed. This is because they often cause differences in how the
model performs on the training set compared to the validation set. Such
inconsistency can alert defenders that their data has been poisoned, allowing
them to take the necessary defensive actions. In this paper, we introduce a
more threatening type of poisoning attack called the Deferred Poisoning Attack.
This new attack allows the model to function normally during the training and
validation phases but makes it very sensitive to evasion attacks or even
natural noise. We achieve this by ensuring the poisoned model's loss function
has a similar value as a normally trained model at each input sample but with a
large local curvature. A similar model loss ensures that there is no obvious
inconsistency between the training and validation accuracy, demonstrating high
stealthiness. On the other hand, the large curvature implies that a small
perturbation may cause a significant increase in model loss, leading to
substantial performance degradation, which reflects a worse robustness. We
fulfill this purpose by making the model have singular Hessian information at
the optimal point via our proposed Singularization Regularization term. We have
conducted both theoretical and empirical analyses of the proposed method and
validated its effectiveness through experiments on image classification tasks.
Furthermore, we have confirmed the hazards of this form of poisoning attack
under more general scenarios using natural noise, offering a new perspective
for research in the field of security.",cs.CR
Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.",cs.CR
NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA,"The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.",cs.CR
Physical Layer Deception in OFDM Systems,"As a promising technology, physical layer security (PLS) enhances security by
leveraging the physical characteristics of communication channels. However, the
conventional PLS approach leads to a considerable disparity in the effort
legitimate users need to secure data compared to eavesdroppers. To address this
issue, we propose a physical layer deception (PLD) framework, which applies
random deceptive ciphering and orthogonal frequency-division multiplexing
(OFDM) to defend against eavesdropping proactively. While ensuring the same
level of confidentiality as traditional PLS methods, the PLD approach
additionally introduces a deception mechanism, even when the eavesdropper
possesses the same knowledge about the transmitter end as the legitimate
receiver. Through thorough theoretical analyses and numerical simulations, we
prove the superiority of our method over the conventional PLS approach.",cs.CR
Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach,"Graph neural networks (GNNs) have attracted considerable attention due to
their diverse applications. However, the scarcity and quality limitations of
graph data present challenges to their training process in practical settings.
To facilitate the development of effective GNNs, companies and researchers
often seek external collaboration. Yet, directly sharing data raises privacy
concerns, motivating data owners to train GNNs on their private graphs and
share the trained models. Unfortunately, these models may still inadvertently
disclose sensitive properties of their training graphs (e.g., average default
rate in a transaction network), leading to severe consequences for data owners.
In this work, we study graph property inference attack to identify the risk of
sensitive property information leakage from shared models. Existing approaches
typically train numerous shadow models for developing such attack, which is
computationally intensive and impractical. To address this issue, we propose an
efficient graph property inference attack by leveraging model approximation
techniques. Our method only requires training a small set of models on graphs,
while generating a sufficient number of approximated shadow models for attacks.
To enhance diversity while reducing errors in the approximated models, we apply
edit distance to quantify the diversity within a group of approximated models
and introduce a theoretically guaranteed criterion to evaluate each model's
error. Subsequently, we propose a novel selection mechanism to ensure that the
retained approximated models achieve high diversity and low error. Extensive
experiments across six real-world scenarios demonstrate our method's
substantial improvement, with average increases of 2.7% in attack accuracy and
4.1% in ROC-AUC, while being 6.5$\times$ faster compared to the best baseline.",cs.CR
Learning Constant-Depth Circuits in Malicious Noise Models,"The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time
algorithm for learning constant-depth circuits ($\mathsf{AC}^0$) with respect
to the uniform distribution on the hypercube. Extending their algorithm to the
setting of malicious noise, where both covariates and labels can be
adversarially corrupted, has remained open. Here we achieve such a result,
inspired by recent work on learning with distribution shift. Our running time
essentially matches their algorithm, which is known to be optimal assuming
various cryptographic primitives.
  Our proof uses a simple outlier-removal method combined with Braverman's
theorem for fooling constant-depth circuits. We attain the best possible
dependence on the noise rate and succeed in the harshest possible noise model
(i.e., contamination or so-called ""nasty noise"").",cs.CR
Towards Personalized Federated Learning via Comprehensive Knowledge Distillation,"Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.",cs.CR
Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.",cs.CR
"EVA-S3PC: Efficient, Verifiable, Accurate Secure Matrix Multiplication Protocol Assembly and Its Application in Regression","Efficient multi-party secure matrix multiplication is crucial for
privacy-preserving machine learning, but existing mixed-protocol frameworks
often face challenges in balancing security, efficiency, and accuracy. This
paper presents an efficient, verifiable and accurate secure three-party
computing (EVA-S3PC) framework that addresses these challenges with elementary
2-party and 3-party matrix operations based on data obfuscation techniques. We
propose basic protocols for secure matrix multiplication, inversion, and hybrid
multiplication, ensuring privacy and result verifiability. Experimental results
demonstrate that EVA-S3PC achieves up to 14 significant decimal digits of
precision in Float64 calculations, while reducing communication overhead by up
to $54.8\%$ compared to state of art methods. Furthermore, 3-party regression
models trained using EVA-S3PC on vertically partitioned data achieve accuracy
nearly identical to plaintext training, which illustrates its potential in
scalable, efficient, and accurate solution for secure collaborative modeling
across domains.",cs.CR
LLMs for Domain Generation Algorithm Detection,"This work analyzes the use of large language models (LLMs) for detecting
domain generation algorithms (DGAs). We perform a detailed evaluation of two
important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning
(SFT), showing how they can improve detection. SFT increases performance by
using domain-specific data, whereas ICL helps the detection model to quickly
adapt to new threats without requiring much retraining. We use Meta's Llama3 8B
model, on a custom dataset with 68 malware families and normal domains,
covering several hard-to-detect schemes, including recent word-based DGAs.
Results proved that LLM-based methods can achieve competitive results in DGA
detection. In particular, the SFT-based LLM DGA detector outperforms
state-of-the-art models using attention layers, achieving 94% accuracy with a
4% false positive rate (FPR) and excelling at detecting word-based DGA domains.",cs.CR
Quantum One-Time Protection of any Randomized Algorithm,"The meteoric rise in power and popularity of machine learning models
dependent on valuable training data has reignited a basic tension between the
power of running a program locally and the risk of exposing details of that
program to the user. At the same time, fundamental properties of quantum states
offer new solutions to data and program security that can require strikingly
few quantum resources to exploit, and offer advantages outside of mere
computational run time. In this work, we demonstrate such a solution with
quantum one-time tokens.
  A quantum one-time token is a quantum state that permits a certain program to
be evaluated exactly once. One-time security guarantees, roughly, that the
token cannot be used to evaluate the program more than once. We propose a
scheme for building quantum one-time tokens for any randomized classical
program, which include generative AI models. We prove that the scheme satisfies
an interesting definition of one-time security as long as outputs of the
classical algorithm have high enough min-entropy, in a black box model.
  Importantly, the classical program being protected does not need to be
implemented coherently on a quantum computer. In fact, the size and complexity
of the quantum one-time token is independent of the program being protected,
and additional quantum resources serve only to increase the security of the
protocol. Due to this flexibility in adjusting the security, we believe that
our proposal is parsimonious enough to serve as a promising candidate for a
near-term useful demonstration of quantum computing in either the NISQ or early
fault tolerant regime.",cs.CR
Oblivious Defense in ML Models: Backdoor Removal without Detection,"As society grows more reliant on machine learning, ensuring the security of
machine learning systems against sophisticated attacks becomes a pressing
concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022)
shows that an adversary can plant undetectable backdoors in machine learning
models, allowing the adversary to covertly control the model's behavior.
Backdoors can be planted in such a way that the backdoored machine learning
model is computationally indistinguishable from an honest model without
backdoors.
  In this paper, we present strategies for defending against backdoors in ML
models, even if they are undetectable. The key observation is that it is
sometimes possible to provably mitigate or even remove backdoors without
needing to detect them, using techniques inspired by the notion of random
self-reducibility. This depends on properties of the ground-truth labels
(chosen by nature), and not of the proposed ML model (which may be chosen by an
attacker).
  We give formal definitions for secure backdoor mitigation, and proceed to
show two types of results. First, we show a ""global mitigation"" technique,
which removes all backdoors from a machine learning model under the assumption
that the ground-truth labels are close to a Fourier-heavy function. Second, we
consider distributions where the ground-truth labels are close to a linear or
polynomial function in $\mathbb{R}^n$. Here, we show ""local mitigation""
techniques, which remove backdoors with high probability for every inputs of
interest, and are computationally cheaper than global mitigation. All of our
constructions are black-box, so our techniques work without needing access to
the model's representation (i.e., its code or parameters). Along the way we
prove a simple result for robust mean estimation.",cs.CR
On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep Support Vector Data Description,"In this paper, we study the problem of promptly detecting the presence of
non-cooperative activity from one or more Reconfigurable Intelligent Surfaces
(RISs) with unknown characteristics lying in the vicinity of a Multiple-Input
Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division
Multiplexing (OFDM) transmissions. We first present a novel wideband channel
model incorporating RISs as well as non-reconfigurable stationary surfaces,
which captures both the effect of the RIS actuation time on the channel in the
frequency domain as well as the difference between changing phase
configurations during or among transmissions. Considering that RISs may operate
under the coordination of a third-party system, and thus, may negatively impact
the communication of the intended MIMO OFDM system, we present a novel RIS
activity detection framework that is unaware of the distribution of the phase
configuration of any of the non-cooperative RISs. In particular, capitalizing
on the knowledge of the data distribution at the multi-antenna receiver, we
design a novel online change point detection statistic that combines a deep
support vector data description model with the scan $B$-test. The presented
numerical investigations demonstrate the improved detection accuracy as well as
decreased computational complexity of the proposed RIS detection approach over
existing change point detection schemes.",cs.CR
Formal Logic-guided Robust Federated Learning against Poisoning Attacks,"Federated Learning (FL) offers a promising solution to the privacy concerns
associated with centralized Machine Learning (ML) by enabling decentralized,
collaborative learning. However, FL is vulnerable to various security threats,
including poisoning attacks, where adversarial clients manipulate the training
data or model updates to degrade overall model performance. Recognizing this
threat, researchers have focused on developing defense mechanisms to counteract
poisoning attacks in FL systems. However, existing robust FL methods
predominantly focus on computer vision tasks, leaving a gap in addressing the
unique challenges of FL with time series data. In this paper, we present
FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated
learning for time-series tasks, even in scenarios with heterogeneous client
data and a large number of adversarial participants. Unlike traditional
model-centric defenses, FLORAL leverages logical reasoning to evaluate client
trustworthiness by aligning their predictions with global time-series patterns,
rather than relying solely on the similarity of client updates. Our approach
extracts logical reasoning properties from clients, then hierarchically infers
global properties, and uses these to verify client updates. Through formal
logic verification, we assess the robustness of each client contribution,
identifying deviations indicative of adversarial behavior. Experimental results
on two datasets demonstrate the superior performance of our approach compared
to existing baseline methods, highlighting its potential to enhance the
robustness of FL to time series applications. Notably, FLORAL reduced the
prediction error by 93.27% in the best-case scenario compared to the
second-best baseline. Our code is available at
https://anonymous.4open.science/r/FLORAL-Robust-FTS.",cs.CR
Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education,"Cyberattacks frequently target higher educational institutions, making
cybersecurity awareness and resilience critical for students. However, limited
research exists on cybersecurity awareness, attitudes, and resilience among
students in higher education. This study addresses this gap using the Theory of
Planned Behavior as a theoretical framework. A modified Human Aspects of
Information Security Questionnaire was employed to gather 266 valid responses
from undergraduate and postgraduate students at a South African higher
education institution. Key dimensions of cybersecurity awareness and behavior,
including password management, email usage, social media practices, and mobile
device security, were assessed. A significant disparity in cybersecurity
awareness and practices, with postgraduate students demonstrating superior
performance across several dimensions was noted. This research postulates the
existence of a Cybersecurity-Education Inflection Point during the transition
to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These
concepts provide a foundation for developing targeted cybersecurity education
initiatives in higher education, particularly highlighting the need for earlier
intervention at the undergraduate level.",cs.CR
Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras,"While security vulnerabilities in traditional Deep Neural Networks (DNNs)
have been extensively studied, the susceptibility of Spiking Neural Networks
(SNNs) to adversarial attacks remains mostly underexplored. Until now, the
mechanisms to inject backdoors into SNN models have been limited to digital
scenarios; thus, we present the first evaluation of backdoor attacks in
real-world environments.
  We begin by assessing the applicability of existing digital backdoor attacks
and identifying their limitations for deployment in physical environments. To
address each of the found limitations, we present three novel backdoor attack
methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess
the effectiveness of traditional backdoor procedures and defenses adapted for
SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that
while these procedures and defenses can mitigate some attacks, they often fail
against stronger methods like Flashy Backdoor or sacrifice too much clean
accuracy, rendering the models unusable.
  Overall, all our methods can achieve up to a 100% Attack Success Rate while
maintaining high clean accuracy in every tested dataset. Additionally, we
evaluate the stealthiness of the triggers with commonly used metrics, finding
them highly stealthy. Thus, we propose new alternatives more suited for
identifying poisoned samples in these scenarios. Our results show that further
research is needed to ensure the security of SNN-based systems against backdoor
attacks and their safe application in real-world scenarios. The code,
experiments, and results are available in our repository.",cs.CR
FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses,"Federated Learning is a privacy preserving decentralized machine learning
paradigm designed to collaboratively train models across multiple clients by
exchanging gradients to the server and keeping private data local.
Nevertheless, recent research has revealed that the security of Federated
Learning is compromised, as private ground truth data can be recovered through
a gradient inversion technique known as Deep Leakage. While these attacks are
crafted with a focus on applications in Federated Learning, they generally are
not evaluated in realistic scenarios. This paper introduces the FEDLAD
Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a
comprehensive benchmark for evaluating Deep Leakage attacks and defenses within
a realistic Federated context. By implementing a unified benchmark that
encompasses multiple state-of-the-art Deep Leakage techniques and various
defense strategies, our framework facilitates the evaluation and comparison of
the efficacy of these methods across different datasets and training states.
This work highlights a crucial trade-off between privacy and model accuracy in
Federated Learning and aims to advance the understanding of security challenges
in decentralized machine learning systems, stimulate future research, and
enhance reproducibility in evaluating Deep Leakage attacks and defenses.",cs.CR
Blockchain-Based Multi-Path Mobile Access Point Selection for Secure 5G VANETs,"This letter presents a blockchain-based multi-path mobile access point (MAP)
selection strategy for secure 5G vehicular ad-hoc networks (VANETs). The
proposed method leverages blockchain technology for decentralized, transparent,
and secure MAP selection, while the multi-path transmission strategy enhances
network reliability and reduces communication delays. A trust-based attack
detection mechanism is integrated to ensure network security. Simulation
results demonstrate that the proposed algorithm reduces both handover frequency
and average communication delay by over 80%, and successfully identifies and
excludes more than 95% of Sybil nodes, ensuring reliable and secure
communication in highly dynamic vehicular environments.",cs.CR
Personal Data Protection in AI-Native 6G Systems,"As 6G evolves into an AI-native technology, the integration of artificial
intelligence (AI) and Generative AI into cellular communication systems
presents unparalleled opportunities for enhancing connectivity, network
optimization, and personalized services. However, these advancements also
introduce significant data protection challenges, as AI models increasingly
depend on vast amounts of personal data for training and decision-making. In
this context, ensuring compliance with stringent data protection regulations,
such as the General Data Protection Regulation (GDPR), becomes critical for the
design and operational integrity of 6G networks. These regulations shape key
system architecture aspects, including transparency, accountability, fairness,
bias mitigation, and data security.
  This paper identifies and examines the primary data protection risks
associated with AI-driven 6G networks, focusing on the complex data flows and
processing activities throughout the 6G lifecycle. By exploring these risks, we
provide a comprehensive analysis of the potential privacy implications and
propose effective mitigation strategies. Our findings stress the necessity of
embedding privacy-by-design and privacy-by-default principles in the
development of 6G standards to ensure both regulatory compliance and the
protection of individual rights.",cs.CR
Region-Guided Attack on the Segment Anything Model (SAM),"The Segment Anything Model (SAM) is a cornerstone of image segmentation,
demonstrating exceptional performance across various applications, particularly
in autonomous driving and medical imaging, where precise segmentation is
crucial. However, SAM is vulnerable to adversarial attacks that can
significantly impair its functionality through minor input perturbations.
Traditional techniques, such as FGSM and PGD, are often ineffective in
segmentation tasks due to their reliance on global perturbations that overlook
spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address
these challenges, but they frequently depend on external cues and do not fully
leverage the structural interdependencies within segmentation processes. This
limitation underscores the need for a novel adversarial strategy that exploits
the unique characteristics of segmentation tasks. In response, we introduce the
Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a
Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted
perturbations that fragment large segments and expand smaller ones, resulting
in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves
high success rates in both white-box and black-box scenarios, emphasizing the
need for robust defenses against such sophisticated attacks. RGA not only
reveals SAM's vulnerabilities but also lays the groundwork for developing more
resilient defenses against adversarial threats in image segmentation.",cs.CR
Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering,"Combating money laundering has become increasingly complex with the rise of
cybercrime and digitalization of financial transactions. Graph-based machine
learning techniques have emerged as promising tools for Anti-Money Laundering
(AML) detection, capturing intricate relationships within money laundering
networks. However, the effectiveness of AML solutions is hindered by data silos
within financial institutions, limiting collaboration and overall efficacy.
This research presents a novel privacy-preserving approach for collaborative
AML machine learning, facilitating secure data sharing across institutions and
borders while preserving privacy and regulatory compliance. Leveraging Fully
Homomorphic Encryption (FHE), computations are directly performed on encrypted
data, ensuring the confidentiality of financial data. Notably, FHE over the
Torus (TFHE) was integrated with graph-based machine learning using Zama
Concrete ML. The research contributes two key privacy-preserving pipelines.
First, the development of a privacy-preserving Graph Neural Network (GNN)
pipeline was explored. Optimization techniques like quantization and pruning
were used to render the GNN FHE-compatible. Second, a privacy-preserving
graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was
successfully developed. Experiments demonstrated strong predictive performance,
with the XGBoost model consistently achieving over 99% accuracy, F1-score,
precision, and recall on the balanced AML dataset in both unencrypted and
FHE-encrypted inference settings. On the imbalanced dataset, the incorporation
of graph-based features improved the F1-score by 8%. The research highlights
the need to balance the trade-off between privacy and computational efficiency.",cs.CR
Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.",cs.CR
Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis,"In the rapidly evolving landscape of 5G technology, safeguarding Radio
Frequency (RF) environments against sophisticated intrusions is paramount,
especially in dynamic spectrum access and management. This paper presents an
enhanced experimental model that integrates a self-attention mechanism with a
Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous
spectral activities in 5G networks at the waveform level. Our approach,
grounded in time-series analysis, processes in-phase and quadrature (I/Q)
samples to identify irregularities that could indicate potential jamming
attacks. The model's architecture, augmented with a self-attention layer,
extends the capabilities of RNN autoencoders, enabling a more nuanced
understanding of temporal dependencies and contextual relationships within the
RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed
constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a
comprehensive stream of data that reflects real-world RF spectrum conditions
and attack scenarios. The model is trained to reconstruct standard signal
behavior, establishing a normative baseline against which deviations,
indicative of security threats, are identified. The proposed architecture is
designed to balance between detection precision and computational efficiency,
so the LSTM network, enriched with self-attention, continues to optimize for
minimal execution latency and power consumption. Conducted on a real-world
SDR-based testbed, our results demonstrate the model's improved performance and
accuracy in threat detection.
  Keywords: self-attention, real-time intrusion detection, RNN autoencoder,
Transformer architecture, LSTM, time series anomaly detection, 5G Security,
spectrum access security.",cs.CR
DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks,"Graph has become increasingly integral to the advancement of recommendation
systems, particularly with the fast development of graph neural network(GNN).
By exploring the virtue of rich node features and link information, GNN is
designed to provide personalized and accurate suggestions. Meanwhile, the
privacy leakage of GNN in such contexts has also captured special attention.
Prior work has revealed that a malicious user can utilize auxiliary knowledge
to extract sensitive link data of the target graph, integral to recommendation
systems, via the decision made by the target GNN model. This poses a
significant risk to the integrity and confidentiality of data used in
recommendation system. Though important, previous works on GNN's privacy
leakage are still challenged in three aspects, i.e., limited stealing attack
scenarios, sub-optimal attack performance, and adaptation against defense. To
address these issues, we propose a diffusion model based link stealing attack,
named DM4Steal. It differs previous work from three critical aspects. (i)
Generality: aiming at six attack scenarios with limited auxiliary knowledge, we
propose a novel training strategy for diffusion models so that DM4Steal is
transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from
the retention of semantic structure in the diffusion model during the training
process, DM4Steal is capable to learn the precise topology of the target graph
through the GNN decision process. (iii) Adaptation: when GNN is defensive
(e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling
the score model multiple times to keep performance degradation to a minimum,
thus DM4Steal implements successful adaptive attack on defensive GNN.",cs.CR
TDDBench: A Benchmark for Training data detection,"Training Data Detection (TDD) is a task aimed at determining whether a
specific data instance is used to train a machine learning model. In the
computer security literature, TDD is also referred to as Membership Inference
Attack (MIA). Given its potential to assess the risks of training data
breaches, ensure copyright authentication, and verify model unlearning, TDD has
garnered significant attention in recent years, leading to the development of
numerous methods. Despite these advancements, there is no comprehensive
benchmark to thoroughly evaluate the effectiveness of TDD methods. In this
work, we introduce TDDBench, which consists of 13 datasets spanning three data
modalities: image, tabular, and text. We benchmark 21 different TDD methods
across four detection paradigms and evaluate their performance from five
perspectives: average detection performance, best detection performance, memory
consumption, and computational efficiency in both time and memory. With
TDDBench, researchers can identify bottlenecks and areas for improvement in TDD
algorithms, while practitioners can make informed trade-offs between
effectiveness and efficiency when selecting TDD algorithms for specific use
cases. Our large-scale benchmarking also reveals the generally unsatisfactory
performance of TDD algorithms across different datasets. To enhance
accessibility and reproducibility, we open-source TDDBench for the research
community.",cs.CR
NinjaDoH: A Censorship-Resistant Moving Target DoH Server Using Hyperscalers and IPNS,"We introduce NinjaDoH, a novel DNS over HTTPS (DoH) protocol that leverages
the InterPlanetary Name System (IPNS), along with public cloud infrastructure,
to create a censorship-resistant moving target DoH service. NinjaDoH is
specifically designed to evade traditional censorship methods that involve
blocking DoH servers by IP addresses or domains by continually altering the
server's network identifiers, significantly increasing the complexity of
effectively censoring NinjaDoH traffic without disruption of other web traffic.
We also present an analysis that quantifies the DNS query latency and financial
costs of running our implementation of this protocol as a service. Further
tests assess the ability of NinjaDoH to elude detection mechanisms, including
both commercial firewall products and advanced machine learning-based detection
systems. The results broadly support NinjaDoH's efficacy as a robust, moving
target DNS solution that can ensure continuous and secure internet access in
environments with heavy DNS-based censorship.",cs.CR
TRANSPOSE: Transitional Approaches for Spatially-Aware LFI Resilient FSM Encoding,"Finite state machines (FSMs) regulate sequential circuits, including access
to sensitive information and privileged CPU states. Courtesy of contemporary
research on laser attacks, laser-based fault injection (LFI) is becoming even
more precise where an adversary can thwart chip security by altering individual
flip-flop (FF) values. Different laser models, e.g., bit flip, bit set, and bit
reset, have been developed to appreciate LFI on practical targets. As
traditional approaches may incorporate substantial overhead, state-based SPARSE
and transition-based TAMED countermeasures were proposed in our prior work to
improve FSM resiliency efficiently. TAMED overcame SPARSE's limitation of being
too conservative, and generating multiple LFI resilient encodings for
contemporary LFI models on demand. SPARSE, however, incorporated design layout
information into its vulnerability estimation which makes its vulnerability
estimation metric more accurate. In this paper, we extend TAMED by proposing a
transition-based encoding CAD framework (TRANSPOSE), that incorporates spatial
transitional vulnerability metrics to quantify design susceptibility of FSMs
based on both the bit flip model and the set-reset models. TRANSPOSE also
incorporates floorplan optimization into its framework to accommodate secure
spatial inter-distance of FF-sensitive regions. All TRANSPOSE approaches are
demonstrated on 5 multifarious benchmarks and outperform existing FSM encoding
schemes/frameworks in terms of security and overhead.",cs.CR
Brewing Vodka: Distilling Pure Knowledge for Lightweight Threat Detection in Audit Logs,"Advanced Persistent Threats (APTs) are continuously evolving, leveraging
their stealthiness and persistence to put increasing pressure on current
provenance-based Intrusion Detection Systems (IDS). This evolution exposes
several critical issues: (1) The dense interaction between malicious and benign
nodes within provenance graphs introduces neighbor noise, hindering effective
detection; (2) The complex prediction mechanisms of existing APTs detection
models lead to the insufficient utilization of prior knowledge embedded in the
data; (3) The high computational cost makes detection impractical.
  To address these challenges, we propose Vodka, a lightweight threat detection
system built on a knowledge distillation framework, capable of node-level
detection within audit log provenance graphs. Specifically, Vodka applies graph
Laplacian regularization to reduce neighbor noise, obtaining smoothed and
denoised graph signals. Subsequently, Vodka employs a teacher model based on
GNNs to extract knowledge, which is then distilled into a lightweight student
model. The student model is designed as a trainable combination of a feature
transformation module and a personalized PageRank random walk label propagation
module, with the former capturing feature knowledge and the latter learning
label and structural knowledge. After distillation, the student model benefits
from the knowledge of the teacher model to perform precise threat detection.
Finally, Vodka reconstructs attack paths from anomalous nodes, providing
insight into the attackers' strategies. We evaluate Vodka through extensive
experiments on three public datasets and compare its performance against
several state-of-the-art IDS solutions. The results demonstrate that Vodka
achieves outstanding detection accuracy across all scenarios and the detection
time is 1.4 to 5.2 times faster than the current state-of-the-art methods.",cs.CR
FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks,"Federated Learning (FL) is a machine learning method for training with
private data locally stored in distributed machines without gathering them into
one place for central learning. Despite its promises, FL is prone to critical
security risks. First, because FL depends on a central server to aggregate
local training models, this is a single point of failure. The server might
function maliciously. Second, due to its distributed nature, FL might encounter
backdoor attacks by participating clients. They can poison the local model
before submitting to the server. Either type of attack, on the server or the
client side, would severely degrade learning accuracy. We propose FedBlock, a
novel blockchain-based FL framework that addresses both of these security
risks. FedBlock is uniquely desirable in that it involves only smart contract
programming, thus deployable atop any blockchain network. Our framework is
substantiated with a comprehensive evaluation study using real-world datasets.
Its robustness against backdoor attacks is competitive with the literature of
FL backdoor defense. The latter, however, does not address the server risk as
we do.",cs.CR
Relating Quantum Tamper-Evident Encryption to Other Cryptographic Notions,"A quantum tamper-evident encryption scheme is a non-interactive symmetric-key
encryption scheme mapping classical messages to quantum ciphertexts such that
an honest recipient of a ciphertext can detect with high probability any
meaningful eavesdropping. This quantum cryptographic primitive was first
introduced by Gottesman in 2003. Beyond formally defining this security notion,
Gottesman's work had three main contributions: showing that any quantum
authentication scheme is also a tamper-evident scheme, noting that a quantum
key distribution scheme can be constructed from any tamper-evident scheme, and
constructing a prepare-and-measure tamper-evident scheme using only Wiesner
states inspired by Shor and Preskill's proof of security for the BB84 quantum
key distribution scheme.
  In this work, we further our understanding of tamper-evident encryption by
formally relating it to other cryptographic primitives in an
information-theoretic setting. In particular, we show that tamper evidence
implies encryption, answering a question left open by Gottesman, we show that
it can be constructed from any encryption scheme with revocation and
vice-versa, and we formalize an existing sketch of a construction of quantum
money from any tamper-evident encryption scheme. These results also yield as a
corollary that any scheme allowing the revocation of a message must be an
encryption scheme. Finally, we show separations between tamper evidence and
other primitives, notably that tamper evidence does not imply authentication
and does not imply uncloneable encryption.",cs.CR
Visually Analyze SHAP Plots to Diagnose Misclassifications in ML-based Intrusion Detection,"Intrusion detection has been a commonly adopted detective security measures
to safeguard systems and networks from various threats. A robust intrusion
detection system (IDS) can essentially mitigate threats by providing alerts. In
networks based IDS, typically we deal with cyber threats like distributed
denial of service (DDoS), spoofing, reconnaissance, brute-force, botnets, and
so on. In order to detect these threats various machine learning (ML) and deep
learning (DL) models have been proposed. However, one of the key challenges
with these predictive approaches is the presence of false positive (FP) and
false negative (FN) instances. This FPs and FNs within any black-box intrusion
detection system (IDS) make the decision-making task of an analyst further
complicated. In this paper, we propose an explainable artificial intelligence
(XAI) based visual analysis approach using overlapping SHAP plots that presents
the feature explanation to identify potential false positive and false
negatives in IDS. Our approach can further provide guidance to security
analysts for effective decision-making. We present case study with multiple
publicly available network traffic datasets to showcase the efficacy of our
approach for identifying false positive and false negative instances. Our
use-case scenarios provide clear guidance for analysts on how to use the visual
analysis approach for reliable course-of-actions against such threats.",cs.CR
Fine Grained Insider Risk Detection,"We present a method to detect departures from business-justified workflows
among support agents. Our goal is to assist auditors in identifying agent
actions that cannot be explained by the activity within their surrounding
context, where normal activity patterns are established from historical data.
We apply our method to help audit millions of actions of over three thousand
support agents.
  We collect logs from the tools used by support agents and construct a
bipartite graph of Actions and Entities representing all the actions of the
agents, as well as background information about entities. From this graph, we
sample subgraphs rooted on security-significant actions taken by the agents.
Each subgraph captures the relevant context of the root action in terms of
other actions, entities and their relationships. We then prioritize the
rooted-subgraphs for auditor review using feed-forward and graph neural
networks, as well as nearest neighbors techniques. To alleviate the issue of
scarce labeling data, we use contrastive learning and domain-specific data
augmentations.
  Expert auditors label the top ranked subgraphs as ``worth auditing"" or ``not
worth auditing"" based on the company's business policies. This system finds
subgraphs that are worth auditing with high enough precision to be used in
production.",cs.CR
Efficacy of EPSS in High Severity CVEs found in KEV,"The Exploit Prediction Scoring System (EPSS) is designed to assess the
probability of a vulnerability being exploited in the next 30 days relative to
other vulnerabilities. The latest version, based on a research paper published
in arXiv, assists defenders in deciding which vulnerabilities to prioritize for
remediation. This study evaluates EPSS's ability to predict exploitation before
vulnerabilities are actively compromised, focusing on high severity CVEs that
are known to have been exploited and included in the CISA KEV catalog. By
analyzing EPSS score history, the availability and simplicity of exploits, the
system's purpose, its value as a target for Threat Actors (TAs), this paper
examines EPSS's potential and identifies areas for improvement.",cs.CR
Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach,"Blockchains are being positioned as the ""technology of trust"" that can be
used to mediate transactions between non-trusting parties without the need for
a central authority. They support transaction types that are native to the
blockchain platform or user-defined via user programs called smart contracts.
Despite the significant flexibility in transaction programmability that smart
contracts offer, they pose several usability, robustness, and performance
challenges.
  This paper proposes an alternative transaction framework that incorporates
more primitives into the native set of transaction types (reducing the
likelihood of requiring user-defined transaction programs often). The framework
is based on the concept of declarative blockchain transactions whose strength
lies in the fact that it addresses several of the limitations of smart
contracts simultaneously. A formal and implementation framework is presented,
and a subset of commonly occurring transaction behaviors are modeled and
implemented as use cases, using an open-source blockchain database,
BigchchainDB, as the implementation context. A performance study comparing the
declarative transaction approach to equivalent smart contract transaction
models reveals several advantages of the proposed approach.",cs.CR
PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption,"Confidential computing on GPUs, like NVIDIA H100, mitigates the security
risks of outsourced Large Language Models (LLMs) by implementing strong
isolation and data encryption. Nonetheless, this encryption incurs a
significant performance overhead, reaching up to 52.8 percent and 88.2 percent
throughput drop when serving OPT-30B and OPT-66B, respectively. To address this
challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM
removes the overhead by overlapping the encryption and GPU computation through
pipelining - an idea inspired by the CPU instruction pipelining - thereby
effectively concealing the latency increase caused by encryption. The primary
technical challenge is that, unlike CPUs, the encryption module lacks prior
knowledge of the specific data needing encryption until it is requested by the
GPUs. To this end, we propose speculative pipelined encryption to predict the
data requiring encryption by analyzing the serving patterns of LLMs. Further,
we have developed an efficient, low-cost pipeline relinquishing approach for
instances of incorrect predictions. Our experiments on NVIDIA H100 GPU show
that compared with vanilla systems without confidential computing (e.g., vLLM,
PEFT, and FlexGen), PipeLLM incurs modest overhead (less than 19.6 percent in
throughput) across various LLM sizes, from 13B to 175B.",cs.CR
Exploring Feature Importance and Explainability Towards Enhanced ML-Based DoS Detection in AI Systems,"Denial of Service (DoS) attacks pose a significant threat in the realm of AI
systems security, causing substantial financial losses and downtime. However,
AI systems' high computational demands, dynamic behavior, and data variability
make monitoring and detecting DoS attacks challenging. Nowadays, statistical
and machine learning (ML)-based DoS classification and detection approaches
utilize a broad range of feature selection mechanisms to select a feature
subset from networking traffic datasets. Feature selection is critical in
enhancing the overall model performance and attack detection accuracy while
reducing the training time. In this paper, we investigate the importance of
feature selection in improving ML-based detection of DoS attacks. Specifically,
we explore feature contribution to the overall components in DoS traffic
datasets by utilizing statistical analysis and feature engineering approaches.
Our experimental findings demonstrate the usefulness of the thorough
statistical analysis of DoS traffic and feature engineering in understanding
the behavior of the attack and identifying the best feature selection for
ML-based DoS classification and detection.",cs.CR
Quantum-Computable One-Way Functions without One-Way Functions,"We construct a classical oracle relative to which $\mathsf{P} = \mathsf{NP}$
but quantum-computable quantum-secure trapdoor one-way functions exist. This is
a substantial strengthening of the result of Kretschmer, Qian, Sinha, and Tal
(STOC 2023), which only achieved single-copy pseudorandom quantum states
relative to an oracle that collapses $\mathsf{NP}$ to $\mathsf{P}$. For
example, our result implies multi-copy pseudorandom states and pseudorandom
unitaries, but also classical-communication public-key encryption, signatures,
and oblivious transfer schemes relative to an oracle on which
$\mathsf{P}=\mathsf{NP}$. Hence, in our new relativized world, classical
computers live in ""Algorithmica"" whereas quantum computers live in
""Cryptomania,"" using the language of Impagliazzo's worlds.
  Our proof relies on a new distributional block-insensitivity lemma for
$\mathsf{AC^0}$ circuits, wherein a single block is resampled from an arbitrary
distribution.",cs.CR
Analysing the cultural dimensions of cybercriminal groups -- A case study on the Conti ransomware group,"Cybercriminal profiling and cyber-attack attribution have been elusive goals
world-wide, due to their effects on societal and geopolitical balance and
stability. Attributing actions to a group or state is a complex endeavour, with
traditional established approaches including cyber threat intelligence and
analysis of technical means such as malware analysis, network forensics and
geopolitical intelligence. However, we propose an additional component for
profiling threat actor groups through analysing cultural aspects of human
behaviours and interactions. We utilise a set of variables which determine
characteristics of national and organisational culture to create a cultural
""footprint"" of cybercriminal groups. As a case study, we conduct thematic
analysis across the six dimensions of the Hofstede national culture
classification and the eight dimensions of the Meyer classification on leaked
internal communications of the ransomware group Conti. We propose that a
systematic analysis of similar communications can serve as a practical tool for
a) understanding the modus operandi of cybercrime and cyberwarfare-related
groups, and b) profiling cybercriminal groups and/or nation-state actors.
Insights from such applications can, first, assist in combating cybercrime and,
second, if combined with additional cyber threat intelligence, can provide a
level of confidence in nuanced cyber-attack attribution processes.",cs.CR
LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks,"In this paper, we present an adaptive framework designed for the continuous
detection, identification and classification of emerging attacks in network
traffic. The framework employs a transformer encoder architecture, which
captures hidden patterns in a bidirectional manner to differentiate between
malicious and legitimate traffic. Initially, the framework focuses on the
accurate detection of malicious activities, achieving a perfect recall of 100\%
in distinguishing between attack and benign traffic. Subsequently, the system
incrementally identifies unknown attack types by leveraging a Gaussian Mixture
Model (GMM) to cluster features derived from high-dimensional BERT embeddings.
This approach allows the framework to dynamically adjust its identification
capabilities as new attack clusters are discovered, maintaining high detection
accuracy. Even after integrating additional unknown attack clusters, the
framework continues to perform at a high level, achieving 95.6\% in both
classification accuracy and recall.The results demonstrate the effectiveness of
the proposed framework in adapting to evolving threats while maintaining high
accuracy in both detection and identification tasks. Our ultimate goal is to
develop a scalable, real-time intrusion detection system that can continuously
evolve with the ever-changing network threat landscape.",cs.CR
Digital Twin-Assisted Federated Learning with Blockchain in Multi-tier Computing Systems,"In Industry 4.0 systems, a considerable number of resource-constrained
Industrial Internet of Things (IIoT) devices engage in frequent data
interactions due to the necessity for model training, which gives rise to
concerns pertaining to security and privacy. In order to address these
challenges, this paper considers a digital twin (DT) and blockchain-assisted
federated learning (FL) scheme. To facilitate the FL process, we initially
employ fog devices with abundant computational capabilities to generate DT for
resource-constrained edge devices, thereby aiding them in local training.
Subsequently, we formulate an FL delay minimization problem for FL, which
considers both of model transmission time and synchronization time, also
incorporates cooperative jamming to ensure secure synchronization of DT. To
address this non-convex optimization problem, we propose a decomposition
algorithm. In particular, we introduce upper limits on the local device
training delay and the effects of aggregation jamming as auxiliary variables,
thereby transforming the problem into a convex optimization problem that can be
decomposed for independent solution. Finally, a blockchain verification
mechanism is employed to guarantee the integrity of the model uploading
throughout the FL process and the identities of the participants. The final
global model is obtained from the verified local and global models within the
blockchain through the application of deep learning techniques. The efficacy of
our proposed cooperative interference-based FL process has been verified
through numerical analysis, which demonstrates that the integrated DT
blockchain-assisted FL scheme significantly outperforms the benchmark schemes
in terms of execution time, block optimization, and accuracy.",cs.CR
FedPID: An Aggregation Method for Federated Learning,"This paper presents FedPID, our submission to the Federated Tumor
Segmentation Challenge 2024 (FETS24). Inspired by FedCostWAvg and FedPIDAvg,
our winning contributions to FETS21 and FETS2022, we propose an improved
aggregation strategy for federated and collaborative learning. FedCostWAvg is a
method that averages results by considering both the number of training samples
in each group and how much the cost function decreased in the last round of
training. This is similar to how the derivative part of a PID controller works.
In FedPIDAvg, we also included the integral part that was missing. Another
challenge we faced were vastly differing dataset sizes at each center. We
solved this by assuming the sizes follow a Poisson distribution and adjusting
the training iterations for each center accordingly. Essentially, this part of
the method controls that outliers that require too much training time are less
frequently used. Based on these contributions we now adapted FedPIDAvg by
changing how the integral part is computed. Instead of integrating the loss
function we measure the global drop in cost since the first round.",cs.CR
CryptoEL: A Novel Experiential Learning Tool for Enhancing K-12 Cryptography Education,"This paper presents an educational tool designed to enhance cryptography
education for K-12 students, utilizing Kolb's Experiential Learning (EL) model
and engaging visual components. Our tool incorporates the four stages of EL --
Concrete Experience, Reflective Observation, Abstract Conceptualization, and
Active Experimentation -- to teach key cryptographic concepts, including
hashing, symmetric cryptography, and asymmetric cryptography. The learning
experience is enriched with real-world simulations, customized AI-based
conversation agents, video demonstrations, interactive scenarios, and a
simplified Python coding terminal focused on cryptography. Targeted at
beginners in cybersecurity, the tool encourages independent learning with
minimal instructor involvement. An evaluation with 51 middle and high school
students showed positive feedback from 93% of participants, who found the
simulations, visualizations, AI reflections, scenarios, and coding capabilities
engaging and conducive to learning. Comprehension surveys indicated a high
understanding of cryptography concepts: hashing (middle school: 89%, high
school: 92%), symmetric cryptography (middle school: 93%, high school: 97%),
and asymmetric cryptography (middle school: 91%, high school: 94%).",cs.CR
Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition,"Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.",cs.CR
BlindexTEE: A Blind Index Approach towards TEE-supported End-to-end Encrypted DBMS,"Using cloud-based applications comes with privacy implications, as the
end-user looses control over their data. While encrypting all data on the
client is possible, it largely reduces the usefulness of database management
systems (DBMS) that are typically built to efficiently query large quantities
of data. We present BlindexTEE, a new component that sits between the
application business-logic and the database. BlindexTEE is shielded from
malicious users or compromised environments by executing inside an SEV-SNP
confidential VM, AMD's trusted execution environment (TEE). BlindexTEE is in
charge of end-to-end encryption of user data while preserving the ability of
the DBMS to efficiently filter data. By decrypting and re-encrypting data, it
builds blind indices, used later on to efficiently query the DBMS. We
demonstrate the practicality of BlindexTEE with MySQL in several micro- and
macro-benchmarks, achieving overheads between 36.1% and 462% over direct
database access depending on the usage scenario.",cs.CR
Fuzzing Processing Pipelines for Zero-Knowledge Circuits,"Zero-knowledge (ZK) protocols have recently found numerous practical
applications, such as in authentication, online-voting, and blockchain systems.
These protocols are powered by highly complex pipelines that process
deterministic programs, called circuits, written in one of many domain-specific
programming languages, e.g., Circom, Noir, and others. Logic bugs in
circuit-processing pipelines could have catastrophic consequences and cause
significant financial and reputational damage. As an example, consider that a
logic bug in a ZK pipeline could result in attackers stealing identities or
assets. It is, therefore, critical to develop effective techniques for checking
their correctness.
  In this paper, we present the first systematic fuzzing technique for ZK
pipelines, which uses metamorphic test oracles to detect critical logic bugs.
We have implemented our technique in an open-source tool called Circuzz. We
used Circuzz to test four significantly different ZK pipelines and found a
total of 16 logic bugs in all pipelines. Due to their critical nature, 15 of
our bugs have already been fixed by the pipeline developers.",cs.CR
R+R:Understanding Hyperparameter Effects in DP-SGD,"Research on the effects of essential hyperparameters of DP-SGD lacks
consensus, verification, and replication. Contradictory and anecdotal
statements on their influence make matters worse. While DP-SGD is the standard
optimization algorithm for privacy-preserving machine learning, its adoption is
still commonly challenged by low performance compared to non-private learning
approaches. As proper hyperparameter settings can improve the privacy-utility
trade-off, understanding the influence of the hyperparameters promises to
simplify their optimization towards better performance, and likely foster
acceptance of private learning. To shed more light on these influences, we
conduct a replication study: We synthesize extant research on hyperparameter
influences of DP-SGD into conjectures, conduct a dedicated factorial study to
independently identify hyperparameter effects, and assess which conjectures can
be replicated across multiple datasets, model architectures, and differential
privacy budgets. While we cannot (consistently) replicate conjectures about the
main and interaction effects of the batch size and the number of epochs, we
were able to replicate the conjectured relationship between the clipping
threshold and learning rate. Furthermore, we were able to quantify the
significant importance of their combination compared to the other
hyperparameters.",cs.CR
Adaptive Optimization of TLS Overhead for Wireless Communication in Critical Infrastructure,"With critical infrastructure increasingly relying on wireless communication,
using end-to-end security such as TLS becomes imperative. However, TLS
introduces significant overhead for resource-constrained devices and networks
prevalent in critical infrastructure. In this paper, we propose to leverage the
degrees of freedom in configuring TLS to dynamically adapt algorithms,
parameters, and other settings to best meet the currently occurring resource
and security constraints in a wireless communication scenario. Consequently, we
can make the best use of scarce resources to provide tightened security for
wireless networks in critical infrastructure.",cs.CR
Differentially private and decentralized randomized power method,"The randomized power method has gained significant interest due to its
simplicity and efficient handling of large-scale spectral analysis and
recommendation tasks. As modern datasets contain sensitive private information,
we need to give formal guarantees on the possible privacy leaks caused by this
method. This paper focuses on enhancing privacy preserving variants of the
method. We propose a strategy to reduce the variance of the noise introduced to
achieve Differential Privacy (DP). We also adapt the method to a decentralized
framework with a low computational and communication overhead, while preserving
the accuracy. We leverage Secure Aggregation (a form of Multi-Party
Computation) to allow the algorithm to perform computations using data
distributed among multiple users or devices, without revealing individual data.
We show that it is possible to use a noise scale in the decentralized setting
that is similar to the one in the centralized setting. We improve upon existing
convergence bounds for both the centralized and decentralized versions. The
proposed method is especially relevant for decentralized applications such as
distributed recommender systems, where privacy concerns are paramount.",cs.CR
"Quantum One-Time Programs, Revisited","One-time programs (Goldwasser, Kalai and Rothblum, CRYPTO 2008) are functions
that can be run on any single input of a user's choice, but not on a second
input. Classically, they are unachievable without trusted hardware, but the
destructive nature of quantum measurements seems to provide a quantum path to
constructing them. Unfortunately, Broadbent, Gutoski and Stebila showed that
even with quantum techniques, a strong notion of one-time programs, similar to
ideal obfuscation, cannot be achieved for any non-trivial quantum function. On
the positive side, Ben-David and Sattath (Quantum, 2023) showed how to
construct a one-time program for a certain (probabilistic) digital signature
scheme, under a weaker notion of one-time program security. There is a vast gap
between achievable and provably impossible notions of one-time program
security, and it is unclear what functionalities are one-time programmable
under the achievable notions of security.
  In this work, we present new, meaningful, yet achievable definitions of
one-time program security for probabilistic classical functions. We show how to
construct one time programs satisfying these definitions for all functions in
the classical oracle model and for constrained pseudorandom functions in the
plain model. Finally, we examine the limits of these notions: we show a class
of functions which cannot be one-time programmed in the plain model, as well as
a class of functions which appears to be highly random given a single query,
but whose one-time program form leaks the entire function even in the oracle
model.",cs.CR
Tabular Data Synthesis with Differential Privacy: A Survey,"Data sharing is a prerequisite for collaborative innovation, enabling
organizations to leverage diverse datasets for deeper insights. In real-world
applications like FinTech and Smart Manufacturing, transactional data, often in
tabular form, are generated and analyzed for insight generation. However, such
datasets typically contain sensitive personal/business information, raising
privacy concerns and regulatory risks. Data synthesis tackles this by
generating artificial datasets that preserve the statistical characteristics of
real data, removing direct links to individuals. However, attackers can still
infer sensitive information using background knowledge. Differential privacy
offers a solution by providing provable and quantifiable privacy protection.
Consequently, differentially private data synthesis has emerged as a promising
approach to privacy-aware data sharing. This paper provides a comprehensive
overview of existing differentially private tabular data synthesis methods,
highlighting the unique challenges of each generation model for generating
tabular data under differential privacy constraints. We classify the methods
into statistical and deep learning-based approaches based on their generation
models, discussing them in both centralized and distributed environments. We
evaluate and compare those methods within each category, highlighting their
strengths and weaknesses in terms of utility, privacy, and computational
complexity. Additionally, we present and discuss various evaluation methods for
assessing the quality of the synthesized data, identify research gaps in the
field and directions for future research.",cs.CR
TabSec: A Collaborative Framework for Novel Insider Threat Detection,"In the era of the Internet of Things (IoT) and data sharing, users frequently
upload their personal information to enterprise databases to enjoy enhanced
service experiences provided by various online services. However, the
widespread presence of system vulnerabilities, remote network intrusions, and
insider threats significantly increases the exposure of private enterprise data
on the internet. If such data is stolen or leaked by attackers, it can result
in severe asset losses and business operation disruptions. To address these
challenges, this paper proposes a novel threat detection framework, TabITD.
This framework integrates Intrusion Detection Systems (IDS) with User and
Entity Behavior Analytics (UEBA) strategies to form a collaborative detection
system that bridges the gaps in existing systems' capabilities. It effectively
addresses the blurred boundaries between external and insider threats caused by
the diversification of attack methods, thereby enhancing the model's learning
ability and overall detection performance. Moreover, the proposed method
leverages the TabNet architecture, which employs a sparse attention feature
selection mechanism that allows TabNet to select the most relevant features at
each decision step, thereby improving the detection of rare-class attacks. We
evaluated our proposed solution on two different datasets, achieving average
accuracies of 96.71% and 97.25%, respectively. The results demonstrate that
this approach can effectively detect malicious behaviors such as masquerade
attacks and external threats, significantly enhancing network security defenses
and the efficiency of network attack detection.",cs.CR
Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.",cs.CR
Token Composition: A Graph Based on EVM Logs,"Tokens have proliferated across blockchains in terms of number, market
capitalisation and utility. Some tokens are tokenised versions of existing
tokens -- known variously as wrapped tokens, fractional tokens, or shares. The
repeated application of this process creates matryoshkian tokens of arbitrary
depth. We perform an empirical analysis of token composition on the Ethereum
blockchain. We introduce a graph that represents the tokenisation of tokens by
other tokens, and we show that the graph contains non-trivial topological
structure. We relate properties of the graph, e.g., connected components and
cyclic structure, to the tokenisation process. For example, we identify the
longest directed path and its corresponding sequence of tokens, and we
visualise the connected components relating to a stablecoin and an NFT
protocol. Our goal is to explore and visualise what has been wrought with
tokens, rather than add yet another brick to the edifice.",cs.CR
Consensus Under Adversary Majority Done Right,"A spectre is haunting consensus protocols-the spectre of adversary majority.
The literature is inconclusive, with possibilities and impossibilities running
abound. Dolev and Strong in 1983 showed an early possibility for up to 99%
adversaries. Yet, we have known impossibility results for adversaries above 1/2
in synchrony, and above 1/3 in partial synchrony. What gives? It is high time
that we pinpoint the culprit of this confusion: the critical role of the
modeling details of clients. Are the clients sleepy or always-on? Are they
silent or communicating? Can validators be sleepy too? We systematize models
for consensus across four dimensions (sleepy/always-on clients,
silent/communicating clients, sleepy/always-on validators, and
synchrony/partial-synchrony), some of which are new, and tightly characterize
the achievable safety and liveness resiliences with matching possibilities and
impossibilities for each of the sixteen models. To this end, we unify folklore
and earlier results, and fill gaps left in the literature with new protocols
and impossibility theorems.",cs.CR
Undermining Image and Text Classification Algorithms Using Adversarial Attacks,"Machine learning models are prone to adversarial attacks, where inputs can be
manipulated in order to cause misclassifications. While previous research has
focused on techniques like Generative Adversarial Networks (GANs), there's
limited exploration of GANs and Synthetic Minority Oversampling Technique
(SMOTE) in text and image classification models to perform adversarial attacks.
Our study addresses this gap by training various machine learning models and
using GANs and SMOTE to generate additional data points aimed at attacking text
classification models. Furthermore, we extend our investigation to face
recognition models, training a Convolutional Neural Network(CNN) and subjecting
it to adversarial attacks with fast gradient sign perturbations on key features
identified by GradCAM, a technique used to highlight key image characteristics
CNNs use in classification. Our experiments reveal a significant vulnerability
in classification models. Specifically, we observe a 20 % decrease in accuracy
for the top-performing text classification models post-attack, along with a 30
% decrease in facial recognition accuracy. This highlights the susceptibility
of these models to manipulation of input data. Adversarial attacks not only
compromise the security but also undermine the reliability of machine learning
systems. By showcasing the impact of adversarial attacks on both text
classification and face recognition models, our study underscores the urgent
need for develop robust defenses against such vulnerabilities.",cs.CR
Fixing Security Vulnerabilities with AI in OSS-Fuzz,"Critical open source software systems undergo significant validation in the
form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased
random search over the domain of program inputs, to find inputs which crash the
software system. Such fuzzing is useful to enhance the security of software
systems in general since even closed source software may use open source
components. Hence testing open source software is of paramount importance.
Currently OSS-Fuzz is the most significant and widely used infrastructure for
continuous validation of open source systems. Unfortunately even though
OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more
software projects, the detected vulnerabilities may remain unpatched, as
vulnerability fixing is often manual in practice. In this work, we rely on the
recent progress in Large Language Model (LLM) agents for autonomous program
improvement including bug fixing. We customise the well-known AutoCodeRover
agent for fixing security vulnerabilities. This is because LLM agents like
AutoCodeRover fix bugs from issue descriptions via code search. Instead for
security patching, we rely on the test execution of the exploit input to
extract code elements relevant to the fix. Our experience with OSS-Fuzz
vulnerability data shows that LLM agent autonomy is useful for successful
security patching, as opposed to approaches like Agentless where the control
flow is fixed. More importantly our findings show that we cannot measure
quality of patches by code similarity of the patch with reference codes (as in
CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores
still fail to pass given the given exploit input. Our findings indicate that
security patch correctness needs to consider dynamic attributes like test
executions as opposed to relying of standard text/code similarity metrics.",cs.CR
Large Language Model Supply Chain: Open Problems From the Security Perspective,"Large Language Model (LLM) is changing the software development paradigm and
has gained huge attention from both academia and industry. Researchers and
developers collaboratively explore how to leverage the powerful problem-solving
ability of LLMs for specific domain tasks. Due to the wide usage of LLM-based
applications, e.g., ChatGPT, multiple works have been proposed to ensure the
security of LLM systems. However, a comprehensive understanding of the entire
processes of LLM system construction (the LLM supply chain) is crucial but
relevant works are limited. More importantly, the security issues hidden in the
LLM SC which could highly impact the reliable usage of LLMs are lack of
exploration. Existing works mainly focus on assuring the quality of LLM from
the model level, security assurance for the entire LLM SC is ignored. In this
work, we take the first step to discuss the potential security risks in each
component as well as the integration between components of LLM SC. We summarize
12 security-related risks and provide promising guidance to help build safer
LLM systems. We hope our work can facilitate the evolution of artificial
general intelligence with secure LLM ecosystems.",cs.CR
"Trustworthy Federated Learning: Privacy, Security, and Beyond","While recent years have witnessed the advancement in big data and Artificial
Intelligence (AI), it is of much importance to safeguard data privacy and
security. As an innovative approach, Federated Learning (FL) addresses these
concerns by facilitating collaborative model training across distributed data
sources without transferring raw data. However, the challenges of robust
security and privacy across decentralized networks catch significant attention
in dealing with the distributed data in FL. In this paper, we conduct an
extensive survey of the security and privacy issues prevalent in FL,
underscoring the vulnerability of communication links and the potential for
cyber threats. We delve into various defensive strategies to mitigate these
risks, explore the applications of FL across different sectors, and propose
research directions. We identify the intricate security challenges that arise
within the FL frameworks, aiming to contribute to the development of secure and
efficient FL systems.",cs.CR
Federated Learning Clients Clustering with Adaptation to Data Drifts,"Federated Learning (FL) enables deep learning model training across edge
devices and protects user privacy by retaining raw data locally. Data
heterogeneity in client distributions slows model convergence and leads to
plateauing with reduced precision. Clustered FL solutions address this by
grouping clients with statistically similar data and training models for each
cluster. However, maintaining consistent client similarity within each group
becomes challenging when data drifts occur, significantly impacting model
accuracy. In this paper, we introduce Fielding, a clustered FL framework that
handles data drifts promptly with low overheads. Fielding detects drifts on all
clients and performs selective label distribution-based re-clustering to
balance cluster optimality and model performance, remaining robust to malicious
clients and varied heterogeneity degrees. Our evaluations show that Fielding
improves model final accuracy by 1.9%-5.9% and reaches target accuracies
1.16x-2.61x faster.",cs.CR
SQL Injection Jailbreak: a structural disaster of large language models,"In recent years, the rapid development of large language models (LLMs) has
brought new vitality to the various domains and generated substantial social
and economic benefits. However, the swift advancement of LLMs has introduced
new security vulnerabilities. Jailbreak, a form of attack that induces LLMs to
output harmful content through carefully crafted prompts, poses a challenge to
the safe and trustworthy development of LLMs. Previous jailbreak attack methods
primarily exploited the internal capabilities of the model. Among them, one
category leverages the model's implicit capabilities for jailbreak attacks,
where the attacker is unaware of the exact reasons for the attack's success.
The other category utilizes the model's explicit capabilities for jailbreak
attacks, where the attacker understands the reasons for the attack's success.
For example, these attacks exploit the model's abilities in coding, contextual
learning, or understanding ASCII characters. However, these earlier jailbreak
attacks have certain limitations, as they only exploit the inherent
capabilities of the model. In this paper, we propose a novel jailbreak method,
SQL Injection Jailbreak (SIJ), which utilizes the construction of input prompts
by LLMs to inject jailbreak information into user prompts, enabling successful
jailbreak of the LLMs. Our SIJ method achieves nearly 100\% attack success
rates on five well-known open-source LLMs in the context of AdvBench, while
incurring lower time costs compared to previous methods. More importantly, SIJ
reveals a new vulnerability in LLMs that urgently needs to be addressed. To
this end, we propose a defense method called Self-Reminder-Key and demonstrate
its effectiveness through experiments. Our code is available at
\href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}.",cs.CR
Anomalous Client Detection in Federated Learning,"Federated learning (FL), with the growing IoT and edge computing, is seen as
a promising solution for applications that are latency- and privacy-aware.
However, due to the widespread dispersion of data across many clients, it is
challenging to monitor client anomalies caused by malfunctioning devices or
unexpected events. The majority of FL solutions now in use concentrate on the
classification problem, ignoring situations in which anomaly detection may also
necessitate privacy preservation and effectiveness. The system in federated
learning is unable to manage the potentially flawed behavior of its clients
completely. These behaviors include sharing arbitrary parameter values and
causing a delay in convergence since clients are chosen at random without
knowing the malfunctioning behavior of the client. Client selection is crucial
in terms of the efficiency of the federated learning framework. The challenges
such as client drift and handling slow clients with low computational
capability are well-studied in FL. However, the detection of anomalous clients
either for security or for overall performance in the FL frameworks is hardly
studied in the literature. In this paper, we propose an anomaly client
detection algorithm to overcome malicious client attacks and client drift in FL
frameworks. Instead of random client selection, our proposed method utilizes
anomaly client detection to remove clients from the FL framework, thereby
enhancing the security and efficiency of the overall system. This proposed
method improves the global model convergence in almost 50\% fewer communication
rounds compared with widely used random client selection using the MNIST
dataset.",cs.CR
A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services,"Large language models (LLMs) have demonstrated exceptional capabilities in
text understanding and generation, and they are increasingly being utilized
across various domains to enhance productivity. However, due to the high costs
of training and maintaining these models, coupled with the fact that some LLMs
are proprietary, individuals often rely on online AI as a Service (AIaaS)
provided by LLM companies. This business model poses significant privacy risks,
as service providers may exploit users' trace patterns and behavioral data. In
this paper, we propose a practical and privacy-preserving framework that
ensures user anonymity by preventing service providers from linking requests to
the individuals who submit them. Our framework is built on partially blind
signatures, which guarantee the unlinkability of user requests. Furthermore, we
introduce two strategies tailored to both subscription-based and API-based
service models, ensuring the protection of both users' privacy and service
providers' interests. The framework is designed to integrate seamlessly with
existing LLM systems, as it does not require modifications to the underlying
architectures. Experimental results demonstrate that our framework incurs
minimal computation and communication overhead, making it a feasible solution
for real-world applications.",cs.CR
Privacy-Preserving Customer Churn Prediction Model in the Context of Telecommunication Industry,"Data is the main fuel of a successful machine learning model. A dataset may
contain sensitive individual records e.g. personal health records, financial
data, industrial information, etc. Training a model using this sensitive data
has become a new privacy concern when someone uses third-party cloud computing.
Trained models also suffer privacy attacks which leads to the leaking of
sensitive information of the training data. This study is conducted to preserve
the privacy of training data in the context of customer churn prediction
modeling for the telecommunications industry (TCI). In this work, we propose a
framework for privacy-preserving customer churn prediction (PPCCP) model in the
cloud environment. We have proposed a novel approach which is a combination of
Generative Adversarial Networks (GANs) and adaptive Weight-of-Evidence (aWOE).
Synthetic data is generated from GANs, and aWOE is applied on the synthetic
training dataset before feeding the data to the classification algorithms. Our
experiments were carried out using eight different machine learning (ML)
classifiers on three openly accessible datasets from the telecommunication
sector. We then evaluated the performance using six commonly employed
evaluation metrics. In addition to presenting a data privacy analysis, we also
performed a statistical significance test. The training and prediction
processes achieve data privacy and the prediction classifiers achieve high
prediction performance (87.1\% in terms of F-Measure for GANs-aWOE based
Na\""{\i}ve Bayes model). In contrast to earlier studies, our suggested approach
demonstrates a prediction enhancement of up to 28.9\% and 27.9\% in terms of
accuracy and F-measure, respectively.",cs.CR
Comparing Security and Efficiency of WebAssembly and Linux Containers in Kubernetes Cloud Computing,"This study investigates the potential of WebAssembly as a more secure and
efficient alternative to Linux containers for executing untrusted code in cloud
computing with Kubernetes. Specifically, it evaluates the security and
performance implications of this shift. Security analyses demonstrate that both
Linux containers and WebAssembly have attack surfaces when executing untrusted
code, but WebAssembly presents a reduced attack surface due to an additional
layer of isolation. The performance analysis further reveals that while
WebAssembly introduces overhead, particularly in startup times, it could be
negligible in long-running computations. However, WebAssembly enhances the core
principle of containerization, offering better security through isolation and
platform-agnostic portability compared to Linux containers. This research
demonstrates that WebAssembly is not a silver bullet for all security concerns
or performance requirements in a Kubernetes environment, but typical attacks
are less likely to succeed and the performance loss is relatively small.",cs.CR
Signer-Optimal Multiple-Time Post-Quantum Hash-Based Signature for Heterogeneous IoT Systems,"Heterogeneous Internet of Things (IoTs) harboring resource-limited devices
like wearable sensors are essential for next-generation networks. Ensuring the
authentication and integrity of security-sensitive telemetry in these
applications is vital. Digital signatures provide scalable authentication with
non-repudiation and public verifiability, making them essential tools for IoTs.
However, emerging quantum computers necessitate post-quantum (PQ) secure
solutions, yet existing NIST-PQC standards are costlier than their conventional
counterparts and unsuitable for resource-limited IoTs. There is a significant
need for lightweight PQ-secure digital signatures that respect the resource
constraints of low-end IoTs.
  We propose a new multiple-time hash-based signature called Maximum
Utilization Multiple HORS (MUM-HORS) that offers PQ security, short signatures,
fast signing, and high key utilization for an extended lifespan. MUM-HORS
addresses the inefficiency and key loss issues of HORS in offline/online
settings by introducing compact key management data structures and optimized
resistance to weak-message attacks. We tested MUM-HORS on two embedded
platforms (ARM Cortex A-72 and 8-bit AVR ATmega2560) and commodity hardware.
Our experiments confirm up to 40x better utilization with the same signing
capacity (2^20 messages, 128-bit security) compared to multiple-time HORS while
achieving 2x and 156-2463x faster signing than conventional-secure and NIST
PQ-secure schemes, respectively, on an ARM Cortex. These features make MUM-HORS
ideal multiple-time PQ-secure signature for heterogeneous IoTs.",cs.CR
How Memory-Safe is IoT? Assessing the Impact of Memory-Protection Solutions for Securing Wireless Gateways,"The rapid development of the Internet of Things (IoT) has enabled novel
user-centred applications, including many in safety-critical areas such as
healthcare, smart environment security, and emergency response systems. The
diversity in IoT manufacturers, standards, and devices creates a combinatorial
explosion of such deployment scenarios, leading to increased security and
safety threats due to the difficulty of managing such heterogeneity. In almost
every IoT deployment, wireless gateways are crucial for interconnecting IoT
devices and providing services, yet they are vulnerable to external threats and
serve as key entry points for large-scale IoT attacks. Memory-based
vulnerabilities are among the most serious threats in software, with no
universal solution yet available. Legacy memory protection mechanisms, such as
canaries, RELRO, NX, and Fortify, have enhanced memory safety but remain
insufficient for comprehensive protection. Emerging technologies like ARM-MTE,
CHERI, and Rust are based on more universal and robust Secure-by-Design (SbD)
memory safety principles, yet each entails different trade-offs in hardware or
code modifications. Given the challenges of balancing security levels with
associated overheads in IoT systems, this paper explores the impact of memory
safety on the IoT domain through an empirical large-scale analysis of
memory-related vulnerabilities in modern wireless gateways. Our results show
that memory vulnerabilities constitute the majority of IoT gateway threats,
underscoring the necessity for SbD solutions, with the choice of
memory-protection technology depending on specific use cases and associated
overheads.",cs.CR
WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy Principles,"In this paper, we introduce WaKA (Wasserstein K-nearest neighbors
Attribution), a novel attribution method that leverages principles from the
LiRA (Likelihood Ratio Attack) framework and applies them to \( k \)-nearest
neighbors classifiers (\( k \)-NN). WaKA efficiently measures the contribution
of individual data points to the model's loss distribution, analyzing every
possible \( k \)-NN that can be constructed using the training set, without
requiring sampling or shadow model training. WaKA can be used \emph{a
posteriori} as a membership inference attack (MIA) to assess privacy risks, and
\emph{a priori} for data minimization and privacy influence measurement. Thus,
WaKA can be seen as bridging the gap between data attribution and membership
inference attack (MIA) literature by distinguishing between the value of a data
point and its privacy risk. For instance, we show that self-attribution values
are more strongly correlated with the attack success rate than the contribution
of a point to model generalization. WaKA's different usages were also evaluated
across diverse real-world datasets, demonstrating performance very close to
LiRA when used as an MIA on \( k \)-NN classifiers, but with greater
computational efficiency.",cs.CR
"Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents","Language model (LM) agents that act on users' behalf for personal tasks can
boost productivity, but are also susceptible to unintended privacy leakage
risks. We present the first study on people's capacity to oversee the privacy
implications of the LM agents. By conducting a task-based survey (N=300), we
investigate how people react to and assess the response generated by LM agents
for asynchronous interpersonal communication tasks, compared with a response
they wrote. We found that people may favor the agent response with more privacy
leakage over the response they drafted or consider both good, leading to an
increased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct
patterns of privacy behaviors, attitudes, and preferences, and the nuanced
interactions between privacy considerations and other factors. Our findings
shed light on designing agentic systems that enable privacy-preserving
interactions and achieve bidirectional alignment on privacy preferences to help
users calibrate trust.",cs.CR
RA-WEBs: Remote Attestation for WEB services,"Data theft and leakage, caused by external adversaries and insiders,
demonstrate the need for protecting user data. Trusted Execution Environments
(TEEs) offer a promising solution by creating secure environments that protect
data and code from such threats. The rise of confidential computing on cloud
platforms facilitates the deployment of TEE-enabled server applications, which
are expected to be widely adopted in web services such as privacy-preserving
LLM inference and secure data logging. One key feature is Remote Attestation
(RA), which enables integrity verification of a TEE.
  However, $\textit{compatibility}$ issues with RA verification arise as no
browsers natively support this feature, making prior solutions cumbersome and
risky. To address these challenges, we propose $\texttt{RA-WEBs}$
($\textbf{R}$emote $\textbf{A}$ttestation for $\textbf{Web}$
$\textbf{s}$ervices), a novel RA protocol designed for high compatibility with
the current web ecosystem. $\texttt{RA-WEBs}$ leverages established web
mechanisms for immediate deployability, enabling RA verification on existing
browsers. We conduct a comprehensive security analysis, demonstrating
$\texttt{RA-WEBs}$'s resilience against various threats. Our contributions
include the $\texttt{RA-WEBs}$ proposal, a proof-of-concept implementation, an
in-depth security analysis, and publicly available code for reproducible
research.",cs.CR
Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles,"We propose a novel approach to effectively detect cloned identities of
social-sensor cloud service providers (i.e. social media users) in the face of
incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed
approach first extracts account pairs with similar usernames or screen names
from a given set of user accounts collected from a social media. It then learns
a multi-view representation associated with a given account and extracts two
categories of features for every single account. These two categories of
features include profile and Weighted Generalised Canonical Correlation
Analysis (WGCCA)-based features that may potentially contain missing values. To
counter the impact of such missing values, a missing value imputer will next
impute the missing values of the aforementioned profile and WGCCA-based
features. After that, the proposed approach further extracts two categories of
augmented features for each account pair identified previously, namely, 1)
similarity and 2) differences-based features. Finally, these features are
concatenated and fed into a Light Gradient Boosting Machine classifier to
detect identity cloning. We evaluated and compared the proposed approach
against the existing state-of-the-art identity cloning approaches and other
machine or deep learning models atop a real-world dataset. The experimental
results show that the proposed approach outperforms the state-of-the-art
approaches and models in terms of Precision, Recall and F1-score.",cs.CR
What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks,"While `jailbreaks' have been central to research on the safety and
reliability of LLMs (large language models), the underlying mechanisms behind
these attacks are not well understood. Some prior works have used linear
methods to analyze jailbreak prompts or model refusal. Here, however, we
compare linear and nonlinear methods to study the features in prompts that
contribute to successful jailbreaks. We do this by probing for jailbreak
success based only on the portions of the latent representations corresponding
to prompt tokens. First, we introduce a dataset of 10,800 jailbreak attempts
from 35 attack methods. We then show that different jailbreaking methods work
via different nonlinear features in prompts. Specifically, we find that while
probes can distinguish between successful and unsuccessful jailbreaking prompts
with a high degree of accuracy, they often transfer poorly to held-out attack
methods. We also show that nonlinear probes can be used to mechanistically
jailbreak the LLM by guiding the design of adversarial latent perturbations.
These mechanistic jailbreaks are able to jailbreak Gemma-7B-IT more reliably
than 34 of the 35 techniques that it was trained on. Ultimately, our results
suggest that jailbreaks cannot be thoroughly understood in terms of universal
or linear prompt features alone.",cs.CR
False Data Injection Attack Detection in Edge-based Smart Metering Networks with Federated Learning,"Smart metering networks are increasingly susceptible to cyber threats, where
false data injection (FDI) appears as a critical attack. Data-driven-based
machine learning (ML) methods have shown immense benefits in detecting FDI
attacks via data learning and prediction abilities. Literature works have
mostly focused on centralized learning and deploying FDI attack detection
models at the control center, which requires data collection from local
utilities like meters and transformers. However, this data sharing may raise
privacy concerns due to the potential disclosure of household information like
energy usage patterns. This paper proposes a new privacy-preserved FDI attack
detection by developing an efficient federated learning (FL) framework in the
smart meter network with edge computing. Distributed edge servers located at
the network edge run an ML-based FDI attack detection model and share the
trained model with the grid operator, aiming to build a strong FDI attack
detection model without data sharing. Simulation results demonstrate the
efficiency of our proposed FL method over the conventional method without
collaboration.",cs.CR
From Federated Learning to Quantum Federated Learning for Space-Air-Ground Integrated Networks,"6G wireless networks are expected to provide seamless and data-based
connections that cover space-air-ground and underwater networks. As a core
partition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN)
have been envisioned to provide countless real-time intelligent applications.
To realize this, promoting AI techniques into SAGIN is an inevitable trend. Due
to the distributed and heterogeneous architecture of SAGIN, federated learning
(FL) and then quantum FL are emerging AI model training techniques for enabling
future privacy-enhanced and computation-efficient SAGINs. In this work, we
explore the vision of using FL/QFL in SAGINs. We present a few representative
applications enabled by the integration of FL and QFL in SAGINs. A case study
of QFL over UAV networks is also given, showing the merit of quantum-enabled
training approach over the conventional FL benchmark. Research challenges along
with standardization for QFL adoption in future SAGINs are also highlighted.",cs.CR
Advancing Biomedical Signal Security: Real-Time ECG Monitoring with Chaotic Encryption,"The real time analysis and secure transmission of electrocardiogram (ECG)
signals are critical for ensuring both effective medical diagnosis and patient
data privacy. In this study, we developed a real time ECG monitoring system
that integrates chaotic encryption to protect the integrity and confidentiality
of ECG signals during acquisition, transmission, and storage. By leveraging the
logistic map as the chaotic function for encryption, our system offers a highly
secure framework that dynamically encrypts ECG signals without adding
significant latency. To validate the system's reliability, we applied a series
of security tests. The results demonstrate that chaotic encryption is effective
in enhancing data security, as evidenced by high entropy values and strong key
sensitivity, ensuring protection against common cryptographic attacks.
Additionally, the system's real time disease detection model, based on deep
learning, operates seamlessly with encrypted data, providing accurate diagnosis
without compromising security. Our findings indicate that chaotic encryption,
paired with real time analysis, is a powerful method for protecting sensitive
medical data, making this approach particularly relevant for telemedicine and
remote patient monitoring applications. The success of this system highlights
its potential for broader application to other biomedical signals, providing a
secure infrastructure for the future of digital health.",cs.CR
ECG-PPS: Privacy Preserving Disease Diagnosis and Monitoring System for Real-Time ECG Signal,"This study introduces the development of a state of the art, real time ECG
monitoring and analysis system, incorporating cutting edge medical technology
and innovative data security measures. Our system performs three distinct
functions thaat real time ECG monitoring and disease detection, encrypted
storage and synchronized visualization, and statistical analysis on encrypted
data. At its core, the system uses a three lead ECG preamplifier connected
through a serial port to capture, display, and record real time ECG data. These
signals are securely stored in the cloud using robust encryption methods.
Authorized medical personnel can access and decrypt this data on their
computers, with AES encryption ensuring synchronized real time data tracking
and visualization. Furthermore, the system performs statistical operations on
the ECG data stored in the cloud without decrypting it, using Fully Homomorphic
Encryption (FHE). This enables privacy preserving data analysis while ensuring
the security and confidentiality of patient information. By integrating these
independent functions, our system significantly enhances the security and
efficiency of health monitoring. It supports critical tasks such as disease
detection, patient monitoring, and preliminary intervention, all while
upholding stringent data privacy standards. We provided detailed discussions on
the system's architecture, hardware configuration, software implementation, and
clinical performance. The results highlight the potential of this system to
improve patient care through secure and efficient ECG monitoring and analysis.
This work represents a significant leap forward in medical technology. By
incorporating FHE into both data transmission and storage processes, we ensure
continuous encryption of data throughout its lifecycle while enabling real time
disease diagnosis.",cs.CR
"PARIS: A Practical, Adaptive Trace-Fetching and Real-Time Malicious Behavior Detection System","The escalating sophistication of cyber-attacks and the widespread utilization
of stealth tactics have led to significant security threats globally.
Nevertheless, the existing static detection methods exhibit limited coverage,
and traditional dynamic monitoring approaches encounter challenges in bypassing
evasion techniques. Thus, it has become imperative to implement nuanced and
dynamic analysis to achieve precise behavior detection in real time. There are
two pressing concerns associated with current dynamic malware behavior
detection solutions. Firstly, the collection and processing of data entail a
significant amount of overhead, making it challenging to be employed for
real-time detection on the end host. Secondly, these approaches tend to treat
malware as a singular entity, thereby overlooking varied behaviors within one
instance. To fill these gaps, we propose PARIS, an adaptive trace fetching,
lightweight, real-time malicious behavior detection system. Specifically, we
monitor malicious behavior with Event Tracing for Windows (ETW) and learn to
selectively collect maliciousness-related APIs or call stacks, significantly
reducing the data collection overhead. As a result, we can monitor a wider
range of APIs and detect more intricate attack behavior. We implemented a
prototype of PARIS and evaluated the system overhead, the accuracy of
comparative behavior recognition, and the impact of different models and
parameters. The result demonstrates that PARIS can reduce over 98.8% of data
compared to the raw ETW trace and hence decreases the overhead on the host in
terms of memory, bandwidth, and CPU usage with a similar detection accuracy to
the baselines that suffer from the high overhead. Furthermore, a breakdown
evaluation shows that 80% of the memory and bandwidth savings and a complete
reduction in CPU usage can be attributed to our adaptive trace-fetching
collector.",cs.CR
Quantum Token Obfuscation via Superposition,"As quantum computing advances, traditional cryptographic security measures,
including token obfuscation, are increasingly vulnerable to quantum attacks.
This paper introduces a quantum-enhanced approach to token obfuscation
leveraging quantum superposition and multi-basis verification to establish a
robust defense against these threats. In our method, tokens are encoded in
superposition states, making them simultaneously exist in multiple states until
measured, thus enhancing obfuscation complexity. Multi-basis verification
further secures these tokens by enforcing validation across multiple quantum
bases, thwarting unauthorized access. Additionally, we incorporate a quantum
decay protocol and a refresh mechanism to manage the token life-cycle securely.
Our experimental results demonstrate significant improvements in token security
and robustness, validating this approach as a promising solution for
quantum-secure cryptographic applications. This work not only highlights the
feasibility of quantum-based token obfuscation but also lays the foundation for
future quantum-safe security architectures.",cs.CR
AutoPT: How Far Are We from the End2End Automated Web Penetration Testing?,"Penetration testing is essential to ensure Web security, which can detect and
fix vulnerabilities in advance, and prevent data leakage and serious
consequences. The powerful inference capabilities of large language models
(LLMs) have made significant progress in various fields, and the development
potential of LLM-based agents can revolutionize the cybersecurity penetration
testing industry. In this work, we establish a comprehensive end-to-end
penetration testing benchmark using a real-world penetration testing
environment to explore the capabilities of LLM-based agents in this domain. Our
results reveal that the agents are familiar with the framework of penetration
testing tasks, but they still face limitations in generating accurate commands
and executing complete processes. Accordingly, we summarize the current
challenges, including the difficulty of maintaining the entire message history
and the tendency for the agent to become stuck.
  Based on the above insights, we propose a Penetration testing State Machine
(PSM) that utilizes the Finite State Machine (FSM) methodology to address these
limitations. Then, we introduce AutoPT, an automated penetration testing agent
based on the principle of PSM driven by LLMs, which utilizes the inherent
inference ability of LLM and the constraint framework of state machines. Our
evaluation results show that AutoPT outperforms the baseline framework ReAct on
the GPT-4o mini model and improves the task completion rate from 22% to 41% on
the benchmark target. Compared with the baseline framework and manual work,
AutoPT also reduces time and economic costs further. Hence, our AutoPT has
facilitated the development of automated penetration testing and significantly
impacted both academia and industry.",cs.CR
Strengthening DeFi Security: A Static Analysis Approach to Flash Loan Vulnerabilities,"The rise of Decentralized Finance (DeFi) has brought novel financial
opportunities but also exposed serious security vulnerabilities, with flash
loans frequently exploited for price manipulation attacks. These attacks,
leveraging the atomic nature of flash loans, allow malicious actors to
manipulate DeFi protocol oracles and pricing mechanisms within a single
transaction, causing substantial financial losses. Traditional smart contract
analysis tools address some security risks but often struggle to detect the
complex, inter-contract dependencies that make flash loan attacks challenging
to identify.
  In response, we introduce FlashDeFier, an advanced detection framework that
enhances static taint analysis to target price manipulation vulnerabilities
arising from flash loans. FlashDeFier expands the scope of taint sources and
sinks, enabling comprehensive analysis of data flows across DeFi protocols. The
framework constructs detailed inter-contract call graphs to capture
sophisticated data flow patterns, significantly improving detection accuracy.
Tested against a dataset of high-profile DeFi incidents, FlashDeFier identifies
76.4% of price manipulation vulnerabilities, marking a 30% improvement over
DeFiTainter. These results highlight the importance of adaptive detection
frameworks that evolve alongside DeFi threats, underscoring the need for hybrid
approaches combining static, dynamic, and symbolic analysis methods for
resilient DeFi security.",cs.CR
Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives,"While open Large Language Models (LLMs) have made significant progress, they
still fall short of matching the performance of their closed, proprietary
counterparts, making the latter attractive even for the use on highly private
data. Recently, various new methods have been proposed to adapt closed LLMs to
private data without leaking private information to third parties and/or the
LLM provider. In this work, we analyze the privacy protection and performance
of the four most recent methods for private adaptation of closed LLMs. By
examining their threat models and thoroughly comparing their performance under
different privacy levels according to differential privacy (DP), various LLM
architectures, and multiple datasets for classification and generation tasks,
we find that: (1) all the methods leak query data, i.e., the (potentially
sensitive) user data that is queried at inference time, to the LLM provider,
(2) three out of four methods also leak large fractions of private training
data to the LLM provider while the method that protects private data requires a
local open LLM, (3) all the methods exhibit lower performance compared to three
private gradient-based adaptation methods for local open LLMs, and (4) the
private adaptation methods for closed LLMs incur higher monetary training and
query costs than running the alternative methods on local open LLMs. This
yields the conclusion that, to achieve truly privacy-preserving LLM adaptations
that yield high performance and more privacy at lower costs, taking into
account current methods and models, one should use open LLMs.",cs.CR
Federated Learning with Relative Fairness,"This paper proposes a federated learning framework designed to achieve
\textit{relative fairness} for clients. Traditional federated learning
frameworks typically ensure absolute fairness by guaranteeing minimum
performance across all client subgroups. However, this approach overlooks
disparities in model performance between subgroups. The proposed framework uses
a minimax problem approach to minimize relative unfairness, extending previous
methods in distributionally robust optimization (DRO). A novel fairness index,
based on the ratio between large and small losses among clients, is introduced,
allowing the framework to assess and improve the relative fairness of trained
models. Theoretical guarantees demonstrate that the framework consistently
reduces unfairness. We also develop an algorithm, named \textsc{Scaff-PD-IA},
which balances communication and computational efficiency while maintaining
minimax-optimal convergence rates. Empirical evaluations on real-world datasets
confirm its effectiveness in maintaining model performance while reducing
disparity.",cs.CR
Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing,"Federated Learning (FL) is essential for efficient data exchange in Internet
of Things (IoT) environments, as it trains Machine Learning (ML) models locally
and shares only model updates. However, FL is vulnerable to privacy threats
like model inversion and membership inference attacks, which can expose
sensitive training data. To address these privacy concerns, Differential
Privacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML
models degrades performance, especially in dynamic IoT systems where
continuous, lifelong FL learning accumulates excessive noise over time. To
mitigate this issue, we introduce Federated HyperDimensional computing with
Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)
framework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy
carefully manages the balance between privacy and performance by theoretically
tracking cumulative noise from previous rounds and adding only the necessary
incremental noise to meet privacy requirements. In a real-world case study
involving in-process monitoring of manufacturing machining operations,
FedHDPrivacy demonstrates robust performance, outperforming standard FL
frameworks-including Federated Averaging (FedAvg), Federated Stochastic
Gradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized
Averaging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy
also shows potential for future enhancements, such as multimodal data fusion.",cs.CR
BinEnhance: A Enhancement Framework Based on External Environment Semantics for Binary Code Search,"Binary code search plays a crucial role in applications like software reuse
detection. Currently, existing models are typically based on either internal
code semantics or a combination of function call graphs (CG) and internal code
semantics. However, these models have limitations. Internal code semantic
models only consider the semantics within the function, ignoring the
inter-function semantics, making it difficult to handle situations such as
function inlining. The combination of CG and internal code semantics is
insufficient for addressing complex real-world scenarios. To address these
limitations, we propose BinEnhance, a novel framework designed to leverage the
inter-function semantics to enhance the expression of internal code semantics
for binary code search. Specifically, BinEnhance constructs an External
Environment Semantic Graph (EESG), which establishes a stable and analogous
external environment for homologous functions by using different inter-function
semantic relations (e.g., call, location, data-co-use). After the construction
of EESG, we utilize the embeddings generated by existing internal code semantic
models to initialize nodes of EESG. Finally, we design a Semantic Enhancement
Model (SEM) that uses Relational Graph Convolutional Networks (RGCNs) and a
residual block to learn valuable external semantics on the EESG for generating
the enhanced semantics embedding. In addition, BinEnhance utilizes data feature
similarity to refine the cosine similarity of semantic embeddings. We conduct
experiments under six different tasks (e.g., under function inlining scenario)
and the results illustrate the performance and robustness of BinEnhance. The
application of BinEnhance to HermesSim, Asm2vec, TREX, Gemini, and Asteria on
two public datasets results in an improvement of Mean Average Precision (MAP)
from 53.6% to 69.7%. Moreover, the efficiency increases fourfold.",cs.CR
Practical hybrid PQC-QKD protocols with enhanced security and performance,"Quantum resistance is vital for emerging cryptographic systems as quantum
technologies continue to advance towards large-scale, fault-tolerant quantum
computers. Resistance may be offered by quantum key distribution (QKD), which
provides information-theoretic security using quantum states of photons, but
may be limited by transmission loss at long distances. An alternative approach
uses classical means and is conjectured to be resistant to quantum attacks,
so-called post-quantum cryptography (PQC), but it is yet to be rigorously
proven, and its current implementations are computationally expensive. To
overcome the security and performance challenges present in each, here we
develop hybrid protocols by which QKD and PQC inter-operate within a joint
quantum-classical network. In particular, we consider different hybrid designs
that may offer enhanced speed and/or security over the individual performance
of either approach. Furthermore, we present a method for analyzing the security
of hybrid protocols in key distribution networks. Our hybrid approach paves the
way for joint quantum-classical communication networks, which leverage the
advantages of both QKD and PQC and can be tailored to the requirements of
various practical networks.",cs.CR
Towards efficient and secure quantum-classical communication networks,"The rapid advancement of quantum technologies calls for the design and
deployment of quantum-safe cryptographic protocols and communication networks.
There are two primary approaches to achieving quantum-resistant security:
quantum key distribution (QKD) and post-quantum cryptography (PQC). While each
offers unique advantages, both have drawbacks in practical implementation. In
this work, we introduce the pros and cons of these protocols and explore how
they can be combined to achieve a higher level of security and/or improved
performance in key distribution. We hope our discussion inspires further
research into the design of hybrid cryptographic protocols for
quantum-classical communication networks.",cs.CR
Privacy Risks of Speculative Decoding in Large Language Models,"Speculative decoding in large language models (LLMs) accelerates token
generation by speculatively predicting multiple tokens cheaply and verifying
them in parallel, and has been widely deployed. In this paper, we provide the
first study demonstrating the privacy risks of speculative decoding. We observe
that input-dependent patterns of correct and incorrect predictions can be
leaked out to an adversary monitoring token generation times and packet sizes,
leading to privacy breaches. By observing the pattern of correctly and
incorrectly speculated tokens, we show that a malicious adversary can
fingerprint queries and learn private user inputs with more than $90\%$
accuracy across three different speculative decoding techniques - REST (almost
$100\%$ accuracy), LADE (up to $92\%$ accuracy), and BiLD (up to $95\%$
accuracy). We show that an adversary can also leak out confidential
intellectual property used to design these techniques, such as data from
data-stores used for prediction (in REST) at a rate of more than $25$ tokens
per second, or even hyper-parameters used for prediction (in LADE). We also
discuss mitigation strategies, such as aggregating tokens across multiple
iterations and padding packets with additional bytes, to avoid such privacy or
confidentiality breaches.",cs.CR
AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity Operations using Fine-tuned and Open-Source LLMs,"Retrieval-augmented generation (RAG) on specialized domain datasets has shown
improved performance when large language models (LLMs) are fine-tuned for
generating responses to user queries. In this study, we develop a cybersecurity
question-answering (Q\&A) dataset, called AttackQA, and employ it to build a
RAG-based Q\&A system designed for analysts in security operations centers. The
dataset comprises 25,335 Q\&A pairs, accompanied by rationales to facilitate
fine-tuning and evaluation. 80\% of the dataset was generated with help of a
lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per
second with full 16-bit precision on SambaNova System's SN40L specialized
hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and
reject low-quality Q\&A pairs. In using the dataset for RAG, we demonstrate
that fine-tuning open-source embeddings and LLMs can yield superior accuracy
compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).
Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,
enabling the creation of a fully open-source, high-speed RAG and evaluation
pipeline with a benchmark for model accuracy.",cs.CR
BACSA: A Bias-Aware Client Selection Algorithm for Privacy-Preserving Federated Learning in Wireless Healthcare Networks,"Federated Learning (FL) has emerged as a transformative approach in
healthcare, enabling collaborative model training across decentralized data
sources while preserving user privacy. However, performance of FL rapidly
degrades in practical scenarios due to the inherent bias in non Independent and
Identically distributed (non-IID) data among participating clients, which poses
significant challenges to model accuracy and generalization. Therefore, we
propose the Bias-Aware Client Selection Algorithm (BACSA), which detects user
bias and strategically selects clients based on their bias profiles. In
addition, the proposed algorithm considers privacy preservation, fairness and
constraints of wireless network environments, making it suitable for sensitive
healthcare applications where Quality of Service (QoS), privacy and security
are paramount. Our approach begins with a novel method for detecting user bias
by analyzing model parameters and correlating them with the distribution of
class-specific data samples. We then formulate a mixed-integer non-linear
client selection problem leveraging the detected bias, alongside wireless
network constraints, to optimize FL performance. We demonstrate that BACSA
improves convergence and accuracy, compared to existing benchmarks, through
evaluations on various data distributions, including Dirichlet and
class-constrained scenarios. Additionally, we explore the trade-offs between
accuracy, fairness, and network constraints, indicating the adaptability and
robustness of BACSA to address diverse healthcare applications.",cs.CR
Identify Backdoored Model in Federated Learning via Individual Unlearning,"Backdoor attacks present a significant threat to the robustness of Federated
Learning (FL) due to their stealth and effectiveness. They maintain both the
main task of the FL system and the backdoor task simultaneously, causing
malicious models to appear statistically similar to benign ones, which enables
them to evade detection by existing defense methods. We find that malicious
parameters in backdoored models are inactive on the main task, resulting in a
significantly large empirical loss during the machine unlearning process on
clean inputs. Inspired by this, we propose MASA, a method that utilizes
individual unlearning on local models to identify malicious models in FL. To
improve the performance of MASA in challenging non-independent and identically
distributed (non-IID) settings, we design pre-unlearning model fusion that
integrates local models with knowledge learned from other datasets to mitigate
the divergence in their unlearning behaviors caused by the non-IID data
distributions of clients. Additionally, we propose a new anomaly detection
metric with minimal hyperparameters to filter out malicious models efficiently.
Extensive experiments on IID and non-IID datasets across six different attacks
validate the effectiveness of MASA. To the best of our knowledge, this is the
first work to leverage machine unlearning to identify malicious models in FL.
Code is available at \url{https://github.com/JiiahaoXU/MASA}.",cs.CR
"OML: Open, Monetizable, and Loyal AI","Artificial Intelligence (AI) has steadily improved across a wide range of
tasks. However, the development and deployment of AI are almost entirely
controlled by a few powerful organizations that are racing to create Artificial
General Intelligence (AGI). The centralized entities make decisions with little
public oversight, shaping the future of humanity, often with unforeseen
consequences. In this paper, we propose OML, which stands for Open,
Monetizable, and Loyal AI, an approach designed to democratize AI development.
OML is realized through an interdisciplinary framework spanning AI, blockchain,
and cryptography. We present several ideas for constructing OML using
technologies such as Trusted Execution Environments (TEE), traditional
cryptographic primitives like fully homomorphic encryption and functional
encryption, obfuscation, and AI-native solutions rooted in the sample
complexity and intrinsic hardness of AI tasks. A key innovation of our work is
introducing a new scientific field: AI-native cryptography. Unlike conventional
cryptography, which focuses on discrete data and binary security guarantees,
AI-native cryptography exploits the continuous nature of AI data
representations and their low-dimensional manifolds, focusing on improving
approximate performance. One core idea is to transform AI attack methods, such
as data poisoning, into security tools. This novel approach serves as a
foundation for OML 1.0 which uses model fingerprinting to protect the integrity
and ownership of AI models. The spirit of OML is to establish a decentralized,
open, and transparent platform for AI development, enabling the community to
contribute, monetize, and take ownership of AI models. By decentralizing
control and ensuring transparency through blockchain technology, OML prevents
the concentration of power and provides accountability in AI development that
has not been possible before.",cs.CR
Face Anonymization Made Simple,"Current face anonymization techniques often depend on identity loss
calculated by face recognition models, which can be inaccurate and unreliable.
Additionally, many methods require supplementary data such as facial landmarks
and masks to guide the synthesis process. In contrast, our approach uses
diffusion models with only a reconstruction loss, eliminating the need for
facial landmarks or masks while still producing images with intricate,
fine-grained details. We validated our results on two public benchmarks through
both quantitative and qualitative evaluations. Our model achieves
state-of-the-art performance in three key areas: identity anonymization, facial
attribute preservation, and image quality. Beyond its primary function of
anonymization, our model can also perform face swapping tasks by incorporating
an additional facial image as input, demonstrating its versatility and
potential for diverse applications. Our code and models are available at
https://github.com/hanweikung/face_anon_simple .",cs.CR
New classes of reversible cellular automata,"A Boolean function $f$ on $k$~bits induces a shift-invariant vectorial
Boolean function $F$ from $n$ bits to $n$ bits for every $n\geq k$. If $F$ is
bijective for every $n$, we say that $f$ is a proper lifting, and it is known
that proper liftings are exactly those functions that arise as local rules of
reversible cellular automata. We construct new families of such liftings for
arbitrary large $k$ and discuss whether all have been identified for $k\leq 6$.",cs.CR
"A General Quantum Duality for Representations of Groups with Applications to Quantum Money, Lightning, and Fire","Aaronson, Atia, and Susskind established that swapping quantum states
$|\psi\rangle$ and $|\phi\rangle$ is computationally equivalent to
distinguishing their superpositions $|\psi\rangle\pm|\phi\rangle$. We extend
this to a general duality principle: manipulating quantum states in one basis
is equivalent to extracting values in a complementary basis. Formally, for any
group, implementing a unitary representation is equivalent to Fourier subspace
extraction from its irreducible representations.
  Building on this duality principle, we present the applications:
  * Quantum money, representing verifiable but unclonable quantum states, and
its stronger variant, quantum lightning, have resisted secure plain-model
constructions. While (public-key) quantum money has been constructed securely
only from the strong assumption of quantum-secure iO, quantum lightning has
lacked such a construction, with past attempts using broken assumptions. We
present the first secure quantum lightning construction based on a plausible
cryptographic assumption by extending Zhandry's construction from Abelian to
non-Abelian group actions, eliminating reliance on a black-box model. Our
construction is realizable with symmetric group actions, including those
implicit in the McEliece cryptosystem.
  * We give an alternative quantum lightning construction from one-way
homomorphisms, with security holding under certain conditions. This scheme
shows equivalence among four security notions: quantum lightning security,
worst-case and average-case cloning security, and security against preparing a
canonical state.
  * Quantum fire describes states that are clonable but not telegraphable: they
cannot be efficiently encoded classically. These states ""spread"" like fire, but
are viable only in coherent quantum form. The only prior construction required
a unitary oracle; we propose the first candidate in the plain model.",cs.CR
Tactical Edge IoT in Defense and National Security,"The deployment of Internet of Things (IoT) systems in Defense and National
Security faces some limitations that can be addressed with Edge Computing
approaches. The Edge Computing and IoT paradigms combined bring potential
benefits, since they confront the limitations of traditional centralized cloud
computing approaches, which enable easy scalability, real-time applications or
mobility support, but whose use poses certain risks in aspects like
cybersecurity. This chapter identifies scenarios in which Defense and National
Security can leverage Commercial Off-The-Shelf (COTS) Edge IoT capabilities to
deliver greater survivability to warfighters or first responders, while
lowering costs and increasing operational efficiency and effectiveness. In
addition, it presents the general design of a Tactical Edge IoT communications
architecture, it identifies the open challenges for a widespread adoption and
provides research guidelines and some recommendations for enabling
cost-effective Edge IoT for Defense and National Security.",cs.CR
